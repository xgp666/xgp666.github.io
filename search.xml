<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>21 k8s的持续集成</title>
      <link href="/posts/c0fd.html"/>
      <url>/posts/c0fd.html</url>
      
        <content type="html"><![CDATA[<h2 id="实验环境">实验环境</h2><table><thead><tr><th>IP</th><th>主机名称</th><th>服务</th></tr></thead><tbody><tr><td><strong>192.168.1.21</strong></td><td><strong>master</strong></td><td><strong>k8s</strong></td></tr><tr><td><strong>192.168.1.22</strong></td><td><strong>node01</strong></td><td><strong>k8s</strong></td></tr><tr><td><strong>192.168.1.10</strong></td><td><strong>git</strong></td><td><strong>gitlab</strong></td></tr><tr><td><strong>192.168.1.13</strong></td><td><strong>jenkins</strong></td><td><strong>jenkins</strong></td></tr></tbody></table><p>![image-20200309134708695](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200309134708695.png)</p><p><strong>总体流程：</strong></p><ul><li><strong>在开发机开发代码后提交到gitlab</strong></li><li><strong>之后通过webhook插件触发jenkins进行构建，jenkins将代码打成docker镜像，push到docker-registry</strong></li><li><strong>之后将在k8s-master上执行rc、service的创建，进而创建Pod，从私服拉取镜像，根据该镜像启动容器</strong></li></ul><p><strong>应用构建和发布流程说明。</strong></p><ol><li><strong>用户向Gitlab提交代码，代码中必须包含<code>Dockerfile</code></strong></li><li><strong>将代码提交到远程仓库</strong></li><li><strong>用户在发布应用时需要填写git仓库地址和分支、服务类型、服务名称、资源数量、实例个数，确定后触发Jenkins自动构建</strong></li><li><strong>Jenkins的CI流水线自动编译代码并打包成docker镜像推送到Harbor镜像仓库</strong></li><li><strong>Jenkins的CI流水线中包括了自定义脚本，根据我们已准备好的kubernetes的YAML模板，将其中的变量替换成用户输入的选项</strong></li><li><strong>生成应用的kubernetes YAML配置文件</strong></li><li><strong>更新Ingress的配置，根据新部署的应用的名称，在ingress的配置文件中增加一条路由信息</strong></li><li><strong>更新PowerDNS，向其中插入一条DNS记录，IP地址是边缘节点的IP地址。关于边缘节点，请查看<a href="https://jimmysong.io/kubernetes-handbook/practice/edge-node-configuration.html" target="_blank" rel="noopener">边缘节点配置</a></strong></li><li><strong>Jenkins调用kubernetes的API，部署应用</strong></li></ol><h1>一、前期工作</h1><h2 id="1、先验证k8s集群（1-21和1-22）">1、先验证k8s集群（1.21和1.22）</h2><pre><code>[root@master ~]# kubectl get nodes</code></pre><p>![image-20200306083959440](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306083959440.png)</p><h2 id="2、master部署私有仓库">2、master部署私有仓库</h2><h3 id="Docker01部署"><strong>Docker01部署</strong></h3><pre><code>72 docker pull registry//下载registry镜像73 docker run -itd --name registry -p 5000:5000 --restart=always registry:latest//基于registry镜像，启动一台容器78 vim /usr/lib/systemd/system/docker.service #13行修改ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.21:5000 80 systemctl daemon-reload81 systemctl restart docker.service//重启docker76 docker tag httpd:latest 192.168.1.11:5000/web:v1 76 docker tag httpd:latest 192.168.1.11:5000/web:v2//把容器重命名一个标签77 docker ps</code></pre><p>![image-20200309101144205](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200309101144205.png)</p><pre><code>78 vim /usr/lib/systemd/system/docker.service #13行修改ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.11:5000 80 systemctl daemon-reload81 systemctl restart docker.service//重启docker100 docker push 192.168.1.11:5000/web:v1100 docker push 192.168.1.11:5000/web:v2//上传容器到私有仓库</code></pre><h3 id="Docker02和docker03加入私有仓库"><strong>Docker02和docker03加入私有仓库</strong></h3><pre><code>78 vim /usr/lib/systemd/system/docker.service #13行修改ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.11:5000  80 systemctl daemon-reload81 systemctl restart docker.service//重启docker99 docker pull 192.168.1.21:5000/web:v1//测试下载</code></pre><h2 id="3、然后重要的地方到了，建立-yaml配置文件让kubernetes自己控制容器集群。"><strong>3、然后重要的地方到了，建立 yaml配置文件让kubernetes自己控制容器集群。</strong></h2><p><em><strong>用来模拟我们部署的服务</strong></em></p><pre><code>[root@master app]# vim deploy.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata:  name: webspec:  replicas: 2  template:    metadata:      labels:        name: web    spec:      containers:      - name: web        image: 192.168.1.21:5000/web:v1        imagePullPolicy: Always     #改为本地仓库下载        ports:        - containerPort: 80</code></pre><p>执行一下</p><pre><code>[root@master app]# kubectl apply -f deploy.yaml</code></pre><p>查看一下</p><pre><code>[root@master app]# kubectl get pod</code></pre><p>![image-20200306085507559](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306085507559.png)</p><h3 id="可是容器的ip只能在容器本机上访问，集群内的其他主机和集群外的主机都没办法访问，这个时候就需要将容器的端口映射到服务器上的端口了，所以需要做一个service的模板。service-模板可以将容器的端口映射到服务器的端口上，并且可以固定映射在服务器上的端口。"><strong>可是容器的ip只能在容器本机上访问，集群内的其他主机和集群外的主机都没办法访问，这个时候就需要将容器的端口映射到服务器上的端口了，所以需要做一个service的模板。service 模板可以将容器的端口映射到服务器的端口上，并且可以固定映射在服务器上的端口。</strong></h3><pre><code>[root@master app]# vim deploy-svc.yamlapiVersion: v1kind: Servicemetadata:  labels:    name: web  name: webspec:  type: NodePort  ports:  - port: 80    targetPort: 80    nodePort: 31234  selector:    name: web</code></pre><p><strong>执行一下</strong></p><pre><code>[root@master app]# kubectl apply -f deploy-svc.yaml</code></pre><p><strong>查看一下</strong></p><pre><code>[root@master app]# kubectl get svc</code></pre><p>![image-20200306085725863](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306085725863.png)</p><p><strong>访问一下http://192.168.1.21:31234/</strong></p><p>![image-20200306085846077](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306085846077.png)</p><h3 id="《ok-kubernetes-完毕，-开始配置-jenkins-gitlab联动》"><strong>《ok kubernetes</strong> <strong>完毕， 开始配置 jenkins+gitlab联动》</strong></h3><h3 id="4、git和jenkins加入私有仓库">4、git和jenkins加入私有仓库</h3><pre><code>78 vim /usr/lib/systemd/system/docker.service #13行修改ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.11:5000  80 systemctl daemon-reload81 systemctl restart docker.service//重启docker99 docker pull 192.168.1.11/busybox:v1//测试下载</code></pre><h3 id="5、jenkins服务器向k8smaster做免密登录">5、jenkins服务器向k8smaster做免密登录</h3><pre><code>100 ssh-copy-id 192.168.1.21</code></pre><h1>二、安装jenkins（1.13）</h1><h3 id="安装java环境">安装java环境</h3><pre><code>[root@jenkins ~]# tar -zxf jdk-8u231-linux-x64.tar.gz[root@jenkins ~]# mv jdk1.8.0_131 /usr/java#注意 这里有位置敏感，不要多一个“/”[root@jenkins ~]# vim /etc/profile #在最下面写export JAVA_HOME=/usr/javaexport JRE_HOME=/usr/java/jreexport PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$PATHexport CLASSPATH=$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar[root@jenkins ~]#  source /etc/profile//环境变量生效[root@jenkins ~]#  java -version//验证环境变量</code></pre><p>![image-20200306091443071](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306091443071.png)</p><h3 id="安装tomcat">安装tomcat</h3><pre><code>[root@jenkins ~]# tar -zxf apache-tomcat-7.0.54.tar.gz [root@jenkins ~]# mv apache-tomcat-7.0.54 /usr/tomcat7[root@jenkins ~]# cd /usr/tomcat7/webapps/[root@jenkins webapps]# rm -rf *[root@jenkins webapps]# cp /root/jenkins.war . #这几步是jenkins的包放进了tomcat里[root@jenkins webapps]# vim /usr/tomcat7/conf/server.xml //修改tomcat的字符集</code></pre><p>![image-20200306092022390](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306092022390.png)</p><pre><code>[root@jenkins webapps]# cd /usr/tomcat7/bin/[root@jenkins bin]# vim catalina.sh #!/bin/shexport CATALINA_OPTS=&quot;-DJENKINS_HOME=/data/jenkins&quot;export JENKINS_JAVA_OPTIONS=&quot;-Djava.awt.headless=true -Dhudson.ClassicPluginStrategy.noBytecodeTransformer=true&quot;//这两行添加的是jenkins的家目录位置，这个很重要[root@jenkins bin]# ./catalina.sh start //启动tomcat</code></pre><p>![image-20200306092523262](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306092523262.png)</p><pre><code>[root@jenkins bin]# netstat -anput | grep 8080</code></pre><p>![image-20200306110511541](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306110511541.png)</p><h3 id="浏览器安装jenkins">浏览器安装jenkins</h3><p><a href="http://192.168.1.11:8080/jenkins" target="_blank" rel="noopener">http://192.168.1.11:8080/jenkins</a></p><p>![image-20200306110627790](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306110627790.png)</p><pre><code>[root@jenkins bin]# cat /data/jenkins/secrets/initialAdminPasswordc577cbf75d934878a94b0f9e00ada328   //复制密码</code></pre><h3 id="（1）推荐安装">（1）推荐安装</h3><p>![image-20200308124155279](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200308124155279.png)</p><p><strong>#左边是自动安装， 右边是自定义安装，我们选左边的，如果不是这个画面则说明网络很卡或者没有网(推荐使用右边的，然后选择不安装插件，之后可以自定义安装）</strong></p><p>![image-20200306151852889](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306151852889.png)</p><h3 id="（2）这个是自定义安装（自己上传的包）">（2）这个是自定义安装（自己上传的包）</h3><pre><code>[root@autoweb bin]# ./catalina.sh stop[root@autoweb ~]# cd /data/jenkins/plugins/[root@autoweb jenkins]# mv plugins plugins/.bk然后上传plugins.tar.gz包：[root@autoweb jenkins]# tar -zxf plugins.tar.gz [root@autoweb ~]# cd /usr/tomcat7/bin/[root@autoweb bin]# ./catalina.sh stop[root@autoweb bin]# ./catalina.sh start</code></pre><p>![image-20200306110627790](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306110627790.png)</p><p><strong>输入密码后断网</strong></p><p>![image-20200308124449039](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200308124449039.png)</p><p>![image-20200308123936170](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200308123936170.png)</p><h3 id="（3）两个剩下的方法一样">（3）两个剩下的方法一样</h3><p>![image-20200306151900827](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306151900827.png)</p><p>![image-20200306151905668](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306151905668.png)</p><p>![image-20200306151911675](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306151911675.png)</p><h4 id="下载中文插件"><strong>下载中文插件</strong></h4><p><strong>系统管理-----&gt;插件管理-----&gt;avalilable(可选)然后搜索localization-zh-cn</strong></p><p>![image-20200306152834083](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306152834083.png)</p><p>![image-20200306152957419](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306152957419.png)</p><p><strong>然后还需要3个插件</strong></p><p>![image-20200306153713286](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306153713286.png)</p><h1>三、安装gitlab（1.10）</h1><p><strong>GitLab CI 是 GitLab 默认集成的 CI 功能，GitLab CI 通过在项目内 .gitlab-ci.yaml 配置文件读取 CI 任务并进行相应处理；GitLab CI 通过其称为 GitLab Runner 的 Agent 端进行 build 操作；Runner 本身可以使用多种方式安装，比如使用 Docker 镜像启动等；Runner 在进行 build 操作时也可以选择多种 build 环境提供者；比如直接在 Runner 所在宿主机 build、通过新创建虚拟机(vmware、virtualbox)进行 build等；同时 Runner 支持 Docker 作为 build 提供者，即每次 build 新启动容器进行 build；GitLab CI 其大致架构如下</strong></p><p>![image-20200309140112958](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200309140112958.png)</p><pre><code># yum -y install curl policycoreutils openssh-server openssh-clients postfix git# systemctl enable sshd# systemctl start sshd# systemctl enable postfix# systemctl start postfix</code></pre><p>![image-20200306112315163](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306112315163.png)</p><h3 id="安装gitlab-ce"><strong>安装gitlab-ce</strong></h3><pre><code>[root@git ~]# curl -sS https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.rpm.sh | sudo bash</code></pre><p><strong>注：由于网络问题，国内用户，使用清华大学的镜像源进行安装：</strong></p><pre><code>[root@git ~]# vim /etc/yum.repos.d/gitlab-ce.repo[gitlab-ce]name=gitlab-cebaseurl=http://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7repo_gpgcheck=0gpgcheck=0enabled=1gpgkey=https://packages.gitlab.com/gpg.key[root@git ~]# yum makecache//保存到本地[root@git ~]# yum -y install gitlab-ce #这两条命令是把gitlab源先加入了yum，然后yum下载gitlab[root@git ~]# vim /etc/gitlab/gitlab.rb //修改端口是为了防止端口冲突，因为80默认是http服务的 external_url 'http://192.168.1.21:90'  #端口， unicorn默认是8080 也是tomcat的端口 unicorn['listen'] = '127.0.0.1'unicorn['port'] = 3000 [root@git ~]# gitlab-ctl reconfigure //启动gitlab，这个过程可能会有点慢[root@git ~]# ls /etc/yum.repos.d///查看一下</code></pre><p>![image-20200306141100803](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306141100803.png)</p><h3 id="访问192-168-1-10-90">访问192.168.1.10:90</h3><p><strong>在网页配置用户密码后则安装完毕。用户默认root，这里让设置一个密码再登录，<a href="http://xn--12345-of3np30ehqhlqe.com" target="_blank" rel="noopener">这里设置12345.com</a>（相对较短的密码不让设置）</strong></p><p>![image-20200306141728312](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306141728312.png)</p><p>![image-20200306142041682](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306142041682.png)</p><p>![image-20200306142219600](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306142219600.png)</p><h1>四、jenkins和gitlab相互关联</h1><p><strong>jenkins：工具集成平台</strong></p><p><strong>gitlab: 软件托管平台</strong></p><p><strong>部署这两个服务的联动，需要经过ssh验证。</strong></p><h2 id="1、首先我们需要在gitlab上绑定jenkins服务器的ssh公钥，这里我们使用的是root用户的公私钥，切记生产环境是不允许随便用root的">1、<strong>首先我们需要在gitlab上绑定jenkins服务器的ssh公钥，这里我们使用的是root用户的公私钥，切记生产环境是不允许随便用root的</strong></h2><h3 id="（1）jenkins"><strong>（1）jenkins</strong></h3><pre><code>[root@jenkins ~]# ssh-keygen -t rsa //然后不输入只回车会生成一对公私钥</code></pre><h4 id="默认在-root-ssh-目录里"><strong>默认在/root/.ssh/目录里</strong></h4><pre><code>[root@jenkins ~]# cat /root/.ssh/id_rsa.pub //查看公钥并复制ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDMA4+je3NsxZrF2v8TPLXJp1ejwy1YokXipEFyGVNo5IbtkiBDwBLOAl5i7yromY8YGgoNNriE2g89IM/44BGC5UDCokQ69Ze9Ta9Kynv3/1PDFXIABJJG0f6LsUqt0nKFaFoGz3ZuYAnl6AzLpXEic8DBDrsFk+UGrxvMfSEqHlYO2b7jRXE1HGRnqI/IcVB190cLT1kmBKi7hSqUNBc1cY6t3a6gGiBpp9tc8PW4r/RcLblhAL1LKx8x37NOZkqox8IMh3eM/wtWwAVFlI8XU+sz9akzJOVmd1ArT5Q4w8WA/uVHCDUGVI/fli/ZRv+mNZyF3EH26runctb5LkCT root@jenkins</code></pre><h3 id="（2）gitlab">（2）gitlab</h3><p>![image-20200306195836403](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306195836403.png)</p><h4 id="在这里放刚才拷贝的公钥保存就行了。"><strong>在这里放刚才拷贝的公钥保存就行了。</strong></h4><p>![image-20200306200026493](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306200026493.png)</p><p>![image-20200306200103458](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306200103458.png)</p><h4 id="我们先在gitlab上创建一个代码仓库-点击-new-project"><strong>我们先在gitlab上创建一个代码仓库 点击 new project</strong></h4><p>![image-20200306200156932](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306200156932.png)</p><p><strong>输入一个仓库的名字，权限选择公共的（public）然后直接点击创建</strong></p><p>![image-20200306200431858](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306200431858.png)</p><h4 id="点击新建一个new-file"><strong>点击新建一个new.file</strong></h4><p>![image-20200306201437862](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306201437862.png)</p><h4 id="写入代码，起一个名字然后保存"><strong>写入代码，起一个名字然后保存</strong></h4><p>![image-20200306201558158](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306201558158.png)</p><h4 id="创建好了，然后在本地测试一下是否可用"><strong>创建好了，然后在本地测试一下是否可用</strong></h4><p>![image-20200306201744679](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306201744679.png)</p><pre><code>[root@git ~]# mkdir xgp[root@git ~]# cd xgp/[root@git xgp]# git clone git@192.168.1.10:root/xgp-demo.git//克隆xgp-demo仓库到本地[root@git xgp]# ls xgp-demo/index.html[root@git xgp]# cat xgp-demo/index.html print: &quot;hello word!!!&quot;//查看一下</code></pre><h3 id="（3）自动构建">（3）自动构建</h3><p><strong>安装插件</strong></p><p><strong>先进入到之前查看插件的地方</strong></p><p><strong>系统设置----插件管理----高级_—上传插件gitlab-oauth、gitlab-plugin、 windows-slaves、ruby-runt ime、gitlab-hook</strong></p><p>![image-20200306212734302](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306212734302.png)</p><h3 id="（4）如果可以用，则打开jenkins-点击新建">（4）如果可以用，则打开jenkins 点击新建</h3><p>![image-20200306202647670](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306202647670.png)</p><p>![image-20200306202724313](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306202724313.png)</p><h4 id="地址粘贴进去以后没有报错则没错"><strong>地址粘贴进去以后没有报错则没错</strong></h4><p>![image-20200306203441474](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306203441474.png)</p><p><strong>但是很伤心它报错了，那是因为jenkins和git没有关联上</strong></p><h3 id="解决">解决</h3><p>![image-20200306203407502](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306203407502.png)</p><h4 id="git主机生成ssh密钥">git主机生成ssh密钥</h4><pre><code>[root@jenkins ~]# ssh-keygen -t rsa //然后不输入只回车会生成一对公私钥[root@jenkins ~]# cat /root/.ssh/id_rsa   //查看密钥并复制</code></pre><p>![image-20200306203947496](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306203947496.png)</p><p>![image-20200306204232588](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306204232588.png)</p><p><strong>下面的这个插件很重要，就是他实现自动化更新的webhook插件，安装过了就会有这条，然后点击这条下面出来的这些东西保持默认就行。同时注意复制</strong></p><p><strong>这个里面写的是jenkins构建时候会执行的shell脚本，这个是最重要的，就是他实现了下端kubernetes自动更新容器的操作。</strong></p><p>![image-20200306204512237](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306204512237.png)</p><p>![image-20200306204948462](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306204948462.png)</p><pre><code>#!/bin/bashbackupcode=&quot;/data/backcode/$JOB_NAME/$BUILD_NUMBER&quot;  mkdir -p $backupcode     #jenkins创建上述目录chmod 644 &quot;$JENKINS_HOME&quot;/workspace/&quot;$JOB_NAME&quot;/*rsync -acP   &quot;$JENKINS_HOME&quot;/workspace/&quot;$JOB_NAME&quot;/*  $backupcode #$JENKINS_HOME和$JOB_NAME同步最新消息#ssh root@192.168.1.21 sed -i 's/v1/v2/g' /root/app/deploy.yaml #更改镜像版本echo From  192.168.1.21:5000/web:v1 &gt; &quot;$JENKINS_HOME&quot;/workspace/Dockerfileecho COPY ./&quot;$JOB_NAME&quot;/* /usr/local/apache2/htdocs/ &gt;&gt; &quot;$JENKINS_HOME&quot;/workspace/Dockerfiledocker rmi 192.168.1.21:5000/web:v1docker build -t 192.168.1.21:5000/web:v1 /&quot;$JENKINS_HOME&quot;/workspace/.docker push 192.168.1.21:5000/web:v1ssh root@192.168.1.21 kubectl delete deployment webssh root@192.168.1.21 kubectl apply -f /root/app/deploy.yaml</code></pre><blockquote><p><strong>$JOB_NAME：项目名称</strong></p><p><strong>$BUILD_NUMBER：第几次构建</strong></p><p><strong>$JENKINS_HOME：jenkins的家目录</strong></p></blockquote><p><strong>完事以后先别保存，首先复制一下上面的jenkins地址，然后去gitlab上绑定webhook</strong></p><p>![image-20200306213050759](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306213050759.png)</p><p><strong>保存，登陆gitlab，点击下图这个设置</strong></p><p>![image-20200306213514819](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306213514819.png)</p><p>![image-20200306213829519](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306213829519.png)</p><p><strong>测试显示下图 的蓝条说明jenkins 已经连通了gitlab</strong></p><p>![image-20200306214117715](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306214117715.png)</p><p>![image-20200306215322180](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306215322180.png)</p><h4 id="回到Jenkins开启匿名访问权限"><strong>回到Jenkins开启匿名访问权限</strong></h4><p>![image-20200306215429619](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306215429619.png)</p><p>![image-20200306215504413](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306215504413.png)</p><p>![](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306215539717.png)</p><p>![image-20200306215611348](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306215611348.png)</p><p><strong>测试显示下图 的蓝条说明jenkins 已经连通了gitlab</strong></p><p>![image-20200306214126410](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200306214126410.png)</p><p><strong>好了，jenkins和gitlab 都已经互相的ssh通过了，然后我们最后需要做的一个ssh是关于jenkins</strong></p><p><strong>///注意，这里是从git和jenkins向master节点做免密登录。</strong></p><pre><code>[root@git ~]# ssh-copy-id root@192.168.1.21[root@jenkins ~]# ssh-copy-id root@192.168.1.21</code></pre><p><strong>好了，环境全部部署完毕！！！。开始测试</strong></p><h1>五、测试</h1><p><strong>测试的方法很简单，就是在gitlab上新建代码，删除代码，修改代码，都会触发webhook进行自动部署。最终会作用在所有的nginx容器中，也就是我们的web服务器。</strong></p><p>![image-20200309100434912](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200309100434912.png)</p><p><strong>这里我修改了之前建立的 index.html文件 保存以后，就打开浏览器 一直访问kubernetes-node 里面的容器了</strong></p><p>![image-20200309100445830](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200309100445830.png)</p><p>![image-20200309100530210](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200309100530210.png)</p><h2 id="访问一下http-192-168-1-21-31234">访问一下http://192.168.1.21:31234/</h2><p><em><strong>如果没有变，应该注意查看是否在jenkins上构建完成，等以小会就可以了。</strong></em></p><p>![image-20200309100557309](G:\四期\虚拟化\kubernetes\k8s文档\21 k8s持续集成.assets\image-20200309100557309.png)</p><p><strong>构建成功</strong></p><h1>六、GitLab CI 总结</h1><p><strong>CS 架构</strong><br>GitLab 作为 Server 端，控制 Runner 端执行一系列的 CI 任务；代码 clone 等无需关心，GitLab 会自动处理好一切；Runner 每次都会启动新的容器执行 CI 任务</p><p><strong>容器即环境</strong><br>在 Runner 使用 Docker build 的前提下；所有依赖切换、环境切换应当由切换不同镜像实现，即 build 那就使用 build 的镜像，deploy 就用带有 deploy 功能的镜像；通过不同镜像容器实现完整的环境隔离</p><p><strong>CI即脚本</strong><br>不同的 CI 任务实际上就是在使用不同镜像的容器中执行 <a href="https://www.centos.bz/tag/shell/" target="_blank" rel="noopener">SHELL</a> 命令，自动化 CI 就是执行预先写好的一些小脚本</p><p><strong>敏感信息走环境变量</strong><br>一切重要的敏感信息，如账户密码等，不要写到 CI 配置中，直接放到 GitLab 的环境变量中；GitLab 会保证将其推送到远端 Runner 的 SHELL 变量中</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>20 k8s的helm模板</title>
      <link href="/posts/c224.html"/>
      <url>/posts/c224.html</url>
      
        <content type="html"><![CDATA[<h1>自定义helm模板</h1><p><a href="https://hub.helm.sh/" target="_blank" rel="noopener">https://hub.helm.sh/</a></p><h2 id="1、开发自己的chare包">1、开发自己的chare包</h2><pre><code>[root@master ~]# helm create mychare//创建一个名为mychare的chare包[root@master ~]# tree -C mychare///以树状图查看一下chare包mychare/├── charts├── Chart.yaml├── templates│   ├── deployment.yaml│   ├── _helpers.tpl│   ├── ingress.yaml│   ├── NOTES.txt│   ├── service.yaml│   └── tests│       └── test-connection.yaml└── values.yaml</code></pre><h2 id="2、调试chart">2、调试chart</h2><pre><code>[root@master mychare]# cd[root@master ~]# helm install --dry-run --debug mychare//检查这个mychare是否有问题</code></pre><h2 id="3、安装chart">3、安装chart</h2><pre><code>[root@node02 ~]# docker pull nginx:stable</code></pre><h3 id="（1）通过仓库安装">（1）通过仓库安装</h3><pre><code>[root@master mychare]# helm search redis//搜索chare包</code></pre><pre><code>[root@master mychare]# helm repo list//查看是否有能访问仓库</code></pre><pre><code>[root@master mychare]# helm install stable/redis//安装</code></pre><h3 id="（2）通过tar包安装">（2）通过tar包安装</h3><pre><code>[root@master ~]# helm fetch stable/redis//直接下载chare包[root@master ~]# tar -zxf redis-1.1.15.tgz//解压下载的chare包[root@master ~]# tree -C redisredis├── Chart.yaml├── README.md├── templates│   ├── deployment.yaml│   ├── _helpers.tpl│   ├── networkpolicy.yaml│   ├── NOTES.txt│   ├── pvc.yaml│   ├── secrets.yaml│   └── svc.yaml└── values.yaml</code></pre><h3 id="（3）通过chare本地目录安装">（3）通过chare本地目录安装</h3><pre><code>[root@master ~]# helm fetch stable/redis//直接下载chare包[root@master ~]# tar -zxf redis-1.1.15.tgz//解压下载的chare包[root@master ~]# helm install redis</code></pre><h3 id="（4）通过URL安装">（4）通过URL安装</h3><pre><code>[root@master ~]# helm install https://example.com/charts/foo-1.2.3.tgz</code></pre><p>使用本地目录安装：</p><pre><code>[root@master ~]# cd mychare/[root@master mychare]# vim values.yaml </code></pre><p>![image-20200304094840738](G:\四期\虚拟化\kubernetes\k8s文档\20 k8s的helm模板.assets\image-20200304094840738.png)</p><pre><code>[root@master mychare]# cd templates/[root@master templates]# vim service.yaml </code></pre><p>![image-20200304095647172](G:\四期\虚拟化\kubernetes\k8s文档\20 k8s的helm模板.assets\image-20200304095647172.png)</p><pre><code>[root@master templates]# cd ..[root@master mychare]# helm install -n test ../mychare/[root@master ~]# helm upgrade test mychare/ -f  mychare/values.yaml </code></pre><h2 id="4、例子">4、例子</h2><p><strong>使用mychart部署一个实例: xgp。使用镜像为私有镜像v1 版本。</strong></p><p><strong>完成之后，镜像版本。</strong></p><p><strong>全部成功之后，将实例做一个升级，将镜像改为v2版本。</strong></p><h3 id="更改镜像为私有镜像">更改镜像为私有镜像</h3><pre><code>[root@master ~]# vim mychare/values.yaml</code></pre><p>![image-20200304104416415](G:\四期\虚拟化\kubernetes\k8s文档\20 k8s的helm模板.assets\image-20200304104416415.png)</p><pre><code>[root@master ~]#  helm install -n xgp mychare/ -f mychare/values.yaml[root@master ~]# kubectl get deployments. -o wide</code></pre><p>![image-20200304104645260](G:\四期\虚拟化\kubernetes\k8s文档\20 k8s的helm模板.assets\image-20200304104645260.png)</p><pre><code>[root@master ~]# vim mychare/values.yaml</code></pre><p>![image-20200304105120894](G:\四期\虚拟化\kubernetes\k8s文档\20 k8s的helm模板.assets\image-20200304105120894.png)</p><pre><code>[root@master ~]# helm upgrade  xgp mychare/  -f mychare/values.yaml [root@master ~]# kubectl get deployments. -o wide</code></pre><p>![image-20200304105211506](G:\四期\虚拟化\kubernetes\k8s文档\20 k8s的helm模板.assets\image-20200304105211506.png)</p><pre><code>[root@master ~]# kubectl edit deployments. xgp-mychare</code></pre><p>![image-20200304105334541](G:\四期\虚拟化\kubernetes\k8s文档\20 k8s的helm模板.assets\image-20200304105334541.png)</p><pre><code>[root@master ~]# kubectl get deployments. -o wide</code></pre><p>![image-20200304105359184](G:\四期\虚拟化\kubernetes\k8s文档\20 k8s的helm模板.assets\image-20200304105359184.png)</p><h1>创建自己的Repo仓库</h1><h2 id="1、node01启动一个httpd的容器">1、node01启动一个httpd的容器</h2><pre><code>[root@node01 ~]# mkdir /var/xgp//创建一个目录[root@node01 ~]# docker pull httpd//下载httpd镜像[root@node02 ~]# docker run -d -p 8080:80 -v /var/xgp:/usr/local/apache2/htdocs httpd//启动一个httpd的容器</code></pre><h2 id="2、master节点上，将mychart目录打包。">2、master节点上，将mychart目录打包。</h2><pre><code>[root@master ~]# helm package mychare/Successfully packaged chart and saved it to: /root/mychare-0.1.0.tgz</code></pre><h2 id="3、生成仓库的index文件。">3、生成仓库的index文件。</h2><pre><code>[root@master ~]# mkdir myrepo//创建一个目录存放打包的chare[root@master ~]# mv mychare-0.1.0.tgz myrepo///移动打包好的文件[root@master ~]# helm repo index myrepo/ --url http://192.168.1.22:8080/charts//生成仓库的index文件[root@master ~]# ls myrepo/index.yaml  mychare-0.1.0.tgz</code></pre><h2 id="4、将生成的tar包和index-yaml上传到node01的-var-www-charts目录下">4、将生成的tar包和index.yaml上传到node01的/var/www/charts目录下.</h2><h3 id="node01创建目录">node01创建目录</h3><pre><code>[root@node01 ~]# mkdir /var/xgp/charts</code></pre><h3 id="master移动动到">master移动动到</h3><pre><code>[root@master ~]# scp myrepo/* node01:/var/xgp/charts/</code></pre><h3 id="node01查看一下">node01查看一下</h3><pre><code>[root@node01 ~]# ls /var/xgp/charts/index.yaml  mychare-0.1.0.tgz</code></pre><h2 id="5、添加新的repo仓库。">5、添加新的repo仓库。</h2><pre><code>[root@master ~]# helm repo add newrepo http://192.168.1.22:8080/charts</code></pre><pre><code>[root@master ~]# helm repo list</code></pre><p>![image-20200304112410286](G:\四期\虚拟化\kubernetes\k8s文档\20 k8s的helm模板.assets\image-20200304112410286.png)</p><pre><code>[root@master ~]# helm search mychare</code></pre><p>![image-20200304112443931](G:\四期\虚拟化\kubernetes\k8s文档\20 k8s的helm模板.assets\image-20200304112443931.png)</p><h2 id="6、我们就可以直接使用新的repo仓库部署实例了。">6、我们就可以直接使用新的repo仓库部署实例了。</h2><pre><code>[root@master ~]# helm install newrepo/mychare -n wsd</code></pre><pre><code>[root@master ~]# helm list </code></pre><p>![image-20200304112515084](G:\四期\虚拟化\kubernetes\k8s文档\20 k8s的helm模板.assets\image-20200304112515084.png)</p><h2 id="7-如果以后仓库中新添加了chart包-需要用helm-repo-update命玲更新本地的index文件。">7.如果以后仓库中新添加了chart包,需要用helm repo update命玲更新本地的index文件。</h2><p>练习：<br>新创建一个bdqn.的chart包。然后将chart包上传到上述repo源中。</p><pre><code>[root@master ~]# helm create bdqn[root@master ~]# helm package bdqn/[root@master ~]# mv bdqn-0.1.0.tgz myrepo/[root@master ~]#  helm repo index myrepo/ --url http://192.168.1.22:8080/charts[root@master myrepo]# scp bdqn-0.1.0.tgz index.yaml  node01:/var/xgp/charts[root@master myrepo]# helm repo update[root@master myrepo]# helm search bdqn[root@master myrepo]# helm install http://192.168.1.22:8080/charts/bdqn-0.1.0.tgz</code></pre><h2 id="1）创建helm的私有仓库，以自己的名字命名。">1）创建helm的私有仓库，以自己的名字命名。</h2><h3 id="1、node01启动一个httpd的容器-2">1、node01启动一个httpd的容器</h3><pre><code>[root@node01 ~]# mkdir /var/xgp//创建一个目录[root@node01 ~]# docker pull httpd//下载httpd镜像[root@node02 ~]# docker run -d -p 8080:80 -v /var/xgp:/usr/local/apache2/htdocs httpd//启动一个httpd的容器</code></pre><h3 id="3、生成仓库的index文件。-2">3、生成仓库的index文件。</h3><pre><code>[root@master ~]# mkdir xgprepo//创建一个目录存放打包的chare[root@master ~]# helm repo index xgprepo/ --url http://192.168.1.22:8080/charts//生成仓库的index文件</code></pre><h3 id="4、将生成的index-yaml上传到node01的-var-www-charts目录下">4、将生成的index.yaml上传到node01的/var/www/charts目录下.</h3><h4 id="node01创建目录-2">node01创建目录</h4><pre><code>[root@node01 ~]# mkdir /var/xgp/charts</code></pre><h4 id="master移动动到-2">master移动动到</h4><pre><code>[root@master ~]# scp xgprepo/* node01:/var/xgp/charts/</code></pre><h4 id="node01查看一下-2">node01查看一下</h4><pre><code>[root@node01 ~]# ls /var/xgp/charts/index.yaml  </code></pre><h3 id="5、添加新的repo仓库">5、添加新的repo仓库</h3><pre><code>[root@master ~]# helm repo add xgp http://192.168.1.22:8080/charts[root@master ~]# helm repo list </code></pre><p>![image-20200304132528938](G:\四期\虚拟化\kubernetes\k8s文档\20 k8s的helm模板.assets\image-20200304132528938.png)</p><h2 id="2）-自定义一个chart包，要求这个包运行一个httpd的服务，使用私有镜像v1版本。3个副本Pod，service类型更改为NodePort，端口指定为-30000">2） 自定义一个chart包，要求这个包运行一个httpd的服务，使用私有镜像v1版本。3个副本Pod，service类型更改为NodePort，端口指定为:30000</h2><h4 id="自定义一个chart包">自定义一个chart包</h4><pre><code>[root@master ~]# helm create wsd//创建一个名为wsd的chares包</code></pre><h4 id="按照要求修改配置文件">按照要求修改配置文件</h4><pre><code>[root@master ~]# cd wsd///进入这个chart包[root@master wsd]# vim values.yaml//修改wsd的配置文件replicaCount: 3                         #三个副本image:  repository: 192.168.1.21:5000/web      #更改镜像为私有镜像  tag: v1                                #镜像标签v1  pullPolicy: IfNotPresent              imagePullSecrets: []nameOverride: &quot;&quot;fullnameOverride: &quot;&quot;service:  type: NodePort              #修改模式为映射端口  port: 80  nodePort: 30000             #添加端口[root@master wsd]# vim templates/service.yaml apiVersion: v1kind: Servicemetadata:  name: {{ include "wsd.fullname" . }}  labels:{{ include "wsd.labels" . | indent 4 }}spec:  type: {{ .Values.service.type }}  ports:    - port: {{ .Values.service.port }}      targetPort: http      protocol: TCP      name: http      nodePort: {{ .Values.service.nodePort }}    #“添加”能让服务识别到nodePort的端口  selector:    app.kubernetes.io/name: {{ include "wsd.name" . }}    app.kubernetes.io/instance: {{ .Release.Name }}</code></pre><h4 id="测试一下">测试一下</h4><pre><code>[root@master ~]# helm install -n wsd  wsd/ -f wsd/values.yaml </code></pre><p>![image-20200304134959273](G:\四期\虚拟化\kubernetes\k8s文档\20 k8s的helm模板.assets\image-20200304134959273.png)</p><h4 id="查看一下镜像版本">查看一下镜像版本</h4><pre><code>[root@master ~]# kubectl get deployments. -o wide</code></pre><p>![image-20200304135106081](G:\四期\虚拟化\kubernetes\k8s文档\20 k8s的helm模板.assets\image-20200304135106081.png)</p><h4 id="访问一下">访问一下</h4><pre><code>[root@master ~]# curl 127.0.0.1:30000</code></pre><p>![image-20200304150609552](G:\四期\虚拟化\kubernetes\k8s文档\20 k8s的helm模板.assets\image-20200304150609552.png)</p><h2 id="3-将实例进行更新，要求镜像生产v2版本。">3)  将实例进行更新，要求镜像生产v2版本。</h2><p><strong>私有镜像和官方镜像升级有所不同，官方的只需通过 （helm upgrade --set imageTag=“标签” 服务名称 charts包名 ）进行更改标签即可，而私有镜像需通过更改values.yaml中的标签才行比较麻烦一点。</strong></p><h3 id="1、修改values-yaml">1、修改values.yaml</h3><pre><code>[root@master ~]# vim wsd/values.yaml # Default values for wsd.# This is a YAML-formatted file.# Declare variables to be passed into your templates.replicaCount: 3image:  repository: 192.168.1.21:5000/web  tag: v2                            #修改标签为v2  pullPolicy: IfNotPresent[root@master ~]# helm upgrade wsd wsd/ -f wsd/values.yaml//基于配置文件刷新一下wsd服务</code></pre><h4 id="查看一下">查看一下</h4><pre><code>[root@master ~]# kubectl get deployments. -o wide</code></pre><p>![image-20200304140054269](G:\四期\虚拟化\kubernetes\k8s文档\20 k8s的helm模板.assets\image-20200304140054269.png)</p><h4 id="访问一下-2">访问一下</h4><pre><code>[root@master ~]# curl 127.0.0.1:30000</code></pre><p>![image-20200304150742815](G:\四期\虚拟化\kubernetes\k8s文档\20 k8s的helm模板.assets\image-20200304150742815.png)</p><h3 id="2、使用edit进行版本更新">2、使用edit进行版本更新</h3><p><em><strong>确定wsd这个服务开启</strong></em></p><pre><code>[root@master ~]# kubectl edit deployments. wsd</code></pre><p>![](G:\四期\虚拟化\kubernetes\k8s文档\20 k8s的helm模板.assets\image-20200304140425336.png)</p><h4 id="查看一下-2">查看一下</h4><pre><code>[root@master ~]# kubectl get deployments. -o wide</code></pre><p>![image-20200304140520342](G:\四期\虚拟化\kubernetes\k8s文档\20 k8s的helm模板.assets\image-20200304140520342.png)</p><h4 id="访问一下-3">访问一下</h4><pre><code>[root@master ~]# curl 127.0.0.1:30000</code></pre><p>![image-20200304150839440](G:\四期\虚拟化\kubernetes\k8s文档\20 k8s的helm模板.assets\image-20200304150839440.png)</p><h2 id="4）重新定义一个chart包，名称为-new-test-将这个包上传到上述私有仓库中。">4）重新定义一个chart包，名称为: new-test,将这个包上传到上述私有仓库中。</h2><pre><code>[root@master ~]# helm repo list </code></pre><p>![image-20200304142059023](G:\四期\虚拟化\kubernetes\k8s文档\20 k8s的helm模板.assets\image-20200304142059023.png)</p><pre><code>[root@master ~]# helm create xgp-wsd//创建一个名为xgp-wsd的charts包[root@master ~]# helm package xgp-wsd///将xgp-wsd打包在当前目录[root@master ~]# mv xgp-wsd-0.1.0.tgz xgprepo///把打包文件放到仓库目录[root@master ~]# helm repo index xgprepo/ --url http://192.168.1.22:8080/charts//把仓库目录新加入的charts包信息记录在index.yaml中，使得其他加入的主机可以识别到，仓库的charts包[root@master ~]# scp xgprepo/* node01:/var/xgp/charts//将仓库目录的文件移动到httpd服务上，使各个主机可以访问，下载仓库的charts包[root@master ~]# helm repo update //更新一下chart存储库</code></pre><h3 id="查看一下-3">查看一下</h3><pre><code>[root@master ~]# helm search xgp-wsd</code></pre><p>![image-20200304142009776](G:\四期\虚拟化\kubernetes\k8s文档\20 k8s的helm模板.assets\image-20200304142009776.png)</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>18 k8s的HPA自动扩容与缩容</title>
      <link href="/posts/2643.html"/>
      <url>/posts/2643.html</url>
      
        <content type="html"><![CDATA[<h1>HPA</h1><p><strong>可以根据当前Pod资源的使用率，比如说CPU、磁盘、内存等进行副本Pod的动态的扩容与缩容。</strong></p><p><strong>前提条件:系统应该能否获取到当前Pod的资源使用情况 (意思是可以执行kubectl top pod命令,并且能够得到反馈信息)。</strong></p><p><strong>heapster：这个组件之前是集成在k8s集群的,不过在1.12版本之后被移除了。如果还想使用此功能，应该部署metricServer, 这个k8s集群资源使用情况的聚合器。</strong></p><p><strong>这里，我们使用一个测试镜像， 这个镜像基于php-apache制作的docker镜像，包含了一些可以运行cpu密集计算任务的代码。</strong></p><h2 id="1、创建一个deployment控制器">1、创建一个deployment控制器</h2><pre><code>[root@master ~]#docker pull mirrorgooglecontainers/hpa-example:latest//下载hpa-example镜像[root@master ~]# kubectl run php-apache --image=mirrorgooglecontainers/hpa-example --requests=cpu=200m --expose  --port=80//基于hpa-example镜像，运行一个deployment控制器，请求CPU的资源为200m，暴露一个80端口</code></pre><h3 id="查看一下">查看一下</h3><pre><code>[root@master ~]# kubectl get deployments.</code></pre><p>![image-20200228102643352](G:\四期\虚拟化\kubernetes\k8s文档\18 HPA自动容与蒲容.assets\image-20200228102643352.png)</p><h2 id="2、创建HPA控制器">2、创建HPA控制器</h2><pre><code>[root@master ~]# kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10//当deployment资源对象的CPU使用率达到50%时，就进行扩容，最多可以扩容到10个</code></pre><h3 id="查看一下-2">查看一下</h3><pre><code>[root@master ~]# kubectl get hpa</code></pre><p>![image-20200228101908398](G:\四期\虚拟化\kubernetes\k8s文档\18 HPA自动容与蒲容.assets\image-20200228101908398.png)</p><h2 id="3、测试（master开启三个端口）">3、测试（master开启三个端口）</h2><p><strong>新开启多个终端，对pod进行死循环请求php-apache的pod</strong></p><h3 id="端口一">端口一</h3><h4 id="（1）创建一个应用，用来不停的访问我们刚刚创建的php-apache的svc资源。">（1）创建一个应用，用来不停的访问我们刚刚创建的php-apache的svc资源。</h4><pre><code>[root@master ~]# kubectl run -i --tty load-generator --image=busybox /bin/sh</code></pre><h4 id="（2）进入Pod内，执行以下这条命令-用来模拟访问php-apache的svc资源。">（2）进入Pod内，执行以下这条命令.用来模拟访问php-apache的svc资源。</h4><pre><code>[root@master ~]# while true; do wget -q -O- http://php-apache.default.svc.cluster.local ; done//不停地向php-apache的svc资源，发送ok</code></pre><h3 id="端口二">端口二</h3><pre><code>[root@master ~]# kubectl get hpa -w//实时查看pod的cpu状态</code></pre><p>![image-20200228133816724](G:\四期\虚拟化\kubernetes\k8s文档\18 k8s的HPA自动容与缩容.assets\image-20200228133816724.png)</p><p><strong>可以看到php-apache的cpu使用情况已经超过了50%</strong></p><h3 id="端口三">端口三</h3><pre><code>[root@master images]# kubectl get pod -w//实时查看pod的状态</code></pre><p>![image-20200228134105507](G:\四期\虚拟化\kubernetes\k8s文档\18 k8s的HPA自动容与缩容.assets\image-20200228134105507.png)</p><p><strong>可以看到当php-apache的cpu使用情况超过50%后，就会不断生成新的php-apache来进行负载均衡（目前设置的上线时10个），当然，如果cpu使用情况下降到50%，master就会陆续地删除php-apache，这样的使用可以减少不必要的资源浪费、资源分配不均等情况。</strong></p><h1>二、资源限制</h1><h2 id="1、基于Pod">1、基于Pod</h2><p><strong>Kubernetes对资源的限制实际上是通过cgroup来控制的，cgroup 是容器的一组用来控制内核如何运行进程的相关属性集合。针对内存、CPU 和各种设备都有对应的cgroup</strong></p><p><strong>默认情况下，Pod运行没有CPU和内存的限额。这意味着系统中的任何 Pod将能够像执行该Pod所在的节点一样，消耗足够多的CPU和内存。一般会针对某些应用的pod资源进行资源限制，这个资源限制是通过</strong></p><p><strong>resources的requests和limits来实现</strong></p><pre><code>[root@master ~]# vim cgroup-pod.yaml</code></pre><p>![image-20200228153809932](G:\四期\虚拟化\kubernetes\k8s文档\18 k8s的HPA自动容与缩容.assets\image-20200228153809932.png)</p><p><strong>requests: 要分配的资源，limits为最高请求的资源值。可以简单的理解为初始值和最大值。</strong></p><h2 id="2、基于名称空间"><strong>2、基于名称空间</strong></h2><h3 id="1）-计算资源配额">1） 计算资源配额</h3><pre><code>[root@master ~]# vim compute-resources.yaml</code></pre><p>![image-20200228153818288](G:\四期\虚拟化\kubernetes\k8s文档\18 k8s的HPA自动容与缩容.assets\image-20200228153818288.png)</p><h3 id="2）配置对象数量配额限制">2）配置对象数量配额限制</h3><pre><code>[root@master ~]# vim object-counts.yaml</code></pre><p>![image-20200228153828002](G:\四期\虚拟化\kubernetes\k8s文档\18 k8s的HPA自动容与缩容.assets\image-20200228153828002.png)</p><h3 id="3）-配置CPU和内存的LimitRange">3） 配置CPU和内存的LimitRange</h3><pre><code>[root@master ~]# vim limitRange.yaml</code></pre><p>![image-20200228153834705](G:\四期\虚拟化\kubernetes\k8s文档\18 k8s的HPA自动容与缩容.assets\image-20200228153834705.png)</p><p><strong>default 即 limit的值。</strong></p><p><strong>defaultRequest 即 request的值。</strong></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>14 k8s的Secret（密文）和configmap（明文）的使用教程</title>
      <link href="/posts/a50d.html"/>
      <url>/posts/a50d.html</url>
      
        <content type="html"><![CDATA[<h1>一、Secret</h1><p><em><strong>Secret :用来保存一些敏感信息，比如数据库的用户名密码或者秘钥。</strong></em></p><h2 id="举例-保存数据库的用户名和密码">举例:保存数据库的用户名和密码</h2><blockquote><p><strong>用户名：</strong><strong>root</strong><br><strong>密码：</strong>   <strong><a href="http://123.com" target="_blank" rel="noopener">123.com</a></strong></p></blockquote><h3 id="1、通过–from-literal（文字的）">1、通过–from-literal（文字的）</h3><pre><code>[root@master secret]# kubectl create secret generic mysecret1 --from-literal=username=root --from-literal=password=123.com</code></pre><blockquote><p><strong>generic：通用的，一般的加密方式</strong></p></blockquote><h4 id="查看一下">查看一下</h4><pre><code>[root@master secret]# kubectl get secrets </code></pre><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200214100419966.png" alt></p><p><strong>类型是Opaque（不透明的）</strong></p><h3 id="2、通过from-file（文件）">2、通过from-file（文件）</h3><h4 id="新建两个文件并分别写入用户名和密码">新建两个文件并分别写入用户名和密码</h4><pre><code>[root@master secret]# echo root &gt; username[root@master secret]# echo 123.com  &gt; password</code></pre><h4 id="创建一个secret">创建一个secret</h4><pre><code>[root@master secret]#  kubectl create secret generic mysecret2 --from-file=username --from-file=password </code></pre><h4 id="查看一下-2">查看一下</h4><pre><code>[root@master secret]# kubectl get secrets</code></pre><p><img src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5C%E8%99%9A%E6%8B%9F%E5%8C%96%5Ckubernetes%5Ck8s%E6%96%87%E6%A1%A3%5C14.assets%5Cimage-20200214103506842.png" alt="image-20200214103506842"></p><h3 id="3、通过-from-env-file">3、通过-- from- env-file:</h3><h4 id="创建一个文件写入用户名和密码">创建一个文件写入用户名和密码</h4><pre><code>[root@master secret]#vim env.txt username=rootpassword=123.com</code></pre><h4 id="创建一个secret-2">创建一个secret</h4><pre><code>[root@master secret]# kubectl create secret generic mysecret3 --from-env-file=env.txt </code></pre><h4 id="查看一下-3">查看一下</h4><pre><code>[root@master secret]# kubectl get secrets </code></pre><p><img src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5C%E8%99%9A%E6%8B%9F%E5%8C%96%5Ckubernetes%5Ck8s%E6%96%87%E6%A1%A3%5C14.assets%5Cimage-20200214103905956.png" alt="image-20200214103905956"></p><h3 id="4、通过yaml配置文件">4、通过yaml配置文件</h3><h4 id="（1）把需要保存的数据加密（”base64“的方式）">（1）把需要保存的数据加密（”base64“的方式）</h4><pre><code>[root@master secret]# echo root | base64cm9vdAo=[root@master secret]# echo 123.com | base64MTIzLmNvbQo=</code></pre><blockquote><p><strong>解码：</strong></p><pre><code>[root@master secret]# echo -n cm9vdAo | base64 --decode root[root@master secret]# echo -n MTIzLmNvbQo | base64 --decode 123.com</code></pre></blockquote><h4 id="（2）编写secre4的yaml文件">（2）编写secre4的yaml文件</h4><pre><code>[root@master secret]# vim secret4.yamlapiVersion: v1kind: Secretmetadata:  name: mysecret4data:  username: cm9vdAo=  password: MTIzLmNvbQo=</code></pre><h5 id="执行一下">执行一下</h5><pre><code>[root@master secret]# kubectl apply -f secret4.yaml </code></pre><h4 id="（3）查看一下">（3）查看一下</h4><pre><code>[root@master secret]# kubectl get secrets </code></pre><p><img src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5C%E8%99%9A%E6%8B%9F%E5%8C%96%5Ckubernetes%5Ck8s%E6%96%87%E6%A1%A3%5C14.assets%5Cimage-20200214104544899.png" alt="image-20200214104544899"></p><h2 id="如果来使用Secret资源">如果来使用Secret资源</h2><h3 id="1-以Volume挂载的方式">1. 以Volume挂载的方式</h3><h4 id="编写pod的yaml文件"><strong>编写pod的yaml文件</strong></h4><pre><code>[root@master secret]# vim pod.yaml apiVersion: v1kind: Podmetadata:  name: mypodspec:  containers:  - name: mypod    image: busybox    args:      - /bin/sh      - -c      - sleep 300000    volumeMounts:    - name: secret-test      mountPath: &quot;/etc/secret-test&quot;  #pod中的路径      readOnly: true                 #是否只读  volumes:  - name: secret-test    secret:      secretName: mysecret1</code></pre><p><strong>还可以自定义存放数据的文件名</strong></p><h4 id="执行一下-2">执行一下</h4><pre><code>[root@master secret]# kubectl apply -f pod.yaml </code></pre><h4 id="进入容器查看保存的数据">进入容器查看保存的数据</h4><pre><code>[root@master secret]# kubectl exec -it mypod /bin/sh/ # cd /etc/secret-test//etc/secret-test # lspasword   username</code></pre><pre><code>/etc/secret-test # cat username root/etc/secret-test # cat pasword 123.com</code></pre><h4 id="测试是否有只读权限">测试是否有只读权限</h4><pre><code>123.com/etc/secret-test # echo admin &gt; username/bin/sh: can't create username: Read-only file system</code></pre><h3 id="1-1-自定义存放数据的文件名的yaml文件">1.1 自定义存放数据的文件名的yaml文件</h3><pre><code>[root@master yaml]#  vim pod.yaml apiVersion: v1kind: Podmetadata:  name: mypodspec:  containers:  - name: mypod    image: busybox    args:      - /bin/sh      - -c      - sleep 300000    volumeMounts:    - name: secret-test      mountPath: &quot;/etc/secret-test&quot;  #pod中的路径      readOnly: true                 #是否只读  volumes:  - name: secret-test    secret:      secretName: mysecret1      items:      - key: username        path: my-group/my-username   #自定义的容器中的目录      - key: password        path: my-group/my-password   #自定义的容器中的目录</code></pre><h4 id="执行一下-3">执行一下</h4><pre><code>[root@master yaml]# kubectl apply -f pod.yaml</code></pre><h4 id="查看一下-4">查看一下</h4><pre><code>[root@master secret]# kubectl exec -it mypod /bin/sh//进入容器查看 # cat /etc/secret-test/my-group/my-password 123.com  # cat /etc/secret-test/my-group/my-username root</code></pre><h3 id="1-2-如果，现在将secret资源内保存的数据进行更新，请问，使用此数据的应用内，数据是是否也会更新">1.2 如果，现在将secret资源内保存的数据进行更新，请问，使用此数据的应用内，数据是是否也会更新?</h3><p><strong>会实时更新(这里引用数据，是以volumes挂 载使用数据的方式)。</strong></p><p><strong>更新mysecret1的数据:   password  —&gt;  admin   YWRtaW4K (base64)</strong></p><p><strong>可以通过edit 命令，直接修改。</strong></p><pre><code>[root@master secret]# kubectl edit secrets mysecret1</code></pre><p><img src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5C%E8%99%9A%E6%8B%9F%E5%8C%96%5Ckubernetes%5Ck8s%E6%96%87%E6%A1%A3%5C14.assets%5Cimage-20200217162834490.png" alt="image-20200217162834490"></p><h4 id="查看一下-5">查看一下</h4><pre><code>[root@master secret]# kubectl exec -it mypod /bin/sh//进入容器查看 # cat /etc/secret-test/my-group/my-password admin # cat /etc/secret-test/my-group/my-username root</code></pre><p><em><strong>数据已经成功更新了</strong></em></p><h3 id="2、以环境变量的方式">2、以环境变量的方式</h3><p><strong>编写pod的yaml文件</strong></p><pre><code>[root@master secret]# vim pod-env.yaml apiVersion: v1kind: Podmetadata:  name: mypod2spec:  containers:  - name: mypod    image: busybox    args:      - /bin/sh      - -c      - sleep 300000    env:      - name: SECRET_USERNAME        valueFrom:          secretKeyRef:            name: mysecret2            key: username      - name: SECRET_PASSWORD        valueFrom:          secretKeyRef:            name: mysecret2            key: password</code></pre><h4 id="执行一下-4">执行一下</h4><pre><code>[root@master secret]# kubectl apply -f pod-env.yaml </code></pre><h4 id="查看一下-6">查看一下</h4><pre><code>[root@master secret]# kubectl get pod</code></pre><p><img src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5C%E8%99%9A%E6%8B%9F%E5%8C%96%5Ckubernetes%5Ck8s%E6%96%87%E6%A1%A3%5C14.assets%5Cimage-20200214111931566.png" alt="image-20200214111931566"></p><h4 id="进入容器查看保存的数据-2">进入容器查看保存的数据</h4><pre><code>[root@master secret]# kubectl exec -it mypod2 /bin/sh/ # echo $SECRET_USERNAMEroot/ # echo $SECRET_PASSWORD123.com</code></pre><h3 id="2-1-更新sevret文件的内容">2.1 更新sevret文件的内容</h3><pre><code> [root@master yaml]# kubectl edit secrets mysecret2 //修改保存文件的内容</code></pre><p><img src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5C%E8%99%9A%E6%8B%9F%E5%8C%96%5Ckubernetes%5Ck8s%E6%96%87%E6%A1%A3%5C14.assets%5Cimage-20200217162834490.png" alt="image-20200217162834490"></p><h4 id="查看一下-7">查看一下</h4><pre><code>[root@master secret]# kubectl exec -it mypod2 /bin/sh/ # echo $SECRET_USERNAMEroot/ # echo $SECRET_PASSWORD123.com</code></pre><p><em><strong>等待了一定时间后，可以看到这个数据并没有没有改变</strong></em></p><h2 id="总结">总结</h2><p><strong>如果引用secret数据的应用， 要求会随着secret资源对象内保存的数据的更新，而实时更新，那么应该使用volumes挂载的方式引用资源因为用环境变量的方式引用不会实时更新数据。</strong></p><h1>二、ConfigMap</h1><p><strong>和Secret资源类似，不同之处在于，secret 资源保存的是敏感信息，而Configmap保存的是以明文方式存放的数据。</strong></p><blockquote><p><strong>username：adam</strong></p><p><strong>age：18</strong></p></blockquote><h2 id="创建的四种方式">创建的四种方式</h2><h3 id="1、通过-from-literal-文字的">1、通过-- from- literal(文字的):</h3><pre><code>[root@master yaml]# kubectl create configmap myconfigmap1 --from-literal=username=adam --from-literal=age=18</code></pre><h4 id="查看一下-8">查看一下</h4><pre><code>[root@master yaml]# kubectl get cm</code></pre><p><img src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5C%E8%99%9A%E6%8B%9F%E5%8C%96%5Ckubernetes%5Ck8s%E6%96%87%E6%A1%A3%5C14.assets%5Cimage-20200217103048235.png" alt="image-20200217103048235"></p><pre><code>[root@master yaml]# kubectl describe cm</code></pre><p><img src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5C%E8%99%9A%E6%8B%9F%E5%8C%96%5Ckubernetes%5Ck8s%E6%96%87%E6%A1%A3%5C14.assets%5Cimage-20200217103123130.png" alt="image-20200217103123130"></p><h3 id="2、通过–from-file-文件">2、通过–from-file (文件) :</h3><pre><code>[root@master yaml]# echo adam &gt; username[root@master yaml]# echo 18 &gt; age</code></pre><h4 id="创建">创建</h4><pre><code>[root@master yaml]# kubectl create configmap myconfigmap2 --from-file=username --from-file=age </code></pre><h4 id="查看一下-9">查看一下</h4><pre><code>[root@master yaml]# kubectl describe cm</code></pre><p><img src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5C%E8%99%9A%E6%8B%9F%E5%8C%96%5Ckubernetes%5Ck8s%E6%96%87%E6%A1%A3%5C14.assets%5Cimage-20200217103509006.png" alt="image-20200217103509006"></p><h3 id="3、通过–from-env-file">3、通过–from- env-file:</h3><pre><code>[root@master yaml]# vim env.txt username=adamage=18</code></pre><h4 id="创建-2">创建</h4><pre><code>[root@master yaml]# kubectl create configmap  myconfigmap3 --from-env-file=env.txt</code></pre><h4 id="查看一下-10">查看一下</h4><pre><code>[root@master configmap]# kubectl describe cm</code></pre><p><img src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5C%E8%99%9A%E6%8B%9F%E5%8C%96%5Ckubernetes%5Ck8s%E6%96%87%E6%A1%A3%5C14.assets%5Cimage-20200217165039190.png" alt="image-20200217165039190"></p><h3 id="4、通过yaml配置文件-2">4、通过yaml配置文件:</h3><pre><code>[root@master yaml]# vim configmap.yamlapiVersion: v1kind: ConfigMapmetadata:  name: myconfigmap4data:  username: 'adam'  age: '18'</code></pre><h4 id="创建-3">创建</h4><pre><code>[root@master yaml]# kubectl apply -f configmap.yaml </code></pre><h4 id="查看一下-11">查看一下</h4><pre><code>[root@master yaml]# kubectl describe cm</code></pre><p><img src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5C%E8%99%9A%E6%8B%9F%E5%8C%96%5Ckubernetes%5Ck8s%E6%96%87%E6%A1%A3%5C14.assets%5Cimage-20200217104428521.png" alt="image-20200217104428521"></p><h2 id="如何来使用configmap资源">如何来使用configmap资源</h2><h3 id="1-以Volume挂载的方式-2">1. 以Volume挂载的方式</h3><pre><code>[root@master yaml]# vim v-pod.yaml apiVersion: v1kind: Podmetadata:  name: pod1spec:  containers:  - name: mypod    image: busybox    args:      - /bin/sh      - -c      - sleep 300000    volumeMounts:    - name: cmp-test      mountPath: &quot;/etc/cmp-test&quot;      readOnly: true  volumes:  - name: cmp-test    configMap:      name: myconfigmap1</code></pre><h4 id="执行一下-5">执行一下</h4><pre><code>[root@master configmap]# kubectl apply -f v-pod.yaml </code></pre><h4 id="查看一下-12">查看一下</h4><pre><code>[root@master configmap]# kubectl exec -it pod1 /bin/sh//进入容器查看一下 # cat /etc/cmp-test/age 18/  # cat /etc/cmp-test/username adam/ </code></pre><h3 id="1-1-自定义存放数据的文件名的yaml文件-2">1.1 自定义存放数据的文件名的yaml文件</h3><pre><code>[root@master configmap]# vim v-pod2.yaml apiVersion: v1kind: Podmetadata:  name: pod3spec:  containers:  - name: mypod    image: busybox    args:      - /bin/sh      - -c      - sleep 300000    volumeMounts:    - name: cmp-test      mountPath: &quot;/etc/cmp-test&quot;      readOnly: true  volumes:  - name: cmp-test    configMap:      name: myconfigmap1      items:      - key: username        path: my-group/my-username   #自定义的容器中的目录      - key: age        path: my-group/my-age   #自定义的容器中的目录 </code></pre><h4 id="执行一下-6">执行一下</h4><pre><code>[root@master configmap]# kubectl apply -f v-pod2.yaml</code></pre><h4 id="查看一下-13">查看一下</h4><pre><code>[root@master configmap]# kubectl exec -it pod3 /bin/sh//进入容器查看# cat /etc/cmp-test/my-group/my-username adam/ # cat /etc/cmp-test/my-group/my-age 18/ </code></pre><h3 id="1-2-如果，现在将secret资源内保存的数据进行更新，请问，使用此数据的应用内，数据是是否也会更新-2">1.2 如果，现在将secret资源内保存的数据进行更新，请问，使用此数据的应用内，数据是是否也会更新?</h3><pre><code>[root@master configmap]# kubectl edit cm myconfigmap1</code></pre><p><img src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5C%E8%99%9A%E6%8B%9F%E5%8C%96%5Ckubernetes%5Ck8s%E6%96%87%E6%A1%A3%5C14.assets%5Cimage-20200217172107999.png" alt="image-20200217172107999"></p><h4 id="查看一下-14">查看一下</h4><pre><code>[root@master configmap]# kubectl exec -it pod3 /bin/sh//进入容器查看# cat /etc/cmp-test/my-group/my-username adam/ # cat /etc/cmp-test/my-group/my-age 10</code></pre><p><em><strong>可以看到更新成功</strong></em></p><h3 id="2-以环境变量的方式">2.以环境变量的方式</h3><pre><code>[root@master configmap]# vim e-pod.yaml apiVersion: v1kind: Podmetadata:  name: pod2spec:  containers:  - name: mypod    image: busybox    args:      - /bin/sh      - -c      - sleep 300000    env:      - name: CONFIGMAP_NAME        valueFrom:          configMapKeyRef:            name: myconfigmap2            key: username      - name: CONFIGMAP_AGE        valueFrom:          configMapKeyRef:            name: myconfigmap2            key: age</code></pre><h4 id="执行一下-7">执行一下</h4><pre><code>[root@master configmap]# kubectl apply -f e-pod.yaml </code></pre><h4 id="查看一下-15">查看一下</h4><pre><code>[root@master configmap]# kubectl exec -it pod2 /bin/sh//进入容器查看一下 # echo $CONFIGMAP_NAMEadam # echo $CONFIGMAP_AGE18</code></pre><h3 id="2-1-更新sevret文件的内容-2">2.1 更新sevret文件的内容</h3><pre><code>[root@master configmap]# kubectl edit cm myconfigmap2 //修改保存文件的内容</code></pre><p><img src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5C%E8%99%9A%E6%8B%9F%E5%8C%96%5Ckubernetes%5Ck8s%E6%96%87%E6%A1%A3%5C14.assets%5Cimage-20200217172701793.png" alt="image-20200217172701793"></p><h4 id="查看一下-16">查看一下</h4><pre><code>[root@master configmap]# kubectl exec -it pod2 /bin/sh//进入容器查看一下 # echo $CONFIGMAP_NAMEadam # echo $CONFIGMAP_AGE18</code></pre><p><em><strong>等待了一定时间后，可以看到这个数据并没有没有改变</strong></em></p><p><strong>可以看出这个configmap和secret的更新效果基本没有区别。</strong></p><h2 id="总结configmap、与secret资源有什么相同和不同之处。"><strong>总结configmap、与secret资源有什么相同和不同之处。</strong></h2><h3 id="Secret-与-ConfigMap-对比">Secret 与 ConfigMap 对比</h3><p><strong>相同点：</strong></p><blockquote><p><strong>key/value的形式</strong></p><p><strong>属于某个特定的namespace</strong></p><p><strong>可以导出到环境变量</strong></p><p><strong>可以通过目录/文件形式挂载</strong></p><p><strong>通过 volume 挂载的配置信息均可热更新</strong></p></blockquote><p><strong>不同点：</strong></p><blockquote><p><strong>Secret 可以被 ServerAccount 关联</strong></p><p><strong>Secret 可以存储 docker register 的鉴权信息，用在 ImagePullSecret 参数中，用于拉取私有仓库的镜像</strong></p><p><strong>Secret 支持 Base64 加密</strong></p><p><strong>Secret 分为 <a href="http://kubernetes.io/service-account-token%E3%80%81kubernetes.io/dockerconfigjson%E3%80%81Opaque" target="_blank" rel="noopener">kubernetes.io/service-account-token、kubernetes.io/dockerconfigjson、Opaque</a> 三种类型，而 Configmap 不区分类型</strong></p></blockquote><h2 id="总结以volumes挂载、和环境变量方式引用资源的相同和不同之处。">总结以volumes挂载、和环境变量方式引用资源的相同和不同之处。</h2><p><strong>volumes挂载(可根据更改数据更新)：引用自己创建的secret（密文）或configmap（明文），挂载到容器中指定的目录下。查看保存的文件时，根据自己所填路径和secret或configmap创建的文件，进行查看。</strong></p><p><strong>环境变量(不因更改数据更新)：引用自己创建的secret（密文）或configmap（明文），挂载到容器中指定的目录下。查看保存的文件时，根据自己环境变量，进行查看。</strong></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>11 k8s持久化存储应用</title>
      <link href="/posts/5849.html"/>
      <url>/posts/5849.html</url>
      
        <content type="html"><![CDATA[<h1>k8s存储: (持久化)</h1><p><strong>docker容器是有生命周期的。</strong></p><p><strong>volume</strong></p><h2 id="1-emptyDir（空目录）：-类似docker-数据持久化的-docer-manager-volume">**1.emptyDir（空目录）：**类似docker 数据持久化的:docer manager volume</h2><p><strong>使用场景:在同一 个Pod里，不同的容器，共享数据卷。</strong></p><p><strong>如果容器被删除，数据仍然存在，如果Pod被 删除，数据也会被删除。</strong></p><blockquote><p><strong>测试编写一个yaml文件</strong></p><pre><code>[root@master yaml]# vim emptyDir.yamlapiVersion: v1kind: Podmetadata:  name: producer-consumerspec:  containers:  - image: busybox    name: producer    volumeMounts:    - mountPath: /producer_dir      name: shared-volume    args:    - /bin/sh    - -c    - echo &quot;hello k8s&quot; &gt; /producer_dir/hello; sleep 30000  - image: busybox    name: consumer    volumeMounts:    - mountPath: /consumer_dir      name: shared-volume    args:    - /bin/sh    - -c    - cat /consumer_dir/hello; sleep 30000  volumes:  - name: shared-volume    emptyDir: {}</code></pre><p><strong>执行一下</strong></p><pre><code>[root@master yaml]# kubectl apply -f emptyDir.yaml </code></pre><p><strong>查看一下</strong></p><pre><code>[root@master yaml]# kubectl get pod  </code></pre><p>![image-20200205095431565](G:\四期\虚拟化\kubernetes\k8s文档\11 k8s的存储.assets\image-20200205095431565.png)</p><p><strong>查看日志</strong></p><pre><code>[root@master yaml]# kubectl logs  producer-consumer producer[root@master yaml]# kubectl logs  producer-consumer consumer</code></pre><p>![image-20200205095543780](G:\四期\虚拟化\kubernetes\k8s文档\11 k8s的存储.assets\image-20200205095543780.png)</p><p><strong>查看挂载的目录</strong></p><p><strong>node节点查看容器名，并通过容器名查看挂载的目录</strong></p><pre><code>[root@node01 shared-volume]# docker ps </code></pre><p>![image-20200205102007328](G:\四期\虚拟化\kubernetes\k8s文档\11 k8s的存储.assets\image-20200205102007328.png)</p><pre><code>[root@node01 shared-volume]# docker inspect k8s_consumer_producer-consumer_default_9ec83f9e-e58b-4bf8-8e16-85b0f83febf9_0</code></pre><p>![image-20200205102048470](G:\四期\虚拟化\kubernetes\k8s文档\11 k8s的存储.assets\image-20200205102048470.png)</p><p><strong>进入挂载目录查看一下</strong></p><p>![image-20200205102128953](G:\四期\虚拟化\kubernetes\k8s文档\11 k8s的存储.assets\image-20200205102128953.png)</p></blockquote><h2 id="2-hostPath-Volume：类似docker-数据持久化的-bind-mount">2.hostPath Volume：类似docker 数据持久化的:bind mount</h2><p><strong>如果Pod被删除，数据会保留，相比较emptyDir要好一点。不过一旦host崩溃，hostPath也无法访问 了。</strong></p><p><strong>docker或者k8s集群本身的存储会采用hostPath这种方式。</strong></p><h2 id="3-Persistent-Volume-PV-持久卷-提前做好的，数据持久化的数据存放目录。">3.Persistent Volume| PV(持久卷) 提前做好的，数据持久化的数据存放目录。</h2><h3 id="Psesistent-Volume-Claim-PVC-持久卷使用声明-申请"><strong>Psesistent Volume Claim| PVC( 持久卷使用声明|申请)</strong></h3><h3 id="（1）基于nfs服务来做的PV和pvc"><strong>（1）基于nfs服务来做的PV和pvc</strong></h3><p><strong>下载nfs所需安装包</strong></p><pre><code>[root@node02 ~]# yum -y install nfs-utils  rpcbind</code></pre><p><strong>创建共享目录</strong></p><pre><code>[root@master ~]# mkdir /nfsdata</code></pre><p><strong>创建共享目录的权限</strong></p><pre><code>[root@master ~]# vim /etc/exports/nfsdata *(rw,sync,no_root_squash)</code></pre><p><strong>开启nfs和rpcbind</strong></p><pre><code>[root@master ~]# systemctl start nfs-server.service [root@master ~]# systemctl start rpcbind</code></pre><p><strong>测试一下</strong></p><pre><code>[root@master ~]# showmount -e</code></pre><p>![image-20200205105654925](G:\四期\虚拟化\kubernetes\k8s文档\11 k8s的存储.assets\image-20200205105654925.png)</p><h4 id="1-创建nfs-pv的yaml文件"><strong>&lt;1&gt;创建nfs-pv的yaml文件</strong></h4><pre><code>[root@master yaml]# cd yaml/[root@master yaml]# vim nfs-pv.yamlapiVersion: v1kind: PersistentVolumemetadata:  name: test-pvspec:  capacity:   #pv容量的大小    storage: 1Gi  accessModes:  #访问pv的模式    - ReadWriteOnce #能以读-写mount到单个的节点  persistentVolumeReclaimPolicy: Recycle  storageClassName: nfs  nfs:    path: /nfsdata/pv1    server: 192.168.1.21</code></pre><blockquote><pre><code>  accessModes:(PV支持的访问模式)    - ReadWriteOnce: 能以读-写mount到单个的节点    - ReadWriteMany: 能以读-写mount到多个的节点。- ReadOnlyMnce:  能以只读的方式mount到多个节点。</code></pre></blockquote><blockquote><pre><code>persistentVolumeReclaimPolicy : (PV存储空间的回收策略是什么)Recycle: 自动清除数据。Retain: 需要管理员手动回收。Delete： 云存储专用。</code></pre></blockquote><h4 id="2-执行一下"><strong>&lt;2&gt;执行一下</strong></h4><pre><code>[root@master yaml]# kubectl apply -f nfs-pv.yaml </code></pre><h4 id="3-查看一下">&lt;3&gt;查看一下</h4><pre><code>[root@master yaml]# kubectl get pv</code></pre><p>![image-20200205111307317](G:\四期\虚拟化\kubernetes\k8s文档\11 k8s的存储.assets\image-20200205111307317.png)</p><h4 id="1-创建nfs-pvc的yaml文件"><strong>&lt;1&gt;创建nfs-pvc的yaml文件</strong></h4><pre><code>[root@master yaml]# vim nfs-pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata:  name: test-pvcspec:  accessModes:    - ReadWriteOnce  resources:    requests:      storage: 1Gi  storageClassName: nfs</code></pre><h4 id="2-执行一下-2"><strong>&lt;2&gt;执行一下</strong></h4><pre><code>[root@master yaml]# kubectl apply -f nfs-pvc.yaml </code></pre><h4 id="3-查看一下-2">&lt;3&gt;查看一下</h4><pre><code>[root@master yaml]# kubectl get pvc</code></pre><p>![image-20200205113407860](G:\四期\虚拟化\kubernetes\k8s文档\11 k8s的存储.assets\image-20200205113407860.png)</p><pre><code>[root@master yaml]# kubectl get pv</code></pre><p>![image-20200205113512580](G:\四期\虚拟化\kubernetes\k8s文档\11 k8s的存储.assets\image-20200205113512580.png)</p><h3 id="（2）创建一个pod资源">（2）创建一个pod资源</h3><pre><code>[root@master yaml]# vim pod.yamlkind: PodapiVersion: v1metadata:  name: test-podspec:  containers:    - name: pod1      image: busybox      args:      - /bin/sh      - -c      - sleep 30000      volumeMounts:      - mountPath: &quot;/mydata&quot;        name: mydata  volumes:    - name: mydata      persistentVolumeClaim:        claimName: test-pvc</code></pre><h4 id="1-执行一下">&lt;1&gt; 执行一下</h4><pre><code>[root@master yaml]# kubectl apply -f pod.yaml </code></pre><h4 id="2-查看一下">&lt;2&gt;查看一下</h4><pre><code>[root@master yaml]# kubectl get pod -o wide</code></pre><p>![image-20200207100212328](G:\四期\虚拟化\kubernetes\k8s文档\11 k8s的存储.assets\image-20200207100212328.png)</p><p><strong>可以看到现在没有开启成功</strong></p><h5 id="查看一下test-pod的信息看看是哪里的问题">查看一下test-pod的信息看看是哪里的问题</h5><pre><code>[root@master yaml]# kubectl describe pod test-pod </code></pre><p>![image-20200207123950227](G:\四期\虚拟化\kubernetes\k8s文档\11 k8s的存储.assets\image-20200207123950227.png)</p><h5 id="那是因为pv的本地挂载目录没有创建好">那是因为pv的本地挂载目录没有创建好</h5><pre><code>[root@master yaml]# mkdir /nfsdata/pv1///要和nfs-pv.yaml的名字一样</code></pre><h5 id="重新创建一下pod">重新创建一下pod</h5><pre><code>[root@master yaml]# kubectl delete -f pod.yaml [root@master yaml]# kubectl apply -f pod.yaml [root@master yaml]# kubectl get pod -o wide</code></pre><p>![image-20200207102822785](G:\四期\虚拟化\kubernetes\k8s文档\11 k8s的存储.assets\image-20200207102822785.png)</p><h3 id="（3）test-pod创建hello创建文件并添加内容">（3）test-pod创建hello创建文件并添加内容</h3><pre><code>[root@master yaml]# kubectl exec test-pod touch /mydata/hello</code></pre><p><strong>进入容器</strong></p><pre><code>[root@master yaml]# kubectl exec -it test-pod  /bin/sh/ # echo 123 &gt; /mydata/hello/ # exit</code></pre><p><strong>挂载目录查看一下</strong></p><pre><code>[root@master yaml]# cat  /nfsdata/pv1/hello </code></pre><p>![image-20200207104239153](G:\四期\虚拟化\kubernetes\k8s文档\11 k8s的存储.assets\image-20200207104239153.png)</p><p><strong>和刚刚的一样</strong></p><h3 id="（4）测试回收策略">（4）测试回收策略</h3><h4 id="删除pod和pvc，pv">删除pod和pvc，pv</h4><pre><code>[root@master yaml]# kubectl delete pod test-pod [root@master yaml]# kubectl delete pvc test-pvc [root@master yaml]# kubectl delete pv test-pv </code></pre><h4 id="查看一下">查看一下</h4><pre><code>[root@master yaml]# kubectl get pv</code></pre><p>![image-20200207104454636](G:\四期\虚拟化\kubernetes\k8s文档\11 k8s的存储.assets\image-20200207104454636.png)</p><pre><code>[root@master yaml]# cat  /nfsdata/pv1/hello</code></pre><p>![image-20200207104520048](G:\四期\虚拟化\kubernetes\k8s文档\11 k8s的存储.assets\image-20200207104520048.png)</p><p><em><strong>文件已被回收</strong></em></p><h3 id="（5）修改pv的回收策略为手动">（5）修改pv的回收策略为手动</h3><h4 id="修改">修改</h4><pre><code>[root@master yaml]# vim nfs-pv.yaml apiVersion: v1kind: PersistentVolumemetadata:  name: test-pvspec :  capacity :    storage: 1Gi  accessModes:    - ReadWriteOnce  persistentVolumeReclaimPolicy: Retain   #修改  storageClassName: nfs  nfs:    path: /nfsdata/pv1    server: 192.168.1.21</code></pre><h4 id="执行一下">执行一下</h4><pre><code>[root@master yaml]# kubectl apply -f nfs-pv.yaml </code></pre><h4 id="创建pod">创建pod</h4><pre><code>[root@master yaml]# kubectl apply -f pod.yaml </code></pre><h4 id="查看一下-2">查看一下</h4><p>![image-20200207105203009](G:\四期\虚拟化\kubernetes\k8s文档\11 k8s的存储.assets\image-20200207105203009.png)</p><pre><code>[root@master yaml]# kubectl describe pod test-pod </code></pre><p>![image-20200207105248025](G:\四期\虚拟化\kubernetes\k8s文档\11 k8s的存储.assets\image-20200207105248025.png)</p><h4 id="创建pvc">创建pvc</h4><pre><code>[root@master yaml]# kubectl apply -f nfs-pvc.yaml </code></pre><h4 id="查看一下pod">查看一下pod</h4><pre><code>[root@master yaml]# kubectl get pod</code></pre><p>![image-20200207105402354](G:\四期\虚拟化\kubernetes\k8s文档\11 k8s的存储.assets\image-20200207105402354.png)</p><h3 id="（6）test-pod创建hello创建文件并添加内容">（6）test-pod创建hello创建文件并添加内容</h3><pre><code>[root@master yaml]# kubectl exec test-pod touch /mydata/k8s</code></pre><h4 id="查看一下挂载目录">查看一下挂载目录</h4><pre><code>[root@master yaml]# ls /nfsdata/pv1/</code></pre><p>![image-20200207105618318](G:\四期\虚拟化\kubernetes\k8s文档\11 k8s的存储.assets\image-20200207105618318.png)</p><h4 id="删除pod和pvc，pv，再次查看挂载目录">删除pod和pvc，pv，再次查看挂载目录</h4><pre><code>[root@master yaml]# kubectl delete pod test-pod [root@master yaml]# kubectl delete pvc test-pvc[root@master yaml]# kubectl delete pv test-pv </code></pre><h4 id="查看挂载目录">查看挂载目录</h4><pre><code>[root@master yaml]# ls /nfsdata/pv1/</code></pre><p>![image-20200207105757641](G:\四期\虚拟化\kubernetes\k8s文档\11 k8s的存储.assets\image-20200207105757641.png)</p><p><em><strong>内容还在</strong></em></p><h2 id="4-mysql对数据持久化的应用">4.mysql对数据持久化的应用</h2><p><strong>最小化安装系统需要</strong></p><pre><code>yum -y install mariadb</code></pre><h2 id="（1）通过之前的yaml文件，创建pv和pvc">（1）通过之前的yaml文件，创建pv和pvc</h2><pre><code>[root@master yaml]# kubectl apply -f  nfs-pv.yaml [root@master yaml]# kubectl apply -f  nfs-pvc.yaml </code></pre><h3 id="查看一下-3">查看一下</h3><pre><code>[root@master yaml]# kubectl get pv</code></pre><p>![image-20200207110132199](G:\四期\虚拟化\kubernetes\k8s文档\11 k8s的存储.assets\image-20200207110132199.png)</p><pre><code>[root@master yaml]# kubectl get pvc</code></pre><p>![image-20200207110140002](G:\四期\虚拟化\kubernetes\k8s文档\11 k8s的存储.assets\image-20200207110140002.png)</p><h2 id="（2）编写一个mysql的yaml文件">（2）编写一个mysql的yaml文件</h2><pre><code>[root@master yaml]# vim mysql.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata:  name: test-mysqlspec:  selector:    matchLabels:    #支持等值的标签      app: mysqlapiVersion: extensions/v1beta1kind: Deploymentmetadata:  name: test-mysqlspec:  selector:    matchLabels:      app: mysql  template:    metadata:      labels:        app: mysql    spec:      containers:      - image: mysql:5.6        name: mysql        env:        - name: MYSQL_ROOT_PASSWORD          value: 123.com        volumeMounts:        - name: mysql-storage          mountPath: /var/lib/mysql      volumes:      - name: mysql-storage        persistentVolumeClaim:          claimName: test-pvc</code></pre><h3 id="执行一下-2">执行一下</h3><pre><code>[root@master yaml]# kubectl apply -f mysql.yaml </code></pre><h3 id="查看一下-4">查看一下</h3><pre><code>[root@master yaml]# kubectl get pod</code></pre><p>![image-20200207110741833](G:\四期\虚拟化\kubernetes\k8s文档\11 k8s的存储.assets\image-20200207110741833.png)</p><h2 id="（3）进入mysql容器">（3）进入mysql容器</h2><pre><code>[root@master yaml]# kubectl exec -it test-mysql-569f8df4db-rkpwm  -- mysql -u root -p123.com </code></pre><h3 id="创建数据库">创建数据库</h3><pre><code>mysql&gt; create database yun33;</code></pre><h3 id="切换数据库">切换数据库</h3><pre><code>mysql&gt; use yun33;</code></pre><h3 id="创建表">创建表</h3><pre><code>mysql&gt; create table my_id( id int(4))；</code></pre><h3 id="在表中插入数据">在表中插入数据</h3><pre><code>mysql&gt; insert my_id values(9527);</code></pre><h3 id="查看表">查看表</h3><pre><code>mysql&gt; select * from my_id;</code></pre><p>![image-20200207113808540](G:\四期\虚拟化\kubernetes\k8s文档\11 k8s的存储.assets\image-20200207113808540.png)</p><h2 id="（4）查看本地的挂载目录">（4）查看本地的挂载目录</h2><pre><code>[root@master yaml]# ls /nfsdata/pv1/</code></pre><p>![image-20200207113909796](G:\四期\虚拟化\kubernetes\k8s文档\11 k8s的存储.assets\image-20200207113909796.png)</p><h3 id="查看一下pod-2">查看一下pod</h3><pre><code>[root@master yaml]# kubectl get pod -o wide -w</code></pre><p>![image-20200207114050117](G:\四期\虚拟化\kubernetes\k8s文档\11 k8s的存储.assets\image-20200207114050117.png)</p><h3 id="挂起node01">挂起node01</h3><p>![image-20200207114607518](G:\四期\虚拟化\kubernetes\k8s文档\11 k8s的存储.assets\image-20200207114607518.png)</p><h2 id="（5）查看node02上面数据是否和刚才一样（验证数据的一致性）">（5）查看node02上面数据是否和刚才一样（验证数据的一致性）</h2><h3 id="进入数据库">进入数据库</h3><pre><code>[root@master yaml]#  kubectl exec -it test-mysql-569f8df4db-nsdnz  -- mysql -u root -p123.com </code></pre><h3 id="查看数据库">查看数据库</h3><pre><code>mysql&gt; show databases;</code></pre><p>![image-20200207115253123](G:\四期\虚拟化\kubernetes\k8s文档\11 k8s的存储.assets\image-20200207115253123.png)</p><h3 id="查看表-2">查看表</h3><pre><code>mysql&gt; show tables;</code></pre><p>![image-20200207115352727](G:\四期\虚拟化\kubernetes\k8s文档\11 k8s的存储.assets\image-20200207115352727.png)</p><pre><code>mysql&gt; select * from my_id;</code></pre><p>![image-20200207113808540](G:\四期\虚拟化\kubernetes\k8s文档\11 k8s的存储.assets\image-20200207113808540.png)</p><p><em><strong>可以看到数据还在</strong></em></p><h2 id="5-总结">5. 总结</h2><h4 id="PV的访问控制类型"><strong>PV的访问控制类型</strong></h4><p><strong>accessModes:(PV支持的访问模式)</strong></p><ul><li><strong>ReadWriteOnce: 能以读-写mount到单个的节点</strong></li><li><strong>ReadWriteMany: 能以读-写mount到多个的节点。</strong></li><li><strong>ReadOnlyOnce: 能以只读的方式mount到单个节点。</strong></li></ul><h4 id="PV的空间回收策略"><strong>PV的空间回收策略</strong></h4><p><strong>persistentVolumeReclaimPolicy : (PV存储空间的回收策略是什么)</strong></p><p>​    <strong>Recycle: 自动清除数据。</strong></p><p>​    <strong>Retain: 需要管理员手动回收。</strong></p><p>​    <strong>Delete： 云存储专用。</strong></p><h4 id="PV和PVC相互关联"><strong>PV和PVC相互关联</strong></h4><p><strong>是通过accessModes和storageClassName模块关联的</strong></p><h4 id="Pod不断的重启">Pod不断的重启:</h4><p><strong>1、swap,没有关闭，导致集群运行不正常。</strong><br><strong>2、内存不足，运行服务也会重后。</strong></p><p>kubectl describe<br>kubectl logs<br>/var/ log/messages<br>查看该节点的kubelet的日志。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>10 复习 </title>
      <link href="/posts/1b18.html"/>
      <url>/posts/1b18.html</url>
      
        <content type="html"><![CDATA[<h1>虚拟化</h1><p><strong>云计算的分类</strong>：</p><blockquote><p><strong>基础及服务：laas</strong><br><strong>平台及服务：paas</strong><br><strong>软件及服务：saas</strong></p></blockquote><p><strong>docker虚拟化的底层原理:</strong> Namespace + Cgroup</p><p><strong>Namespace六项隔离:</strong></p><blockquote><p><strong>IPC: 共享内存,消息列队</strong><br><strong>MNT: 挂载点 文件系统</strong><br><strong>NET: 网络栈</strong><br><strong>PID: 进程编号</strong><br><strong>USER: 用户 组</strong><br><strong>UTS: 主机名 域名</strong><br><strong>namespace 六项隔离 实现了容器与宿主机 容器与容器之间的隔离</strong></p></blockquote><p><strong>Cgroup 四项作用：</strong></p><blockquote><p>**1） 资源的限制：**cgroup可以对进程组使用的资源总额进行限制<br>**2） 优先级分配：**通过分配的cpu时间片数量以及硬盘IO带宽的大小，实际上相当于控制了进程运行的优先级别<br><strong>3） 资源统计：</strong> group可以统计系统资源使用量，比如gpu使用时间，内存使用量等，用于按量计费。同时还支持挂起动能，也就是说通过cgroup把所有 资源限制起来,对资源都不能使用，注意着并不是说我们的程序不能使用了,知识不能使用资源，处于等待状态。<br>**4） 进程控制：**可以对进程组执行挂起、恢复等操作。</p></blockquote><p><strong>镜像是容器运行的核心，容器是镜像运行的后的实例。</strong></p><p><strong>DockerHub| registry  ----&gt;  pull</strong><br><strong>image :     save &gt;   |  load &lt;</strong><br><strong>run    ----&gt;  Container    ----&gt;   commit</strong>*<br><strong>Dockerfile</strong></p><p><strong>Docker 三剑客。</strong></p><blockquote><p><strong>docker  machine :自动化部署多台dockerHost 。</strong></p><p>​        <strong>Docker-compose: 它可以同时控制多个容器。</strong></p><p>​        <strong>yaml。</strong></p><p><strong>Docker Swarm：</strong></p><p>​        <strong>从单个的服务向集群的形势发展。</strong></p><p>​         <strong>高可用、高性能、高并发 ：为了防止单点故障。</strong></p><p>​         <strong>Service：服务  ----&gt; 包括运行什么服务，需要多个                          rep1icas（副本）, 外网如何访问。</strong></p></blockquote><h3 id="k8s"><strong>k8s</strong></h3><p>关闭防火墙、禁用selinux、修改主机名并加入域名解析、关闭swap 、时间同步、免密登录、打开iptables桥接</p><blockquote><p><strong>对硬件的基本要求： CPU：2核   MEM：2G</strong></p><p><strong>主机名：master node01 node02</strong></p><p><strong>时间必须同步</strong></p></blockquote><p><strong>kubctl：k8s客户端      kubeadm：工具  kubelet：客户端代理</strong></p><p><strong>组件：</strong></p><p>​<strong>三层网络： DockerHost  &gt;   Pod  &gt; Service</strong></p><p>​        <strong>Deployment:        Service:</strong></p><h3 id="master组件">**master组件: **</h3><p><strong>kube- api( application interface) k8s的前端接口</strong></p><p>**Scheduler[集群分发调度器]**负责决定将Pod放在哪个Node上运行。在调度时，会充分考虑集群的拓扑结构，当前各个节点的负载情况，以及应对高可用、性能、数据亲和性和需求。</p><p><strong>Controller Manager[内部管理控制中心]</strong>：负责管理集群的各种资源，保证资源处于预期的状态。它由多种Controller组成，包括Replication Controller、Endpoints Controller、Namespace Controller、Serviceaccounts Controller等。</p><p>**Etcd：**负责保存k8s集群的配置信息和各种资源的状态信息。当数据发生变化时，etcd会快速的通知k8s相关组件。<a href>（第三方组件）它有可替换方案。Consul、zookeeper</a></p><p>**Flanner：**是k8s集群网络，可以保证Pod的跨主机通信。也有替换方案。</p><h3 id="Node组件：">Node组件：</h3><p><strong>Kubelet[节点上的Pod管家]</strong>：它是Node的agent(代理)，当Scheduler确定某 个Node上运行Pod之后，会将Pod的具体配置信息发送给该节点的kubelet,kubelet会根据这些信息创建和运行容器，并向Master报告运行状态。</p><p>**kube-proxy[负载均衡、路由转发]:**负责将访问service的TCP/UDP数据流转发到后端的容器。如果有多个副本，kube-proxy会实现负载均衡。</p><h3 id="yaml文件的一级字段">yaml文件的一级字段:</h3><p>​<strong>VERSION:</strong><br>​<strong>KIND:</strong><br>​<strong>METADATA:</strong><br>​<strong>SPEC :</strong></p><pre><code>[root@master ~]# vim web.yamlkind: Deployment  #资源对象是控制器apiVersion: extensions/v1beta1   #api的版本metadata:      #描述kind（资源类型）  name: web   #定义控制器名称  namespace：  #名称空间spec:  replicas: 2   #副本数量  template:     #模板    metadata:          labels:   #标签        app: web_server    spec:      containers:   #指定容器      - name: nginx  #容器名称        image: nginx   #使用的镜像</code></pre><p>​<strong>Deployment（控制器)：</strong></p><p>​**ReplicationController：**用来确保由其管控的Pod对象副本数量，能够满足用户期望，多则删除，少则通过模本创建</p><p>​**RS（RpelicaSet）:**RS也是用于保证与label selector匹配的pod数量维持在期望状态</p><p>​<strong>Service：</strong></p><p>​<strong>type：默认Cluster IP</strong></p><p>​<strong>NodePort：  30000-32767</strong></p><p>​<strong>Deployment和Service关联：标签和标签选择器</strong></p><p>​<strong>Namespace：</strong></p><p>​<strong>Pod：最小单位</strong></p><p>​<strong>镜像的下载策略：</strong></p><blockquote><p>​**Always：**镜像标签为“laster”或镜像不存在时，总是从指定的仓库中获取镜像。</p><p>​**IfNotPresent：**仅当本地镜像不存在时才从目标仓库下载。</p><p>​**Never：**禁止从仓库中下载镜像，即只使用本地镜像。</p></blockquote><p>​<strong>默认的标签 为latest：always</strong></p><p>​<strong>Pod的重启策略：</strong></p><blockquote><p>​<strong>Always：</strong>（默认情况下使用）但凡Pod对象终止就将其重启；<br>​**OnFailure：**仅在Pod对象出现错误时才将其重启；<br>​**Never：**从不重启；</p></blockquote><p>​<strong>Pod的健康检查:</strong><br>​Liveness:   探测失败重启pod<br>​Readiness: 探测失败将pod设置为不可用<br>kubelet：控制pod</p><p>DaemonSet :会在每一个节点都会运行，并且只运行一个Pod</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>09 Job资源对象</title>
      <link href="/posts/e9be.html"/>
      <url>/posts/e9be.html</url>
      
        <content type="html"><![CDATA[<h1>Job资源对象</h1><blockquote><p>**服务类的Pod容器：**RC、RS、DS、Deployment</p><p>**工作类的Pod容器：**Job—&gt;执行一次，或者批量执行处理程序，完成之后退出容器。</p></blockquote><p><strong>注意： 如果容器内执行任务有误，会根据容器的重启策略操作容器，不过这里</strong><br><strong>的容器重启策略只能是: Never和 OnFailure。</strong></p><h1>概念</h1><p><strong>在有些场景下，是想要运行一些容器执行某种特定的任务，任务一旦执行完成，容器也就没有存在的必要了。在这种场景下，创建pod就显得不那么合适。于是就是了Job，Job指的就是那些一次性任务。通过Job运行一个容器，当其任务执行完以后，就自动退出，集群也不再重新将其唤醒。</strong></p><p><strong>从程序的运行形态上来区分，可以将Pod分为两类：长时运行服务（jboss、mysql等）和一次性任务（数据计算、测试）。RC创建的Pod都是长时运行的服务，Job多用于执行一次性任务、批处理工作等，执行完成后便会停止（status.phase变为Succeeded）。</strong></p><h1>一、kubernetes支持以下几种job</h1><blockquote><ul><li><strong>非并行job：通常创建一个pod直至其成功结束。</strong></li><li><strong>固定结束次数的job：设置spec.completions,创建多个pod，直到.spec.completions个pod成功结束。</strong></li><li><strong>带有工作队列的并行job：设置.spec.Parallelism但不设置.spec.completions,当所有pod结束并且至少一个成功时，job就认为是成功。</strong></li></ul></blockquote><h2 id="Job-Controller">Job Controller</h2><p><strong>Job Controller负责根据Job Spec创建pod，并持续监控pod的状态，直至其成功结束，如果失败，则根据restartPolicy（只支持OnFailure和Never，不支持Always）决定是否创建新的pod再次重试任务。</strong></p><h2 id="例子"><strong>例子</strong></h2><h3 id="（1）编写一个job的yaml文件">（1）编写一个job的yaml文件</h3><pre><code>[root@master yaml]# vim jop.yamlkind: JobapiVersion: batch/v1metadata:  name: test-jobspec:  template:    metadata:      name: test-job    spec:      containers:      - name: hello        image: busybox        command: [&quot;echo&quot;,&quot;hello k8s job!&quot;]      restartPolicy: Never</code></pre><h3 id="（2）执行一下">（2）执行一下</h3><pre><code>[root@master yaml]# kubectl apply -f jop.yaml </code></pre><h3 id="（3）查看一下">（3）查看一下</h3><pre><code>[root@master yaml]# kubectl get pod</code></pre><p>![image-20200115090831524](G:\四期\虚拟化\kubernetes\k8s文档\09 Jop资源对象.assets\image-20200115090831524.png)</p><h4 id="查看日志">查看日志</h4><pre><code>[root@master yaml]# kubectl logs test-job-gs45w </code></pre><p>![image-20200115091213349](G:\四期\虚拟化\kubernetes\k8s文档\09 Jop资源对象.assets\image-20200115091213349.png)</p><p><strong>我们可以看到job与其他资源对象不同，仅执行一次性任务，默认pod借宿运行后job即结束，状态为Completed。</strong></p><h3 id="（4）修改一下jop的yaml文件，把echo命令换成乱码">（4）修改一下jop的yaml文件，把echo命令换成乱码</h3><pre><code>[root@master yaml]# vim jop.yamlkind: JobapiVersion: batch/v1metadata:  name: test-jobspec:  template:    metadata:      name: test-job    spec:      containers:      - name: hello        image: busybox        command: [&quot;asdasxsddwefew&quot;,&quot;hello k8s job!&quot;] #修改      restartPolicy: Never</code></pre><h3 id="（5）先删除之前的pod">（5）先删除之前的pod</h3><pre><code>[root@master yaml]# kubectl delete jobs.batch test-job  </code></pre><h3 id="（6）执行一下">（6）执行一下</h3><pre><code>[root@master yaml]# kubectl apply -f jop.yaml </code></pre><h3 id="（7）查看一下">（7）查看一下</h3><pre><code>[root@master yaml]# kubectl get pod -w</code></pre><p>![image-20200115091647925](G:\四期\虚拟化\kubernetes\k8s文档\09 Jop资源对象.assets\image-20200115091647925.png)</p><p><em><strong>它会一直创建pod直到完成命令。</strong></em></p><h3 id="（8）修改一下jop的yaml文件，修改重启策略">（8）修改一下jop的yaml文件，修改重启策略</h3><pre><code>[root@master yaml]# vim jop.yaml kind: JobapiVersion: batch/v1metadata:  name: test-jobspec:  template:    metadata:      name: test-job    spec:      containers:      - name: hello        image: busybox        command: [&quot;asdasxsddwefew&quot;,&quot;hello k8s job!&quot;]      restartPolicy: OnFailure</code></pre><h3 id="（9）先删除之前的pod">（9）先删除之前的pod</h3><pre><code>[root@master yaml]# kubectl delete jobs.batch test-job </code></pre><h3 id="（10）执行一下">（10）执行一下</h3><pre><code>[root@master yaml]# kubectl apply -f jop.yaml </code></pre><h3 id="（11）查看一下">（11）查看一下</h3><pre><code>[root@master yaml]# kubectl get pod -w</code></pre><p>![image-20200115092801882](G:\四期\虚拟化\kubernetes\k8s文档\09 Job资源对象.assets\image-20200115092801882.png)</p><p><em><strong>它会一直重启pod完成命令，直到重启到一定次数就会删除job。</strong></em></p><h1>二、提高Job的执行效率</h1><h2 id="1-我们可以在Job-spec字段下加上parallelism选项。表示同时运行多少个Pod执行任务。">1. 我们可以在Job.spec字段下加上<a href>parallelism</a>选项。表示同时运行多少个Pod执行任务。</h2><hr><h3 id="（1）编写一个job的yaml文件-2">（1）编写一个job的yaml文件</h3><pre><code>[root@master yaml]# vim jop.yamlkind: JobapiVersion: batch/v1metadata:  name: test-jobspec:  parallelism: 2    #同时启用几个pod  template:    metadata:      name: test-job    spec:      containers:      - name: hello        image: busybox        command: [&quot;echo&quot;,&quot;hello k8s job!&quot;]      restartPolicy: OnFailure</code></pre><h3 id="（3）执行一下">（3）执行一下</h3><pre><code>[root@master yaml]# kubectl apply -f jop.yaml </code></pre><h3 id="（4）查看一下">（4）查看一下</h3><pre><code>[root@master yaml]# kubectl get pod</code></pre><p>![image-20200115093854913](G:\四期\虚拟化\kubernetes\k8s文档\09 Job资源对象.assets\image-20200115093854913.png)</p><h4 id="查看日志-2">查看日志</h4><p>![image-20200115094002236](G:\四期\虚拟化\kubernetes\k8s文档\09 Job资源对象.assets\image-20200115094002236.png)</p><h2 id="2-我们可以在Job-spec字段下加上complations选项。表示总共需要完成Pod的数量">2. 我们可以在Job.spec字段下加上complations选项。表示总共需要完成Pod的数量</h2><h3 id="（1）编写一个job的yaml文件-3">（1）编写一个job的yaml文件</h3><pre><code>[root@master yaml]# vim jop.yamlkind: JobapiVersion: batch/v1metadata:  name: test-jobspec:  complations: 8            #运行pod的总数量8个  parallelism: 2            #同时运行2个pod  template:    metadata:      name: test-job    spec:      containers:      - name: hello        image: busybox        command: [&quot;echo&quot;,&quot;hello k8s job!&quot;]      restartPolicy: OnFailure</code></pre><p><strong>job 字段解释：</strong></p><blockquote><p><strong>标志Job结束需要成功运行的Pod个数，默认为1</strong><br><strong>parallelism：标志并行运行的Pod的个数，默认为1</strong><br><strong>activeDeadlineSeconds：标志失败Pod的重试最大时间，超过这个时间不会继续重试.</strong></p></blockquote><h3 id="（2）先删除之前的pod">（2）先删除之前的pod</h3><pre><code>[root@master yaml]# kubectl delete jobs.batch test-job </code></pre><h3 id="（3）执行一下-2">（3）执行一下</h3><pre><code>[root@master yaml]# kubectl apply -f jop.yaml </code></pre><h3 id="（4）查看一下-2">（4）查看一下</h3><pre><code>[root@master yaml]# kubectl get pod</code></pre><p>![image-20200115094519494](G:\四期\虚拟化\kubernetes\k8s文档\09 Job资源对象.assets\image-20200115094519494.png)</p><p><strong>可以看到pod是两个两个的启动的。</strong></p><h2 id="3-如何定时执行Job">3. 如何定时执行Job</h2><h3 id="（1）编写一个cronjob的yaml文件">（1）编写一个cronjob的yaml文件</h3><pre><code>[root@master yaml]# vim cronjop.yamlkind: CronJobapiVersion: batch/v1beta1metadata:  name: hellospec:  schedule: &quot;*/1 * * * *&quot; #限定时间  jobTemplate:    spec:      template:        spec:          containers:          - name: hello            image: busybox            command: [&quot;echo&quot;,&quot;hello&quot;,&quot;cronjob&quot;]          restartPolicy: OnFailure</code></pre><h3 id="（2）先删除之前的pod-2">（2）先删除之前的pod</h3><pre><code>[root@master yaml]# kubectl delete jobs.batch test-job </code></pre><h3 id="（3）执行一下-3">（3）执行一下</h3><pre><code>[root@master yaml]# kubectl apply -f jop.yaml </code></pre><h3 id="（4）查看一下-3">（4）查看一下</h3><pre><code>[root@master yaml]# kubectl get pod</code></pre><p>![image-20200115095857428](G:\四期\虚拟化\kubernetes\k8s文档\09 Job资源对象.assets\image-20200115095857428.png)</p><pre><code>[root@master yaml]# kubectl get cronjobs.batch </code></pre><p>![image-20200115095920740](G:\四期\虚拟化\kubernetes\k8s文档\09 Job资源对象.assets\image-20200115095920740.png)</p><p><strong>此时查看Pod的状态，会发现，每分钟都会运行一个新的Pod来执行命令规定的任</strong><br><strong>务。</strong></p><h2 id="练习：规定2020-1-15-10-5分运行上面的crontab任务。">练习：规定2020.1.15.10.5分运行上面的crontab任务。</h2><h3 id="（1）编写一个cronjob的yaml文件-2">（1）编写一个cronjob的yaml文件</h3><pre><code>[root@master yaml]# vim cronjop.yamlkind: CronJobapiVersion: batch/v1beta1metadata:  name: hellospec:  schedule: &quot;5 10 15 1 *&quot; #限定时间  jobTemplate:    spec:      template:        spec:          containers:          - name: hello            image: busybox            command: [&quot;echo&quot;,&quot;hello&quot;,&quot;cronjob&quot;]          restartPolicy: OnFailure</code></pre><h3 id="（2）先删除之前的pod-3">（2）先删除之前的pod</h3><pre><code>[root@master yaml]# kubectl delete cronjobs.batch hello </code></pre><h3 id="（3）执行一下-4">（3）执行一下</h3><pre><code>[root@master yaml]# kubectl apply -f jop.yaml </code></pre><h3 id="（4）查看一下-4">（4）查看一下</h3><pre><code>[root@master yaml]# kubectl get pod</code></pre><p>![image-20200115100855819](G:\四期\虚拟化\kubernetes\k8s文档\09 Job资源对象.assets\image-20200115100855819.png)</p><p><strong>这时会发现，如果规定具体时间，可能并不会执行任务。</strong></p><h3 id="（5）添加apiVersion库">（5）添加apiVersion库</h3><pre><code>[root@master yaml]# vim /etc/kubernetes/manifests/kube-apiserver.yaml spec:  containers:  - command:    - kube-apiserver    - --runtime-config=batch/v2alpha1=true    #添加</code></pre><p>![image-20200115104218361](G:\四期\虚拟化\kubernetes\k8s文档\09 Job资源对象.assets\image-20200115104218361.png)</p><h3 id="（6）重启kubelet">（6）重启kubelet</h3><pre><code>[root@master yaml]# systemctl restart kubelet.service </code></pre><h3 id="（7）查看api版本">（7）查看api版本</h3><pre><code>[root@master yaml]# kubectl api-versions </code></pre><p>![image-20200115104521662](G:\四期\虚拟化\kubernetes\k8s文档\09 Job资源对象.assets\image-20200115104521662.png)</p><h3 id="（8）编写一个cronjob的yaml文件">（8）编写一个cronjob的yaml文件</h3><pre><code>[root@master yaml]# vim cronjop.yamlkind: CronJobapiVersion: batch/v1beta1metadata:  name: hellospec:  schedule: &quot;47 10 15 1 *&quot; #限定时间  jobTemplate:    spec:      template:        spec:          containers:          - name: hello            image: busybox            command: [&quot;echo&quot;,&quot;hello&quot;,&quot;cronjob&quot;]          restartPolicy: OnFailure</code></pre><h3 id="（9）执行一下">（9）执行一下</h3><pre><code>[root@master yaml]# kubectl apply -f jop.yaml </code></pre><h3 id="（4）查看一下-5">（4）查看一下</h3><pre><code>[root@master yaml]# kubectl get pod -w</code></pre><p>![image-20200115100855819](G:\四期\虚拟化\kubernetes\k8s文档\09 Job资源对象.assets\image-20200115100855819.png)</p><p><strong>注意：此时仍然不能正常运行指定时间的Job，这是因为K8s官方在cronjob这个资源对象的支持中还没有完善此功能，还待开发。</strong></p><p><strong>跟Job资源一样在cronjob.spec.jobTemplate.spec 下同样支持并发Job参数:</strong><br><strong>parallelism，也支持完成Pod的总数参数: completionsr</strong></p><h1>总结</h1><p><strong>Job 作为 Kubernetes 中用于处理任务的资源，与其他的资源没有太多的区别，它也使用 Kubernetes 中常见的控制器模式，监听 Informer 中的事件并运行 <code>syncHandler</code> 同步任务</strong></p><p><strong>而 CronJob 由于其功能的特殊性，每隔 10s 会从 apiserver 中取出资源并进行检查是否应该触发调度创建新的资源，需要注意的是 CronJob 并不能保证在准确的目标时间执行，执行会有一定程度的滞后。</strong></p><p><strong>两个控制器的实现都比较清晰，只是边界条件比较多，分析其实现原理时一定要多注意。</strong></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>08 ReplicaSet、DaemonSet</title>
      <link href="/posts/7772.html"/>
      <url>/posts/7772.html</url>
      
        <content type="html"><![CDATA[<h1>ReplicaSet简单介绍</h1><h2 id="1-RC：ReplicationController（老一代的pod控制器）">1. RC：ReplicationController（老一代的pod控制器）</h2><p><strong>用来确保由其管控的Pod对象副本数量，能够满足用户期望，多则删除，少则通过模本创建</strong></p><h3 id="特点：">特点：</h3><ul><li>​<strong>确保Pod资源对象的数量精准。</strong></li><li>​<strong>确保pod健康运行。</strong></li><li>​<strong>弹性伸缩</strong></li></ul><p><strong>同样，它也可以通过yaml或json格式的资源清单来创建。其中spec字段一般嵌套以下字段：</strong></p><ul><li>​<strong>replicas：期望的Pod对象副本数量。</strong></li><li>​<strong>selector：当前控制器匹配Pod对此项副本的标签选择器</strong></li><li>​<strong>template：pod副本的模板</strong></li></ul><p><strong>与RC相比而言，RS不仅支持<em>基于等值</em>的标签选择器，而且还支持<em>基于集合</em>的标签选择器。</strong></p><h2 id="2-标签：解决同类型的资源对象，为了更好的管理，按照标签分组。">2. 标签：解决同类型的资源对象，为了更好的管理，按照标签分组。</h2><h3 id="常用的标签分类：">常用的标签分类：</h3><ul><li>​<strong>release（版本）：stable（稳定版）、canary（金丝雀版本）、beta（测试版本）</strong></li><li>​<strong>environment（环境变量）：dev（开发）、qa（测试）、production（生产）</strong></li><li>​<strong>application（应用）：ui、as（application software应用软件）、pc、sc</strong></li><li>​<strong>tier（架构层级）：frontend（前端）、backend（后端）、cache（缓存）</strong></li><li>​<strong>partition（分区）：customerA（客户A）、customerB（客户B）</strong></li><li>​<strong>track（品控级别）：daily（每天）、weekly（每周）</strong></li></ul><p><strong>标签要做到：见名知意。</strong></p><h2 id="3-测试">3.测试</h2><h3 id="（1）编写一个pod的yaml文件">（1）编写一个pod的yaml文件</h3><pre><code>[root@master ~]# vim label.yaml kind: PodapiVersion: v1metadata:  name: labels  labels:    env: qa    tier: frontendspec:  containers:  - name: myapp    image: httpd</code></pre><h4 id="1-执行一下">&lt;1&gt;执行一下</h4><pre><code>[root@master ~]# kubectl apply -f label.yaml  --record </code></pre><h4 id="2-查看一下">&lt;2&gt;查看一下</h4><pre><code>[root@master ~]# kubectl get pod  --show-labels //通过--show-labels显示资源对象的</code></pre><p>![image-20200114095943595](G:\四期\虚拟化\kubernetes\k8s文档\08 ReplicaS儿童、DaemonSet.assets\image-20200114095943595.png)</p><pre><code>[root@master ~]# kubectl get po -L env,tier//显示某个键对应的值</code></pre><p>![image-20200114100043922](G:\四期\虚拟化\kubernetes\k8s文档\08 ReplicaS儿童、DaemonSet.assets\image-20200114100043922.png)</p><pre><code>[root@master ~]# kubectl get po -l env,tier//通过-l 查看仅包含某个标签的资源。</code></pre><p>![image-20200114100200895](G:\四期\虚拟化\kubernetes\k8s文档\08 ReplicaS儿童、DaemonSet.assets\image-20200114100200895.png)</p><h3 id="（2）添加标签">（2）添加标签</h3><pre><code>[root@master ~]# kubectl label pod  labels app=pc//给pod资源添加标签</code></pre><h3 id="（3）修改标签">（3）修改标签</h3><pre><code>[root@master ~]# kubectl label pod labels env=dev --overwrite//修改标签</code></pre><pre><code>[root@master ~]# kubectl get pod -l tier --show-labels //查看标签</code></pre><p>![image-20200114100607585](G:\四期\虚拟化\kubernetes\k8s文档\08 ReplicaS儿童、DaemonSet.assets\image-20200114100607585.png)</p><h3 id="（4）编写一个service的yaml文件">（4）编写一个service的yaml文件</h3><pre><code>[root@master ~]# vim service.yamlkind: ServiceapiVersion: v1metadata:  name: servicespec:  type: NodePort  selector:    env: qa  ports:  - protocol: TCP    port: 90    targetPort: 80    nodePort: 30123</code></pre><h4 id="1-执行一下-2">&lt;1&gt;执行一下</h4><pre><code>[root@master ~]# kubectl apply -f service.yaml </code></pre><h4 id="2-查看一下-2">&lt;2&gt;查看一下</h4><pre><code>[root@master ~]# kubectl describe svc</code></pre><p>![image-20200114101837151](G:\四期\虚拟化\kubernetes\k8s文档\08 ReplicaSet、DaemonSet.assets\image-20200114101837151.png)</p><h4 id="3-访问一下">&lt;3&gt;访问一下</h4><pre><code>[root@master ~]# curl 127.0.0.1:30123</code></pre><p>![image-20200114101915248](G:\四期\虚拟化\kubernetes\k8s文档\08 ReplicaSet、DaemonSet.assets\image-20200114101915248.png)</p><p><strong>如果标签有多个，标签选择器选择其中一个，也可以关联成功。相反，如果选择器有多个，那么标签必须完全满足条件，才可以关联成功。</strong></p><h2 id="4-标签选择器：标签的查询过滤条件。">4. 标签选择器：标签的查询过滤条件。</h2><p><strong><a href>基于等值关系的（equality-based）</a>：“=”，“==”，“！ =”前面两个都是相等，最后一个是不等于。</strong></p><p><strong><a href>基于集合关系（set-based）</a>:in、notin、exists三种。选择器列表间为“逻辑与”关系，使用ln或者NotIn操作时，其valuas不强制要求为非空的字符串列表，而使用Exists或DostNotExist时，其values必须为空</strong></p><h4 id="使用标签选择器的逻辑：">使用标签选择器的逻辑：</h4><ul><li><strong>同时指定的多个选择器之间的逻辑关系为“与”操作。</strong></li><li><strong>使用空值的标签选择器意味着每个资源对象都将把选中。</strong></li><li><strong>空的标签选择器无法选中任何资源。</strong></li></ul><h3 id="（1）例子">（1）例子</h3><p>![image-20200114110334223](G:\四期\虚拟化\kubernetes\k8s文档\08 ReplicaSet、DaemonSet.assets\image-20200114110334223.png)</p><h4 id="编写一个selector的yaml’文件">编写一个selector的yaml’文件</h4><pre class=" language-language-yaml"><code class="language-language-yaml">[root@master ~]# vim selector.yamlselector:  matchLabels:    app: nginx  mathExpressions:    - {key: name,operator: In,values: [zhangsan,lisi]}    - {key: age,operator: Exists,values:}</code></pre><ul><li><strong>selector：当前控制器匹配Pod对此项副本的标签选择器</strong></li><li><strong>matchLabels: 指定键值对表示的标签选择器。</strong></li><li><strong>mathExpressions:：基于表达式来指定的标签选择器。</strong></li></ul><h1>DaemonSet</h1><p><em><strong>它也是一种pod控制器。</strong></em></p><p><em><strong>RC，RS , deployment , daemonset.都是pod控制器。statfukSet，RBAC</strong></em></p><h3 id="1-使用场景：">1. 使用场景：</h3><p><strong>如果必须将pod运行在固定的某个或某几个节点，且要优先于其他的pod的启动。通常情况下，默认会将每一个节点都运行，并且只能运行一个pod。这种情况推荐使用DeamonSet资源对象。</strong></p><ul><li><strong>监控程序；</strong></li><li><strong>日志收集程序；</strong></li><li><strong>集群存储程序；</strong></li></ul><pre><code>[root@master ~]# kubectl get ds -n kube-system //查看一下DaemonSet</code></pre><h3 id="2-DaemonSet-与-Deployment-的区别">2. DaemonSet 与 Deployment 的区别</h3><ul><li><strong>Deployment 部署的副本 Pod 会分布在各个 Node 上，每个 Node 都可能运行好几个副本。</strong></li><li><strong>DaemonSet 的不同之处在于：每个 Node 上最多只能运行一个副本。</strong></li></ul><h3 id="3-运行一个web服务，在每一个节点运行一个pod。">3. 运行一个web服务，在每一个节点运行一个pod。</h3><pre><code>[root@master ~]# vim daemonset.yamlkind: DaemonSetapiVersion: extensions/v1beta1metadata:  name: test-dsspec:  template:    metadata:      labels:        name: test-ds    spec:      containers:      - name: test-ds        image: httpd</code></pre><h4 id="1-执行一下-3">&lt;1&gt;执行一下</h4><pre><code>[root@master ~]# kubectl apply -f daemonset.yaml </code></pre><h4 id="2-查看一下-3">&lt;2&gt;查看一下</h4><pre><code>[root@master ~]# kubectl get ds</code></pre><p>![image-20200114112936161](G:\四期\虚拟化\kubernetes\k8s文档\08 ReplicaSet、DaemonSet.assets\image-20200114112936161.png)</p><h1>总结</h1><h2 id="1）总结RC、RS、Deplyment、DaemonSet控制器的特点及使用场景。"><strong>1）总结RC、RS、Deplyment、DaemonSet控制器的特点及使用场景。</strong></h2><h3 id="1-Replication-Controller（RC）">&lt;1&gt;Replication Controller（RC）</h3><h4 id="介绍及使用场景">介绍及使用场景</h4><p><strong><code>Replication Controller</code>简称<code>RC</code>，<code>RC</code>是<code>Kubernetes</code>系统中的核心概念之一，简单来说，<code>RC</code>可以保证在任意时间运行<code>Pod</code>的副本数量，能够保证<code>Pod</code>总是可用的。如果实际<code>Pod</code>数量比指定的多那就结束掉多余的，如果实际数量比指定的少就新启动一些<code>Pod</code>，当<code>Pod</code>失败、被删除或者挂掉后，<code>RC</code>都会去自动创建新的<code>Pod</code>来保证副本数量，所以即使只有一个<code>Pod</code>，我们也应该使用<code>RC</code>来管理我们的<code>Pod</code>。</strong></p><h4 id="主要功能">主要功能</h4><ul><li><strong>确保pod数量：RC用来管理正常运行Pod数量，一个RC可以由一个或多个Pod组成，在RC被创建后，系统会根据定义好的副本数来创建Pod数量。在运行过程中，如果Pod数量小于定义的，就会重启停止的或重新分配Pod，反之则杀死多余的。</strong></li><li><strong>确保pod健康：当pod不健康，运行出错或者无法提供服务时，RC也会杀死不健康的pod，重新创建新的。</strong></li><li><strong>弹性伸缩 ：在业务高峰或者低峰期的时候，可以通过RC动态的调整pod的数量来提高资源的利用率。同时，配置相应的监控功能（Hroizontal Pod Autoscaler），会定时自动从监控平台获取RC关联pod的整体资源使用情况，做到自动伸缩。</strong></li><li><strong>滚动升级：滚动升级为一种平滑的升级方式，通过逐步替换的策略，保证整体系统的稳定，在初始化升级的时候就可以及时发现和解决问题，避免问题不断扩大。</strong></li></ul><h3 id="2-Replication-Set（RS）">&lt;2&gt;Replication Set（RS）</h3><p><strong>被认为 是“升级版”的RC。RS也是用于保证与label selector匹配的pod数量维持在期望状态。</strong></p><blockquote><p><strong>实际上<code>RS</code>和<code>RC</code>的功能基本一致，目前唯一的一个区别就是<code>RC</code>只支持基于等式的<code>selector</code>（env=dev或app=nginx），但<code>RS</code>还支持基于集合的<code>selector</code>（version in (v1, v2)），这对复杂的运维管理就非常方便了。</strong></p><p><strong><code>kubectl</code>命令行工具中关于<code>RC</code>的大部分命令同样适用于我们的<code>RS</code>资源对象。不过我们也很少会去单独使用<code>RS</code>，它主要被<code>Deployment</code>这个更加高层的资源对象使用，除非用户需要自定义升级功能或根本不需要升级<code>Pod</code>，在一般情况下，我们推荐使用<code>Deployment</code>而不直接使用<code>Replica Set</code>。</strong></p></blockquote><h4 id="区别在于">区别在于</h4><p><strong>1、RC只支持基于等式的selector（env=dev或environment!=qa），但RS还支持新的，基于集合的selector（version in (v1.0, v2.0)或env notin (dev, qa)），这对复杂的运维管理很方便。</strong></p><p><strong>2、升级方式</strong></p><ul><li><strong>RS不能使用kubectlrolling-update进行升级</strong></li><li><strong>kubectl rolling-update专用于rc</strong></li><li><strong>RS升级使用deployment或者kubectl replace命令</strong></li><li><strong>社区引入这一API的初衷是用于取代vl中的RC，也就是说当v1版本被废弃时，RC就完成了它的历史使命，而由RS来接管其工作</strong></li></ul><h3 id="3-DaemonSet">&lt;3&gt;DaemonSet</h3><h4 id="1-特点：">1. 特点：</h4><p><strong>如果必须将pod运行在固定的某个或某几个节点，且要优先于其他的pod的启动。通常情况下，默认会将每一个节点都运行，并且只能运行一个pod。这种情况推荐使用DeamonSet资源对象。</strong></p><p><strong>一个DaemonSet对象能确保其创建的Pod在集群中的每一台（或指定）Node上都运行一个副本。如果集群中动态加入了新的Node，DaemonSet中的Pod也会被添加在新加入Node上运行。删除一个DaemonSet也会级联删除所有其创建的Pod。</strong></p><h4 id="2-使用环境"><strong>2. 使用环境</strong></h4><ul><li><strong>监控程序；</strong></li><li><strong>日志收集程序；</strong></li><li><strong>集群存储程序；</strong></li></ul><h3 id="4-Deployment">&lt;4&gt;Deployment</h3><h4 id="1-什么是Deployment">1. 什么是Deployment</h4><p><strong>Kubernetes Deployment提供了官方的用于更新Pod和Replica Set（下一代的Replication Controller）的方法，您可以在Deployment对象中只描述您所期望的理想状态（预期的运行状态），Deployment控制器为您将现在的实际状态转换成您期望的状态，例如，您想将所有的webapp:v1.0.9升级成webapp:v1.1.0，您只需创建一个Deployment，Kubernetes会按照Deployment自动进行升级。现在，您可以通过Deployment来创建新的资源（pod，rs，rc），替换已经存在的资源等。</strong></p><p><strong>你只需要在Deployment中描述你想要的目标状态是什么，Deployment controller就会帮你将Pod和Replica Set的实际状态改变到你的目标状态。你可以定义一个全新的Deployment，也可以创建一个新的替换旧的Deployment。</strong></p><h4 id="2-典型的用例">2. 典型的用例</h4><ul><li><strong>使用Deployment来创建ReplicaSet。ReplicaSet在后台创建pod。检查启动状态，看它是成功还是失败。</strong></li><li><strong>然后，通过更新Deployment的PodTemplateSpec字段来声明Pod的新状态。这会创建一个新的ReplicaSet，Deployment会按照控制的速率将pod从旧的ReplicaSet移动到新的ReplicaSet中。</strong></li><li><strong>如果当前状态不稳定，回滚到之前的Deployment revision。每次回滚都会更新Deployment的revision。</strong></li><li><strong>扩容Deployment以满足更高的负载。</strong></li><li><strong>暂停Deployment来应用PodTemplateSpec的多个修复，然后恢复上线。</strong></li><li><strong>根据Deployment 的状态判断上线是否hang住了。</strong></li><li><strong>清除旧的不必要的ReplicaSet。</strong></li></ul><h4 id="3-使用环境">3. 使用环境</h4><p><strong>Deployment集成了上线部署、滚动升级、创建副本、暂停上线任务，恢复上线任务，回滚到以前某一版本（成功/稳定）的Deployment等功能，在某种程度上，Deployment可以帮我们实现无人值守的上线，大大降低我们的上线过程的复杂沟通、操作风险。</strong></p><ul><li><strong>定义Deployment来创建Pod和ReplicaSet</strong></li><li><strong>滚动升级和回滚应用</strong></li><li><strong>扩容和缩容</strong></li><li><strong>暂停和继续Deployment</strong></li></ul><h4 id="3-DaemonSet-与-Deployment-的区别">3. DaemonSet 与 Deployment 的区别</h4><ul><li><strong>Deployment 部署的副本 Pod 会分布在各个 Node 上，每个 Node 都可能运行好几个副本。</strong></li><li><strong>DaemonSet 的不同之处在于：每个 Node 上最多只能运行一个副本。</strong></li></ul><h2 id="2）使用DaemonSet控制器运行httpd服务，要求名称以自己的名称命名。标签为：tier-backend-env-dev"><strong>2）使用DaemonSet控制器运行httpd服务，要求名称以自己的名称命名。标签为：tier=backend,env=dev.</strong></h2><pre><code>[root@master ~]# vim daemonset.yaml kind: DaemonSetapiVersion: extensions/v1beta1metadata:  name: xgp-dsspec:  template:    metadata:      labels:        tier: backend        env: dev    spec:      containers:      - name: xgp-ds        image: httpd</code></pre><h3 id="查看一下">查看一下</h3><pre><code>[root@master ~]# kubectl get pod  --show-labels </code></pre><p>![image-20200114100043922](G:\四期\虚拟化\kubernetes\k8s文档\08 ReplicaS儿童、DaemonSet.assets\image-20200114100043922.png)</p><pre><code>[root@master ~]# kubectl get pod -L env,tier</code></pre><p>![image-20200114095943595](G:\四期\虚拟化\kubernetes\k8s文档\08 ReplicaSet、DaemonSet.assets\image-20200114095943595.png)</p><h2 id="3-创建service资源对象与上述资源进行关联，要有验证。"><strong>3) 创建service资源对象与上述资源进行关联，要有验证。</strong></h2><pre><code>[root@master ~]# vim service.yaml kind: ServiceapiVersion: v1metadata:  name: servicespec:   type: NodePort  selector:     env: dev  ports:      - protocol: TCP    port: 90     targetPort: 80    nodePort: 30123 </code></pre><h3 id="执行一下">执行一下</h3><pre><code>[root@master ~]# kubectl apply -f service.yaml </code></pre><h3 id="查看一下-2">查看一下</h3><pre><code>[root@master ~]# kubectl describe svc</code></pre><p>![image-20200114120345596](G:\四期\虚拟化\kubernetes\k8s文档\08 ReplicaSet、DaemonSet.assets\image-20200114120345596.png)</p><h3 id="访问一下">访问一下</h3><pre><code>[root@master ~]# curl 127.0.0.1:30123</code></pre><p>![image-20200114120444524](G:\四期\虚拟化\kubernetes\k8s文档\08 ReplicaSet、DaemonSet.assets\image-20200114120444524.png)</p><h2 id="4）整理关于标签和标签选择器都有什么作用？"><strong>4）整理关于标签和标签选择器都有什么作用？</strong></h2><p><strong>&lt;1&gt;标签：解决同类型的资源对象，为了更好的管理，按照标签分组。</strong></p><p><strong>&lt;2&gt;标签选择器：标签的查询过滤条件。</strong></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>06 pod资源对象</title>
      <link href="/posts/cf38.html"/>
      <url>/posts/cf38.html</url>
      
        <content type="html"><![CDATA[<h1>一，k8s的资源对象</h1><p><em><strong>Deployment、Service、Pod是k8s最核心的3个资源对象</strong></em></p><blockquote><p>**Deployment：**最常见的无状态应用的控制器，支持应用的扩缩容、滚动升级等操作。</p><p>**Service：**为弹性变动且存在生命周期的Pod对象提供了一个固定的访问接口，用于服务发现和服务访问。</p><p>**Pod：**是运行容器以及调度的最小单位。同一个pod可以同时运行多个容器，这些容器共享net、UTS、IPC，除此之外还有USER、PID、MOUNT。</p><p>**ReplicationController：**用于确保每个Pod副本在任意时刻都能满足目标数量，简单来说，它用于每个容器或容器组总是运行并且可以访问的：老一代无状态的Pod应用控制器。</p><p>**RwplicatSet：**新一代的无状态的Pod应用控制器，它与RC的不同之处在于支持的标签选择器不同，RC只支持等值选择器（键值对），RS还额外支持基于集合的选择器。</p><p>**StatefulSet：**用于管理有状态的持久化应用，如database服务程序，它与Deployment不同之处在于，它会为每一个pod创建一个独有的持久性标识符，并确保每个pod之间的顺序性。</p><p>**DaemonSet：**用于确保每一个节点都运行了某个pod的一个副本，新增的节点一样会被添加到此类pod，在节点移除时，此pod会被回收。</p><p>**Job：**用于管理运行完成后即可终止的应用，例如批量处理做作业任务；</p><p>**volume：**pv pvc<br><strong>ConfigMap：</strong><br><strong>Secret：</strong><br><strong>Role：</strong><br><strong>ClusterRole：</strong><br><strong>RoleBinding：</strong><br><strong>cluster RoleBinding：</strong><br><strong>service account：</strong><br><strong>Helm：</strong></p></blockquote><h2 id="Pod的生命周期被定义为以下几个阶段。">Pod的生命周期被定义为以下几个阶段。</h2><blockquote><ul><li><strong>Pending：Pod已经被创建，但是一个或者多个容器还未创建，这包括Pod调度阶段，以及容器镜像的下载过程。</strong></li><li><strong>Running：Pod已经被调度到Node，所有容器已经创建，并且至少一个容器在运行或者正在重启。</strong></li><li><strong>Succeeded：Pod中所有容器正常退出。</strong></li><li><strong>Failed：Pod中所有容器退出，至少有一个容器是一次退出的。</strong></li></ul></blockquote><h1>环境介绍</h1><table><thead><tr><th>主机</th><th>IP地址</th><th>服务</th></tr></thead><tbody><tr><td>master</td><td>192.168.1.21</td><td>k8s</td></tr><tr><td>node01</td><td>192.168.1.22</td><td>k8s</td></tr><tr><td>node02</td><td>192.168.1.23</td><td>k8s</td></tr></tbody></table><h1>二，Namespace：名称空间</h1><p><strong>默认的名称空间：</strong></p><blockquote><p><strong>Namespace（命名空间）是kubernetes系统中的另一个重要的概念，通过将系统内部的对象“分配”到不同的Namespace中，形成逻辑上分组的不同项目、小组或用户组，便于不同的分组在共享使用整个集群的资源的同时还能被分别管理。</strong></p><p><strong>Kubernetes集群在启动后，会创建一个名为“default”的Namespace，如果不特别指明Namespace，则用户创建的Pod、RC、Service都被系统创建到“default”的Namespace中。</strong></p></blockquote><h2 id="1-查看名称空间">1.查看名称空间</h2><pre><code>[root@master ~]# kubectl get namespaces</code></pre><p>![image-20200109094700728](G:\四期\虚拟化\kubernetes\k8s文档\06 pod资源对象.assets\image-20200109094700728.png)</p><h2 id="2-查看名称空间详细信息">2.查看名称空间详细信息</h2><pre><code>[root@master ~]# kubectl describe ns default</code></pre><p>![image-20200109095006067](G:\四期\虚拟化\kubernetes\k8s文档\06 pod资源对象.assets\image-20200109095006067.png)</p><h2 id="3-创建名称空间">3.创建名称空间</h2><pre><code>[root@master ~]# kubectl create ns bdqn</code></pre><h3 id="查看一下">查看一下</h3><pre><code>[root@master ~]# kubectl get namespaces</code></pre><p>![image-20200109095153448](G:\四期\虚拟化\kubernetes\k8s文档\06 pod资源对象.assets\image-20200109095153448.png)</p><h2 id="4-创建namespace的yaml文件">4.创建namespace的yaml文件</h2><h3 id="（1）查看格式">（1）查看格式</h3><pre><code>[root@master ~]# kubectl explain ns//查看nasespace的yaml文件的格式</code></pre><h3 id="（2）创建namespace的yaml文件">（2）创建namespace的yaml文件</h3><pre><code>[root@master ~]# vim test-ns.yamlapiVersion: v1kind: Namespacemetadata:  name: test</code></pre><h3 id="（3）运行namespace的yaml文件">（3）运行namespace的yaml文件</h3><pre><code>[root@master ~]# kubectl apply -f test-ns.yaml </code></pre><h3 id="（4）查看一下">（4）查看一下</h3><pre><code>[root@master ~]# kubectl get ns</code></pre><p>![image-20200109095808777](G:\四期\虚拟化\kubernetes\k8s文档\06 pod资源对象.assets\image-20200109095808777.png)</p><h2 id="4-删除名称空间">4.删除名称空间</h2><pre><code>[root@master ~]# kubectl delete ns test [root@master ~]# kubectl delete -f test-ns.yaml </code></pre><p><strong>注意：namespace资源对象进用于资源对象的隔离，并不能隔绝不同名称空间的Pod之间的通信。那是网络策略资源的功能。</strong></p><h2 id="5-查看指定名称空间">5.查看指定名称空间</h2><p><strong>可使用–namespace或-n选项</strong></p><pre><code>[root@master ~]# kubectl get pod -n kube-system [root@master ~]# kubectl get pod --namespace kube-system </code></pre><h1>三，Pod</h1><h2 id="1-编写一个pod的yaml文件">1.编写一个pod的yaml文件</h2><pre><code>[root@master ~]# vim pod.yamlkind: PodapiVersion: v1metadata:  name: test-podspec:  containers:  - name: test-app    image: 192.168.1.21:5000/web:v1</code></pre><p><em><strong>pod的yaml文件不支持replicas字段</strong></em></p><h3 id="（1）运行一下">（1）运行一下</h3><pre><code>[root@master ~]# kubectl apply -f pod.yaml </code></pre><h3 id="（2）查看一下">（2）查看一下</h3><pre><code>[root@master ~]# kubectl get pod</code></pre><p>![image-20200109100836911](G:\四期\虚拟化\kubernetes\k8s文档\06 pod资源对象.assets\image-20200109100836911.png)</p><p><em><strong>ps：这个pod因为是自己创建的，所以删除之后k8s并不会自动生成，相当于docker中创建</strong></em></p><h2 id="2-指定pod的namespace名称空间">2.指定pod的namespace名称空间</h2><h3 id="（1）修改pod的yaml文件">（1）修改pod的yaml文件</h3><pre><code>[root@master ~]# vim pod.yamlkind: Pod        #资源类型apiVersion: v1   #api版本metadata:  name: test-pod    #指定控制器名称  namespace: bdqn   #指定namespace（名称空间）spec:  containers:      #容器  - name: test-app  #容器名称    image: 192.168.1.21:5000/web:v1  #镜像</code></pre><h5 id="执行一下">执行一下</h5><pre><code>[root@master ~]# kubectl apply -f pod.yaml </code></pre><h3 id="（2）查看一下-2">（2）查看一下</h3><pre><code>[root@master ~]#  kubectl get pod -n bdqn //根据namespace名称查看</code></pre><p>![image-20200109101521992](G:\四期\虚拟化\kubernetes\k8s文档\06 pod资源对象.assets\image-20200109101521992.png)</p><h2 id="3-pod中镜像获取策略">3.pod中镜像获取策略</h2><blockquote><p>**Always：**镜像标签为“laster”或镜像不存在时，总是从指定的仓库中获取镜像。</p><p>**IfNotPresent：**仅当本地镜像不存在时才从目标仓库下载。</p><p>**Never：**禁止从仓库中下载镜像，即只使用本地镜像。</p></blockquote><p><em><strong>注意：对于标签为“laster”或者标签不存在，其默认的镜像下载策略为“Always”，而对于其他的标签镜像，默认策略为“IfNotPresent”。</strong></em></p><h2 id="4-观察pod和service的不同并关联">4.观察pod和service的不同并关联</h2><h3 id="（1）pod的yaml文件（指定端口）">（1）pod的yaml文件（指定端口）</h3><pre><code>[root@master ~]# vim pod.yaml kind: Pod          #资源类型apiVersion: v1      #api版本metadata:  name: test-pod       #指定控制器名称  namespace: bdqn   #指定namespace（名称空间）spec:  containers:                          #容器  - name: test-app                    #容器名称    image: 192.168.1.21:5000/web:v1   #镜像    imagePullPolicy: IfNotPresent   #获取的策略    ports:    - protocol: TCP      containerPort: 80  </code></pre><h4 id="1-删除之前的pod">&lt;1&gt;删除之前的pod</h4><pre><code>[root@master ~]# kubectl delete pod -n bdqn test-pod </code></pre><h4 id="2-执行一下">&lt;2&gt;执行一下</h4><pre><code>[root@master ~]# kubectl apply -f pod.yaml </code></pre><h4 id="3-查看一下">&lt;3&gt;查看一下</h4><pre><code>[root@master ~]# kubectl get pod -n bdqn </code></pre><p>![image-20200109110215669](G:\四期\虚拟化\kubernetes\k8s文档\06 pod资源对象.assets\image-20200109110215669.png)</p><h3 id="（2）pod的yaml文件（修改端口）">（2）pod的yaml文件（修改端口）</h3><pre><code>[root@master ~]# vim pod.yaml kind: PodapiVersion: v1metadata:  name: test-pod  namespace: bdqnspec:  containers:  - name: test-app    image: 192.168.1.21:5000/web:v1    imagePullPolicy: IfNotPresent    ports:    - protocol: TCP      containerPort: 90   #改一下端口</code></pre><h4 id="1-删除之前的pod-2">&lt;1&gt;删除之前的pod</h4><pre><code>[root@master ~]# kubectl delete pod -n bdqn test-pod </code></pre><h4 id="2-执行一下-2">&lt;2&gt;执行一下</h4><pre><code>[root@master ~]# kubectl apply -f pod.yaml </code></pre><h4 id="3-查看一下-2">&lt;3&gt;查看一下</h4><pre><code>[root@master ~]# kubectl get pod -n bdqn -o wide</code></pre><p>![image-20200109110409584](G:\四期\虚拟化\kubernetes\k8s文档\06 pod资源对象.assets\image-20200109110409584.png)</p><h4 id="4-访问一下">&lt;4&gt;访问一下</h4><p>![image-20200109110430334](G:\四期\虚拟化\kubernetes\k8s文档\06 pod资源对象.assets\image-20200109110430334.png)</p><p><strong>会发现修改的90端口并不生效，他只是一个提示字段并不生效。</strong></p><h3 id="（3）pod的yaml文件（添加标签）">（3）pod的yaml文件（添加标签）</h3><pre><code>[root@master ~]# vim pod.yaml kind: PodapiVersion: v1metadata:  name: test-pod  namespace: bdqn  labels:                 #标签    app: test-web          #标签名称spec:  containers:  - name: test-app    image: 192.168.1.21:5000/web:v1    imagePullPolicy: IfNotPresent    ports:    - protocol: TCP      containerPort: 90   #改一下端口</code></pre><h4 id="pod">--------------------------------------pod---------------------------------------------</h4><h2 id="（4）编写一个service的yaml文件">（4）编写一个service的yaml文件</h2><pre><code>[root@master ~]# vim test-svc.yaml apiVersion: v1      #api版本kind: Service          #资源类型metadata:  name: test-svc       #指定控制器名称  namespace: bdqn   #指定namespace（名称空间）spec:  selector:          #标签    app: test-web    #标签名称（须和pod的标签名称一致）  ports:                - port: 80          #宿主机端口    targetPort: 80    #容器端口</code></pre><p><em><strong>会发现添加的80端口生效了，所以不能乱改。</strong></em></p><h4 id="1-执行一下">&lt;1&gt;执行一下</h4><pre><code>[root@master ~]# kubectl apply -f test-svc.yaml</code></pre><h4 id="2-查看一下">&lt;2&gt;查看一下</h4><pre><code>[root@master ~]# kubectl get svc -n bdqn </code></pre><p>![image-20200109121106859](G:\四期\虚拟化\kubernetes\k8s文档\06 pod资源对象.assets\image-20200109121106859.png)</p><pre><code>[root@master ~]# kubectl describe svc -n bdqn test-svc </code></pre><p>![image-20200109121139399](G:\四期\虚拟化\kubernetes\k8s文档\06 pod资源对象.assets\image-20200109121139399.png)</p><h4 id="4-访问一下-2">&lt;4&gt;访问一下</h4><pre><code>[root@master ~]# curl 10.98.57.97 </code></pre><p>![image-20200109121205607](G:\四期\虚拟化\kubernetes\k8s文档\06 pod资源对象.assets\image-20200109121205607.png)</p><h4 id="service">--------------------------------------service---------------------------------------------</h4><h1>四，容器的重启策略</h1><p><strong>Pod的重启策略（RestartPolicy）应用与Pod内所有容器，并且仅在Pod所处的Node上由kubelet进行判断和重启操作。当某个容器异常退出或者健康检查失败时，kubelet将根据RestartPolicy的设置来进行相应的操作。</strong></p><blockquote><p><strong>Always：</strong>（默认情况下使用）但凡Pod对象终止就将其重启；<br>**OnFailure：**仅在Pod对象出现错误时才将其重启；<br>**Never：**从不重启；</p></blockquote><h1>五，pod的默认健康检查</h1><p><strong>每个容器启动时都会执行一个进程，此进程由 Dockerfile 的 CMD 或 ENTRYPOINT 指定。如果进程退出时返回码非零，则认为容器发生故障，Kubernetes 就会根据 <code>restartPolicy</code> 重启容器。</strong></p><h2 id="（1）编写健康检查的yaml文件">（1）编写健康检查的yaml文件</h2><p><strong>下面我们模拟一个容器发生故障的场景，Pod 配置文件如下：</strong></p><pre><code>[root@master ~]# vim healcheck.yaml apiVersion: v1kind: Podmetadata:  labels:    test: healcheck  name:  healcheckspec:  restartPolicy: OnFailure  #指定重启策略  containers:  - name:  healcheck    image: busybox:latest    args:                   #生成pod时运行的命令    - /bin/sh    - -c    - sleep 20; exit 1 </code></pre><h3 id="1-执行一下-2">&lt;1&gt;执行一下</h3><pre><code>[root@master ~]# kubectl apply -f  healcheck.yaml</code></pre><h3 id="2-查看一下-2">&lt;2&gt;查看一下</h3><pre><code>[root@master ~]# kubectl get pod -o wide</code></pre><p>![image-20200109121809350](G:\四期\虚拟化\kubernetes\k8s文档\06 pod资源对象.assets\image-20200109121809350.png)</p><pre><code>[root@master ~]# kubectl get pod -w | grep healcheck</code></pre><p>![image-20200109121817775](G:\四期\虚拟化\kubernetes\k8s文档\06 pod资源对象.assets\image-20200109121817775.png)</p><p><strong>在上面的例子中，容器进程返回值非零，Kubernetes 则认为容器发生故障，需要重启。但有不少情况是发生了故障，但进程并不会退出。</strong></p><h1>六，小实验</h1><h2 id="1）以自己的名称创建一个k8s名称空间，以下所有操作都在此名称空间中。">1）以自己的名称创建一个k8s名称空间，以下所有操作都在此名称空间中。</h2><h3 id="（1）创建名称空间">（1）创建名称空间</h3><pre><code>[root@master ~]# kubectl create ns xgp</code></pre><h3 id="（2）查看一下-3">（2）查看一下</h3><pre><code>[root@master ~]# kubectl get ns xgp </code></pre><p>![image-20200109133106300](G:\四期\虚拟化\kubernetes\k8s文档\06 pod资源对象.assets\image-20200109133106300.png)</p><h2 id="2）创建一个Pod资源对象，使用的是私有仓库中私有镜像，其镜像的下载策略为：NEVER。-Pod的重启策略为：-Never">2）创建一个Pod资源对象，使用的是私有仓库中私有镜像，其镜像的下载策略为：NEVER。 Pod的重启策略为： Never.</h2><pre><code>[root@master ~]# vim pod.yamlkind: PodapiVersion: v1metadata:  name: test-pod  namespace: xgp  labels:    app: test-webspec:  restartPolicy: Never  containers:  - name: www    image: 192.168.1.21:5000/web:v1    imagePullPolicy: Never    args:                       - /bin/sh    - -c    - sleep 90; exit 1    ports:    - protocol: TCP      containerPort: 80</code></pre><h2 id="3）创建出容器之后，执行非正常退出，查看Pod的最终状态。">3）创建出容器之后，执行非正常退出，查看Pod的最终状态。</h2><h3 id="（1）执行一下上面pod的yaml文件">（1）执行一下上面pod的yaml文件</h3><pre><code>[root@master ~]# kubectl apply -f pod.yaml </code></pre><h3 id="（2）动态查看ns中test-pod的信息">（2）动态查看ns中test-pod的信息</h3><pre><code>[root@master ~]# kubectl get pod -n xgp  -w | grep test-pod</code></pre><p>![image-20200109135543482](G:\四期\虚拟化\kubernetes\k8s文档\06 pod资源对象.assets\image-20200109135543482.png)</p><blockquote><p><strong>删除test-pod</strong></p><pre><code>[root@master ~]# kubectl delete pod -n xgp test-pod </code></pre></blockquote><h2 id="4-创建一个Service资源对象，与上述Pod对象关联，验证他们的关联性。">4) 创建一个Service资源对象，与上述Pod对象关联，验证他们的关联性。</h2><h3 id="（1）修改pod的yaml文件-2">（1）修改pod的yaml文件</h3><pre><code>[root@master ~]# vim pod.yamlkind: PodapiVersion: v1metadata:  name: test-pod  namespace: xgp  labels:    app: test-webspec:  restartPolicy: Never  containers:  - name: www    image: 192.168.1.21:5000/web:v1    imagePullPolicy: Never    ports:    - protocol: TCP      containerPort: 80</code></pre><h3 id="（1）编写service的yaml文件">（1）编写service的yaml文件</h3><pre><code>[root@master ~]# vim svc.yaml apiVersion: v1kind: Servicemetadata:  name: test-svc  namespace: xgpspec:  selector:    app: test-web  ports:  - port: 80    targetPort: 80</code></pre><h3 id="（2）执行一下">（2）执行一下</h3><pre><code>[root@master ~]# kubectl apply -f svc.yaml </code></pre><h3 id="（3）查看一下">（3）查看一下</h3><pre><code>[root@master ~]# kubectl get  pod -o wide -n xgp </code></pre><p>![image-20200109141712910](G:\四期\虚拟化\kubernetes\k8s文档\06 pod资源对象.assets\image-20200109141712910.png)</p><h3 id="（4）访问一下">（4）访问一下</h3><pre><code>[root@master ~]# curl 10.244.1.21</code></pre><p>![image-20200109141749352](G:\四期\虚拟化\kubernetes\k8s文档\06 pod资源对象.assets\image-20200109141749352.png)</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>05 Delpoyment、service</title>
      <link href="/posts/936d.html"/>
      <url>/posts/936d.html</url>
      
        <content type="html"><![CDATA[<h1>Deployment介绍</h1><p><strong>Deployment是kubernetes 1.2引入的概念，用来解决Pod的编排问题。Deployment可以理解为RC的升级版（RC+Reolicat Set）。特点在于可以随时知道Pod的部署进度，即对Pod的创建、调度、绑定节点、启动容器完整过程的进度展示。</strong></p><h2 id="使用场景">使用场景</h2><blockquote><p>创建一个Deployment对象来生成对应的Replica Set并完成Pod副本的创建过程。<br>检查Deployment的状态来确认部署动作是否完成（Pod副本的数量是否达到预期值）。<br>更新Deployment以创建新的Pod(例如镜像升级的场景)。<br>如果当前Deployment不稳定，回退到上一个Deployment版本。<br>挂起或恢复一个Deployment。</p></blockquote><h1>Service介绍</h1><p><img src="https://img-blog.csdn.net/20170809212910268?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaHV3aF8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p><p><strong>Service定义了一个服务的访问入口地址，前端应用通过这个入口地址访问其背后的一组由Pod副本组成的集群实例，Service与其后端的Pod副本集群之间是通过Label Selector来实现“无缝对接”。RC保证Service的Pod副本实例数目保持预期水平。</strong></p><h2 id="外部系统访问Service的问题">外部系统访问Service的问题</h2><table><thead><tr><th style="text-align:left">IP类型</th><th style="text-align:left">说明</th></tr></thead><tbody><tr><td style="text-align:left">Node IP</td><td style="text-align:left">Node节点的IP地址</td></tr><tr><td style="text-align:left">Pod IP</td><td style="text-align:left">Pod的IP地址</td></tr><tr><td style="text-align:left">Cluster IP</td><td style="text-align:left">Service的IP地址</td></tr></tbody></table><h1>环境介绍</h1><table><thead><tr><th>主机</th><th>IP地址</th><th>服务</th></tr></thead><tbody><tr><td><strong>master</strong></td><td><strong>192.168.1.21</strong></td><td><strong>k8s</strong></td></tr><tr><td><strong>node01</strong></td><td><strong>192.168.1.22</strong></td><td><strong>k8s</strong></td></tr><tr><td><strong>node02</strong></td><td><strong>192.168.1.23</strong></td><td><strong>k8s</strong></td></tr></tbody></table><h1>一，Delpoyment和service的简单使用</h1><h2 id="1-练习写一个yaml文件，要求使用自己的私有镜像，要求副本数量为三个。">1.练习写一个yaml文件，要求使用自己的私有镜像，要求副本数量为三个。</h2><pre><code>[root@master ~]# vim xgp.yamlkind: DeploymentapiVersion: extensions/v1beta1metadata:  name: xgp-webspec:  replicas: 3  template:    metadata:      labels:        app: xgp-server    spec:      containers:      - name: web        image: 192.168.1.21:5000/web:v1</code></pre><h3 id="（1）执行一下">（1）执行一下</h3><pre><code>[root@master ~]# kubectl apply -f xgp.yaml  --recore</code></pre><h3 id="（2）查看一下">（2）查看一下</h3><pre><code>[root@master ~]# kubectl get pod</code></pre><p>![image-20200108090638488](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108090638488.png)</p><h3 id="（3）访问一下">（3）访问一下</h3><pre><code>[root@master ~]# curl 10.244.2.16</code></pre><p>![image-20200108090817058](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108090817058.png)</p><h3 id="（4）更新一下yaml文件，副本加一">（4）更新一下yaml文件，副本加一</h3><pre><code>[root@master ~]# vim xgp.yamlkind: DeploymentapiVersion: extensions/v1beta1metadata:  name: xgp-webspec:  replicas: 4  template:    metadata:      labels:        app: xgp-server    spec:      containers:      - name: web        image: 192.168.1.21:5000/web:v1</code></pre><h4 id="1-执行一下">&lt;1&gt;执行一下</h4><pre><code>[root@master ~]# kubectl apply -f xgp.yaml --recore</code></pre><h4 id="2-查看一下">&lt;2&gt;查看一下</h4><pre><code>[root@master ~]# kubectl get pod</code></pre><p>![image-20200108091104534](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108091104534.png)</p><p><em><strong>副本数量加一，如果yaml文件的副本为0，则副本数量还是之前的状态，并不会更新。</strong></em></p><h2 id="2-练习写一个service文件">2.练习写一个service文件</h2><pre><code>[root@master ~]# vim xgp-svc.yamlkind: ServiceapiVersion: v1metadata:  name: xgp-svcspec:  selector:    app: xgp-server  ports:    - protocol: TCP      port: 80      targetPort: 80</code></pre><h3 id="（1）执行一下-2">（1）执行一下</h3><pre><code>[root@master ~]# kubectl apply -f xgp-svc.yaml </code></pre><h3 id="（2）查看一下-2">（2）查看一下</h3><pre><code>[root@master ~]# kubectl get svc</code></pre><p>![image-20200108091909396](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108091909396.png)</p><h3 id="（3）访问一下-2">（3）访问一下</h3><pre><code>[root@master ~]# curl 10.107.119.49</code></pre><p>![image-20200108092011164](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108092011164.png)</p><h2 id="3-修改yaml文件">3.修改yaml文件</h2><pre><code>[root@master ~]# vim xgp.yaml kind: DeploymentapiVersion: extensions/v1beta1metadata:  name: xgp-webspec:  replicas: 3  template:    metadata:      labels:        app: xgp-server    spec:      containers:      - name: web        image: 192.168.1.21:5000/web:v1        ports:          - containerPort: 80  #提示端口</code></pre><p><em><strong>注意：在Delpoyment资源对象中，可以添加Port字段，但此字段仅供用户查看，并不实际生效</strong></em></p><h3 id="执行一下">执行一下</h3><pre><code>[root@master ~]# kubectl apply -f xgp.yaml --recore</code></pre><h2 id="4-service文件映射端口">4.service文件映射端口</h2><pre><code>[root@master ~]# vim xgp-svc.yaml kind: ServiceapiVersion: v1metadata:  name: xgp-svcspec:  type: NodePort  selector:    app: xgp-server  ports:    - protocol: TCP      port: 80      targetPort: 80      nodePort: 30123</code></pre><h3 id="执行一下-2">执行一下</h3><pre><code>[root@master ~]# kubectl apply -f xgp-svc.yaml </code></pre><h3 id="查看一下">查看一下</h3><pre><code>[root@master ~]# kubectl get svc</code></pre><p>![image-20200108094404773](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108094404773.png)</p><h3 id="访问一下">访问一下</h3><pre><code>[root@master ~]# curl 127.0.0.1:30123</code></pre><p>![image-20200108094439682](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108094439682.png)</p><p>![image-20200108094501253](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108094501253.png)</p><h2 id="5-修改三个pod页面内容">5.修改三个pod页面内容</h2><h3 id="（1）查看一下pod信息">（1）查看一下pod信息</h3><pre><code>[root@master ~]# kubectl get pod -o wide</code></pre><p>![image-20200108094953119](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108094953119.png)</p><h3 id="（2）修改POD页面内容（三台不一样）">（2）修改POD页面内容（三台不一样）</h3><pre><code>[root@master ~]# kubectl exec -it xgp-web-8d5f9656f-8z7d9 /bin/bash//根据pod名称进入pod之中</code></pre><h3 id="进入容器后修改页面内容">进入容器后修改页面内容</h3><pre><code>root@xgp-web-8d5f9656f-8z7d9:/usr/local/apache2# echo xgp-v1 &gt; htdocs/index.html root@xgp-web-8d5f9656f-8z7d9:/usr/local/apache2# exit</code></pre><h3 id="访问一下-2">访问一下</h3><pre><code>[root@master ~]# curl 127.0.0.1:30123</code></pre><p>![image-20200108095626532](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108095626532.png)</p><h1>二.分析一下k8s负载均衡原理</h1><h3 id="（1）查看service的暴露IP">（1）查看service的暴露IP</h3><pre><code>[root@master ~]# kubectl get svc</code></pre><p>![image-20200108101539835](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108101539835.png)</p><h3 id="（2）查看一下iptabes规则">（2）查看一下iptabes规则</h3><pre><code>[root@master ~]# iptables-save //查看已配置的规则</code></pre><blockquote><p>SNAT：Source NAT（源地址转换）</p><p>DNAT：Destination NAT（目标地址转换）</p><p>MASQ：动态的源地址转换</p></blockquote><h3 id="（3）根据service的暴露IP，查看对应的iptabes规则">（3）根据service的暴露IP，查看对应的iptabes规则</h3><pre><code>[root@master ~]# iptables-save | grep 10.107.119.49</code></pre><p>![image-20200108101726315](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108101726315.png)</p><pre><code>[root@master ~]# iptables-save | grep KUBE-SVC-ESI7C72YHAUGMG5S</code></pre><p>![image-20200108102003596](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108102003596.png)</p><h3 id="（4）对应一下IP是否一致">（4）对应一下IP是否一致</h3><pre><code>[root@master ~]# iptables-save | grep KUBE-SEP-ZHDQ73ZKUBMELLJB</code></pre><p>![image-20200108102137062](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108102137062.png)</p><pre><code>[root@master ~]# kubectl get pod -o wide</code></pre><p>![image-20200108102203144](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108102203144.png)</p><p><strong>Service实现的负载均衡：默认使用的是iptables规则。IPVS</strong></p><h1>三.回滚到指定版本</h1><h3 id="（1）删除之前创建的delpoy和service">（1）删除之前创建的delpoy和service</h3><pre><code>[root@master ~]# kubectl  delete -f xgp.yaml [root@master ~]# kubectl  delete -f xgp-svc.yaml </code></pre><h3 id="（2）准备三个版本所使用的私有镜像，来模拟每次升级不同的镜像">（2）准备三个版本所使用的私有镜像，来模拟每次升级不同的镜像</h3><pre><code>[root@master ~]# vim xgp1.yaml  （三个文件名不相同）kind: DeploymentapiVersion: extensions/v1beta1metadata:  name: xgp-webspec:  revisionHistoryLimit: 10  replicas: 3  template:    metadata:      labels:        app: xgp-server    spec:      containers:      - name: web        image: 192.168.1.21:5000/web:v1  （三台版本不同）        ports:          - containerPort: 80</code></pre><p>此处3个yaml文件 指定不同版本的镜像</p><h3 id="（3）运行三个服务，并记录三个版本信息">（3）运行三个服务，并记录三个版本信息</h3><pre><code>[root@master ~]# kubectl apply -f xgp-1.yaml --record [root@master ~]# kubectl apply -f xgp-2.yaml --record [root@master ~]# kubectl apply -f xgp-3.yaml --record </code></pre><h3 id="（4）查看有哪些版本信息">（4）查看有哪些版本信息</h3><pre><code>[root@master ~]# kubectl rollout history deployment xgp-web </code></pre><p>![image-20200108105842447](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108105842447.png)</p><h3 id="（5）运行之前的service文件">（5）运行之前的service文件</h3><pre><code>[root@master ~]# kubectl apply -f xgp-svc.yaml</code></pre><h3 id="（6）查看service暴露端口">（6）查看service暴露端口</h3><pre><code>[root@master ~]# kubectl get svc</code></pre><p>![image-20200108110014614](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108110014614.png)</p><h3 id="（7）测试访问">（7）测试访问</h3><pre><code>[root@master ~]# curl 127.0.0.1:30123</code></pre><p>![image-20200108110049396](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108110049396.png)</p><h3 id="（8）回滚到指定版本">（8）回滚到指定版本</h3><pre><code>[root@master ~]# kubectl rollout undo deployment xgp-web --to-revision=1//这里指定的是版本信息的编号</code></pre><h4 id="1-访问一下">&lt;1&gt;访问一下</h4><pre><code>[root@master ~]# curl 127.0.0.1:30123</code></pre><p>![image-20200108110337266](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108110337266.png)</p><h4 id="2-查看有哪些版本信息">&lt;2&gt;查看有哪些版本信息</h4><pre><code>[root@master ~]# kubectl rollout history deployment xgp-web </code></pre><p>![image-20200108110443558](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108110443558.png)</p><p><em><strong>编号1已经被编号2替代，从而生的是一个新的编号4</strong></em></p><h1>四.用label控制pod的位置</h1><blockquote><p>默认情况下，scheduler会将pod调度到所有可用的Node，不过有些情况我们希望将 Pod 部署到指定的 Node，比如将有大量磁盘 I/O 的 Pod 部署到配置了 SSD 的 Node；或者 Pod 需要 GPU，需要运行在配置了 GPU 的节点上。</p><p>kubernetes通过label来实现这个功能</p><p>label 是 key-value 对，各种资源都可以设置 label，灵活添加各种<strong>自定义属性</strong>。比如执行如下命令标注 k8s-node1 是配置了 SSD 的节点</p></blockquote><h4 id="首先我们给node1节点打上一个ssd的标签">首先我们给node1节点打上一个ssd的标签</h4><pre><code>[root@master ~]# kubectl label nodes node02 disk=ssd</code></pre><h3 id="（1）查看标签">（1）查看标签</h3><pre><code>[root@master ~]# kubectl get nodes --show-labels | grep node02</code></pre><p>![image-20200108111354832](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108111354832.png)</p><h3 id="（2）删除副本一">（2）删除副本一</h3><pre><code>[root@master ~]# kubectl delete -f xgp-1.yaml deployment.extensions &quot;xgp-web&quot; deleted[root@master ~]# kubectl delete svc xgp-svc </code></pre><h3 id="（3）修改副本一的yaml文件">（3）修改副本一的yaml文件</h3><pre><code>[root@master ~]# vim xgp-1.yaml kind: DeploymentapiVersion: extensions/v1beta1metadata:  name: xgp-webspec:  revisionHistoryLimit: 10  replicas: 3  template:    metadata:      labels:        app: xgp-server    spec:      containers:      - name: web        image: 192.168.1.21:5000/web:v1        ports:          - containerPort: 80      nodeSelector:    #添加节点选择器        disk: ssd      #和标签内容一致</code></pre><h3 id="（4）执行一下">（4）执行一下</h3><pre><code>[root@master ~]# kubectl apply -f xgp-1.yaml </code></pre><h4 id="查看一下-2">查看一下</h4><pre><code>[root@master ~]# kubectl get pod -o wide</code></pre><p>![image-20200108112059395](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108112059395.png)</p><p><em><strong>现在pod都在node02上运行</strong></em></p><h3 id="（5）删除标签">（5）删除标签</h3><pre><code>[root@master ~]# kubectl  label nodes node02 disk-</code></pre><h4 id="查看一下-3">查看一下</h4><pre><code>[root@master ~]# kubectl get nodes --show-labels | grep node02</code></pre><p>![image-20200108112245347](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108112245347.png)</p><p><em><strong>没有disk标签了</strong></em></p><h1>五，小实验</h1><h3 id="1）使用私有镜像v1版本部署一个Deployment资源对象，要求副本Pod数量为3个，并创建一个Service资源对象相互关联，指定要求3个副本Pod全部运行在node01节点上，记录一个版本。"><strong>1）使用私有镜像v1版本部署一个Deployment资源对象，要求副本Pod数量为3个，并创建一个Service资源对象相互关联，指定要求3个副本Pod全部运行在node01节点上，记录一个版本。</strong></h3><h4 id="（1）用label控制pod的位置">（1）用label控制pod的位置</h4><pre><code>[root@master ~]# kubectl label nodes node01 disk=ssd</code></pre><h4 id="（2）编写源yaml文件">（2）编写源yaml文件</h4><pre><code>[root@master ~]# vim xgp.yamlkind: DeploymentapiVersion: extensions/v1beta1metadata:  name: xgp-webspec:  replicas: 3  template:    metadata:      labels:        app: xgp-server    spec:      containers:      - name: web        image: 192.168.1.21:5000/web:v1        ports:          - containerPort: 80      nodeSelector:            disk: ssd  </code></pre><h4 id="（3）编写源service文件">（3）编写源service文件</h4><pre><code>[root@master ~]# vim xgp-svc.yamlkind: ServiceapiVersion: v1metadata:  name: xgp-svcspec:  type: NodePort  selector:    app: xgp-server  ports:    - protocol: TCP      port: 80      targetPort: 80      nodePort: 30123</code></pre><h4 id="（4）执行yaml文件，创建控制器。执行service文件创建映射端口">（4）执行yaml文件，创建控制器。执行service文件创建映射端口</h4><pre><code>[root@master ~]# kubectl apply -f  xgp.yaml [root@master ~]# kubectl apply -f xgp-svc.yaml </code></pre><h4 id="（5）查看一下pod节点">（5）查看一下pod节点</h4><pre><code>[root@master ~]# kubectl get pod -o wide</code></pre><p>![image-20200108122424654](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108122424654.png)</p><h4 id="（6）记录一个版本">（6）记录一个版本</h4><pre><code>[root@master ~]# kubectl rollout history deployment xgp-web &gt; pod.txt</code></pre><p>![image-20200108142016701](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108142016701.png)</p><h4 id="（7）访问一下">（7）访问一下</h4><p>![image-20200108122518278](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108122518278.png)</p><p>![image-20200108122534683](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108122534683.png)</p><h3 id="2）根据上述Deployment，升级为v2版本，记录一个版本。"><strong>2）根据上述Deployment，升级为v2版本，记录一个版本。</strong></h3><h4 id="（1）修改yaml文件镜像版本">（1）修改yaml文件镜像版本</h4><pre><code>[root@master ~]# vim xgp.yaml kind: DeploymentapiVersion: extensions/v1beta1metadata:  name: xgp-webspec:  replicas: 3  template:    metadata:      labels:        app: xgp-server    spec:      containers:      - name: web        image: 192.168.1.21:5000/web:v2    #修改版本为二        ports:          - containerPort: 80      nodeSelector:        disk: ssd</code></pre><h4 id="（2）刷新一下yaml文件">（2）刷新一下yaml文件</h4><pre><code>[root@master ~]# kubectl apply -f xgp.yaml --recore</code></pre><h4 id="（3）访问一下-3">（3）访问一下</h4><p>![image-20200108141825924](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108141825924.png)</p><h4 id="（4）记录一个版本">（4）记录一个版本</h4><pre><code>[root@master ~]# kubectl rollout history deployment xgp-web &gt; pod.txt</code></pre><p>![image-20200108142030157](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108142030157.png)</p><h3 id="3）最后升级到v3版本，这时，查看Service关联，并且分析访问流量的负载均衡详细情况。"><strong>3）最后升级到v3版本，这时，查看Service关联，并且分析访问流量的负载均衡详细情况。</strong></h3><h4 id="1）修改yaml文件镜像版本">1）修改yaml文件镜像版本</h4><pre><code>[root@master ~]# vim xgp.yaml kind: DeploymentapiVersion: extensions/v1beta1metadata:  name: xgp-webspec:  replicas: 3  template:    metadata:      labels:        app: xgp-server    spec:      containers:      - name: web        image: 192.168.1.21:5000/web:v3   #修改版本为二        ports:          - containerPort: 80      nodeSelector:        disk: ssd</code></pre><h4 id="（2）刷新一下yaml文件-2">（2）刷新一下yaml文件</h4><pre><code>[root@master ~]# kubectl apply -f xgp.yaml --recore</code></pre><h4 id="（3）访问一下-4">（3）访问一下</h4><p>![image-20200108142329749](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108142329749.png)</p><h4 id="（5）分析访问流量的负载均衡详细情况">（5）分析访问流量的负载均衡详细情况</h4><h5 id="1-查看一下service映射端口">&lt;1&gt;查看一下service映射端口</h5><p>![image-20200108142504637](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108142504637.png)</p><h5 id="2-以ip为起点，分析访问流量的负载均衡详细情况">&lt;2&gt;以ip为起点，分析访问流量的负载均衡详细情况</h5><p><strong>Service实现的负载均衡：默认使用的是iptables规则。IPVS</strong></p><pre><code>[root@master ~]# iptables-save | grep 10.107.27.229//根据service的暴露IP，查看对应的iptabes规则</code></pre><p>![image-20200108143052433](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108143052433.png)</p><pre><code>[root@master ~]# iptables-save | grep KUBE-SVC-ESI7C72YHAUGMG5S</code></pre><p>![image-20200108143359463](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108143359463.png)</p><p><em><strong>这里显示了各节点的负载比例</strong></em></p><h5 id="3-对应一下IP是否一致">&lt;3&gt;对应一下IP是否一致</h5><pre><code>[root@master ~]# iptables-save | grep KUBE-SEP-VDKW5WQIWOLZMJ6G</code></pre><p>![image-20200108143547946](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108143547946.png)</p><pre><code>[root@master ~]# kubectl get pod -o wide</code></pre><p>![image-20200108143608942](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108143608942.png)</p><h3 id="4）回滚到指定版本v1，并作验证。"><strong>4）回滚到指定版本v1，并作验证。</strong></h3><h4 id="1-回滚到指定版本">&lt;1&gt;回滚到指定版本</h4><pre><code>[root@master ~]# kubectl rollout undo deployment xgp-web --to-revision=1//这里指定的是版本信息的编号</code></pre><h4 id="2-访问一下">&lt;2&gt;访问一下</h4><pre><code>[root@master ~]# curl 127.0.0.1:30123</code></pre><p>![image-20200108110337266](G:\四期\虚拟化\kubernetes\k8s文档\05 Delpoyment、service.assets\image-20200108110337266.png)</p><blockquote><p><strong>排错思路</strong></p><pre><code>[root@master ~]# less /var/log/messages  | grep kubelet[root@master ~]# kubectl  logs -n  kube-system kube-scheduler-master [root@master ~]# kubectl describe pod xgp-web-7d478f5bb7-bd4bj </code></pre></blockquote>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>04 配置清单</title>
      <link href="/posts/748b.html"/>
      <url>/posts/748b.html</url>
      
        <content type="html"><![CDATA[<h1>一，两种创建资源的方法</h1><h2 id="1-基于命令的方式：">1. 基于命令的方式：</h2><ol><li><strong>简单直观快捷，上手快。</strong></li><li><strong>适合临时测试或实验。</strong></li></ol><h2 id="2-基于配置清单的方式：">2. 基于配置清单的方式：</h2><ol><li><strong>配置文件描述了 <code>What</code>，即应用最终要达到的状态。</strong></li><li><strong>配置文件提供了创建资源的模板，能够重复部署。</strong></li><li><strong>可以像管理代码一样管理部署。</strong></li><li><strong>适合正式的、跨环境的、规模化部署。</strong></li><li><strong>这种方式要求熟悉配置文件的语法，有一定难度。</strong></li></ol><h2 id="环境介绍">环境介绍</h2><table><thead><tr><th>主机</th><th>IP地址</th><th>服务</th></tr></thead><tbody><tr><td>master</td><td>192.168.1.21</td><td>k8s</td></tr><tr><td>node01</td><td>192.168.1.22</td><td>k8s</td></tr><tr><td>node02</td><td>192.168.1.23</td><td>k8s</td></tr></tbody></table><h1>二. 配置清单（yam，yaml）</h1><p><strong>在k8s中，一般使用yaml格式的文件来创建符合我们预期期望的pod，这样的yaml文件我们一般称为资源清单</strong></p><blockquote><p><strong>/etc/kubernetes/manifests/</strong>    k8s存放（yam、yaml）文件的地方</p><p>**kubectl explain deployment（通过explain参数加上资源类别就能看到该资源应该怎么定义）</p><p><strong>kubectl explain deployment.metadata</strong> 通过资源类别加上带有Object标记的字段，我们就可以看到一级字段下二级字段的内容有那些怎么去定义等</p><p><strong>kubectl explain deployment.metadata.ownerReferences</strong> 通过加上不同级别的字段名称来看下字段下的内容，而且前面的[]号代表对象列表</p></blockquote><h2 id="1-常见yaml文件写法，以及字段的作用">1.常见yaml文件写法，以及字段的作用</h2><p><strong>(1) apiVersion：api版本信息</strong></p><p><em><strong>（用来定义当前属于哪个组和那个版本，这个直接关系到最终提供使用的是那个版本）</strong></em></p><pre><code>[root@master manifests]# kubectl api-versions//查看到当前所有api的版本</code></pre><p><strong>(2) kind: 资源对象的类别</strong></p><p><em><strong>(用来定义创建的对象是属于什么类别，是pod，service，还是deployment等对象，可以按照其固定的语法格式来自定义。)</strong></em><br><strong>(3) metadata: 元数据 名称字段（必写）</strong></p><blockquote><p><strong>提供以下几个字段</strong>：<br>　　<strong>creationTimestamp: &quot;2019-06-24T12:18:48Z&quot;</strong><br>　　<strong>generateName: myweb-5b59c8b9d-</strong><br>　　<strong>labels: （对象标签）</strong><br>　　　　<strong>pod-template-hash: 5b59c8b9d</strong><br>　　　　<strong>run: myweb</strong><br>　　<strong>name: myweb-5b59c8b9d-gwzz5 （pods对象的名称，同一个类别当中的pod对象名称是唯一的，不能重复）</strong><br>　　<strong>namespace: default （对象所属的名称空间，同一名称空间内可以重复，这个名称空间也是k8s级别的名称空间，不和容器的名称空间混淆）</strong><br>　　<strong>ownerReferences:</strong></p><p>- <strong>apiVersion: apps/v1</strong><br>　　　　<strong>blockOwnerDeletion: true</strong><br>　　　　<strong>controller: true</strong><br>　　　　<strong>kind: ReplicaSet</strong><br>　　　　<strong>name: myweb-5b59c8b9d</strong><br>　　　　<strong>uid: 37f38f64-967a-11e9-8b4b-000c291028e5</strong><br>　　<strong>resourceVersion: &quot;943&quot;</strong><br>　　<strong>selfLink: /api/v1/namespaces/default/pods/myweb-5b59c8b9d-gwzz5</strong><br>　　<strong>uid: 37f653a6-967a-11e9-8b4b-000c291028e5</strong><br>　　<strong>annotations（资源注解，这个需要提前定义，默认是没有的）</strong><br><strong>通过这些标识定义了每个资源引用的path：即/api/group/version/namespaces/名称空间/资源类别/对象名称</strong></p></blockquote><p><strong>(4) spec： 用户期望的状态</strong></p><p><em><strong>（这个字段最重要，因为spec是用来定义目标状态的‘disired state’，而且资源不通导致spec所嵌套的字段也各不相同，也就因为spec重要且字段不相同，k8s在内部自建了一个spec的说明用于查询）</strong></em></p><p><strong>(5) status：资源现在处于什么样的状态</strong></p><p><em><strong>（当前状态，’current state‘，这个字段有k8s集群来生成和维护，不能自定义，属于一个只读字段）</strong></em></p><h2 id="2-编写一个yaml文件">2.编写一个yaml文件</h2><pre><code>[root@master ~]# vim web.yamlkind: Deployment  #资源对象是控制器apiVersion: extensions/v1beta1   #api的版本metadata:      #描述kind（资源类型）  name: web   #定义控制器名称spec:  replicas: 2   #副本数量  template:     #模板    metadata:          labels:   #标签        app: web_server    spec:      containers:   #指定容器      - name: nginx  #容器名称        image: nginx   #使用的镜像</code></pre><h3 id="执行一下">执行一下</h3><pre><code>[root@master ~]# kubectl apply -f web.yaml </code></pre><h3 id="查看一下">查看一下</h3><pre><code>[root@master ~]# kubectl get deployments.  -o wide//查看控制器信息</code></pre><p><img src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5C%E8%99%9A%E6%8B%9F%E5%8C%96%5Ckubernetes%5Ck8s%E6%96%87%E6%A1%A3%5C04%E9%85%8D%E7%BD%AE%E6%B8%85%E5%8D%95.assets%5Cimage-20200107100450262.png" alt="image-20200107100450262"></p><pre><code>[root@master ~]# kubectl get pod -o wide//查看pod节点信息</code></pre><p><img src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5C%E8%99%9A%E6%8B%9F%E5%8C%96%5Ckubernetes%5Ck8s%E6%96%87%E6%A1%A3%5C04%E9%85%8D%E7%BD%AE%E6%B8%85%E5%8D%95.assets%5Cimage-20200107101803209.png" alt="image-20200107101803209"></p><h2 id="3-编写一个service-yaml文件">3.编写一个service.yaml文件</h2><pre><code>[root@master ~]# vim web-svc.yamlkind: Service  #资源对象是副本apiVersion: v1   #api的版本metadata:  name: web-svcspec:  selector:     #标签选择器    app: web-server  #须和web.yaml的标签一致  ports:              #端口  - protocol: TCP    port: 80            #宿主机的端口    targetPort: 80      #容器的端口</code></pre><blockquote><p><strong>使用相同标签和标签选择器内容，使两个资源对象相互关联。</strong></p><p><strong>创建的service资源对象，默认的type为ClusterIP，意味着集群内任意节点都可访问。它的作用是为后端真正服务的pod提供一个统一的接口。如果想要外网能够访问服务，应该把type改为NodePort</strong></p></blockquote><h3 id="（1）执行一下">（1）执行一下</h3><pre><code>[root@master ~]# kubectl apply -f web-svc.yaml </code></pre><h3 id="（2）查看一下">（2）查看一下</h3><pre><code>[root@master ~]# kubectl get svc//查看控制器信息</code></pre><p><img src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5C%E8%99%9A%E6%8B%9F%E5%8C%96%5Ckubernetes%5Ck8s%E6%96%87%E6%A1%A3%5C04%E9%85%8D%E7%BD%AE%E6%B8%85%E5%8D%95.assets%5Cimage-20200107110717972.png" alt="image-20200107110717972"></p><h3 id="（3）访问一下">（3）访问一下</h3><pre><code>[root@master ~]# curl 10.111.193.168</code></pre><p><img src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5C%E8%99%9A%E6%8B%9F%E5%8C%96%5Ckubernetes%5Ck8s%E6%96%87%E6%A1%A3%5C04%E9%85%8D%E7%BD%AE%E6%B8%85%E5%8D%95.assets%5Cimage-20200107110837353.png" alt="image-20200107110837353"></p><h2 id="4-外网能够访问服务">4.外网能够访问服务</h2><h3 id="（1）修改web-svc-yaml文件">（1）修改web-svc.yaml文件</h3><pre><code>kind: Service  #资源对象是副本apiVersion: v1   #api的版本metadata:  name: web-svcspec:  type: NodePort    #添加 更改网络类型  selector:     #标签选择器    app: web_server  #须和web.yaml的标签一致  ports:              #端口  - protocol: TCP    port: 80            #宿主机的端口    targetPort: 80      #容器的端口    nodePort: 30086     #指定群集映射端口，范围是30000-32767</code></pre><h3 id="（2）刷新一下">（2）刷新一下</h3><pre><code>[root@master ~]#  kubectl apply -f web-svc.yaml </code></pre><h3 id="（3）查看一下">（3）查看一下</h3><pre><code>[root@master ~]# kubectl get svc</code></pre><p><img src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5C%E8%99%9A%E6%8B%9F%E5%8C%96%5Ckubernetes%5Ck8s%E6%96%87%E6%A1%A3%5C04%E9%85%8D%E7%BD%AE%E6%B8%85%E5%8D%95.assets%5Cimage-20200107111338940.png" alt="image-20200107111338940"></p><h3 id="（4）浏览器测试">（4）浏览器测试</h3><p><img src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5C%E8%99%9A%E6%8B%9F%E5%8C%96%5Ckubernetes%5Ck8s%E6%96%87%E6%A1%A3%5C04%E9%85%8D%E7%BD%AE%E6%B8%85%E5%8D%95.assets%5Cimage-20200107111451952.png" alt="image-20200107111451952"></p><h1>三、小实验</h1><blockquote><p><strong>基于上一篇博客实验继续进行</strong></p></blockquote><h3 id="1-使用yaml文件的方式创建一个Deployment资源对象，要求镜像使用个人私有镜像v1版本。replicas为3个。">1.使用yaml文件的方式创建一个Deployment资源对象，要求镜像使用个人私有镜像v1版本。replicas为3个。</h3><h3 id="编写yaml文件">编写yaml文件</h3><pre><code>[root@master ~]# vim www.yamlkind: DeploymentapiVersion: extensions/v1beta1metadata:  name: xgpspec:  replicas: 3  template:    metadata:      labels:        app: www_server    spec:      containers:      - name: web        image: 192.168.1.21:5000/web:v1   </code></pre><h4 id="（1）执行一下-2">（1）执行一下</h4><pre><code>[root@master ~]# kubectl apply -f web-svc.yaml </code></pre><h4 id="（2）查看一下-2">（2）查看一下</h4><pre><code>[root@master ~]# kubectl get deployments. -o wide//查看控制器信息</code></pre><p><img src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5C%E8%99%9A%E6%8B%9F%E5%8C%96%5Ckubernetes%5Ck8s%E6%96%87%E6%A1%A3%5C04%E9%85%8D%E7%BD%AE%E6%B8%85%E5%8D%95.assets%5Cimage-20200107120901208.png" alt="image-20200107120901208"></p><pre><code>[root@master ~]# kubectl get pod -o wide//查看pod节点信息</code></pre><p><img src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5C%E8%99%9A%E6%8B%9F%E5%8C%96%5Ckubernetes%5Ck8s%E6%96%87%E6%A1%A3%5C04%E9%85%8D%E7%BD%AE%E6%B8%85%E5%8D%95.assets%5Cimage-20200107121002152.png" alt="image-20200107121002152"></p><h4 id="（3）访问一下-2">（3）访问一下</h4><p><img src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5C%E8%99%9A%E6%8B%9F%E5%8C%96%5Ckubernetes%5Ck8s%E6%96%87%E6%A1%A3%5C04%E9%85%8D%E7%BD%AE%E6%B8%85%E5%8D%95.assets%5Cimage-20200107121147669.png" alt="image-20200107121147669"></p><h3 id="2-使用yaml文件的方式创建一个Service资源对象，要与上述Deployment资源对象关联，type类型为：-NodePort，端口为-30123"><strong>2.</strong>  使用yaml文件的方式创建一个Service资源对象，要与上述Deployment资源对象关联，type类型为： NodePort，端口为:30123.</h3><h4 id="编写service文件">编写service文件</h4><pre><code>[root@master ~]# vim www-svc.yamlkind: ServiceapiVersion: v1metadata:  name: www-svcspec:  type: NodePort  selector:    app: www_server  ports:  - protocol: TCP    port: 80    targetPort: 80    nodePort: 30123</code></pre><h4 id="执行一下-2">执行一下</h4><pre><code>[root@master ~]# kubectl apply -f www-svc.yaml </code></pre><h4 id="查看一下-2">查看一下</h4><pre><code>[root@master ~]# kubectl get svc</code></pre><p><img src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5C%E8%99%9A%E6%8B%9F%E5%8C%96%5Ckubernetes%5Ck8s%E6%96%87%E6%A1%A3%5C04%E9%85%8D%E7%BD%AE%E6%B8%85%E5%8D%95.assets%5Cimage-20200107121929525.png" alt="image-20200107121929525"></p><h4 id="访问一下">访问一下</h4><p><img src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5C%E8%99%9A%E6%8B%9F%E5%8C%96%5Ckubernetes%5Ck8s%E6%96%87%E6%A1%A3%5C04%E9%85%8D%E7%BD%AE%E6%B8%85%E5%8D%95.assets%5Cimage-20200107122015559.png" alt="image-20200107122015559"></p><h1>四. 总结</h1><h2 id="1-Pod的作用"><strong>1. Pod的作用</strong></h2><blockquote><p>在k8s中pod是最小的管理单位，在一个pod中通常会包含一个或多个容器。大多数情况下，一个Pod内只有一个Container容器。<br>在每一个Pod中都有一个特殊的Pause容器和一个或多个业务容器，Pause来源于pause-amd64镜像,Pause容器在Pod中具有非常重要的作用：</p><ul><li>Pause容器作为Pod容器的根容器，其本地于业务容器无关，它的状态代表了整个pod的状态。</li><li>Pod里的多个业务容器共享Pause容器的IP，每个Pod被分配一个独立的IP地址，Pod中的每个容器共享网络命名空间，包括IP地址和网络端口。Pod内的容器可以使用localhost相互通信。k8s支持底层网络集群内任意两个Pod之间进行通信。</li><li>Pod中的所有容器都可以访问共享volumes，允许这些容器共享数据。volumes还用于Pod中的数据持久化，以防其中一个容器需要重新启动而丢失数据。</li></ul></blockquote><h2 id="2-Service的作用"><strong>2. Service的作用</strong></h2><p><strong>Service 是后端真实服务的抽象，一个 Service 可以代表多个相同的后端服务</strong></p><p><strong>Service 为 POD 控制器控制的 POD 集群提供一个固定的访问端点，Service 的工作还依赖于 K8s 中的一个附件，就是 CoreDNS ，它将 Service 地址提供一个域名解析。</strong></p><h3 id="NodePort-类型的-service">NodePort 类型的 service</h3><blockquote><p><strong>clusterIP</strong>：指定 Service 处于 service 网络的哪个 IP，默认为动态分配</p><p><strong>NodePort 是在 ClusterIP 类型上增加了一个暴露在了 node 的网络命名空间上的一个 nodePort，所以用户可以从集群外部访问到集群了，因而用户的请求流程是：Client -&gt; NodeIP:NodePort -&gt; ClusterIP:ServicePort -&gt; PodIP:ContainerPort。</strong></p><p><strong>可以理解为 NodePort 增强了 ClusterIP 的功能，让客户端可以在每个集群外部访问任意一个 nodeip 从而访问到 clusterIP，再由 clusterIP 进行负载均衡至 POD。</strong></p></blockquote><h2 id="3-流量走向">3.流量走向</h2><p><strong>我们在创建完成一个服务之后，用户首先应该访问的是nginx反向代理的ip，然后通过nginx访问到后端的k8s服务器（master节点）的“NodePort暴露IP 及 映射的端口“，master的apiserver接受到客户端发送来的访问指令，将访问指令通知Controller Manager控制器，Scheduler执行调度任务，将访问指令分发到各节点之上，通过”master节点“的“ip+映射端口”访问到后端k8s节点的信息，节点的Kubelet（pod代理）当Scheduler确定让那个节点返回访问信息之后，kube-proxy将访问信息负载均衡到该节点的容器上，各容器返回信息，并向Master报告运行状态</strong></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>03 创建资源的两种方式</title>
      <link href="/posts/6989.html"/>
      <url>/posts/6989.html</url>
      
        <content type="html"><![CDATA[<table><thead><tr><th>主机</th><th>IP地址</th><th></th></tr></thead><tbody><tr><td>master</td><td>192.168.1.21</td><td></td></tr><tr><td>node01</td><td>192.168.1.22</td><td></td></tr><tr><td>node02</td><td>192.168.1.23</td><td></td></tr></tbody></table><h1>两种创建资源的方法</h1><h2 id="基于命令的方式：">基于命令的方式：</h2><ol><li><strong>简单直观快捷，上手快。</strong></li><li><strong>适合临时测试或实验。</strong></li></ol><h2 id="基于配置文件的方式：">基于配置文件的方式：</h2><ol><li><strong>配置文件描述了 <code>What</code>，即应用最终要达到的状态。</strong></li><li><strong>配置文件提供了创建资源的模板，能够重复部署。</strong></li><li><strong>可以像管理代码一样管理部署。</strong></li><li><strong>适合正式的、跨环境的、规模化部署。</strong></li><li><strong>这种方式要求熟悉配置文件的语法，有一定难度。</strong></li></ol><h1>一，用命令行的方式创建资源</h1><h3 id="仅接受json格式"><em>仅接受json格式</em></h3><h2 id="配置清单（yml、yaml）">配置清单（yml、yaml）</h2><pre><code>[root@master ~]# cd /etc/kubernetes/manifests///k8s的yml、yaml文件</code></pre><h2 id="1-node01和node02下载nginx镜像">1.node01和node02下载nginx镜像</h2><pre><code>docker pull nginx//下载nginx镜像</code></pre><h2 id="2-master创建Pod控制器（test-web），deployment">2.master创建Pod控制器（test-web），deployment</h2><pre><code>[root@master ~]# kubectl run test-web --image=nginx --replicas=5//创建Pod控制器，deployment</code></pre><h2 id="3-查看控制器情况">3.查看控制器情况</h2><h3 id="（1）">（1）</h3><pre><code>[root@master ~]# kubectl get deployments.//查看控制器情况</code></pre><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106093615852.png" alt="image-20200106093615852"></p><pre><code>[root@master ~]# kubectl get pod --all-namespaces -o wide//显示pod的节点信息</code></pre><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106093922849.png" alt="image-20200106093922849"></p><h3 id="（2）">（2）</h3><pre><code>[root@master ~]# kubectl get namespaces //查看k8s名称空间</code></pre><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106093850247.png" alt="image-20200106093850247"></p><pre><code>[root@master ~]# kubectl describe deployments. test-web//查看资源详细信息</code></pre><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106093723330.png" alt="image-20200106093723330"></p><p><em><strong>查看某种资源对象，没有指定名称空间，默认是在default名称空间。可以加上-n选项，查看指定名称空间的资源。</strong></em></p><pre><code>[root@master ~]# kubectl get pod -n kube-system </code></pre><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106094343401.png" alt="image-20200106094343401"></p><h2 id="3-删除test-web控制器">3.删除test-web控制器</h2><pre><code>[root@master ~]# kubectl delete deployments. test-web </code></pre><h2 id="4-master创建Pod控制器（web），deployment">4.master创建Pod控制器（web），deployment</h2><pre><code>[root@master ~]# kubectl run web --image=nginx --replicas=5</code></pre><h3 id="查看一下pod信息">查看一下pod信息</h3><pre><code>[root@master ~]# kubectl get pod -o wide//查看一下pod的节点信息</code></pre><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106095722353.png" alt="image-20200106095722353"></p><pre><code>[root@master ~]# kubectl describe deployments. web //查看资源详细信息</code></pre><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106100606861.png" alt="image-20200106100606861"></p><p><em><strong>注意：直接运行创建的deployment资源对象，是经常使用的一个控制器资源类型，除了deployment，还有rc、rs等等pod控制器，deployment是一个高级的pod控制器。</strong></em></p><h3 id="本机测试访问nginx">本机测试访问nginx</h3><pre><code>[root@master ~]# curl 10.244.1.7</code></pre><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106100827131.png" alt="image-20200106100827131"></p><h2 id="5-创建service资源类型">5.创建service资源类型</h2><pre><code>[root@master ~]# kubectl expose deployment web --name=web-xgp --port=80 --type=NodePort//创建service资源类型，这里我们设置了映射端口</code></pre><p><em><strong>如果想要外网能够访问服务，可以暴露deployment资源，得到service资源，但svc资源的类型必须为NodePort。</strong></em></p><p><strong>映射端口范围：30000-32767</strong></p><h3 id="查看service信息">查看service信息</h3><pre><code>[root@master ~]# kubectl get svc</code></pre><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106101443348.png" alt="image-20200106101443348"></p><h3 id="浏览器测试访问http-192-168-1-21-30493">浏览器测试访问http://192.168.1.21:30493/</h3><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106101624954.png" alt="image-20200106101624954"></p><h1>二、服务的扩容与缩容</h1><h2 id="1-查看控制器信息">1. 查看控制器信息</h2><pre><code>[root@master ~]# kubectl get deployments. -o wide</code></pre><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106104638757.png" alt="image-20200106104638757"></p><h2 id="2-扩容">2.扩容</h2><pre><code>[root@master ~]# kubectl scale deployment web --replicas=8</code></pre><h3 id="查看一下">查看一下</h3><pre><code>[root@master ~]# kubectl get deployments. -o wide</code></pre><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106104757123.png" alt="image-20200106104757123"></p><h2 id="3-缩容">3.缩容</h2><pre><code>[root@master ~]# kubectl scale deployment web --replicas=4</code></pre><h3 id="查看一下-2">查看一下</h3><pre><code>[root@master ~]# kubectl get deployments. -o wide</code></pre><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106105536316.png" alt="image-20200106105536316"></p><h2 id="3-通过修改web的yaml文件进行扩容缩容">3.通过修改web的yaml文件进行扩容缩容</h2><h3 id="备份web的yaml文件">备份web的yaml文件</h3><pre><code>[root@master ~]# kubectl get deployments. -o yaml &gt; web.yaml</code></pre><h3 id="使用edit修改web的yaml文件">使用edit修改web的yaml文件</h3><pre><code>[root@master ~]# kubectl edit deployments. web </code></pre><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106105924531.png" alt="image-20200106105924531"></p><h3 id="查看一下-3">查看一下</h3><pre><code>[root@master ~]# kubectl get deployments. -o wide</code></pre><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106105816339.png" alt="image-20200106105816339"></p><h1>三、服务的升级与回滚</h1><h2 id="node01和node02下载1-15版本的nginx">node01和node02下载1.15版本的nginx</h2><pre><code>[root@master ~]# docker pull nginx:1.15</code></pre><h2 id="1-master设置服务升级">1.master设置服务升级</h2><pre><code>[root@master ~]#  kubectl set image deployment web web=nginx:1.15</code></pre><h3 id="查看一下-4">查看一下</h3><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106111227960.png" alt="image-20200106111227960"></p><h2 id="2-master设置服务回滚">2.master设置服务回滚</h2><h3 id="（1）修改配置文件回滚">（1）修改配置文件回滚</h3><h3 id="使用edit修改web的yaml文件-2">使用edit修改web的yaml文件</h3><pre><code>[root@master ~]# kubectl edit deployments. web </code></pre><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106111523148.png" alt="image-20200106111523148"></p><h3 id="查看一下-5">查看一下</h3><pre><code>[root@master ~]# kubectl get deployments. -o wide</code></pre><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106111319699.png" alt="image-20200106111319699"></p><h3 id="（2）命令回滚">（2）命令回滚</h3><pre><code>[root@master ~]# kubectl rollout undo deployment web </code></pre><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106111733617.png" alt="image-20200106111733617"></p><p><em><strong>注意:只能回滚到上一次操作的状态</strong></em></p><h1>四、实验环境</h1><table><thead><tr><th>主机</th><th>IP地址</th><th>服务</th></tr></thead><tbody><tr><td>master</td><td>192.168.1.21</td><td>registry+Deployment</td></tr><tr><td>node01</td><td>192.168.1.22</td><td></td></tr><tr><td>node02</td><td>192.168.1.23</td><td></td></tr></tbody></table><h2 id="1-master-基于httpd制作自己的镜像，需要3个版本，v1-v2-v3-并且对应的版本镜像，访问的主目录内容不一样">1.master 基于httpd制作自己的镜像，需要3个版本，v1,v2,v3.并且对应的版本镜像，访问的主目录内容不一样</h2><h3 id="（1）master下载httpd镜像">（1）master下载httpd镜像</h3><pre><code>[root@master ~]# docker pull httpd</code></pre><h3 id="（2）编写Dockerfile">（2）编写Dockerfile</h3><pre><code>[root@master xgp]# vim DockerfileFROM httpdCOPY index.html /usr/local/apache2/htdocs/index.html</code></pre><h3 id="（3）创建测试网页v1">（3）创建测试网页v1</h3><pre><code>[root@master xgp]#echo &quot;&lt;h1&gt;xgp | test-web | httpd:v1&lt;h1&gt;&quot; &gt; index.html</code></pre><h3 id="（4）基于Dockerfile创建镜像-web1">（4）基于Dockerfile创建镜像 web1</h3><pre><code>[root@master xgp]# docker build -t web1 .</code></pre><h3 id="（5）创建测试网页v2">（5）创建测试网页v2</h3><pre><code>[root@master xgp]#echo &quot;&lt;h1&gt;xgp | test-web | httpd:v1&lt;h1&gt;&quot; &gt; index.html</code></pre><h3 id="（6）基于Dockerfile创建镜像-web2">（6）基于Dockerfile创建镜像 web2</h3><pre><code>[root@master xgp]# docker build -t web2 .</code></pre><h3 id="（7）创建测试网页v3">（7）创建测试网页v3</h3><pre><code>[root@master xgp]# echo &quot;&lt;h1&gt;xgp | test-web | httpd:v3&lt;h1&gt;&quot; &gt; index.html</code></pre><h3 id="（8）基于Dockerfile创建镜像-web3">（8）基于Dockerfile创建镜像 web3</h3><pre><code>[root@master xgp]# docker build -t web3 .</code></pre><h2 id="2-master部署私有仓库">2.master部署私有仓库</h2><h3 id="（1）master下载registry镜像">（1）master下载registry镜像</h3><pre><code>[root@master ~]# docker pull registry</code></pre><h3 id="（2）启动registry">（2）启动registry</h3><pre><code>[root@master xgp]# docker run -itd --name registry -p 5000:5000 --restart=always registry:latest </code></pre><h3 id="（3）修改docker配置文件，加入私有仓库（三台）">（3）修改docker配置文件，加入私有仓库（三台）</h3><pre><code>[root@master xgp]# vim /usr/lib/systemd/system/docker.serviceExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.21:5000</code></pre><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106120848869.png" alt="image-20200106120848869"></p><h3 id="（4）重启docker（三台）">（4）重启docker（三台）</h3><pre><code>[root@master xgp]# systemctl daemon-reload [root@master xgp]# systemctl restart docker</code></pre><h2 id="3-上传之前创建的三个web镜像到私有仓库">3.上传之前创建的三个web镜像到私有仓库</h2><h3 id="（1）修改镜像标签">（1）修改镜像标签</h3><pre><code>[root@master xgp]# docker tag web1:latest 192.168.1.21:5000/web1:latest[root@master xgp]# docker tag web2:latest 192.168.1.21:5000/web2:latest[root@master xgp]# docker tag web3:latest 192.168.1.21:5000/web3:latest</code></pre><h3 id="（2）将三个web镜像上传到私有仓库">（2）将三个web镜像上传到私有仓库</h3><pre><code>[root@master xgp]# docker push  192.168.1.21:5000/web1:latest [root@master xgp]# docker push  192.168.1.21:5000/web2:latest[root@master xgp]# docker push  192.168.1.21:5000/web3:latest </code></pre><h2 id="4-部署一个Deployment资源对象，要求镜像使用上述私有镜像v1版本。6个副本Pod。">4.部署一个Deployment资源对象，要求镜像使用上述私有镜像v1版本。6个副本Pod。</h2><pre><code>[root@master xgp]# kubectl run www1 --image=192.168.1.21:5000/web1:latest --replicas=6</code></pre><h3 id="查看一下-6">查看一下</h3><pre><code>[root@master xgp]# kubectl get pod</code></pre><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106122026271.png" alt="image-20200106122026271"></p><h3 id="本地访问一下">本地访问一下</h3><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106122426308.png" alt="image-20200106122426308"></p><h3 id="5-将上述Deployment暴露一个service资源对象，使外网能否访问服务。">5.将上述Deployment暴露一个service资源对象，使外网能否访问服务。</h3><pre><code>[root@master xgp]#  kubectl expose deployment www1 --name=web-xgp --port=80 --type=NodePort</code></pre><h3 id="查看一下-7">查看一下</h3><pre><code>[root@master xgp]# kubectl get svc</code></pre><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106122313996.png" alt="image-20200106122313996"></p><h3 id="浏览器访问一下">浏览器访问一下</h3><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106122340747.png" alt="image-20200106122340747"></p><h2 id="6-将上述Deployment进行扩容和缩容操作，扩容为8个副本Pod，然后缩容为4个副本Pod。">6.将上述Deployment进行扩容和缩容操作，扩容为8个副本Pod，然后缩容为4个副本Pod。</h2><h2 id="（1）扩容">（1）扩容</h2><pre><code>[root@master xgp]# kubectl scale deployment www1 --replicas=8</code></pre><h3 id="查看一下-8">查看一下</h3><pre><code>[root@master xgp]# kubectl get deployments. -o wide</code></pre><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106122722977.png" alt="image-20200106122722977"></p><h2 id="（2）缩容">（2）缩容</h2><h3 id="修改k8s配置文件">修改k8s配置文件</h3><h3 id="备份web的yaml文件-2">备份web的yaml文件</h3><pre><code>[root@master ~]# kubectl get deployments. -o yaml &gt; www1.yaml</code></pre><h3 id="使用edit修改web的yaml文件-3">使用edit修改web的yaml文件</h3><pre><code>[root@master ~]# kubectl edit deployments. www1</code></pre><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106105924531.png" alt="image-20200106105924531"></p><h3 id="查看一下-9">查看一下</h3><pre><code>[root@master xgp]# kubectl get deployments. -o wide</code></pre><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106122953397.png" alt="image-20200106122953397"></p><h2 id="7-将上述Deployment进行升级与回滚操作，将v1版本，升级到v2版本。">7.将上述Deployment进行升级与回滚操作，将v1版本，升级到v2版本。</h2><h2 id="（1）升级版本为web2">（1）升级版本为web2</h2><pre><code>[root@master ~]# kubectl set image deployment www1 www1=192.168.1.21:5000/web2</code></pre><h3 id="本机测试访问">本机测试访问</h3><pre><code>[root@master ~]# curl 127.0.0.1:30996&lt;h1&gt;xgp | test-web | httpd:v2&lt;h1&gt;</code></pre><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106125722931.png" alt="image-20200106125722931"></p><h3 id="浏览器测试访问">浏览器测试访问</h3><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106125750021.png" alt="image-20200106125750021"></p><h2 id="（2）回滚版本到web1">（2）回滚版本到web1</h2><h3 id="1-修改配置文件回滚">&lt;1&gt;修改配置文件回滚</h3><h3 id="使用edit修改web的yaml文件-4">使用edit修改web的yaml文件</h3><pre><code>[root@master ~]# kubectl edit deployments. www1</code></pre><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106130010344.png" alt="image-20200106130010344"></p><h3 id="查看一下-10">查看一下</h3><pre><code>[root@master ~]# kubectl get deployments. -o wide</code></pre><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106130304423.png" alt="image-20200106130304423"></p><h3 id="访问一下">访问一下</h3><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106130435212.png" alt="image-20200106130435212"></p><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106130447693.png" alt="image-20200106130447693"></p><h3 id="2-命令回滚">&lt;2&gt;命令回滚</h3><pre><code>[root@master ~]# kubectl rollout undo deployment www1</code></pre><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106130317956.png" alt="image-20200106130317956"></p><p><em><strong>注意:只能回滚到上一次操作的状态</strong></em></p><h3 id="访问一下-2">访问一下</h3><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106130357339.png" alt="image-20200106130357339"></p><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106130414060.png" alt="image-20200106130414060"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>02 k8s架构，基本概念</title>
      <link href="/posts/cd85.html"/>
      <url>/posts/cd85.html</url>
      
        <content type="html"><![CDATA[<table><thead><tr><th>主机名</th><th>IP地址</th><th>服务</th></tr></thead><tbody><tr><td>master</td><td>192.168.1.21</td><td></td></tr><tr><td>node01</td><td>192.168.1.22</td><td></td></tr><tr><td>node02</td><td>192.168.1.23</td><td></td></tr></tbody></table><h1>kubernetes架构</h1><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/qqq.png" alt="image-20200104100759831"></p><p><strong>kubectl</strong>：k8s是命令行端，用来发送客户的操作指令。</p><h2 id="master节点">master节点</h2><p><strong>1. API server[资源操作入口]</strong>：是k8s集群的前端接口，各种各样客户端工具以及k8s的其他组件可以通过它管理k8s集群的各种资源。它提供了HTTP/HTTPS RESTful API,即K8S API。</p><blockquote><ul><li><strong>提供了资源对象的唯一操作入口，其他所有组件都必须通过它提供的API来操作资源数据，只有API Server与存储通信，其他模块通过API Server访问集群状态。</strong></li></ul><p><strong>第一，是为了保证集群状态访问的安全。</strong></p><p><strong>第二，是为了隔离集群状态访问的方式和后端存储实现的方式：API Server是状态访问的方式，不会因为后端存储技术etcd的改变而改变。</strong></p><ul><li><strong>作为kubernetes系统的入口，封装了核心对象的增删改查操作，以<a href="https://www.centos.bz/tag/restful/" target="_blank" rel="noopener">RESTFul</a>接口方式提供给外部客户和内部组件调用。对相关的资源数据“全量查询”+“变化监听”，实时完成相关的业务功能。</strong></li></ul></blockquote><p><strong>2. Scheduler[集群分发调度器]</strong>：负责决定将Pod放在哪个Node上运行。在调度时，会充分考虑集群的拓扑结构，当前各个节点的负载情况，以及应对高可用、性能、数据亲和性和需求。</p><blockquote><p><strong>1.Scheduler收集和分析当前Kubernetes集群中所有Minion节点的资源(内存、CPU)负载情况，然后依此分发新建的Pod到Kubernetes集群中可用的节点。</strong></p><p><strong>2.实时监测Kubernetes集群中未分发和已分发的所有运行的Pod。</strong></p><p><strong>3.Scheduler也监测Minion节点信息，由于会频繁查找Minion节点，Scheduler会缓存一份最新的信息在本地。</strong></p><p><strong>4.最后，Scheduler在分发Pod到指定的Minion节点后，会把Pod相关的信息Binding写回API Server。</strong></p></blockquote><p><strong>4. Controller Manager[内部管理控制中心]</strong>：负责管理集群的各种资源，保证资源处于预期的状态。它由多种Controller组成，包括Replication Controller、Endpoints Controller、Namespace Controller、Serviceaccounts Controller等。</p><blockquote><p><strong>实现集群故障检测和恢复的自动化工作，负责执行各种控制器，主要有：</strong></p><p><strong>1.endpoint-controller：定期关联<a href="https://www.centos.bz/tag/service/" target="_blank" rel="noopener">service</a>和pod(关联信息由endpoint对象维护)，保证service到pod的映射总是最新的。</strong></p><p><strong>2.replication-controller：定期关联replicationController和pod，保证replicationController定义的复制数量与实际运行pod的数量总是一致的。</strong></p></blockquote><p><strong>5. Etcd</strong>：负责保存k8s集群的配置信息和各种资源的状态信息。当数据发生变化时，etcd会快速的通知k8s相关组件。<a href>（第三方组件）它有可替换方案。Consul、zookeeper</a></p><p><strong>6. Pod:</strong> k8s集群的最小组成单位。一个Pod内，可以运行一个或多个容器。大多数情况下，一个Pod内只有一个Container容器。</p><p><strong>7. Flanner</strong>：是k8s集群网络，可以保证Pod的跨主机通信。也有替换方案。</p><pre><code>[root@master ~]# kubectl get pod --all-namespaces//查看pod信息</code></pre><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200104100759831.png" alt="image-20200104100759831"></p><pre><code>[root@master ~]# kubectl get pod --all-namespaces -o wide//显示pod的节点信息</code></pre><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200104101023909.png" alt="image-20200104101023909"></p><h2 id="Node节点">Node节点</h2><p><strong>Kubelet[节点上的Pod管家]</strong>：它是Node的agent(代理)，当Scheduler确定某 个Node上运行Pod之后，会将Pod的具体配置信息发送给该节点的kubelet,kubelet会根据这些信息创建和运行容器，并向Master报告运行状态。</p><blockquote><ul><li><strong>负责Node节点上pod的创建、修改、监控、删除等全生命周期的管理</strong></li><li><strong>定时上报本Node的状态信息给API Server。</strong></li><li><strong>kubelet是Master API Server和Minion之间的桥梁，接收Master API Server分配给它的commands和work，与持久性键值存储etcd、file、server和http进行交互，读取配置信息。</strong></li><li><strong>具体的工作如下：</strong></li></ul><p><strong>设置容器的环境变量、给容器绑定<a href="https://www.centos.bz/tag/volume/" target="_blank" rel="noopener">Volume</a>、给容器绑定Port、根据指定的Pod运行一个单一容器、给指定的Pod创建network 容器。</strong></p><p><strong>同步Pod的状态、同步Pod的状态、从<a href="https://www.centos.bz/tag/cadvisor/" target="_blank" rel="noopener">cAdvisor</a>获取<a href="https://www.centos.bz/tag/container/" target="_blank" rel="noopener">Container</a> info、 pod info、 root info、 <a href="https://www.centos.bz/tag/machine/" target="_blank" rel="noopener">machine</a> info。</strong></p><p><strong>在容器中运行命令、杀死容器、删除Pod的所有容器。</strong></p></blockquote><p>**kube-proxy[负载均衡、路由转发]:**负责将访问service的TCP/UDP数据流转发到后端的容器。如果有多个副本，kube-proxy会实现负载均衡。</p><blockquote><ul><li><strong>Proxy是为了解决外部网络能够访问跨机器集群中容器提供的应用服务而设计的，运行在每个Node上。Proxy提供TCP/UDP sockets的proxy，每创建一种Service，Proxy主要从etcd获取Services和Endpoints的配置信息（也可以从file获取），然后根据配置信息在Minion上启动一个Proxy的进程并监听相应的服务端口，当外部请求发生时，Proxy会根据Load Balancer将请求分发到后端正确的容器处理。</strong></li><li><strong>Proxy不但解决了同一主宿机相同服务端口冲突的问题，还提供了Service转发服务端口对外提供服务的能力，Proxy后端使用了随机、轮循负载均衡算法。</strong></li></ul></blockquote><h2 id="范例">范例</h2><blockquote><h3 id="分析各个组件的作用以及架构工作流程">分析各个组件的作用以及架构工作流程:</h3><p><strong>1) kubectl发送部署 请求到API server</strong><br><strong>2) APIserver通知Controller Manager创建一个Deployment资源。</strong><br><strong>3) Scheduler执行调度任务,将两个副本Pod分发到node01和node02. 上。</strong><br><strong>4) node01和node02, 上的kubelet在各自节点上创建并运行Pod。</strong></p><h3 id="补充">补充</h3><p><strong>1.应用的配置和当前的状态信息保存在etcd中，执行kubectl get pod时API server会从etcd中读取这些数据。</strong></p><p><strong>2.flannel会为每个Pod分配一个IP。 但此时没有创建Service资源，目前kube-proxy还没有参与进来。</strong></p></blockquote><h3 id="运行一个例子（创建一个deployment资源对象-pod控制器-）">运行一个例子（创建一个deployment资源对象&lt;pod控制器&gt;）</h3><pre><code>[root@master ~]# kubectl run test-web --image=httpd --replicas=2//创建一个deployment资源对象。</code></pre><p><em><strong>运行完成之后，如果有镜像可直接开启，没有的话需要等待一会儿，node节点要在docker hup上下载</strong></em></p><h4 id="查看一下">查看一下</h4><pre><code>[root@master ~]# kubectl get  deployments.或 kubectl get  deploy</code></pre><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200104110812772.png" alt="image-20200104110812772"></p><pre><code>[root@master ~]# kubectl get pod</code></pre><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200104110954406.png" alt="image-20200104110954406"></p><pre><code>[root@master ~]# kubectl get pod  -o wide//显示pod的节点信息</code></pre><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200104111128779.png" alt="image-20200104111128779"></p><p><em><strong>如果，node节点没有运行test-web服务，需要在节点上重启一下<systemctl restart kubelet></systemctl></strong></em></p><h3 id="如果删除一个pod">如果删除一个pod</h3><pre><code>[root@master ~]# kubectl delete pod test-web-5b56bdff65-2njqf </code></pre><h4 id="查看一下-2">查看一下</h4><pre><code>[root@master ~]# kubectl get pod -o wide</code></pre><p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200104112418012.png" alt="image-20200104112418012"></p><p><em><strong>现在发现容器还存在，因为控制器会自动发现，一旦与之前执行的命令有误差，他会自动补全。</strong></em></p><p><a href="https://blog.csdn.net/gongxsh00/article/details/79932136" target="_blank" rel="noopener">https://blog.csdn.net/gongxsh00/article/details/79932136</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>01 部署k8s集群</title>
      <link href="/posts/xgp.html"/>
      <url>/posts/xgp.html</url>
      
        <content type="html"><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <div class="hbe-input-container">  <input type="password" id="hbePass" placeholder="" />    <label for="hbePass">Hey, password is required here.</label>    <div class="bottom-line"></div>  </div>  <script id="hbeData" type="hbeData" data-hmacdigest="fd746d330c8214d5afb79beb10558e36c9077157acc472122b4fc60ee984bd05">a9abbb8fcc4bf5116e4dc8f25abf611a128cbdf3ff8ddb27d6297e66f736d2b4aa7527a03b34a33375141a193afac697c2de7d3f6fa7357244284136aa6f5c591713b07b5a3f5cca4711d6721b7ca9b6c9c0bb5aeaf71e85375fb49c6c699cae4f568a5317a8eefab83f5bf41faac26d80913f770c4e5211b1d09837dd87983affaa830ca2864b0a4b615c63cbfe8b93360f404f81eb80e763ee5a96b5c851213fb678e2c6893e5619e7fa45303836baca454622c118fe0fbfc492e32aeba9e515bacd321332963ae69e85cd80276bffdc77fef9ac2f84d5183a5da76e26008706ee76af5ad5c08b2854d0c912483f425c3a9be21f2ba898bfe0c362f6dbe5611ddb614fda309ecabfb7acf02ea6e9b339dd390ca4ec82d8b0635e82b28a9988c5ceb425e80c8dbdd8e7b07e5c173f9230f685397bcdb51480d6608fe2555e995e8ee43fc80cb0d0ac867708215b185f6bca9a3c5b62ebcf50661f7d3695ad0ac457a0ddd66b980b15087b26eae8e0656f2037d70a8dea38f2a9278dece2d6a60c1809aff7496eda648a325b34dfc4b61c468e89bc43f1d5cf6d22ec01e0e0c66190f5f1f6fd6db706fd715c923ab4475a23345f17e26c5259cdb861abb37f2275b4fc9bebffc8519232076f49199ed5fbc9d2895765a831cdcf057ace18cffe14e60939225f7a250c11972f11cacc3cf03905211e2dcb134d4039dedacd1fc3f65c8b48cf36a2624c69c5ec4d005109f7e90d28918cf5f2e5a8540ad7050a7f8ef0df71005ac44b0f389ac99e0392a9dd60352115567d4de5a974ba2a55bd8fcaf05969fbcf50be0d8e845140703dc6885b5e619418749d03e18db00cdea0d47d431432ad8709b23cb9d439457fcac45a8a9b73aea71c2c029f6de0c87887e6499031b0c9668d6570b1b10861cdad37243ed66c9da4c66648872e18a6eb5dc71476a7c660c93488f167f8be59dce3388b27b0f5668ab92949043070c189761d118f186c633e206e357eadb40df302f773948c36e8348e5ace73f96b77d9af186334f17254bc7b73465e457696f357562946e2a160b9f0e69870134a394181265f0ba1f3d4dc18c506a261b482f7ef155ac1482e49586972723001119d711650a68fd52aef285e71791fdfc558fa05dc9d497abc8c4f2beac062d4bf2e39a6bfaeed1597f79c2a12fca38a40f9fff13da712dc3ed974a2998348c27815f6845fda2d446249299f2721669ef6c29f313c4da1b8f9d6ac630d9c09b77ce1f3b75696ac006c09dbd553bd24b4a202fc8158a186289d421354bb69007ccede3970a898998e6b44a18a885063434c1f1144c844c1a81c3548adeb876e16b7b7672ef50b8211d46a6b596d873e9c7d41d453e95a4cbc3430d23df6c563f134f3ae5fd8dfec0f5f85342a38ce9d95b7ce2c66cd379bd2c1add14b11f1b2e9f8fc0730fc8f14d6a33175912bd6155231d1568303a43c82dd07248dcfbc5b737cde66018e81f0e1f75c0d8bf3a2d7d21678d2d84ff7e21caefdad19847d28512d06207e77170191dedde573d671dabf9d50af97440ed3615d3ed5768924e50e26d311f83ae2f0124afc62b2f11063391e5fba9814ad2d7e58d15ac6baf9b664951659c9b74bca10cd8d3979998af589aa23a1617fa9161b465b05df2d7b8309614d3e309258ac94104b29cef51de869641b02b35320392df9fed29bfbde3451a252e87fa0112417c04d7383ba662c0b80db6017220d7dea9ac574cee087f4f7bce83fd7cf44f08c1c9cab3b5becc309c4333b08b9777179e29e6d8e5df5c7e0f868f817cee3ead34f1ed9d617294538cbe558affdc57c13bcebd459b09deb9fbb8225551008ef717c2b272360ea1e0dcddd79099ab506512a403ec7b64145a3388b0d6bcf30704837f346d058feb43fe091b1524b3a14c9b3c780fb32ccc023e982682a0ae39adc5159bb163e3ac836aa02c19650dbcbb0682721c27a6898da95ce9ab8145bd3866d61be3e898817ed4db64d8dd0123ea67097d9fc970e8e2b5f186f35a9a443b038a1e5b66bc602b2c835c598c4b38e95972f6e2d1b77ad0c7c3351516e497fb0cbf4618c1fedd72d1a892d7f6f8caece8ed9a071b19f21c5f2414c4a05048b2172f0c3a431e4292dcedb0617f154d6bbd300870c7938ab5e2fc449325f3e1a0b877c779606103119baea0042301bc0fd0db3321c89af2b584115a809bed055a51a94906bb43eb5e6679ad309700a7b01eafc7721dd78095dcd6c266a9d6b4582f1701746d0724c5ed68efa18a79bd0fddf23fdc9989e74b8e55647594872804962c0ee6b5d9a497d17db589c31ffa757c7cde8ca0ebfa674a16055906b8e82169c7d03a2932b17eca0a28d94bcc6be346eb82076e8111cf849619b7ee0df6f40fec5bc2ec77d61155fbeeb1e0eb64e994272193ccfdc1c7f68c22f36e91fd5655ef5c971b92fc22a8b5baa25b686fc50f37750a3fc94b01ce3b5153a3b0249cf54999111c1e0810388327234310e39a6d98006d5267169cc419fba05fa3fbec13bbceabc94b7cb99953fe69ce5dcd95c7da7340207048a44655e9c75705e1912f148e9fdac196a33930fe82b7c39fec2de0c3cc25c18e63d014b93b03b3e252ea101df51b3b3c06f8cfc8000b01b22e235b4086baf301b2a8fde157d10d0cd71e9c0ea4f93b195e14ced27d48d6ba2e3ad24c9e4d94c6fbdb5aaeb2f72531a862f2fe119feb1a4df91803bee03eeab465643971d39385a9064e39d802f2c755d7164bc5f790a948e4da70ee9d0d7d98c9a6c66af4ab59fc5b3174e7c0b2c892dbeffba5b34371d60bb0e17311e2991e6f5f429183d50b1d1280407f3ee1fdfe98c216d8b543aa8468ca762a6a4eb1d79e4e139fc028251367ab65c27d065beecdc21ecdbb4f43c2caa00e8d1e15a14785a3d63f53e27adfa34e06e7161f4b766cb6b4e7505ea1d7a98cea3032fa42520102a19b7dccf49c0c0b472915441300f1e3c813e06f30d4848e4d208f045f0daec733568194475f37f11e5153a90ad6b23bb0dae193f30ecbbaba34a3ade57b4b225bd44331aadd432acaf421d4db868ac0b9665aa9cf187b9849381e1afdcd56107d14946edafdd8d9954e03d73cd145469e8582f0bd043682a7562a36de39a8bf556df1ba3bc7f9b2e72ec20f513c39c36b4dcbc93e5fe690a13abeb0ac67ed63af2a8a31f2ac94a3dd42d119bac9d9ab302940d626fdbbbd360a107aad1c2cc12fe5ab2ca19a8e2eff1e919790c5d382a869402e5a006e6572029640618207d1b6722d0d967667e7249607b66fe8b9c80ac1d0efe276877772c6903ffbdd212a5b21a09ad6bf0d9f2c4c28187615f6fb5b8f6eda9ed940b04fe26f4a2c597070a306713655327d95e9aeee5fd5b0c985d1b73d25bd697c426022498d620d98a932bef206c9d3bf998ca309a46d31cf0db274b9f26a75e68c6ee02e3c766440266183313ab717d28650b82d266a08a0e572d9c0176219547475d63ed399a25c7a2b6c37729dc38af91b67fa47d5ff55340e9cf6b76655ae40288c945696f85323c296378f9f46d6731c52b527f72a32886bb28d516cbcce35b70b5e0ff3f57173a62d7b8d5fe1e0840c80698071af400e93433b2725346db40c77b538d2f59309ce61e47b95a07dcbf852d0f8dadc6ed5e41c74117c95eb4520c353a7ffd2936ef97e437d7dd6c5d126f89a0ba3eadfa96b45172223517a60d803c2f8a97993ff7ddc896fdf8ac4ffd5bb8811e8a9959d7b3d784bbead563febef1649f72c4897d28737c66dae2c15f10b14580083fd49554a0677e17bc1d334f28323c3beb5e642d821e789bdca43d0a92e13fd463143a8aec327642788272abfbb6d1d10b3d955678bea0beef63c3c62f55bcf31ce4f00ec51a93d5fa87ceb2d03085ea4dc650da086867e72f54ac2ec4440fa931c1a0e1e58a070ad23d335f018383c18f69a071564ce7e1871f8c6d2ca43cd26ed90c2c8aef8943066dfaf62fc402d91fbf6ec90938ee6754d3815fd729dd9c71415e3cac67519b7d2d57e37a01d2d4be1b58493ac3b528f56ffe15b895db197a564c1b202c94901aca1adf543d43a74b0c6e3bccb3e1d59e0efdd08826f3db003bbf56f270439c8f0c9d9f8f920dd3271805b1700aa9a0f780d98fccc443feb02984ca01b801048697ea22a65690acf3a57328a5883aaa82c1808f68bbac8fd08113993f47701f5ddd205377c4b6a5447ae4b7779a72d3fec35b350c6790f78a8fd781a766ab5e94438921971cfdada034cd4f6308fc27bdf8900de2a6cb9a99293a239f8302d70ba0dac243efeedf2f9daf49270e1c8e7f48fedd2925d2bfcbe8d939ef6522b56f5a34ca21898bea7aa6b72105e31b807b9760479ecc3475b8343f860b3c8990ab7ec52ec79dab84ae3cd8919812c9ca30702b9aece2d5a2d03cb7b52a952879b1233efa5b5fc1ef8b31b97f1a47463f5787bfd12ec4700a389651c259bf80e1e5d4e9b9b83b8ae5d0d8495519d2f8357d088941b28e40acc4519c3a07dcf26d4148bb9d55d7ca0210b8e089d4a6a397315a28e6545c849d2ad2a756da64ea1b2542e790038bad9d702fd6b03c7bed082c6228310361a2f06cae623896d2db579f98c367dce02335c721c5083b25a74af6fb2f0b213d15367593e825b0048f14790ba511f9c4a81c1fd162f6038406f0c9c54b3754660869adb4f8713cda38a31da4200ec7bd5fc8996150e3a09823c1d0dc92eb98ba3471c50aa3d41a49c005283b77fa81c5582a6039e7731ff38264c39c06ee5f9ae7891a1b7b647f974c6168947fefdf82a0f2aa5acd58f78697c9e774918b15bcbe1fcd023795a39c43ac16a7e4e2a543f318ac1f71837a3de084f4dfdde4dfad6e0a6d262bf727098a08e1c48fffe5a4f271ad36d993af927d098bfbbc5ab3ebf9e73377d96dda6f59511d880c9d3e811dc4dc61de18bf20b973569946d8ebfe4590f086ccbb95b7e86a83e6f4c66e49c44308bad4a1615ecc958edd6c592987fde8ec1a8062514db5f94547bc54f1374d62613ba0b474559571e4d09080b4cb5f4f0264f76cdf48d53ff329fde8e86c6a1f7aed2a6d748bf7cf0ede317da32756186f1fc54e875bf041f146fc64d47dc1560c785f0b4ce98265f10932f1a36e3ec1e6be7ff3aaf865497da5791147605840878c48ddcacaaec2fa7bc04044c0aad584519f4d334fc771bdef056493a8c3dafef29e2f85d686aace3e87f3cf1091e166c35ecdc0c4309d52fdbf5e3900ea6874b01d7ec7bf30a4d58049ec90fa5dbd2909a65bdf06b5ff6098bf35e3b0d4232f1c7790f2afda5619f160c52af19d1d35bcf1e69a7851bc658d2e316705b9143a763d52cb4db7ebbfd2193901ba494e6f2f6b3989745d731e5c0e08b8575d29195c2462a196794c08b107e1fc3fb34d7aba63fd91c68a4a586c09832fe128fa8304ff4b4fe3c3389481e42dfdb79cb152cd92cdc48071b2f5e029c046f4c833915b7fb9afbacb8393c88ec1f56f24727e454b5552ee465db6197671a6956a141807823dbb61a35ac9c67b5ddac5d9ff3a66180e89d2e304f79696d04840161b6a625b2ede5aafc777f5e712e5627fda2afa5e91268cb512e2329fe1e639f3445c65ab62f3821215c4ab67d16347925151915be341b9d5faceabc55b1f093d17e9edc70d6c878c7d92214daa860558527e8c60ef53efc9f469cb3edf176bd1bbabd54220169e42bc4e1a36a5295238c635e37c7a658df63aeea8e2def2cde8bdd7e9972722682596876e36e7791e21ce0ff19b523d2bf64845b0f5fad2226ab617a8eb18eb7513aee59243e958943f4d1ca28ca249364ebd31a20098177454a18439b28f28566503a4b8854628762bf695704e9230a49f8edfff283876b1584a1612b054a9418e9a6d72c2d71de10f8b4c536f4d1d0b4ed62992c756592e954f99b97dcac683d1384415aa5b3a20e346ecbd3318f63a9270093f3a653eaac17b16582c58ec62d3010376cb503aefb92f75f0bc59708d850b6775e3aaad25a8f4009bd335f484edd0a5b8d34959ccbbb4718c84081f0bad18d95920d949dde26855a484cc8dae2bddf84bf4589cab1b4ee234f9497437c625fa1caccb322dde14da23928de0f7d7e79561dc1e0d11210fc8540d4e3d937215466a957c38af03d5425b47dadd5a0a04231d0c776f51ff6b38717293be2c1ed884a3897f1e37596e7b31699501fb864c9e20e71455966fa7f4bdc46698dc3dc3ce9982ab8ae52bb72eff507cc70e8c8f93f88637dd5e6c0249a9bf8eb5414fbcbbdf72fd9c71e9141b2085c7f88fe77014ef51c1e1afd46c5b75ef620a39d060f087f76281b2285181ac3bb9bd188a1353e8e00a2c39885201b7fcde895f8002d0fcaf5e2c4b69bc910a74873c56848482a366def7f507a59196852ace19afeeff61fe4338e895f09b5bc4442d8ba3c3a8579df5a2ffaf2efb86e75c7fad78e8cfec233c614e38e86d78b642b0c501a8ec546a51768070fd3b98ce78abb0fdb24a54d4867f448452a0190754f2162b371435ad08250ccc66708670a76e70ed47c42687d8b938accb69bf9de2371b6e871dddd8402d681f9894c2fbf053de2a4ec5b77ade0e7982e0a4f71641c7ad3277f9359d490d1be8b3924abf001a91dae9bc845f6e085972da61b2387e6c171c9282ec1821f909ffff2824106a312906e2ad795aebbd3049e9666f3abb72ecf9753cca7c8f50f3f4f75bfd17d60b69d7067eedeafa28d60b8768c21f27bf253648b55a1d0427ab03b904ea9461257b398793908bc2412e23f69417629d244d6638636c9164d1c9a892fe009a80bcf852740a326f7c1eb023684194bd8a399b98be1e2ccf38a9a25f7ba25602b87ac36989328929b22c0cf636a6d140bcf8b888809cd46dcf7c15a614988c5f0f95b2e1fdebe9230b1a2c3b2cb393dc3205b858302e82dcdefd048dc42145d7ef19e592ac609a270567a7e28f9e56da512b844394b1b1cf5d43dbaa3c67d141af8e2d33f1fe5e4aad34ee4ef1681dac49409764d07fdb7a5004686e6de11dbf11d22394b828c5a5f9c6a2a7dae1cb547c3bd404d003f214039b90379515aad8b923813658c9ae7a85d4ed969231e7054a7003733c32410a5a3b6d019e688ad10e2f30f941c1d8f9b9419c2bc36912f66f89f010dd6d9dc6c16a894edb9a8f695a5649f6b405e7e95f43d7e272323b79f0a4fb3c48194719087afd4c43921aa91dd534bdad776c112d99944d8c3b37ee8416f379ac91b9e8f63fa9dfee65240ee0b96d179e241f2b32308a7376623ad04da191d9976ce54bec22e7868f3a4112b769b9e6df41338420d4b2f98b1567b05e44856fbd8d68527c63376f342a762e351d0a04774f6f246a03eedf93f88df11c91237d4b903544866b0c75e71cdbdc8598b3f492850a6985a2797bc7bcc1494174ad2fd6cdc49a507d8f7ca4a7d57c5415a9a9f0ff95516af279d66283e63421aba3c77d081210d2808c449f7a62c2444fdfdcc226bb8f811a3e9604d14f3bdade2860409e4088ae747edf27cec934ca11b09955642ef5470ab92fbc9b5efc86f1b86da41e7b6d2f62ee77db211c939795e22cfe93a0c06d68b8334be7039f480d3197bfa33e0e05dd865041063fd6230fbef9cca6e2b197d0f2f919aa114c351c2502654102074a862549371a16d5637bb3e90459b822a010b93b02a8a6d752ff43dab181e005b079b9070e08a5cfb7d0b66db93064f7dcbd97c28deeebb41c9e0cdb2bc75613a4f99a33cfac548c6fc26195ace543e1eaf143131684f7e85958073908f44ef5d70a69975e48dddb1b4fa110a0d7c603d15e84a81622f4a1b1bfea34862d991e9c12c7d9abb449d56725fd0460566a23d50e34aa2db0efe9ed30999a1f5d0d2e7209bd484ad4b41323dc9d75d5af190f4de09ec0ef15fe8eed323a3abf4c8dfa1beee0e98a092060ffb5e051734e6c5a7c7897534bb4f32013466efe5c85b4dcbd8b737110d06d123ae0f753a55d6c42ceecdab001e5bbbe057c123a7bf617d77f42220b2cd205abe5235248a240f873cdc408932f2283c46a84dfa75ac37c035c91de8f4a28e9ad6ef5d4ce15e65770574bd32066912d3842c23925fff59e50c81eac9b9ec84214a6adb46958cf9f6726810cea7e8ba640d25cb13eab9154b66e924f610aee3e80a431d5d5a62d2952bb0fa79b4a189ea64b69dc9bb08746ff930fef695a08af539c2b8ddf23b2c5702ff917ff3ea75092be6ee17d8070636cf30d0d9081b223d598bd44127607547076baa2c794e0752713095810b66f3172c2a78ce56c2c56914a5882df7d9f584098416fc3129f9399a037f3605bf9441af57666b6dba343d49bbd5a510efb3f7feba103a5a829803d569e72a97692b3b72f0bd6f8018f7e5ec8850829084c1aaf9d08413c2806a7103ffb348402aad5e2aac5b0a81c5cd2d20831178a19682b49a655ff2036c0630e6f9e885b2e985a3b1a580df61da6e53bcdc48a46fb9bcfe0172a7284bbdbee0d5450ac6c316cc20903810aab4c5c9e3a6bdf843a97af5b700f00875cc20d323df58b692a46909b6ec411227a71296a192440ec6839d6dbaf5436bf35eaf45fe91ac521aed379838d7718aff727f2801c609e0985ce96439a499b8971126dc87fff9c98458950482ee8bce70e18498e9eae118f6e99e33c50e8ddd13b297202a34c3dcf942681e5f460104858ff85e2064ea0da70fdbebb2364e3f7efb59c9103b447d4883017cdaaa28f2e71b554c496754066292250060e8b5ae6d9c2ac1155e885f811654b6242204b9d1f592225725f7d3332bb8d062ccb3258993fabc6b3f5ce4af307b98a52db593b4e8941174dc5ab947c306357eb272be06c85ce4fc415ea3274b330640a836a9b927456b3a5243f4e7e50084188e9550dde1dffeccae95de9fdac39b382afaf5fc38ce0f8a3dc7c02c1283f08c9f3d30332a0dc11865bf6b3a73200cace8f89de2e954f8a4a10e19c7ed1e2b23045c336780542d6c0ec6e128c7614664758b79b700112ebe240b4934fdc2ac7e09a18a1861d4401ebed7d6e7b43f1871e784f8ad24ac7015c2322d5de7cc679429bbb4b6daa784e42d3e42185fa660b592ddbdc746574cdf27e277de671ff18d9c455b79629768a0d46ac9868e55409ca5429f78e0209c48b1ff0b6b8973a159fb594a9a7a8303aeeef024ee50ad35c8c916e90da4513c70f567e08aff6ff46e68a8b7435954a5134feb8d6809b8e8d5b1c6e2c92846522bfdef37eef88bd37f682a05d9b5557447634f69dc15cc8c21e809f2e1dfcd6a40505080fb078de4d389c4f34781e0af8dac068105fc09f91a99462044fe1095b60390f383f405c02706013e6472957ea4a7020853320d69373d9b61929a1c9465facb7ad8b5d9198bc91da398980458124622b2066803b8738e7595de35aa01b2bfff56e8555001a6f7a205ad81b3c101730b04a0ab15892b25b40e02ce0603583239de5a3410956e06c787e2029ac0b628353fb97736a07b2c1239afb04bd9d12bfb5511e39924a4a6e7ef9f9638153d2d224072606e704c702ad71a469769da376a74dd104a83e9b3682989d7fcd47e3502b87ea75fd7a527924b0ad7e536b6b23a9b56afa778f02488a6a58a74cd8624dd62e81706b0690e20ab86c239d0a7e555d9b95d2af71cd8fdcc0e40e82bb83ad867eb4f3d6028c90d205908dc49cb955fb1b1992378ac8ba5e9a44993b92be4c1ea84f0b2f0bd2c8dee82f9fbb40755d38439c195b1343286de67938fe91d51dfd5902b83091503c9df00d9e55be5b71e1b3519d2432a0ccbcfee90ae748499135cb6e94c8cd63b8418d7e667fdba371b30624e5cc7f94433ea258abe66abf4d43e2af25dd6b643974724886721ece6863f66c5776fa73d89fb9790e90e7ca431f4d9aba0cef576b666c66f29bcbf8b9779bcf9ee5c14224315e08736d4ce13bc00ec3b6a9f999ecc2d6e8f0c4bdfd4d7c84e605fc3e4e6b6e9bd77e0bb25afd30ee7ceefa6d4fc03491b9072ca6f2e958c5c6627df8cc183f8d99a620db50a555954ec31d3db2501a9ba14c858c1c56716b77520f4d6887e4891ba249fbc6e221df1a9baa4111fa8769cdd6fcadab64743b068edac31011ecaf5ca31ffc54c6fb57c24e7d4b36096811d822b43fa1619eaa720e1db25cfbdaadb7c3a58d537bd624fe467efccbdfdf9a8c57d47f957bd44101f158f73d9f4be18a950a7e5ecfee6397e2ff8dab7dc41dce52e57720d279c360425a612c7642eed901ecb08578c1c91ff77cc5dc731d5408dd7fa3b609247945fde4341571b1489ac0912813d4142f1b66d90a7b40a2fb44a0071b4e63e335b65e4258a10d26e9adf8534ff30e1f2e4f76c9448bd37209b0fda6b90da13de16e38796612edca30ee86c87ffeae95bd8fe1cd5c941ceb6ec4d4dd19981fd2691a27392013957e518c5b774bb7168132af18af5785876df5df46d82c43b0a414f9af64c0a585599af5f428122894d843b1d2bd62e01e6e612ff795582315337804d0aad34fbb7cc77d553d388df06cf7e1de0fa3ea350939ed0e7f009da2d3928a1dbb1b2c752acd631074c9451d179655366edcd80d16cdac75de3d8bcba1eacb131b54ad4eead3eb50aab03c4d0ab39987fbf1b60ebed82f66f03a35373d5f24d9764dc7138b29797951e54d26e4233de3715612cef86833694c1328025abff57049daac7211107922f513f6bc69f3971305701f15317a33da303a6a16ae381a9a7a972746a87669d0e5bfbd9d52daa71d5af40196db1bcf52f0fe1fa83851a02bf4bf27cd4a00bdf65b34385f631776d40b99ef07067a0c9c1269d189ca558d57b9be15730b3f7ba6f0fdf87deab38ddd8b5bbb9dd7f19bbf98d82370f7a191f9408b024059540c267370bf59d6008ebf7064672ec9d243456babe1b9dbcdfa4adb4b6f0d9b07f0fd3ffbbf607c57f3f8ebf7592de1ae35bb32e09bb4486ff51a2759edc252f57dab2357cf2d51e725068a2f802330f30213c13a61c1473d0105617430ad54e5bccbd7a8b08b2d0654f5ddf61f1d749217dea7eace749ed34213fcfb724ac03beaccbd7a8dc169df2c7fd2f54216912cf3880430132fc377b17809255e7edae523ddeffa8811959b34fb02f73e2b25c5537e055d05fc5296ec23bcf2cf2e216ae1f8d7a3809ad66be7a1b43ed5e17016f5108b84ad3cb0414c72856aaef7f58f78e775757a2a6cf2443cbc852af3630be39a53978195513307dd6decb735357b9d9b793163b631ff8610cb7d36c5f6165552e9a8a240b1e0a855837909d4b5f1bb3700123c0d09fc35bd0d5d0529174b5b807606b7e07c9b04fe5bfd12ae3a2ebf2670a79de57da5ac4df73e0e24a3fa45c6b7e7c718c14f1f24fbc79f9dbc390c300abf9a2bcf062324592806592f0f97057e286aa563f30edf3c2c75b3b913085e437b38781b06fba4716d3d2dbc1cae1426afb34dfdb7855690d81d8a723128cd675c07768bbf8a554f4668a0d1329921afdade80125e49b98505380953a2bc7f433c93c232d3f0e92c90f2e5ea5b9832e830c7e93a536644be4b7406fabaf124463c49eb36244f134f0d9e02fe5606e41d3d779749e8f83f3be9c5fa2ddd8511ac3419375b6349409aa7a0d713def05be85c03954418af5834d955e1009f0b0bfe57e0a08a943d428eb6f43fe48ba923f4d829740224ce215c483f85ce8186a5832e9f94cf90da8b7add28ffff5bf63acae9cb654e835cf60911dd16ca7a8f873afbe987adb7c9bcf592e225947d9c7b59103dc6724fb2939ca42eb804f7227296064dad97e12f59c1cf77649ae82825b792f8b07b2785b0e3a8cd67fbf63e28c55addbebef7963a963be49d00887b2584cfe8901df604dcb4a6507bda4394548cde1abe8055de10efdbeeecfa703431a64f1067c18ebf6a5df3678bd6c421344dde64cca44cfabb94e45d6aaa9fca215281628d4146e87817406daed21fb65929a2f6719d9859275f7ad0c52d1d63d80c5c5f54e144af9878a9072c4d7f735cb6514d948b6fbf6c1cb6bdd2de169432f26ed25fbac773e6954e9833927f06607af47949a9787dfd9a2382d69547fedeb83d3ef62dc17e2ae4db62838fb5c09fd58c06cf92878c167d728b58bd11e6e43162778c6c8f78dd3a1d83690e989fbeefbd77b136a02aea2cab7899b19a33f887100fe8f6aa5b4e9457bbd6997ab3f73674dfebf4b381b7d512f66eb9994d054c547b59242942894c8cd1b44ae80380d335d5710bd2c718071c51f64b2a807f1df109fc9c498a3c6b3645e0793d65f4d549753a5c476df301b6caea5b8421d3b0c43a5fb9e9887ed4379eea1b365ab376b615b901325b526f41ff59a83ba64f3c3c5cbaf38e9512fce0214b4a0603c98ed8fc28478efd6cf8b338ebc0d20b21ba470e398e3507547fcae9f05d88b74648312b881416bd874bfc259818e2bdc680e48777825aed26790117822e6ed11c2b14880ff4b9bbb9c378c65a0cfd52056bb3b699daeb6b1631d07ddfe7d00edfe680ba1e8262ff85f5f02f0567da402d0d149c6d8b2eea7c3834915b36ee06268c6c0a0a315a89ef171ccc79fc488a440d75e0cda84341436e7cdf565102cac1a4b77695e66d57c7cb6c979f07645f9ba2ec5a43d03ef27f0aa815627fdda4b912bca035ab50abc735ce381ec7fdbf8c20436b2717b22be003f679daa5912799190be0f625d98be959b94c2005ba4264996ca27825477cedcfc607630de57a542e67962d18255c685a3f8cdee16d4cb956075993b1dd9bf738787a145e23e20baddf43d3531c4d27947a069706aedbcc868338b72f85980814ee8451304d1b0bff83e00982efeec824f010a191a2cc74d1a73aecd36f912aa1531fc05f5e17644de9423e0274ee9c482e908feb77d9483280476109c7db6c8d8fb664027f75521dc1fa41633259a9b173d7440c80d36fdf77468a9a5ed0c17561e2e9d34dfb86c40fe2d91031be1117b41e3fe54417846bfc67a1d119b0e69f2cef0ca3649486492c5dc8ad254d9a1304304d93e76f755622e6f175cf588f3327f8e259421fb1b0516578f8e91dd5cdd40f6947bb222b525ff97e3af3320898a63429b88f8c8a32b91b8c256809427a1e9f6d32b0e064cd7c2462cab211e09e9a75c137655e0d4f4c061f9cb4a29ad3700d51a095d76b35342760e0fcd6947d39ab57ded0c1a48f8bff3decbe5783bdda2e2b8560504a036563739ae72dbb12dda0f9e8b017e371d2d4bddaf89edf46632160cc5d719ff249f69bb0e5acc00dba3709b96046041c155a5f96712f4cb3b0e12642db4bfc0bd8322e4776294463a9451ee34ceceb7deac27da8abc5533d09389666ff4ace6372a2b95705f644a2a837c38e756ba9a8230c80771e4f0847ea5d2848c524705b6160d3c3285535f304e3dd3c8a27965b758d34c240d80776cabd46f31f47c12ba6b563769885273949f0c96dffe7f7dadde238c4c4bfb3e518916f5a0658d0d4280ee02d56cfc96d7f55e488d275e9cca9bd8a233c31d3020c6935a18b09d39c4710730ae76a39479e0aba682b07a3ddbc88b72fc6d0d33885fed2ce87ec3641962ca57ca17544ddc78aae609285b1faac188449189045f544580c8bb2a71931d3d0adfe7b66efcb479591bc3db193967b7097da7b0f39ac9e1617116be66a7dca7d8a0e9af1c015e18b27fb2561a1ef6b595aef4675bcae84d1bf2a0f2514171eaba805a0b53895d787316938e29cc1040748c03af1819380a7d395ed98d90c0a95edca6e0ebf35ee78dc52bbad98e6fe58937c176cab094709a876bfa92ca8ee5847f88f8ff1a3294ce5b9c0d57be90cc77005c1acf11dbfb04b77beee95d5041ac463fb21d030df67dc66583bcad84f95dd5aae994b0052fc808481dbdef2c1336d5ad7b08956a464a7a0e03ef2cc26d431b66df8241a755f59f25b5e838245fd4492034f5304fe48e5b99da78577aad04c8080242006176032c1b3c7c9f1272b64c2d01f09eb6cda82231c80a12aa4e3ffda1bdf943246da676b1476fce7c4dcb89d96ba5fa5a4b11057b2b73aea6fafaabe4b23b1a2e0522bb0167854ed7dce9b1e6b149cb7e4b43f2ff9ddf0000e23f1781ed2b1b213e369cb3e01d6244b184c7df46222754abc152779c3be006ea49c2b8804e14696dbae6fa126a919e294ecf768a3e2d3f8e13733b16616c3bfddf1201082a3b3347a47da2e0cc1f55af8e83fe9aedefbab71b4031080af27d19f032f737de40dfa107897bff75557d8d7f8e4ade89eb0baeae99a5b53ccf0bbc4717bd1860269db4eaad1814a4d099cd79530df0a15138ed4b40e3f0c491cefe168d9c757acd87e2b17831efccca8c6afdb61787582d6c6d434390fc67efcc3c7938a978c4690ac2d603737a65f683fd5c92b1145f6ba1aa07d74a55b9966ae79c0f09a6968a0a9578def76a20901633a2bf12050f2270905130928f7ac727e7ffc83c0cdbdc539ef1dee6bbb3dd1b3e53c5c857eca086e0edfb1c77e9037cdfa0694d285913d96d79f981b983c9c76b78ae1ea89b6626276f301a0025e6e7373c585e4872d40344526aa6f87233490d78d0c29669b4aef71ed563d7bfec23b07212e89ab730378b0c5166993a8f833c8fa8c4ebb6a18fcfd4eb86769849b82bfafd846c38871c2c1ce5196afd7f465190bb3b6b89b00ed652f1fe71e6464a96125fef298af8e6f50f3b078f6bf95ab9d82bf1d8d05b3cdacc9c4470106bd9f0b82ad178ffbdbc2c6776d9ef6b5d757206e69bb2ab86b1c2e7257ed7f2f717d2a169c9474b95dd06d5204d4f88ad20c8e41b6fe601d8f3a031034655594e333aadcbbd3475228840a3631c0e415784d54bf041a82e5997413f44af1657090b0ed7ff5a9299a0b9824d33046c95b790661704da94a310a315b6ce07fafdc5ed645b8e3c5a6765061d7a5cf0a734fb990f2b193a5197bccd86400d39b6966288f095c16dde9c6243e835d80e6692b292f88f6cd038ca85f44d31294ff530487c1be8bd0064d95c7026d441c3e8444a948c40b51ed1c66452f358932f9fe0dd7f47708784681eb296eba7db323c032725cd2430ae316a89286d701e0ca44df0c4d110e6ffc5f1da94092db65e7c0a2ece400d028d028c511ae119c232a0d8b16f8087a8a0f77e0a380fdfbbe8292e133316dc6e182d7dd19c245160a9eca1b65e26f217f34bbc7843dd3b8365451dcefabcd349f8cf421485734f6bf6168b458bbc55bb521c880fddf8be06cc641f659945d11f57db89e5dc991b49ebf6f7332204a15eabf9112e1f09ed4797ad2fd9c16f4fdf2c832eccea55d8882bf666739164eb362878181cd297494be27efcfa20c3b6d378bb0db373590794f63368ba78d17ff61bc7075d0822aa1997b5382900873aa18d136f4bfe862386ea8fc79ed05b0f8129f3fb4748470360bf3366c1d2f30f9094fc0490592ff34b370a785c82c278aabee330e3edf17a1f1c90cb41a27eb5aac6cd947c83bafa013cb2450a0e2bdd3cbde8dd8356368311aa7b1a3ba41e3a4752dcfe09227f06c9f4ea28c805d95cbc37bc41559519fb6440217d05fc34fef221ba21a5b45b23fa7fd436d27543700ded62b6c78301284ddf811c45537c61b98628085fd32a0d1b61ecee1953f5b3e18f33fce68f86cecd3754576909e38fc47ea2a4b61f1665d5577db23515d4f32849f62733cd400792d0769302444c301140b7e53df1d26c85b52833773a85f6c1c47c99defc294133ceb6d616ad63a19e2842dfe98de8b3f0f1518d570da212c5147050a279c900cfad44439ba8cd3ffd36584a37372a93bb2e8b8f8c34fc485d1184929767c06fc847b84b4a1a278e3a000c873f52483d5a36821e3c9c5b0ea33173d7e8cd30e479666b095beb8ae9801925f879c6079d4c541dc68fdeb931a893a89d92948dcaa11ad0a013677ec293559f0157bb2987c48a1a98b9e8993c01a9ee0d4510c1f747a612ad6a0cfd5b63084445267f3357b89f6be268b9e31a6e96f6ed64beb3bbba01bc0641d79d002be953487db8f04dbcfe0a9975e8b6aa6d60119164a3578ac664736d3c2042c5ba8bc5851bccbe13e1158f3e7d3f7d2957c656e12ba8500549a737fd31e707243db3c7e9f1d7901c884e3b06df2cb13938e545575a2c889c8522bcfbbcd9f0e859a5551ad410bb0694473e4f0a88b3bea35551d8200cdba5be68fbc7b1fa8566fc19106a830feda21fefb43d488f40bf88c753e09351ddc58e3ea74a5542f153c2d16350748b36bbc543e584777c3d7a3c310689ab65a5d426d7f4ee2a043b79cc266d2f01461135614f65b4c2964d851e1294024045c12f8d5fb0cf143b928bf042f6e039ae6abeafd7632f2e546be1f2ed161dd016018fc11b14bd42d2179ccdfd5e6a64dba9c16cb5e48f428db0898e47c45f654c05a53de1da23dc640837f6328416dd872eaabda8536d605909761a354f48a171abcaf66d007cfd1fffa39592dcd94ad6134381e79567add1c54655ed8fe88773b45a48b5889cc73f46c77b7f72c519295423c7820c7d4d45df05f59c235fbafa463291d034eb5b93fec8945fc93342b68522ca46b8a37d7f11162698a6a5952eecd8f900a837c6f075da075780bb112db34c98517f7ee999c16b79ee405615025211fc443e622746e01405274e182f7d4f66a172709b6b1e81ec4d5c2458fa2dc8072136ac5124be85cbacdf03423b84a53805207d8cf601de4deaf97d64ba4c10123a65697e51e2aa8c1ce63fcb11679ddeed7e2dc75d47e117e43da26ce8578e1899a9e928599ca71d14bd0d250ac04612fa8dcb0a8a9ac5eb699f4ab882551cfcd815a2ccec92ae9afab1855df4e694d1429a8dce3299a4feb9f4967eb5b565f9667fb3e14e4bbbcafec0bb41f1faf2f21fb8a6a4b0af6703253d0cac346bdff218ad442c7ca14d83673a434e651132ee13c62badb60f6d759e85cf179dedecbe3c640f9494763bc0263fda8e1b474cc97f004adf49793009b556aebbf69adddfbc9c2c453c7098091f78d8e91f6c9fe12d4cc29b077f309996461f281fc8dd01e46bf472f8caec7a471337cf56b25a89fbc87e639aadbe787bfa829d1e3b693fb0490eea1f0695ace2fd9a4c5bf95f43b6561fdd38db2e4a60b7111f2ca098e840536b565dc0f5bafd5614757c3241fd274be08842a06d2c9388ca4304938e89b99b15124c14eda407702bc76d561fea5b275b7ccc76d89b372e9ea58c88729dddbadab65a72451529917bd3410073c998c3876e16f44ed8df3b7eb20caa253140730295bb4a2145cce464a594e902975e94d0d0579cbef235474a9239edb9f909ff2a340ff3db999cf84893ff7914a7aeb1bfe92bfdeba36bbbc6d29e0c8ebc921301901ff39b9139fe4625c989db9eeb470663b9def9e1a64c8b75495c66b3f22fe5470173592d41464116dfe2ff6efea67ab8b546de4f8c415d3ec2259b402329099f0fedeec6fa8475db36f8a159cdce223206a0629f921934aff96102534b064e323b288ab6c4f2b767c24da002a92c179bf14ee7f7ce26d6a42b4153d6e171afd3194d1bd21d2b027c6403fb69399ad5f6a45e2b2f7aeb56794717965d5b8e86b27b7f402e013dca8a6b16a6649d6ae10ddb0e92541b643a427a08d0002d089714420b7d78e17097cefcbb557130320d8bc4df957aaedc8fcc7a9dc2490507c0faf731aeb9cfbd15fa320502171d4270725432615b927307f94c1944824f1d43d456b8810090d402218e03270bb6a919c7a28f0d8fc73ef2be9588b4426120e5aa87e50a846b32b8ef1de7b4aaaa9a20e561756d78a4c2c0aced4f09f3ee049d4e77c8f7d4c05f228198bec8f4cd3a495c1a104715c8acadd0d2bcb1c4e08e86e5095a0c2594355f2e562633804e83b2b1aa7f2ee0ba2244f63576ea1bc57d9db3016dfe992c31a77fa82cfa59d41bd30535fbec1b6bf20e0dc0c84d7b1fbd052d6391bde91ec5c553fc289721ec8f55f85318452939bfc54930f5d9dd7240219fb4184075e7c312c4ff7dc6a2c855573b3b278d6f2ab2f1f36b15d50c06e2495a3ec583230c4c0d390e2f9b37b7888e1c9d906472ffd1c42b45a3da3b989f33a918864f6070347ad1c30302406a8b1a8e2c9629317133c2f02df7d514876cb13bb212ece626231f54b7ee406f5a1b11c7704b49322fcd1382e5ffe71661bf8bc41b3ccfa7cfe0e4f1f4be0f6bf1157343af4ad6c2fac2635ebd1cc04078fc3dbcfd4b171a65628fc92ad0188cce5c4e2c92ba4d91bed8c21c0e2846bb070388b09b6802599a52469f4225fa07eef9656bca363d75304c724362b9b446ff744ace22d19c55163d20f890a7044a61cdfbd0495eb68784004196ab1e7dd1641700c2a0dcdc9b6874dd1b1b1cd9283db5860bf946739367d16b7db597a2b1343bf1f7cbb054538459146255282246dadd06acc793d45872be753614d4b07bf43c5540a5b78f51e19e850cd4973678fd42593c8eb83f9daf5f4a8e6428459215f12d53c6579fc5c23f8a7ec1b481874bf388b9d75bf536f6cae2b6f5a477e4b6c10208ffc1773fc78f52c4da2e2045d4dfe4a8b294c1cb4306be5263251701f0980992c974080321dd253b0fd4923918bdbaa281f6fe88a54978754523b8dcefefa11c60a139514ab223167b26a102925259880761ee067cc628d9e46dfb20eb52b9715650fc356613120c6767d3305426ce26d8c8b606f2bff44aba745e9c7b8d350dbdbd027f94e5328b5602db27ff6fb13231ff07825c24bc5b694f5af66e5888b27297479594287c8f018bd27dc7b67e5efce361a9832262dcd1063d123b8acb79a27f7fc58404902d5ba40d1f136e20e5919c3737ae2bb8b73e06d703f08d1a7e21dd2cc2bf599ddb363a0eb96ca7c4c7cd8789feefbaae9cad8d28383e24e6cae8b246cac34325a73dacd68c05e0612ee1038316d499a3bf84a677bf4688eba5c22ef0cdb81b358ff4ee963b4d7f52a3b5402bef96b58179668563ab894bf77ef96aacb361f2faab8c0abc3793852e957cc6da2b1947c0c6a3a38faf8977da424c10b2e11fd49a0e6ac2175d667077141d4061a3897f370191c6b86706aa6919e0da22337245a05da6e949e75aed5bd6549835304ae82bf55abbb9f06ec7d067b2ce76d10c3e714d87f57d50b62cf3f7b2ccb209433df90ef0d6a96e79a29b6b103d8aa82ffe561a606252956aa7a83b1bf360cf2b6df89e7ab1360da2a46e34ca214c42c64497a7961d2b133cd66544ccfa21a604af16d1a5c6ed9fb13f9074f374eeaf67266e8e21c8418529aa7a4ad5de37a1ebf284bba4855ee4cbc8b395c87dd9315f35a11c1f65b65f20eca370d9e5d20abf22717e34a5102e64d2e881109071ab8884b0ac8bcdd2d2836345ff2a811d8dcf3d796b7cb7efba1fbbea7dc0398584d821149424f7a0b97e77ac63fd4479c107accb8ebfd61480a3ae098b057e27ae70573a43253940928a348b8a8f327bf312ffc972e474bda07b1530b9f13b0a5d7588052fe20e82121f4e04b504d76ad3a2fa8ec40dbb27af2690f307d7e03644b90b9bd16222942632ab08c64466943a67ffbe55af607e332d93e11c33631ed419e44cc4f1df55b99691a6ba8cfae097d1129243c40e25dbcdbc45f66fb728c38a9bffbb3beac153e40dd897004b94f80818f685c95b8339209f5f3bee1072cda7da1b4fc3ec249d1feaa6891b5910bc06df73918608a7b3c89e03ea38230154c3395e95b8008a147caac82478c527fca94b740ec99044cd34509f218f463ea15bd79d1a40d6ca695d339e13013d8235161df2c13646e9920938c68748cd8b2afd03056e068e08e725b7a8a8b4580223d6a0d0332c0675ec7b4252e2d1661e9d22ee9de5fa9e5e42e9a500691f3269270b618ca81882f66f3f09c2175ef65af4ec89176cfd18c81c6bc82be3c3a46995818a18622cfe15513887a371f3a47ca47658adba6800f31f4561d68437aed58e1f4c433145022029079ef7f8162ef1882ad9a20408e1f2d83b1b0546a12e552495adf3fd6f7fa081708e45c7fa5f490f79558fd1a637251caa6ec849a4fb770228db609b8a8b75bd0113f2a6f013c5a33b97461b6c7f0ec64d12a1144be9a01fdb63d932791a96980167fd5d6578efc14daa248b879bbda7757cafa01d02ab5c723c2a383fc2edf95f7fca603abadd54e3372da701a07764527af382656bd6768d176243da80d79e82aaf0adedd15d50f1f6650f948ea876044c160f3d8fcdc164d2ffe56ab63a12a01b7c708c725b1051cb17132a91e632a94de4e62e5a051b8b3feb88745dd6bff4784622ed4f8bee1adb9ea59d250518b9190381a2525f39140bd045a42e06a5520fe54a772f791fe2b1f09dc016b56c0fc0c25afad159f97d4be23e404f01badda4da9790ea0e5745e61c8b3f23869625146846b8c8a12d9a9e486c8fbbc5ab8e2d487eca63846c4dedcb2360b6f22010a01a8f7844941289440e7268d4f3858550d20f8c7ae5b36345066706fac0d256a23f27258525c6bebde866adbd1241f27681a8668ae92116873890dd7e577688eaf2e91ee41ce1695be47dc18fabb2dce161f9bcdd8368872dfa65bcbbb963ce7e895f5de8793bf03096ab199bfe8db952ca590f1414387df1ffe9f849dcbb841b15e65f095cdeabc51ce10ba3e3e8dcbc77c6238f66b2b730bec0c8d2af5176b8b5c4b71355cea1d050c3b97e65c163db6509caa2c2dce02a032989de665f4b42e78dca91cf224776387ab233746bfe25262fde01f994a0760c21ff83241d4cefc526f8f58d8664b7e94eee250e3da632d4e0740933eca99a4b8045b4ac2ae9c51b6c7d55b49f52934795929114ff9487e8baa1d940333c6bff91c9a18ec4de4bbd5fcc2364e3e5fdacbfd930dace4c10182286f2e0512b3f5fdcd19bbde3a298d72d15be14c67c7da4b8b78e928769875b4bccc003b680a41112f3493f7c4f272f5d91ce52e28b3bddfac270156b198c793b6c8919fb5eb45993b91a92498eb52f1c5b5aa801d8cf769abb48b0aa7e7830bfa2d76c0b7240baa9597217c6da84ecf1c627ad0f976d0e6afea8dde64b639277051511a6df871dfb0e33acd1f7e0b80f130e66edcc74a90dea3680b9dfe0f4fa3f86286b27d36122bd916fd3e266904a91b86096cfd48caa3f77b4307d9e0d05e1ee4746589087c559a2ab9e1e35f13b1eb498beb0d3dcfc1ec3ea2c974a720a285f8da538eda1d0579bf458d50ea28c52062f899ae16668f95f6b72a2dec3e55e714940e5d7347db42772755419f4477e6dac93ce26264fb64fb27df0d78c081c851454d1a2137ba39b63fd75348fdd7ab916fcc5dd38f4525dc5cd38d382738eb07a64ed4aad00ac8f3f73fe48d2adfb6a4b80f21024b5dc5ddd81afb471fc08f5e1df7c0e9f62c058fadf91d4c14f71cfe81ebdc241a281e77b1ebae55dc3d081f5187ae16592536b8c9565bffdd82362e941b9f13d6b30a5d8a7ad7cb1ec53ff719178bc4e9db0492b5ff530c42f0e8f5d82b1b32d99e3aab10cafc0cf3ad550ea8f051ad20f6bc7ee4da9544160fbf7c2d1e10d3a91365c954fd37c1eb5ce7c0efb6eab3fc0f6b8a9ee933c5fff48d9790dd701a145ec3699f2c19f16e16e39cc334a7aa09741cf77236ee66e2db15cfd0bb789cd7792c95ff7b810f9342119d8d220a29d942cbaf6787e923fc934be939a6ddd7f3ecd57df621f2f46d4ec05a735b44610101c4ea58697d5981ea8d40fe089bbca34db1c1cbb0cf53375ce0e507910f4b4f2955be212a1d1336f0d844166e6930b7a78ceb788a6c0e89bf69edee5630219c43e5c5264764c5bb39daa6da0ec69087ebaf4e7a5600fcea7620b8c02a93e5cbeb6694ee7a7177060102d16ca463d9994e6fc28c008a3949a8258c00179dd5bdb6001288d788403653db3fbe0eec85ba839dbb7c3316474c902521747163c433af6a1d8a387756fb07be132b124cd5d20b7224db58c9b18321d598f3ba806db5d95f13dd3f679b</script></div><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s部署 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>xgp</title>
      <link href="/posts/cfe.html"/>
      <url>/posts/cfe.html</url>
      
        <content type="html"><![CDATA[<p><img src="http://xgp-cunchu.test.upcdn.net/docker/image-20191224101214064.png" alt="img"></p><p><a href="http://xgp-cunchu.test.upcdn.net/xgp/image-20200102093813472.png" target="_blank" rel="noopener">http://xgp-cunchu.test.upcdn.net/xgp/image-20200102093813472.png</a></p><p><a href="http://xgp-cunchu.test.upcdn.net/xgp/image-20200102094039749.png" target="_blank" rel="noopener">http://xgp-cunchu.test.upcdn.net/xgp/image-20200102094039749.png</a></p><img src="/posts/cfe/1.png" class title="This is an example image"><img src="/posts/cfe/1.png" class title="This is an example image"><img src="/posts/cfe/1.png" class title="This is an example image"><p>./xgp/1.png</p><p><img src="/posts/1.png" alt="1"></p><p><img src="/posts/1.png" alt="1"></p>]]></content>
      
      
      <categories>
          
          <category> 测试 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 测试 </tag>
            
            <tag> 图片 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
    
    
    <entry>
      <title>可爱的我</title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>网站感想</title>
      <link href="/about/site.html"/>
      <url>/about/site.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>tags</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>contact</title>
      <link href="/contact/index.html"/>
      <url>/contact/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>categories</title>
      <link href="/categories/index.html"/>
      <url>/categories/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/lib/canvas-nest/canvas-nest-nomobile.min.js"/>
      <url>/lib/canvas-nest/canvas-nest-nomobile.min.js</url>
      
        <content type="html"><![CDATA[!function(){var userAgentInfo = navigator.userAgent;    var Agents = ["iPad", "iPhone", "Android",           "SymbianOS", "Windows Phone",           "iPod", "webOS", "BlackBerry", "IEMobile"];    for (var v = 0; v < Agents.length; v++) {      if (userAgentInfo.indexOf(Agents[v]) > 0) {        return;      }    }function o(w,v,i){return w.getAttribute(v)||i}function j(i){return document.getElementsByTagName(i)}function l(){var i=j("script"),w=i.length,v=i[w-1];return{l:w,z:o(v,"zIndex",-1),o:o(v,"opacity",0.5),c:o(v,"color","0,0,0"),n:o(v,"count",99)}}function k(){r=u.width=window.innerWidth||document.documentElement.clientWidth||document.body.clientWidth,n=u.height=window.innerHeight||document.documentElement.clientHeight||document.body.clientHeight}function b(){e.clearRect(0,0,r,n);var w=[f].concat(t);var x,v,A,B,z,y;t.forEach(function(i){i.x+=i.xa,i.y+=i.ya,i.xa*=i.x>r||i.x<0?-1:1,i.ya*=i.y>n||i.y<0?-1:1,e.fillRect(i.x-0.5,i.y-0.5,1,1);for(v=0;v<w.length;v++){x=w[v];if(i!==x&&null!==x.x&&null!==x.y){B=i.x-x.x,z=i.y-x.y,y=B*B+z*z;y<x.max&&(x===f&&y>=x.max/2&&(i.x-=0.03*B,i.y-=0.03*z),A=(x.max-y)/x.max,e.beginPath(),e.lineWidth=A/2,e.strokeStyle="rgba("+s.c+","+(A+0.2)+")",e.moveTo(i.x,i.y),e.lineTo(x.x,x.y),e.stroke())}}w.splice(w.indexOf(i),1)}),m(b)}var u=document.createElement("canvas"),s=l(),c="c_n"+s.l,e=u.getContext("2d"),r,n,m=window.requestAnimationFrame||window.webkitRequestAnimationFrame||window.mozRequestAnimationFrame||window.oRequestAnimationFrame||window.msRequestAnimationFrame||function(i){window.setTimeout(i,1000/45)},a=Math.random,f={x:null,y:null,max:20000};u.id=c;u.style.cssText="position:fixed;top:0;left:0;z-index:"+s.z+";opacity:"+s.o;j("body")[0].appendChild(u);k(),window.onresize=k;window.onmousemove=function(i){i=i||window.event,f.x=i.clientX,f.y=i.clientY},window.onmouseout=function(){f.x=null,f.y=null};for(var t=[],p=0;s.n>p;p++){var h=a()*r,g=a()*n,q=2*a()-1,d=2*a()-1;t.push({x:h,y:g,xa:q,ya:d,max:6000})}setTimeout(function(){b()},100)}();</0?-1:1,e.fillRect(i.x-0.5,i.y-0.5,1,1);for(v=0;v<w.length;v++){x=w[v];if(i!==x&&null!==x.x&&null!==x.y){B=i.x-x.x,z=i.y-x.y,y=B*B+z*z;y<x.max&&(x===f&&y></0?-1:1,i.ya*=i.y>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/lib/canvas-nest/README.html"/>
      <url>/lib/canvas-nest/README.html</url>
      
        <content type="html"><![CDATA[<h1>Theme NexT Canvas Nest</h1><p><img src="https://img.shields.io/badge/NexT-v7.3.0+-blue?style=flat-square" alt="Theme Version"></p><p><a href="https://github.com/hustcc/canvas-nest.js" target="_blank" rel="noopener">canvas-nest.js</a> for <a href="https://github.com/theme-next" target="_blank" rel="noopener">NexT</a>.</p><h2 id="Install">Install</h2><h3 id="Step-1-→-Go-to-Hexo-dir">Step 1 → Go to Hexo dir</h3><p>Change dir to <strong>Hexo</strong> directory. There must be <code>scaffolds</code>, <code>source</code>, <code>themes</code> and other directories:</p><pre class=" language-language-sh"><code class="language-language-sh">$ cd hexo$ lsscaffolds  source  themes  _config.yml  package.json</code></pre><h3 id="Step-2-→-Create-footer-swig">Step 2 → Create <code>footer.swig</code></h3><p>Create a file named <code>footer.swig</code> in <code>hexo/source/_data</code> directory (create <code>_data</code> directory if it does not exist).</p><p>Edit this file and add the following content:</p><pre class=" language-language-xml"><code class="language-language-xml"><script color="0,0,255" opacity="0.5" zIndex="-1" count="99" src="https://cdn.jsdelivr.net/npm/canvas-nest.js@1/dist/canvas-nest.js"></script></code></pre><p>You can customize these options.</p><h3 id="Step-3-→-Set-it-up">Step 3 → Set it up</h3><p>In the NexT <code>_config.yml</code>, uncomment <code>footer</code> under the <code>custom_file_path</code> section.</p><pre class=" language-language-yml"><code class="language-language-yml"># Define custom file paths.# Create your custom files in site directory `source/_data` and uncomment needed files below.custom_file_path:  #head: source/_data/head.swig  #header: source/_data/header.swig  #sidebar: source/_data/sidebar.swig  #postMeta: source/_data/post-meta.swig  #postBodyEnd: source/_data/post-body-end.swig  footer: source/_data/footer.swig  #bodyEnd: source/_data/body-end.swig  #variable: source/_data/variables.styl  #mixin: source/_data/mixins.styl  #style: source/_data/styles.styl</code></pre>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/lib/canvas-nest/canvas-nest.min.js"/>
      <url>/lib/canvas-nest/canvas-nest.min.js</url>
      
        <content type="html"><![CDATA[!function(){function o(w,v,i){return w.getAttribute(v)||i}function j(i){return document.getElementsByTagName(i)}function l(){var i=j("script"),w=i.length,v=i[w-1];return{l:w,z:o(v,"zIndex",-1),o:o(v,"opacity",0.5),c:o(v,"color","0,0,0"),n:o(v,"count",99)}}function k(){r=u.width=window.innerWidth||document.documentElement.clientWidth||document.body.clientWidth,n=u.height=window.innerHeight||document.documentElement.clientHeight||document.body.clientHeight}function b(){e.clearRect(0,0,r,n);var w=[f].concat(t);var x,v,A,B,z,y;t.forEach(function(i){i.x+=i.xa,i.y+=i.ya,i.xa*=i.x>r||i.x<0?-1:1,i.ya*=i.y>n||i.y<0?-1:1,e.fillRect(i.x-0.5,i.y-0.5,1,1);for(v=0;v<w.length;v++){x=w[v];if(i!==x&&null!==x.x&&null!==x.y){B=i.x-x.x,z=i.y-x.y,y=B*B+z*z;y<x.max&&(x===f&&y>=x.max/2&&(i.x-=0.03*B,i.y-=0.03*z),A=(x.max-y)/x.max,e.beginPath(),e.lineWidth=A/2,e.strokeStyle="rgba("+s.c+","+(A+0.2)+")",e.moveTo(i.x,i.y),e.lineTo(x.x,x.y),e.stroke())}}w.splice(w.indexOf(i),1)}),m(b)}var u=document.createElement("canvas"),s=l(),c="c_n"+s.l,e=u.getContext("2d"),r,n,m=window.requestAnimationFrame||window.webkitRequestAnimationFrame||window.mozRequestAnimationFrame||window.oRequestAnimationFrame||window.msRequestAnimationFrame||function(i){window.setTimeout(i,1000/45)},a=Math.random,f={x:null,y:null,max:20000};u.id=c;u.style.cssText="position:fixed;top:0;left:0;z-index:"+s.z+";opacity:"+s.o;j("body")[0].appendChild(u);k(),window.onresize=k;window.onmousemove=function(i){i=i||window.event,f.x=i.clientX,f.y=i.clientY},window.onmouseout=function(){f.x=null,f.y=null};for(var t=[],p=0;s.n>p;p++){var h=a()*r,g=a()*n,q=2*a()-1,d=2*a()-1;t.push({x:h,y:g,xa:q,ya:d,max:6000})}setTimeout(function(){b()},100)}();</0?-1:1,e.fillRect(i.x-0.5,i.y-0.5,1,1);for(v=0;v<w.length;v++){x=w[v];if(i!==x&&null!==x.x&&null!==x.y){B=i.x-x.x,z=i.y-x.y,y=B*B+z*z;y<x.max&&(x===f&&y></0?-1:1,i.ya*=i.y>]]></content>
      
    </entry>
    
    
  
</search>
