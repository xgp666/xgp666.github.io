<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>01 部署k8s集群</title>
      <link href="/posts/2cda.html"/>
      <url>/posts/2cda.html</url>
      
        <content type="html"><![CDATA[<h2 id="k8s">k8s</h2><h4 id="最基本的硬件要求"><strong>最基本的硬件要求</strong></h4><p><strong>CPU: 双核</strong><br><strong>Mem: 2G</strong><br><strong>3台dockerhost</strong><br><strong>时间必须同步</strong></p><h1>实验环境</h1><table><thead><tr><th>主机名</th><th>IP地址</th><th>服务</th></tr></thead><tbody><tr><td>master</td><td>192.168.1.21</td><td>dockerhost</td></tr><tr><td>node01</td><td>192.168.1.22</td><td>dockerhost</td></tr><tr><td>node02</td><td>192.168.1.23</td><td>dockerhost</td></tr></tbody></table><h2 id="环境准备">环境准备</h2><p><strong>分别将3台虚拟机命名，设置好对应IP，并将其写入域名解析/etc/hosts中，关闭防火墙，iptables，禁用selinux。还有要做到，时间必须一致。全部禁用swap</strong></p><h3 id="1-给三台docker命名">1.给三台docker命名</h3><p><strong>k8.1</strong></p><pre><code>[root@localhost ~]# hostnamectl set-hostname master[root@localhost ~]# su -</code></pre><p><strong>k8.2</strong></p><pre><code>[root@localhost ~]# hostnamectl set-hostname node01[root@localhost ~]# su -</code></pre><p><strong>k8.3</strong></p><pre><code>[root@localhost ~]# hostnamectl set-hostname node02[root@localhost ~]# su -</code></pre><p>验证docker是否能使用及版本是否一样</p><pre><code>[root@master ~]# docker -v</code></pre><p><img src="/" class="lazyload" data-src="/posts/image-20200102093813472.png"  alt="image-20200102093813472"></p><h3 id="2-关闭防火墙及禁用selinux">2.关闭防火墙及禁用selinux</h3><pre><code>[root@master ~]# systemctl stop firewalld[root@master ~]# systemctl disable firewalld [root@master ~]# vim /etc/selinux/config</code></pre><p><img src="/" class="lazyload" data-src="/posts/picture_path"  alt="description" title="image-20200102115453524.png"></p><p><img src="/" class="lazyload" data-src="/posts/image-20200102115453524.png"  alt="image-20200501001702921"></p><h3 id="3-禁用swap（三台）">3.  禁用swap（三台）</h3><pre><code>[root@master ~]# swapoff -a//临时禁用swap[root@master ~]# free -h[root@master ~]# vim /etc/fstab </code></pre><h3 id="4-添加域名解析（三台）">4.添加域名解析（三台）</h3><pre><code>[root@master ~]# echo 192.168.1.21 master &gt;&gt; /etc/hosts[root@master ~]# echo 192.168.1.22 node01 &gt;&gt; /etc/hosts[root@master ~]# echo 192.168.1.23 node02 &gt;&gt; /etc/hosts</code></pre><h3 id="5-做免密登陆（三台）">5.做免密登陆（三台）</h3><pre><code>[root@master ~]# ssh-keygen -t rsa//生成密钥</code></pre><p><strong>复制密钥到其他主机</strong></p><pre><code>   54  ssh-copy-id node01   55  ssh-copy-id node02</code></pre><h4 id="把域名解析复制到其他主机"><strong>把域名解析复制到其他主机</strong></h4><pre><code>   63  scp /etc/hosts node01:/etc   64  scp /etc/hosts node02:/etc</code></pre><h3 id="6-打开路由转发和iptables桥接功能（三台）">6.打开路由转发和iptables桥接功能（三台）</h3><pre><code>[root@master ~]# vim /etc/sysctl.d/k8s.conf//开启iptables桥接功能net.bridge.bridge-nf-call-iptables = 1net.bridge.bridge-nf-call-ip6tables = 1[root@master ~]# echo net.ipv4.ip_forward = 1 &gt;&gt; /etc/sysctl.conf //**打开路由转发[root@master ~]# sysctl -p /etc/sysctl.d/k8s.conf [root@master ~]# sysctl -p //刷新一下</code></pre><p><strong>如果以上命令执行失败可能是缺少模块，可执行以下命令</strong></p><pre><code>[root@master ~]# modprobe br_netfiler</code></pre><p><strong>把路由转发和iptables桥接复制到其他主机</strong></p><pre><code>[root@master ~]# scp /etc/sysctl.d/k8s.conf  node01:/etc/sysctl.d/[root@master ~]# scp /etc/sysctl.d/k8s.conf  node02:/etc/sysctl.d/[root@master ~]# scp /etc/sysctl.conf  node02:/etc/[root@master ~]# scp /etc/sysctl.conf  node01:/etc/</code></pre><p><strong>记得node01和node02也要执行以下命令</strong></p><pre><code>[root@master ~]# sysctl -p /etc/sysctl.d/k8s.conf [root@master ~]# sysctl -p </code></pre><h1>master节点安装部署k8s</h1><h2 id="指定yum安装kubernetes的yum源（三台）">指定yum安装kubernetes的yum源（三台）</h2><pre><code>cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF</code></pre><p><strong>下载完成之后，查看一下仓库是否可用</strong></p><pre><code>[root@master ~]# yum repolist </code></pre><p>![image-20200102095945934](./01 部署k8s集群/image-20200102095945934.png)</p><p><strong>创建本地缓存（三台）</strong></p><pre><code>[root@master ~]# yum makecache fast</code></pre><h2 id="各节点安装所需安装包">各节点安装所需安装包</h2><h3 id="master下载"><strong>master下载</strong></h3><pre><code>[root@master ~]# yum -y install kubeadm-1.15.0-0 kubelet-1.15.0-0 kubectl-1.15.0-0</code></pre><h3 id="node01和node02下载"><strong>node01和node02下载</strong></h3><pre><code>[root@node01 ~]# yum -y install kubeadm-1.15.0-0 kubelet-1.15.0-0</code></pre><h3 id="三台主机把-kubelet加入开机自启"><strong>三台主机把 kubelet加入开机自启</strong></h3><pre><code>[root@master ~]# systemctl enable kubelet</code></pre><h2 id="master导入，之前准备好的镜像"><strong>master导入，之前准备好的镜像</strong></h2><pre><code>[root@master ~]# mkdir images[root@master ~]# cd images/[root@master images]# ls</code></pre><p>![image-20200102101531123](./01 部署k8s集群/image-20200102101531123.png)</p><h3 id="创建一个导入镜像的脚本"><strong>创建一个导入镜像的脚本</strong></h3><pre><code>[root@master images]# cat &gt; image.sh &lt;&lt;EOF&gt; #!/bin/bash&gt; for i in /root/images/*&gt; do&gt; docker load &lt; $i &gt; done&gt; EOF[root@master images]# chmod +x image.sh </code></pre><h3 id="导入镜像">导入镜像</h3><pre><code>[root@master images]# sh image.sh </code></pre><h3 id="初始化Kubernetes集群">初始化Kubernetes集群</h3><pre><code>[root@master ~]#  kubeadm init --kubernetes-version=v1.15.0 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 --ignore-preflight-errors=Swap</code></pre><p><strong>如果以上的命令报错，找出问题后先重置一下（下面的命令），然后再执行以上命令</strong></p><pre><code>[root@master ~]# kubeadm reset//重置kubeadm</code></pre><p>![image-20200102122213788](./01 部署k8s集群/image-20200102122213788.png)</p><pre><code>[root@master images]# kubectl get node//查看当前节点信息</code></pre><p>![image-20200102110808239](./01 部署k8s集群/image-20200102110808239.png)</p><p><strong>可以看出master的状态是未就绪（NotReady），之所以是这种状态是因为还缺少一个附件flannel，没有网络各Pod是无法通信的</strong></p><h3 id="也可以通过检查组件的健康状态"><strong>也可以通过检查组件的健康状态</strong></h3><pre><code>[root@master images]# kubectl get cs</code></pre><p>![image-20200102122413443](./01 部署k8s集群/image-20200102122413443.png)</p><h3 id="添加网络组件（flannel）">添加网络组件（flannel）</h3><p><strong>组件flannel可以通过https://github.com/coreos/flannel中获取</strong></p><pre><code>[root@master ~]# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</code></pre><p>以上只是方式之一，在网络状况良好的情况下建议使用上述方法（调用远端文件执行一下），<strong>若网速较差，建议使用以下方法：</strong></p><pre><code>[root@master images]# wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml#将github官网指定的.yml配置文件下载到本地[root@master images]# ls | grep flannel.yml   #确定下载到了当前目录kube-flannel.yml[root@master images]# kubectl apply -f kube-flannel.yml  #指定下载的.yml文件执行相应命令</code></pre><p><em><strong>上述方法，二选一进行配置即可。</strong></em></p><p><strong>看到很多东西被创建是还不够的，还需要查看flannel是否处于正常启动并运行的状态，才算正在的部署完成</strong></p><pre><code>[root@master images]# kubectl get pods --all-namespaces//查看所有的名称空间的pod（可以看到flannel网络运行正常）</code></pre><p>![image-20200102122732587](./01 部署k8s集群/image-20200102122732587.png)</p><pre><code>[root@master images]# kubectl get pod -n kube-system//查看名称空间为kube-system的pod</code></pre><p>![image-20200102122826221](./01 部署k8s集群/image-20200102122826221.png)</p><p>查看当前节点信息</p><pre><code>kubectl get node//查看当前节点信息（已经准备好了）</code></pre><p>![image-20200102111853910](./01 部署k8s集群/image-20200102111853910.png)</p><h1>node两台节点，导入镜像并加入群集</h1><h2 id="导入镜像-2">导入镜像</h2><p>上传所需镜像包，也可以使用docker pull下载</p><p>![image-20200102133744555](./01 部署k8s集群/image-20200102133744555.png)</p><pre><code>[root@node01 images]# docker load &lt; kube-proxy-1-15.tar &amp;&amp; docker load -i myflannel-11-0.tar  &amp;&amp; docker load -i pause-3-1.tar</code></pre><pre><code>[root@node01 images]# docker images//查看本地镜像</code></pre><p>![image-20200102134006724](./01 部署k8s集群/image-20200102134006724.png)</p><h2 id="node01和node02加入群集">node01和node02加入群集</h2><p><strong>这时使用的命令是初始化群集之后生成的令牌（只有24小时的时效）</strong></p><p>![image-20200102134336922](./01 部署k8s集群/image-20200102134336922.png)</p><pre><code>[root@node01 ~]# kubeadm join 192.168.1.21:6443 --token z0vknh.s6ib4eu4f8bre2nu     --discovery-token-ca-cert-hash sha256:8da72cc83f45d1247f42ce888658129b43726fe2af4ffc0c4e79faedb4050359</code></pre><h2 id="加入群集之后查看一下">加入群集之后查看一下</h2><pre><code>[root@master images]# kubectl get node</code></pre><p>![image-20200102114628989](./01 部署k8s集群/image-20200102114628989.png)</p><h1>各节点优化一下</h1><h2 id="设置table键的默认间距；">设置table键的默认间距；</h2><pre><code>[root@master ~]# vim .vimrcset tabstop=2[root@master ~]# source .vimrc </code></pre><h2 id="设置kubectl命令自动补全">设置kubectl命令自动补全</h2><pre><code>[root@master ~]# yum  -y install bash-completion[root@master ~]#  source /usr/share/bash-completion/bash_completion [root@master ~]# source &lt;(kubectl completion bash)[root@master ~]# echo "source &lt;(kubectl completion bash)" &gt;&gt; ~/.bashrc</code></pre><h2 id="确认k8s群集没有问题，并设置为开机自启">确认k8s群集没有问题，并设置为开机自启</h2><h3 id="master主机操作如下："><strong>master主机操作如下：</strong></h3><pre><code>[root@master ~]# kubectl get pod -n kube-system   #查看pod资源，类似于docker中的容器，确保返回的信息都是running#“-n kube-system”：是k8s的名称空间</code></pre><p>![image-20200102142028971](./01 部署k8s集群/image-20200102142028971.png)</p><h3 id="master和node节点上都需要进行以下操作，以便设置为开机自启："><strong>master和node节点上都需要进行以下操作，以便设置为开机自启：</strong></h3><pre><code>[root@master ~]# systemctl enable kubelet[root@master ~]# systemctl enable docker </code></pre><p><strong>设置为开机自启后，k8s群集的配置基本完成了，现在可以重启一下这三台服务器，如果重启后，执行下面的命令，状态都还是running，则表示绝对没有问题了。</strong></p><pre><code>[root@master ~]# kubectl get pod -n kube-system    #重启后验证状态是否还都是running</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>xgp</title>
      <link href="/posts/xxxx.html"/>
      <url>/posts/xxxx.html</url>
      
        <content type="html"><![CDATA[<p><img src="/" class="lazyload" data-src="/posts/%E8%BD%AF%E4%BB%B6/%E5%8D%9A%E5%AE%A2/Blog/blog/source/_posts/xgp.assets/image-20200102110722214.png"  alt="image-20200501002500703"></p><p><img src="/" class="lazyload" data-src="/posts/.%5Cxgp%5Cimage-20200102110722214.png"  alt="image-20200501002500703"></p><p><img src="/" class="lazyload" data-src="/posts/image-20200102110722214-1588267414224.png"  alt="image-20200501002500703"></p><p><img src="/" class="lazyload" data-src="/posts/image-20200102110722214-1588267414224.png"  alt="图片1"></p><img src="/" class="lazyload" data-src="/posts/image-20200102110722214-1588267414224.png"  width="100%" height="100%" align="center/"><img src="/" class="lazyload" data-src="/posts/image-20200102110722214-1588267414224.png"  width="100%" height="100%" align="center/"><p><img src="/" class="lazyload" data-src="/posts/image-20200102110722214-1588267414224.png"  alt="image-20200102110722214-1588267414224"></p> class="lazyload" data-src="/posts/image-20200102110722214-1588267414224.png" <img src="/"><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>26 Python监控Linux系统</title>
      <link href="/posts/9bcf.html"/>
      <url>/posts/9bcf.html</url>
      
        <content type="html"><![CDATA[<h1>使用Python监控Linux系统</h1><p><strong>Linux下有许多使用Python语言编写的监控工具，如inotify-sync, dstat和glances. 此外，如果要根据业务编写简单的监控脚本，很多工程师也会选择Python语言。Python语言是一门简单 易学/语法清晰/表达能力强的编程语言,非常适合于编写监控程序的场景。使用Python语言编写监控程序具有以下几个优势:</strong></p><ul><li><strong>Python语言开发效率高。 Python语言有自 己的优势与劣势，使用Python开发监控程序是一个充分发挥Python优势，避免Python劣势的领域。对于监控程序来说，能够利用Python语言开发效率高的优势尽快完成程序的编写工作。同时，监控程序也不要求性能，因此避免了Python语言性能不如C、C++和Java的劣势。</strong></li><li><strong>Python语言表达能力强。相信任何-个学习Linux的工程师都使用过shel脚本编写过监控程序。虽然Linux下有很多监控工具,也有很多文本处理程序，但是获取监控与解析结果是完全不同的工具。解析监控结果的程序不理解监控程序输出结果的具体含义。Python语言中有非常丰富的数据结构，可以用各种方式保存监控结果，以便后续处理。</strong></li><li><strong>利用第三方库开发监控程序。Python的标准库本身非常强大,被称为”连电池都包含在内”。对于-个问题,如果标准库没有提供相应的工具,那么也会有开源的项目来填补这个空白。监控程序正式这样一种情况， 在Python语言中，具有非常成熟的第三方库帮助开发者简化监控程序的编写工作。</strong></li></ul><h1>一、Python编写的监控工具</h1><p><strong>我们将介绍两个Python语言编写的监控工具，分别是dstat和glances。</strong></p><h2 id="1、多功能系统资源统计工具dstat">1、多功能系统资源统计工具dstat</h2><p><strong>dstat是一个用Python语言实现的多功能系统资源统计工具，用来取代Linux下的vmstat、iostat、netstat和ifstat等命令。并且，dstat克服了这些命令的限制，增加了额外的功能、以及更多的计数器与更好的灵活性。dstat可以在一个界面上展示非常全面的监控信息，因此，在系统监控、基准测试和故障排除等应用场景下特别有用。</strong></p><p><strong>我们可以使用dstat监控所有系统资源的使用情况，并且可以结合不同的场景定制监控的资源。例如，在同一时间段以相同的时间频率比较网络带宽与磁盘的吞吐率。</strong></p><p><strong>dstat将以列表的形式显示监控信息，并且用不同的颜色进行输出，以可读性较强的单位展示监控数值。例如，对于字节数值，dstat自动根据数值的大小，以K、M、G等单位进行显示，避免了开发者使用其他命令时因为数值太大造成的困惑和错误。此外，使用dstat还可以非常方便地编写插件用来收集默认情况下没有收集的监控信息。dstat是专门为人们实时查看监控信息设计的，因此，默认将监控结果输出到屏幕终端。我们也可以将监控信息以CSV格式输出到文件，以便后续处理。</strong></p><h2 id="2、dstat介绍">2、dstat介绍</h2><p><strong>作为一个多功能系统资源统计工具， dstat具有以下特性:</strong></p><ul><li><strong>结合了 vmstat, iostat, ifstat, netstat 等监控工具的功能，并且提供了更多的监控信息；</strong></li><li><strong>实时显示监控数据；</strong></li><li><strong>在问题分析和故障排查时，可以监视最重要的计数器，也可以对计数器进行排序；</strong></li><li><strong>模块化设计；</strong></li><li><strong>使用 Python 语言编写，更方便扩展现有的工作任务；</strong></li><li><strong>容易扩展，便于添加自定义的计数器；</strong></li><li><strong>包含的许多扩展插件充分说明了增加新的监控项目是很方便的；</strong></li><li><strong>可以分组统计块设备/网络设备，并给出、汇总信息；</strong></li><li><strong>可以显示每台设备中断信息；</strong></li><li><strong>非常准确的时间精度，即便是系统负荷较高也不会延迟显示；</strong></li><li><strong>准确显示单位，限制转换误差范围；</strong></li><li><strong>用不同的颜色显示不同的单位，增加可读性；</strong></li><li><strong>显示中间结果延时小于1秒</strong></li><li><strong>支持 csv 格式输出，便于将监控信息导人 Gnumeric 和 Excel 以生成图形。</strong></li></ul><h2 id="3、安装使用">3、安装使用</h2><p><strong>如果操作系统默认没有安装dstat.那么需要我们手动进行安装。如下所示:</strong></p><pre class=" language-language-shell"><code class="language-language-shell">[root@python ~]# yum -y install dstat</code></pre><p><strong>&lt;1&gt;dstat命令的–version选项，除了显示出tat的版本以外，还会显示操作系统的版本、Python语言的版本、cpu的个数，以及dstat支持的插件列表等详细信息。如下所示：</strong></p><pre class=" language-language-shell"><code class="language-language-shell">[root@python ~]# dstat --version</code></pre><p><img src="/" class="lazyload" data-src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5Cpython%5Cpython%E6%96%87%E6%A1%A3%5C26.assets%5Cimage-20200429141843251.png"  alt="image-20200429141843251"></p><p><strong>&lt;2&gt;dstat --list获取dstat的插件列表</strong></p><pre class=" language-language-shell"><code class="language-language-shell">dstat --list</code></pre><p><img src="/" class="lazyload" data-src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5Cpython%5Cpython%E6%96%87%E6%A1%A3%5C26.assets%5Cimage-20200429141907299.png"  alt="image-20200429141907299"></p><p><strong>&lt;3&gt;直接在终端输入dstat命令，dstat将以默认参数运行。默认情况下，dstat会收集cpu、磁盘、网络、换页和系统信息，并以一秒钟一次的频率进行输出，直到我们按 ctrl+c 结束。</strong></p><pre class=" language-language-shell"><code class="language-language-shell">[root@python ~]# dstat </code></pre><p><img src="/" class="lazyload" data-src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5Cpython%5Cpython%E6%96%87%E6%A1%A3%5C26.assets%5Cimage-20200429141632842.png"  alt="image-20200429141632842"></p><h2 id="4、常用选项如下">4、常用选项如下: .</h2><blockquote><p><strong>直接跟数字，表示#秒收集一次数据，默认为1秒; dstat 5表示5秒更新一次</strong></p></blockquote><ul><li><p><strong>-c,–cpu：统计CPU状态，包括system，user，idle，wait，hardware interrupt，software，interrupt等;</strong></p></li><li><p><strong>-d，–disk：统计磁盘读写状态</strong></p></li><li><p><strong>-D total,sda：统计指定磁盘或汇总信息</strong></p></li><li><p><strong>-l，–load：统计系统负载情况，包括1分钟、5分钟、15分钟平均值</strong></p></li><li><p><strong>-m，- -mem：统计系统物理内存使用情况，包括used, buffers， cache，free</strong></p></li><li><p><strong>-s，–swap：统计swap已使用和剩余量</strong></p></li><li><p><strong>-n，–net：统计网络使用情况，包括接收和发送数据</strong></p></li><li><p><strong>-N eth1,total 统计eth1接口汇总流量</strong></p></li><li><p><strong>-r，–io：统计I/0请求，包括读写请求</strong></p></li><li><p><strong>-p，–proc：统计进程信息，包括runnable、uninterruptible、new</strong></p></li><li><p><strong>-y，–sys：统计系统信息，包括中断、上下文切换</strong></p></li><li><p><strong>-t：显示统计时时间，对分析历史数据非常有用</strong></p></li><li><p><strong>–fs：统计文件打开数和inodes数</strong></p></li></ul><p><strong>除了前面介绍的与监控相关的参数以外，dstat还可以像vmstat和iostat- 样使用参数控制报告的时间间隔，或者同时指定时间间隔与报告次数。</strong></p><p><strong>例如，下面的命令表示以默认的选项运行dstat,每2秒钟输出1条监控信息，并在输出10条监控信息以后退出dstat。如下所示:</strong></p><pre class=" language-language-shell"><code class="language-language-shell">[root@python ~]# dstat 2 10You did not select any stats, using -cdngy by default.Terminal width too small, trimming output.----total-cpu-usage---- -dsk/total- -net/total- ---paging-->usr sys idl wai hiq siq| read  writ| recv  send|  in   out >  2   1  96   0   0   0| 270k  233k|   0     0 |  75B 1812B>  0   0 100   0   0   0|   0     0 |  60B  510B|   0     0 >  0   0 100   0   0   0|   0     0 |  60B  294B|   0     0 >  0   1 100   0   0   0|   0     0 |  60B  294B|   0     0 >  0   0 100   0   0   0|   0     0 | 182B  294B|   0     0 >  1   0 100   0   0   0|   0     0 |  60B  294B|   0     0 >  0   1  99   0   0   0|   0     0 |  60B  294B|   0     0 >  0   1 100   0   0   0|   0     0 |  60B  294B|   0     0 >  0   0 100   0   0   0|   0     0 |  60B  294B|   0     0 >  1   0 100   0   0   0|   0     0 |  60B  294B|   0     0 >  0   0 100   0   0   0|   0    25k|  60B  298B|   0     0 ></code></pre><p><strong>dstat命令中有很多参数可选，你可以通过man dstat命令查看，大多数常用的参数有这些：</strong></p><ul><li><strong>-l ：显示负载统计量</strong></li><li><strong>-m ：显示内存使用率（包括used，buffer，cache，free值）</strong></li><li><strong>-r ：显示I/O统计</strong></li><li><strong>-s ：显示交换分区使用情况</strong></li><li><strong>-t ：将当前时间显示在第一行</strong></li><li><strong>–fs ：显示文件系统统计数据（包括文件总数量和inodes值）</strong></li><li><strong>–nocolor ：不显示颜色（有时候有用）</strong></li><li><strong>–socket ：显示网络统计数据</strong></li><li><strong>–tcp ：显示常用的TCP统计</strong></li><li><strong>–udp ：显示监听的UDP接口及其当前用量的一些动态数据</strong></li></ul><p><strong>dstat附带了一些插件很大程度地扩展了它的功能。你可以通过查看/usr/share/dstat目录来查看它们的一些使用方法，常用的有这些：</strong></p><ul><li><strong>-–disk-util ：显示某一时间磁盘的忙碌状况</strong></li><li><strong>-–freespace ：显示当前磁盘空间使用率</strong></li><li><strong>-–proc-count ：显示正在运行的程序数量</strong></li><li><strong>-–top-bio ：指出块I/O最大的进程</strong></li><li><strong>-–top-cpu ：图形化显示CPU占用最大的进程</strong></li><li><strong>-–top-io ：显示正常I/O最大的进程</strong></li><li><strong>-–top-mem ：显示占用最多内存的进程</strong></li></ul><h2 id="5、-dstat高级用法">5、 dstat高级用法</h2><p><strong>dstat的强大之处不仅仅是因为它聚合了多种工具的监控结果，还因为它能通过附带的插件事项一些更高级功能。</strong></p><p><strong>如:找出磁盘重占用资源最高的进程和用户。</strong></p><p><strong>dstat -cdlmnpsyt 5 可以得到较全面的系统性能数据。</strong></p><p><strong>dstat的–top-(io|bio|cpu|cputime|cputime-avg |mem)通过这几个选项，可以看到具体是那个用户哪个进程占用了相关系统资源, 对系统调优非常有效。如查看当前占用I/O、 cpu、内存等最高的进程信息可以使用dstat --top-mem --top-io --top-cpu选项。以下示例演示了如何找出占用资源最多的进程。</strong></p><pre class=" language-language-shell"><code class="language-language-shell">[root@python scripts]# dstat --top-mem --top-io --top-cpu//查看当前占用I/O、CPU、内存等最高的进程信息--most-expensive- ----most-expensive---- -most-expensive-  memory process |     i/o process      |  cpu process   gnome-shell 53.0M|bash        420k  119k|vmtoolsd     0.1gnome-shell 53.0M|BT-Task    1026B    0 |                gnome-shell 53.0M|gnome-shell 352B   82k|kworker/0:0  1.0gnome-shell 53.0M|sshd: root@ 230B  196B|                gnome-shell 53.0M|sshd: root@ 155B  196B|                gnome-shell 53.0M|sshd: root@ 155B  196B|                gnome-shell 53.0M|sshd: root@ 155B  196B|                gnome-shell 53.0M|BT-Task    1406B    0 |                </code></pre><p><em><strong>dstat的插件保存在/usr/share/dstat目录下， 我们可以参考它们的实现，编写自己的插件。</strong></em></p><h2 id="6、将结果输出到CSV文件">6、将结果输出到CSV文件</h2><p><strong>dstat还可以将监控信息保存到CSV文件中，以便后续进行处理。通过–output选项指定监控数据输出的文件。如下所示:</strong></p><pre class=" language-language-shell"><code class="language-language-shell">[root@python ~]# dstat -a --output dstat_output.csvTerminal width too small, trimming output.----total-cpu-usage---- -dsk/total- -net/total- ---paging-->usr sys idl wai hiq siq| read  writ| recv  send|  in   out >  2   1  97   0   0   0| 175k  158k|   0     0 |  59B 1973B>  0   0 100   0   0   0|   0     0 | 150B  822B|   0     0 >  0   0 100   0   0   0|   0     0 |  60B  298B|   0     0 >  0   0 100   0   0   0|   0     0 |  60B  298B|   0     0 >  0   0 100   0   0   0|   0     0 |  60B  298B|   0     0 >  0   1  99   0   0   0|   0     0 |  60B  298B|   0     0 >  0   0 100   0   0   0|   0     0 | 210B  448B|   0     0 >  1   0  99   0   0   0|   0    49k|  60B  298B|   0     0 >  0   0 100   0   0   0|   0     0 | 210B  396B|   0     0 >  0   0 100   0   0   0|   0     0 |  60B  298B|   0     0 >^C</code></pre><h3 id="用excel查看信息">用excel查看信息</h3><pre class=" language-language-shell"><code class="language-language-shell">[root@python ~]# sz dstat_output.csv//导出本地文件到windows指定位置</code></pre><p><img src="/" class="lazyload" data-src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5Cpython%5Cpython%E6%96%87%E6%A1%A3%5C26.assets%5Cimage-20200429160149926.png"  alt="image-20200429160149926"></p><h1>二、交互性监控工具glances</h1><h2 id="1、glances简介">1、glances简介</h2><p><strong>glances是一款使用Python语言开发、基于psutil的跨平台系统监控工具。在所有的Linux命令行工具中，它与top命令最相似，都是命令行交互式监控工具。但是，glances实现了比top命令更齐全的监控，提供了更加丰富的功能。</strong></p><p><strong>在紧急情况下，工程师需要在尽可能短的时间内查看尽可能多的信息。此时，glances是一个不错的选择。 glances的设计初衷就是在当前窗口中尽可能多地显示系统消息。</strong></p><p><strong>glances可以在用户终端上实时显示重要的系统信息，并动态刷新内容。glances每隔3秒钟对其进行刷新，我们也可以使用命令行参数修改刷新的频率。与dstat相同的是，glances可以将捕获到的数据保存到文件中；而不同的是glances提供了API接口以便应用程序从glances中获取数据。</strong></p><h2 id="2、glances-提供的系统信息">2、glances 提供的系统信息</h2><ul><li><strong>CPU使用率；</strong></li><li><strong>内存使用情况；</strong></li><li><strong>内核统计信息和运行队列信息；</strong></li><li><strong>磁盘I/O速度、传输和读/写比率；</strong></li><li><strong>文件系统中的可用空间；</strong></li><li><strong>磁盘适配器；</strong></li><li><strong>网络I/O速度、传输和读/写比率；</strong></li><li><strong>页面空间和页面速度；</strong></li><li><strong>消耗资源最多的进程；</strong></li><li><strong>计算机信息和系统资源。</strong></li></ul><p><strong>glances 工具可以在用户的终端上实时显示重要的系统信息，并动态地对其进行更新。这个高效的工具可以工作于任何终端屏幕。另外它并不会消耗大量的 CPU 资源，通常低于百分之二。glances 在屏幕上对数据进行显示，并且每隔2秒钟对其进行更新。您也可以自己将这个时间间隔更改为更长或更短的数值。</strong></p><p><strong>glances 工具还可以将相同的数据捕获到一个文件，便于以后对报告进行分析和绘制图形。输出文件可以是电子表格的格式 (.csv) 或者 html 格式。</strong></p><h2 id="3、Linux下安装glances">3、Linux下安装glances</h2><pre class=" language-language-shell"><code class="language-language-shell">#需要epel-release yum -y install epel-release yum -y install glances</code></pre><p><strong>或</strong></p><pre class=" language-language-shell"><code class="language-language-shell">#需要python-develyum -y install python-devel -ypip install glances</code></pre><h2 id="4、glances的使用">4、glances的使用</h2><h3 id="（1）glances的默认输出">（1）glances的默认输出</h3><p><strong>glances的使用非常简单，直接输入glances命令便进入了一个类似于top命令的交互式界面。在这个界面中，显示了比top更加全面，更加具有可读性的信息。</strong></p><p><strong>为了增加可读性，glances会以不同的颜色表示不同的状态。其中，绿色表示性能良好，元须做任何额外工作；蓝色表示系统性能有一些小问题，用户应当开始关注系统性能；紫色表示性能报警，应当采取措施；红色表示性能问题严重，应当立即处理。</strong></p><p><strong>lances是一个交互式的工具．因此，我们也可以输入命令来控制glances的行为。</strong></p><pre class=" language-language-shrell"><code class="language-language-shrell">[root@python ~]# glances</code></pre><p><img src="/" class="lazyload" data-src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5Cpython%5Cpython%E6%96%87%E6%A1%A3%5C26.assets%5Cimage-20200429151108154.png"  alt="image-20200429151108154"></p><h4 id="glances-工作界面的说明"><strong>glances 工作界面的说明 :</strong></h4><p><strong>在图 1 的上部是 CPU 、Load（负载）、Mem（内存使用）、 Swap（交换分区）的使用情况。在图 1 的中上部是网络接口、Processes（进程）的使用情况。通常包括如下字段：</strong></p><ul><li><strong>VIRT: 虚拟内存大小</strong></li><li><strong>RES: 进程占用的物理内存值</strong></li><li><strong>%CPU：该进程占用的 CPU 使用率</strong></li><li><strong>%MEM：该进程占用的物理内存和总内存的百分比</strong></li><li><strong>PID: 进程 ID 号</strong></li><li><strong>USER: 进程所有者的用户名</strong></li><li><strong>TIME+: 该进程启动后占用的总的 CPU 时间</strong></li><li><strong>IO_R 和 IO_W: 进程的读写 I/O 速率</strong></li><li><strong>NAME: 进程名称</strong></li><li><strong>NI: 进程优先级</strong></li><li><strong>S: 进程状态，其中 S 表示休眠，R 表示正在运行，Z 表示僵死状态。</strong></li></ul><h3 id="（2）glances的可读性">（2）glances的可读性</h3><p><strong>对比可以发现，glances对屏幕的利用率比top明显高很多，信息量很大，有许多top所没有显示的数据。而且，glances的实时变动比top颜值高太多了。</strong></p><p><strong>Glances 会用一下几种颜色来代表状态，如下所示：</strong></p><ul><li><strong>绿色：OK（一切正常）</strong></li><li><strong>蓝色：CAREFUL（需要注意）</strong></li><li><strong>紫色：WARNING（警告）</strong></li><li><strong>红色：CRITICAL（严重）</strong></li></ul><h3 id="（3）glances中常见的命令">（3）glances中常见的命令</h3><ul><li><strong>h：显示帮助信息</strong></li><li><strong>q：离开程序退出</strong></li><li><strong>c：按照 CPU 实时负载对系统进程进行排序</strong></li><li><strong>m：按照内存使用状况对系统进程排序</strong></li><li><strong>i：按照 I/O 使用状况对系统进程排序</strong></li><li><strong>p：按照进程名称排序</strong></li><li><strong>d：显示磁盘读写状况</strong></li><li><strong>w：删除日志文件</strong></li><li><strong>l：显示日志</strong></li><li><strong>s：显示传感器信息</strong></li><li><strong>f：显示系统信息</strong></li><li><strong>1：轮流显示每个 CPU 内核的使用情况（次选项仅仅使用在多核 CPU 系统）</strong></li></ul><p><strong>glances还支持将采集的数据导入到其他服务中心，包括InfluxDB、 Cassandra. CouchDB、 OpenTSDB、Prometheus. StatsD、 ElasticSearch, RabbitMQ/ActiveMQ、ZeroMQ、 Kafaka和Riemann.</strong></p><h3 id="（4）如果我们安装了-Bottle-这个-web-框架，还能够通过-web-浏览器显示和命令行终端相同的监控界面。">（4）如果我们安装了 Bottle 这个 web 框架，还能够通过 web 浏览器显示和命令行终端相同的监控界面。</h3><p><strong>glances还支持将采集的数据导人到其他服务中心，包括InfluxDB，Cassandra，CouchDB，OpenTSDB，Prometheus，StatsD，ElasticSearch，RabbitMQ/ActiveMQ，ZeroMQ，Kafka和Riemann。</strong></p><pre class=" language-language-shell"><code class="language-language-shell">[root@python ~]# pip install bottle//安装Bottle框架[root@python ~]#  glances -w              ##默认端口是61208，访问地址没有限制Glances Web User Interface started on http://0.0.0.0:61208/</code></pre><h4 id="web访问如下图："><a href="127.0.0.1:61208">web访问如下图：</a></h4><p><img src="/" class="lazyload" data-src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5Cpython%5Cpython%E6%96%87%E6%A1%A3%5C26.assets%5Cimage-20200429185614710.png"  alt="image-20200429185614710"></p><h1>三、Python监控Linux</h1><p><strong>shell查看磁盘的监控信息，如下所示：</strong></p><pre class=" language-language-shell"><code class="language-language-shell">[root@python proc]# cat /proc/diskstats    8       0 sda 85935 21845 10913707 101067 3119 81257 743486 15647 0 31410 109079   8       1 sda1 1822 0 12456 397 4 0 4096 74 0 457 462   8       2 sda2 84082 21845 10897907 100659 3115 81257 739390 15573 0 30950 108604  11       0 sr0 0 0 0 0 0 0 0 0 0 0 0 253       0 dm-0 80726 0 10688467 99971 2275 0 82606 10224 0 27927 110196 253       1 dm-1 25123 0 205184 7367 82098 0 656784 616558 0 5167 623924</code></pre><h2 id="1、使用shell脚本监控">1、使用shell脚本监控</h2><h3 id="（1）安装转换工具">（1）安装转换工具</h3><p><strong>dos2unix 和 unix2dos 命令将纯文本文件从 DOS 或 Mac 格式转换为 Unix，反之亦然。</strong></p><pre><code>[root@python scripts]# yum -y install dos2unix//下载dos2unix</code></pre><h3 id="（2）编写shell脚本">（2）编写shell脚本</h3><pre class=" language-language-shell"><code class="language-language-shell">[root@python scripts]# vim monitor.sh#/bin/shcpu_idle=$(top -n2 | grep 'Cpu' | tail -n 1 | awk '{print $8}')cpu_usage=$(printf "%.2f" `echo "scale=2; 100 - $cpu_idle" | bc`)mem_free=$(free -m | awk '/Mem:/{print $4 + $6 +$7}')mem_total=$(free -m | awk '/Mem:/{print $2}')mem_used=$(echo "$mem_total - $mem_free" | bc)mem_rate=$(echo "$mem_used * 100 / $mem_total" | bc)disk_usage=$(df -h / | tail -n 1 | awk '{print $5}')disk_used=$(df -h / | tail -n 1 | awk '{print $3}')echo "CPU利用率：$cpu_usage %"echo "内存使用量: $mem_used M"echo "内存利用率：$mem_rate %"echo "磁盘空间使用量：$disk_used"echo "磁盘空间利用率：$disk_usage"</code></pre><h3 id="（3）转换并执行">（3）转换并执行</h3><pre class=" language-language-shell"><code class="language-language-shell">[root@python scripts]# dos2unix monitor.sh//转换为格式为Unix[root@python scripts]# cat monitor.sh#/bin/shcpu_idle=$(top -n2 | grep 'Cpu' | tail -n 1 | awk '{print $8}')cpu_usage=$(printf "%.2f" `echo "scale=2; 100 - $cpu_idle" | bc`)mem_free=$(free -m | awk '/Mem:/{print $4 + $6 +$7}')mem_total=$(free -m | awk '/Mem:/{print $2}')mem_used=$(echo "$mem_total - $mem_free" | bc)mem_rate=$(echo "$mem_used * 100 / $mem_total" | bc)disk_usage=$(df -h / | tail -n 1 | awk '{print $5}')disk_used=$(df -h / | tail -n 1 | awk '{print $3}')echo "CPU利用率：$cpu_usage %"echo "内存使用量: $mem_used M"echo "内存利用率：$mem_rate %"echo "磁盘空间使用量：$disk_used"echo "磁盘空间利用率：$disk_usage"[root@python scripts]# sh monitor.sh//执行编写好的脚本</code></pre><p><img src="/" class="lazyload" data-src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5Cpython%5Cpython%E6%96%87%E6%A1%A3%5C26.assets%5Cimage-20200429165132049.png"  alt="image-20200429165132049"></p><h2 id="2、使用python脚本监控">2、使用python脚本监控</h2><p><strong>编写一个Python脚本，监控磁盘信息，如下所示：</strong></p><h3 id="（1）查看服务启动情况">（1）查看服务启动情况</h3><pre class=" language-language-shell"><code class="language-language-shell">[root@python scripts]# vim proc_count.py import osn = 0for item in os.listdir('/proc'):    if item.isdigit():        n = n+1        # print(len(item))print(n)</code></pre><h5 id="执行结果如下">执行结果如下:</h5><pre class=" language-language-shell"><code class="language-language-shell">[root@python scripts]# python3 proc_count.py 175</code></pre><h3 id="（2）简易版">（2）简易版</h3><pre class=" language-language-python"><code class="language-language-python">[root@python scripts]# vim monitor_dick.py# coding=utf-8# !/usr/bin/pythonfrom __future__ import print_functionfrom collections import namedtupledisk = namedtuple('Disk', 'major_number minor_number device_name'                          ' read_count read_merged_count read_sections'                          ' time_spent_reading write_count write_merged_count'                          ' write_sections time_spent_write io_requests'                          ' time_spent_doing_io weighted_time_spent_dong_io')def get_disk_info(device):    with open('/proc/diskstats') as f:        for line in f:            if line.split()[2] == device:                return disk(*(line.split()))    raise RuntimeError('设备({0})没找到。。。'.format(device))def main():    disk_info = get_disk_info('sda1')    print(disk_info)if __name__ == '__main__':    main()</code></pre><h4 id="执行脚本：">执行脚本：</h4><pre class=" language-language-shell"><code class="language-language-shell">[root@python scripts]# python3 monitor_dick.py</code></pre><p><img src="/" class="lazyload" data-src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5Cpython%5Cpython%E6%96%87%E6%A1%A3%5C26.assets%5Cimage-20200429173414704.png"  alt="image-20200429173414704"></p><h3 id="（3）直观图">（3）直观图</h3><pre class=" language-language-python"><code class="language-language-python"># coding=utf-8# !/usr/bin/pythonfrom __future__ import print_functionfrom collections import namedtupledisk = namedtuple('Disk', 'major_number minor_number device_name'                          ' read_count read_merged_count read_sections'                          ' time_spent_reading write_count write_merged_count'                          ' write_sections time_spent_write io_requests'                          ' time_spent_doing_io weighted_time_spent_dong_io')def get_disk_info(device):    with open('/proc/diskstats') as f:        for line in f:            if line.split()[2] == device:                return disk(*(line.split()))    raise RuntimeError('设备({0})没找到。。。'.format(device))def main(device):    disk_info = get_disk_info(device)    print(disk_info)    print("磁盘写入次数:{0}".format(disk_info.write_count))    print("磁盘写入的字节数:{0}".format(float(disk_info.write_sections) * 512))    print("磁盘写入的延时:{0}".format(disk_info.time_spent_write))if __name__ == '__main__':    main('sda1')</code></pre><h4 id="执行脚本：-2">执行脚本：</h4><pre class=" language-language-shell"><code class="language-language-shell">[root@python scripts]# python3 monitor_dick.py</code></pre><p><img src="/" class="lazyload" data-src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5Cpython%5Cpython%E6%96%87%E6%A1%A3%5C26.assets%5Cimage-20200429180049979.png"  alt="image-20200429180049979"></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>20 k8s的helm模板</title>
      <link href="/posts/c224.html"/>
      <url>/posts/c224.html</url>
      
        <content type="html"><![CDATA[<h1>自定义helm模板</h1><p><a href="https://hub.helm.sh/" target="_blank" rel="noopener">https://hub.helm.sh/</a></p><h2 id="1、开发自己的chare包">1、开发自己的chare包</h2><pre><code>[root@master ~]# helm create mychare//创建一个名为mychare的chare包[root@master ~]# tree -C mychare///以树状图查看一下chare包mychare/├── charts├── Chart.yaml├── templates│&nbsp;&nbsp; ├── deployment.yaml│&nbsp;&nbsp; ├── _helpers.tpl│&nbsp;&nbsp; ├── ingress.yaml│&nbsp;&nbsp; ├── NOTES.txt│&nbsp;&nbsp; ├── service.yaml│&nbsp;&nbsp; └── tests│&nbsp;&nbsp;     └── test-connection.yaml└── values.yaml</code></pre><h2 id="2、调试chart">2、调试chart</h2><pre><code>[root@master mychare]# cd[root@master ~]# helm install --dry-run --debug mychare//检查这个mychare是否有问题</code></pre><h2 id="3、安装chart">3、安装chart</h2><pre><code>[root@node02 ~]# docker pull nginx:stable</code></pre><h3 id="（1）通过仓库安装">（1）通过仓库安装</h3><pre><code>[root@master mychare]# helm search redis//搜索chare包</code></pre><pre><code>[root@master mychare]# helm repo list//查看是否有能访问仓库</code></pre><pre><code>[root@master mychare]# helm install stable/redis//安装</code></pre><h3 id="（2）通过tar包安装">（2）通过tar包安装</h3><pre><code>[root@master ~]# helm fetch stable/redis//直接下载chare包[root@master ~]# tar -zxf redis-1.1.15.tgz//解压下载的chare包[root@master ~]# tree -C redisredis├── Chart.yaml├── README.md├── templates│&nbsp;&nbsp; ├── deployment.yaml│&nbsp;&nbsp; ├── _helpers.tpl│&nbsp;&nbsp; ├── networkpolicy.yaml│&nbsp;&nbsp; ├── NOTES.txt│&nbsp;&nbsp; ├── pvc.yaml│&nbsp;&nbsp; ├── secrets.yaml│&nbsp;&nbsp; └── svc.yaml└── values.yaml</code></pre><h3 id="（3）通过chare本地目录安装">（3）通过chare本地目录安装</h3><pre><code>[root@master ~]# helm fetch stable/redis//直接下载chare包[root@master ~]# tar -zxf redis-1.1.15.tgz//解压下载的chare包[root@master ~]# helm install redis</code></pre><h3 id="（4）通过URL安装">（4）通过URL安装</h3><pre><code>[root@master ~]# helm install https://example.com/charts/foo-1.2.3.tgz</code></pre><p>使用本地目录安装：</p><pre><code>[root@master ~]# cd mychare/[root@master mychare]# vim values.yaml </code></pre><p>![image-20200304094840738](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304094840738.png)</p><pre><code>[root@master mychare]# cd templates/[root@master templates]# vim service.yaml </code></pre><p>![image-20200304095647172](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304095647172.png)</p><pre><code>[root@master templates]# cd ..[root@master mychare]# helm install -n test ../mychare/[root@master ~]# helm upgrade test mychare/ -f  mychare/values.yaml </code></pre><h2 id="4、例子">4、例子</h2><p><strong>使用mychart部署一个实例: xgp。使用镜像为私有镜像v1 版本。</strong></p><p><strong>完成之后，镜像版本。</strong></p><p><strong>全部成功之后，将实例做一个升级，将镜像改为v2版本。</strong></p><h3 id="更改镜像为私有镜像">更改镜像为私有镜像</h3><pre><code>[root@master ~]# vim mychare/values.yaml</code></pre><p>![image-20200304104416415](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304104416415.png)</p><pre><code>[root@master ~]#  helm install -n xgp mychare/ -f mychare/values.yaml[root@master ~]# kubectl get deployments. -o wide</code></pre><p>![image-20200304104645260](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304104645260.png)</p><pre><code>[root@master ~]# vim mychare/values.yaml</code></pre><p>![image-20200304105120894](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304105120894.png)</p><pre><code>[root@master ~]# helm upgrade  xgp mychare/  -f mychare/values.yaml [root@master ~]# kubectl get deployments. -o wide</code></pre><p>![image-20200304105211506](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304105211506.png)</p><pre><code>[root@master ~]# kubectl edit deployments. xgp-mychare</code></pre><p>![image-20200304105334541](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304105334541.png)</p><pre><code>[root@master ~]# kubectl get deployments. -o wide</code></pre><p>![image-20200304105359184](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304105359184.png)</p><h1>创建自己的Repo仓库</h1><h2 id="1、node01启动一个httpd的容器">1、node01启动一个httpd的容器</h2><pre><code>[root@node01 ~]# mkdir /var/xgp//创建一个目录[root@node01 ~]# docker pull httpd//下载httpd镜像[root@node02 ~]# docker run -d -p 8080:80 -v /var/xgp:/usr/local/apache2/htdocs httpd//启动一个httpd的容器</code></pre><h2 id="2、master节点上，将mychart目录打包。">2、master节点上，将mychart目录打包。</h2><pre><code>[root@master ~]# helm package mychare/Successfully packaged chart and saved it to: /root/mychare-0.1.0.tgz</code></pre><h2 id="3、生成仓库的index文件。">3、生成仓库的index文件。</h2><pre><code>[root@master ~]# mkdir myrepo//创建一个目录存放打包的chare[root@master ~]# mv mychare-0.1.0.tgz myrepo///移动打包好的文件[root@master ~]# helm repo index myrepo/ --url http://192.168.1.22:8080/charts//生成仓库的index文件[root@master ~]# ls myrepo/index.yaml  mychare-0.1.0.tgz</code></pre><h2 id="4、将生成的tar包和index-yaml上传到node01的-var-www-charts目录下">4、将生成的tar包和index.yaml上传到node01的/var/www/charts目录下.</h2><h3 id="node01创建目录">node01创建目录</h3><pre><code>[root@node01 ~]# mkdir /var/xgp/charts</code></pre><h3 id="master移动动到">master移动动到</h3><pre><code>[root@master ~]# scp myrepo/* node01:/var/xgp/charts/</code></pre><h3 id="node01查看一下">node01查看一下</h3><pre><code>[root@node01 ~]# ls /var/xgp/charts/index.yaml  mychare-0.1.0.tgz</code></pre><h2 id="5、添加新的repo仓库。">5、添加新的repo仓库。</h2><pre><code>[root@master ~]# helm repo add newrepo http://192.168.1.22:8080/charts</code></pre><pre><code>[root@master ~]# helm repo list</code></pre><p>![image-20200304112410286](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304112410286.png)</p><pre><code>[root@master ~]# helm search mychare</code></pre><p>![image-20200304112443931](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304112443931.png)</p><h2 id="6、我们就可以直接使用新的repo仓库部署实例了。">6、我们就可以直接使用新的repo仓库部署实例了。</h2><pre><code>[root@master ~]# helm install newrepo/mychare -n wsd</code></pre><pre><code>[root@master ~]# helm list </code></pre><p>![image-20200304112515084](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304112515084.png)</p><h2 id="7-如果以后仓库中新添加了chart包-需要用helm-repo-update命玲更新本地的index文件。">7.如果以后仓库中新添加了chart包,需要用helm repo update命玲更新本地的index文件。</h2><p>练习：<br>新创建一个bdqn.的chart包。然后将chart包上传到上述repo源中。</p><pre><code>[root@master ~]# helm create bdqn[root@master ~]# helm package bdqn/[root@master ~]# mv bdqn-0.1.0.tgz myrepo/[root@master ~]#  helm repo index myrepo/ --url http://192.168.1.22:8080/charts[root@master myrepo]# scp bdqn-0.1.0.tgz index.yaml  node01:/var/xgp/charts[root@master myrepo]# helm repo update[root@master myrepo]# helm search bdqn[root@master myrepo]# helm install http://192.168.1.22:8080/charts/bdqn-0.1.0.tgz</code></pre><h2 id="1）创建helm的私有仓库，以自己的名字命名。">1）创建helm的私有仓库，以自己的名字命名。</h2><h3 id="1、node01启动一个httpd的容器-2">1、node01启动一个httpd的容器</h3><pre><code>[root@node01 ~]# mkdir /var/xgp//创建一个目录[root@node01 ~]# docker pull httpd//下载httpd镜像[root@node02 ~]# docker run -d -p 8080:80 -v /var/xgp:/usr/local/apache2/htdocs httpd//启动一个httpd的容器</code></pre><h3 id="3、生成仓库的index文件。-2">3、生成仓库的index文件。</h3><pre><code>[root@master ~]# mkdir xgprepo//创建一个目录存放打包的chare[root@master ~]# helm repo index xgprepo/ --url http://192.168.1.22:8080/charts//生成仓库的index文件</code></pre><h3 id="4、将生成的index-yaml上传到node01的-var-www-charts目录下">4、将生成的index.yaml上传到node01的/var/www/charts目录下.</h3><h4 id="node01创建目录-2">node01创建目录</h4><pre><code>[root@node01 ~]# mkdir /var/xgp/charts</code></pre><h4 id="master移动动到-2">master移动动到</h4><pre><code>[root@master ~]# scp xgprepo/* node01:/var/xgp/charts/</code></pre><h4 id="node01查看一下-2">node01查看一下</h4><pre><code>[root@node01 ~]# ls /var/xgp/charts/index.yaml  </code></pre><h3 id="5、添加新的repo仓库">5、添加新的repo仓库</h3><pre><code>[root@master ~]# helm repo add xgp http://192.168.1.22:8080/charts[root@master ~]# helm repo list </code></pre><p>![image-20200304132528938](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304132528938.png)</p><h2 id="2）-自定义一个chart包，要求这个包运行一个httpd的服务，使用私有镜像v1版本。3个副本Pod，service类型更改为NodePort，端口指定为-30000">2） 自定义一个chart包，要求这个包运行一个httpd的服务，使用私有镜像v1版本。3个副本Pod，service类型更改为NodePort，端口指定为:30000</h2><h4 id="自定义一个chart包">自定义一个chart包</h4><pre><code>[root@master ~]# helm create wsd//创建一个名为wsd的chares包</code></pre><h4 id="按照要求修改配置文件">按照要求修改配置文件</h4><pre><code>[root@master ~]# cd wsd///进入这个chart包[root@master wsd]# vim values.yaml//修改wsd的配置文件replicaCount: 3                         #三个副本image:  repository: 192.168.1.21:5000/web      #更改镜像为私有镜像  tag: v1                                #镜像标签v1  pullPolicy: IfNotPresent              imagePullSecrets: []nameOverride: ""fullnameOverride: ""service:  type: NodePort              #修改模式为映射端口  port: 80  nodePort: 30000             #添加端口[root@master wsd]# vim templates/service.yaml apiVersion: v1kind: Servicemetadata:  name: {{ include "wsd.fullname" . }}  labels:{{ include "wsd.labels" . | indent 4 }}spec:  type: {{ .Values.service.type }}  ports:    - port: {{ .Values.service.port }}      targetPort: http      protocol: TCP      name: http      nodePort: {{ .Values.service.nodePort }}    #“添加”能让服务识别到nodePort的端口  selector:    app.kubernetes.io/name: {{ include "wsd.name" . }}    app.kubernetes.io/instance: {{ .Release.Name }}</code></pre><h4 id="测试一下">测试一下</h4><pre><code>[root@master ~]# helm install -n wsd  wsd/ -f wsd/values.yaml </code></pre><p>![image-20200304134959273](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304134959273.png)</p><h4 id="查看一下镜像版本">查看一下镜像版本</h4><pre><code>[root@master ~]# kubectl get deployments. -o wide</code></pre><p>![image-20200304135106081](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304135106081.png)</p><h4 id="访问一下">访问一下</h4><pre><code>[root@master ~]# curl 127.0.0.1:30000</code></pre><p>![image-20200304150609552](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304150609552.png)</p><h2 id="3-将实例进行更新，要求镜像生产v2版本。">3)  将实例进行更新，要求镜像生产v2版本。</h2><p><strong>私有镜像和官方镜像升级有所不同，官方的只需通过 （helm upgrade --set imageTag=“标签” 服务名称 charts包名 ）进行更改标签即可，而私有镜像需通过更改values.yaml中的标签才行比较麻烦一点。</strong></p><h3 id="1、修改values-yaml">1、修改values.yaml</h3><pre><code>[root@master ~]# vim wsd/values.yaml # Default values for wsd.# This is a YAML-formatted file.# Declare variables to be passed into your templates.replicaCount: 3image:  repository: 192.168.1.21:5000/web  tag: v2                            #修改标签为v2  pullPolicy: IfNotPresent[root@master ~]# helm upgrade wsd wsd/ -f wsd/values.yaml//基于配置文件刷新一下wsd服务</code></pre><h4 id="查看一下">查看一下</h4><pre><code>[root@master ~]# kubectl get deployments. -o wide</code></pre><p>![image-20200304140054269](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304140054269.png)</p><h4 id="访问一下-2">访问一下</h4><pre><code>[root@master ~]# curl 127.0.0.1:30000</code></pre><p>![image-20200304150742815](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304150742815.png)</p><h3 id="2、使用edit进行版本更新">2、使用edit进行版本更新</h3><p><em><strong>确定wsd这个服务开启</strong></em></p><pre><code>[root@master ~]# kubectl edit deployments. wsd</code></pre><p>![](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304140425336.png)</p><h4 id="查看一下-2">查看一下</h4><pre><code>[root@master ~]# kubectl get deployments. -o wide</code></pre><p>![image-20200304140520342](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304140520342.png)</p><h4 id="访问一下-3">访问一下</h4><pre><code>[root@master ~]# curl 127.0.0.1:30000</code></pre><p>![image-20200304150839440](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304150839440.png)</p><h2 id="4）重新定义一个chart包，名称为-new-test-将这个包上传到上述私有仓库中。">4）重新定义一个chart包，名称为: new-test,将这个包上传到上述私有仓库中。</h2><pre><code>[root@master ~]# helm repo list </code></pre><p>![image-20200304142059023](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304142059023.png)</p><pre><code>[root@master ~]# helm create xgp-wsd//创建一个名为xgp-wsd的charts包[root@master ~]# helm package xgp-wsd///将xgp-wsd打包在当前目录[root@master ~]# mv xgp-wsd-0.1.0.tgz xgprepo///把打包文件放到仓库目录[root@master ~]# helm repo index xgprepo/ --url http://192.168.1.22:8080/charts//把仓库目录新加入的charts包信息记录在index.yaml中，使得其他加入的主机可以识别到，仓库的charts包[root@master ~]# scp xgprepo/* node01:/var/xgp/charts//将仓库目录的文件移动到httpd服务上，使各个主机可以访问，下载仓库的charts包[root@master ~]# helm repo update //更新一下chart存储库</code></pre><h3 id="查看一下-3">查看一下</h3><pre><code>[root@master ~]# helm search xgp-wsd</code></pre><p>![image-20200304142009776](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304142009776.png)</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>18 k8s的HPA自动扩容与缩容</title>
      <link href="/posts/2643.html"/>
      <url>/posts/2643.html</url>
      
        <content type="html"><![CDATA[<h1>HPA</h1><p><strong>可以根据当前Pod资源的使用率，比如说CPU、磁盘、内存等进行副本Pod的动态的扩容与缩容。</strong></p><p><strong>前提条件:系统应该能否获取到当前Pod的资源使用情况 (意思是可以执行kubectl top pod命令,并且能够得到反馈信息)。</strong></p><p><strong>heapster：这个组件之前是集成在k8s集群的,不过在1.12版本之后被移除了。如果还想使用此功能，应该部署metricServer, 这个k8s集群资源使用情况的聚合器。</strong></p><p><strong>这里，我们使用一个测试镜像， 这个镜像基于php-apache制作的docker镜像，包含了一些可以运行cpu密集计算任务的代码。</strong></p><h2 id="1、创建一个deployment控制器">1、创建一个deployment控制器</h2><pre><code>[root@master ~]#docker pull mirrorgooglecontainers/hpa-example:latest//下载hpa-example镜像[root@master ~]# kubectl run php-apache --image=mirrorgooglecontainers/hpa-example --requests=cpu=200m --expose  --port=80//基于hpa-example镜像，运行一个deployment控制器，请求CPU的资源为200m，暴露一个80端口</code></pre><h3 id="查看一下">查看一下</h3><pre><code>[root@master ~]# kubectl get deployments.</code></pre><p>![image-20200228102643352](E:\软件\博客\Blog\blog\source_posts\18 HPA自动容与蒲容.assets\image-20200228102643352.png)</p><h2 id="2、创建HPA控制器">2、创建HPA控制器</h2><pre><code>[root@master ~]# kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10//当deployment资源对象的CPU使用率达到50%时，就进行扩容，最多可以扩容到10个</code></pre><h3 id="查看一下-2">查看一下</h3><pre><code>[root@master ~]# kubectl get hpa</code></pre><p>![image-20200228101908398](E:\软件\博客\Blog\blog\source_posts\18 HPA自动容与蒲容.assets\image-20200228101908398.png)</p><h2 id="3、测试（master开启三个端口）">3、测试（master开启三个端口）</h2><p><strong>新开启多个终端，对pod进行死循环请求php-apache的pod</strong></p><h3 id="端口一">端口一</h3><h4 id="（1）创建一个应用，用来不停的访问我们刚刚创建的php-apache的svc资源。">（1）创建一个应用，用来不停的访问我们刚刚创建的php-apache的svc资源。</h4><pre><code>[root@master ~]# kubectl run -i --tty load-generator --image=busybox /bin/sh</code></pre><h4 id="（2）进入Pod内，执行以下这条命令-用来模拟访问php-apache的svc资源。">（2）进入Pod内，执行以下这条命令.用来模拟访问php-apache的svc资源。</h4><pre><code>[root@master ~]# while true; do wget -q -O- http://php-apache.default.svc.cluster.local ; done//不停地向php-apache的svc资源，发送ok</code></pre><h3 id="端口二">端口二</h3><pre><code>[root@master ~]# kubectl get hpa -w//实时查看pod的cpu状态</code></pre><p>![image-20200228133816724](E:\软件\博客\Blog\blog\source_posts\18 k8s的HPA自动容与缩容.assets\image-20200228133816724.png)</p><p><strong>可以看到php-apache的cpu使用情况已经超过了50%</strong></p><h3 id="端口三">端口三</h3><pre><code>[root@master images]# kubectl get pod -w//实时查看pod的状态</code></pre><p>![image-20200228134105507](E:\软件\博客\Blog\blog\source_posts\18 k8s的HPA自动容与缩容.assets\image-20200228134105507.png)</p><p><strong>可以看到当php-apache的cpu使用情况超过50%后，就会不断生成新的php-apache来进行负载均衡（目前设置的上线时10个），当然，如果cpu使用情况下降到50%，master就会陆续地删除php-apache，这样的使用可以减少不必要的资源浪费、资源分配不均等情况。</strong></p><h1>二、资源限制</h1><h2 id="1、基于Pod">1、基于Pod</h2><p><strong>Kubernetes对资源的限制实际上是通过cgroup来控制的，cgroup 是容器的一组用来控制内核如何运行进程的相关属性集合。针对内存、CPU 和各种设备都有对应的cgroup</strong></p><p><strong>默认情况下，Pod运行没有CPU和内存的限额。这意味着系统中的任何 Pod将能够像执行该Pod所在的节点一样，消耗足够多的CPU和内存。一般会针对某些应用的pod资源进行资源限制，这个资源限制是通过</strong></p><p><strong>resources的requests和limits来实现</strong></p><pre><code>[root@master ~]# vim cgroup-pod.yaml</code></pre><p>![image-20200228153809932](E:\软件\博客\Blog\blog\source_posts\18 k8s的HPA自动容与缩容.assets\image-20200228153809932.png)</p><p><strong>requests: 要分配的资源，limits为最高请求的资源值。可以简单的理解为初始值和最大值。</strong></p><h2 id="2、基于名称空间"><strong>2、基于名称空间</strong></h2><h3 id="1）-计算资源配额">1） 计算资源配额</h3><pre><code>[root@master ~]# vim compute-resources.yaml</code></pre><p>![image-20200228153818288](E:\软件\博客\Blog\blog\source_posts\18 k8s的HPA自动容与缩容.assets\image-20200228153818288.png)</p><h3 id="2）配置对象数量配额限制">2）配置对象数量配额限制</h3><pre><code>[root@master ~]# vim object-counts.yaml</code></pre><p>![image-20200228153828002](E:\软件\博客\Blog\blog\source_posts\18 k8s的HPA自动容与缩容.assets\image-20200228153828002.png)</p><h3 id="3）-配置CPU和内存的LimitRange">3） 配置CPU和内存的LimitRange</h3><pre><code>[root@master ~]# vim limitRange.yaml</code></pre><p>![image-20200228153834705](E:\软件\博客\Blog\blog\source_posts\18 k8s的HPA自动容与缩容.assets\image-20200228153834705.png)</p><p><strong>default 即 limit的值。</strong></p><p><strong>defaultRequest 即 request的值。</strong></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>14 k8s的Secret（密文）和configmap（明文）的使用教程</title>
      <link href="/posts/a50d.html"/>
      <url>/posts/a50d.html</url>
      
        <content type="html"><![CDATA[<h1>一、Secret</h1><p><em><strong>Secret :用来保存一些敏感信息，比如数据库的用户名密码或者秘钥。</strong></em></p><h2 id="举例-保存数据库的用户名和密码">举例:保存数据库的用户名和密码</h2><blockquote><p><strong>用户名：</strong><strong>root</strong><br><strong>密码：</strong>   <strong><a href="http://123.com" target="_blank" rel="noopener">123.com</a></strong></p></blockquote><h3 id="1、通过–from-literal（文字的）">1、通过–from-literal（文字的）</h3><pre><code>[root@master secret]# kubectl create secret generic mysecret1 --from-literal=username=root --from-literal=password=123.com</code></pre><blockquote><p><strong>generic：通用的，一般的加密方式</strong></p></blockquote><h4 id="查看一下">查看一下</h4><pre><code>[root@master secret]# kubectl get secrets </code></pre><p><img src="/" class="lazyload" data-src="/posts/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C14.assets%5Cimage-20200214100419966.png"  alt=""></p><p><strong>类型是Opaque（不透明的）</strong></p><h3 id="2、通过from-file（文件）">2、通过from-file（文件）</h3><h4 id="新建两个文件并分别写入用户名和密码">新建两个文件并分别写入用户名和密码</h4><pre><code>[root@master secret]# echo root &gt; username[root@master secret]# echo 123.com  &gt; password</code></pre><h4 id="创建一个secret">创建一个secret</h4><pre><code>[root@master secret]#  kubectl create secret generic mysecret2 --from-file=username --from-file=password </code></pre><h4 id="查看一下-2">查看一下</h4><pre><code>[root@master secret]# kubectl get secrets</code></pre><p><img src="/" class="lazyload" data-src="/posts/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C14.assets%5Cimage-20200214103506842.png"  alt="image-20200214103506842"></p><h3 id="3、通过-from-env-file">3、通过-- from- env-file:</h3><h4 id="创建一个文件写入用户名和密码">创建一个文件写入用户名和密码</h4><pre><code>[root@master secret]#vim env.txt username=rootpassword=123.com</code></pre><h4 id="创建一个secret-2">创建一个secret</h4><pre><code>[root@master secret]# kubectl create secret generic mysecret3 --from-env-file=env.txt </code></pre><h4 id="查看一下-3">查看一下</h4><pre><code>[root@master secret]# kubectl get secrets </code></pre><p><img src="/" class="lazyload" data-src="/posts/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C14.assets%5Cimage-20200214103905956.png"  alt="image-20200214103905956"></p><h3 id="4、通过yaml配置文件">4、通过yaml配置文件</h3><h4 id="（1）把需要保存的数据加密（”base64“的方式）">（1）把需要保存的数据加密（”base64“的方式）</h4><pre><code>[root@master secret]# echo root | base64cm9vdAo=[root@master secret]# echo 123.com | base64MTIzLmNvbQo=</code></pre><blockquote><p><strong>解码：</strong></p><pre><code>[root@master secret]# echo -n cm9vdAo | base64 --decode root[root@master secret]# echo -n MTIzLmNvbQo | base64 --decode 123.com</code></pre></blockquote><h4 id="（2）编写secre4的yaml文件">（2）编写secre4的yaml文件</h4><pre><code>[root@master secret]# vim secret4.yamlapiVersion: v1kind: Secretmetadata:  name: mysecret4data:  username: cm9vdAo=  password: MTIzLmNvbQo=</code></pre><h5 id="执行一下">执行一下</h5><pre><code>[root@master secret]# kubectl apply -f secret4.yaml </code></pre><h4 id="（3）查看一下">（3）查看一下</h4><pre><code>[root@master secret]# kubectl get secrets </code></pre><p><img src="/" class="lazyload" data-src="/posts/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C14.assets%5Cimage-20200214104544899.png"  alt="image-20200214104544899"></p><h2 id="如果来使用Secret资源">如果来使用Secret资源</h2><h3 id="1-以Volume挂载的方式">1. 以Volume挂载的方式</h3><h4 id="编写pod的yaml文件"><strong>编写pod的yaml文件</strong></h4><pre><code>[root@master secret]# vim pod.yaml apiVersion: v1kind: Podmetadata:  name: mypodspec:  containers:  - name: mypod    image: busybox    args:      - /bin/sh      - -c      - sleep 300000    volumeMounts:    - name: secret-test      mountPath: "/etc/secret-test"  #pod中的路径      readOnly: true                 #是否只读  volumes:  - name: secret-test    secret:      secretName: mysecret1</code></pre><p><strong>还可以自定义存放数据的文件名</strong></p><h4 id="执行一下-2">执行一下</h4><pre><code>[root@master secret]# kubectl apply -f pod.yaml </code></pre><h4 id="进入容器查看保存的数据">进入容器查看保存的数据</h4><pre><code>[root@master secret]# kubectl exec -it mypod /bin/sh/ # cd /etc/secret-test//etc/secret-test # lspasword   username</code></pre><pre><code>/etc/secret-test # cat username root/etc/secret-test # cat pasword 123.com</code></pre><h4 id="测试是否有只读权限">测试是否有只读权限</h4><pre><code>123.com/etc/secret-test # echo admin &gt; username/bin/sh: can't create username: Read-only file system</code></pre><h3 id="1-1-自定义存放数据的文件名的yaml文件">1.1 自定义存放数据的文件名的yaml文件</h3><pre><code>[root@master yaml]#  vim pod.yaml apiVersion: v1kind: Podmetadata:  name: mypodspec:  containers:  - name: mypod    image: busybox    args:      - /bin/sh      - -c      - sleep 300000    volumeMounts:    - name: secret-test      mountPath: "/etc/secret-test"  #pod中的路径      readOnly: true                 #是否只读  volumes:  - name: secret-test    secret:      secretName: mysecret1      items:      - key: username        path: my-group/my-username   #自定义的容器中的目录      - key: password        path: my-group/my-password   #自定义的容器中的目录</code></pre><h4 id="执行一下-3">执行一下</h4><pre><code>[root@master yaml]# kubectl apply -f pod.yaml</code></pre><h4 id="查看一下-4">查看一下</h4><pre><code>[root@master secret]# kubectl exec -it mypod /bin/sh//进入容器查看 # cat /etc/secret-test/my-group/my-password 123.com  # cat /etc/secret-test/my-group/my-username root</code></pre><h3 id="1-2-如果，现在将secret资源内保存的数据进行更新，请问，使用此数据的应用内，数据是是否也会更新">1.2 如果，现在将secret资源内保存的数据进行更新，请问，使用此数据的应用内，数据是是否也会更新?</h3><p><strong>会实时更新(这里引用数据，是以volumes挂 载使用数据的方式)。</strong></p><p><strong>更新mysecret1的数据:   password  —&gt;  admin   YWRtaW4K (base64)</strong></p><p><strong>可以通过edit 命令，直接修改。</strong></p><pre><code>[root@master secret]# kubectl edit secrets mysecret1</code></pre><p><img src="/" class="lazyload" data-src="/posts/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C14.assets%5Cimage-20200217162834490.png"  alt="image-20200217162834490"></p><h4 id="查看一下-5">查看一下</h4><pre><code>[root@master secret]# kubectl exec -it mypod /bin/sh//进入容器查看 # cat /etc/secret-test/my-group/my-password admin # cat /etc/secret-test/my-group/my-username root</code></pre><p><em><strong>数据已经成功更新了</strong></em></p><h3 id="2、以环境变量的方式">2、以环境变量的方式</h3><p><strong>编写pod的yaml文件</strong></p><pre><code>[root@master secret]# vim pod-env.yaml apiVersion: v1kind: Podmetadata:  name: mypod2spec:  containers:  - name: mypod    image: busybox    args:      - /bin/sh      - -c      - sleep 300000    env:      - name: SECRET_USERNAME        valueFrom:          secretKeyRef:            name: mysecret2            key: username      - name: SECRET_PASSWORD        valueFrom:          secretKeyRef:            name: mysecret2            key: password</code></pre><h4 id="执行一下-4">执行一下</h4><pre><code>[root@master secret]# kubectl apply -f pod-env.yaml </code></pre><h4 id="查看一下-6">查看一下</h4><pre><code>[root@master secret]# kubectl get pod</code></pre><p><img src="/" class="lazyload" data-src="/posts/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C14.assets%5Cimage-20200214111931566.png"  alt="image-20200214111931566"></p><h4 id="进入容器查看保存的数据-2">进入容器查看保存的数据</h4><pre><code>[root@master secret]# kubectl exec -it mypod2 /bin/sh/ # echo $SECRET_USERNAMEroot/ # echo $SECRET_PASSWORD123.com</code></pre><h3 id="2-1-更新sevret文件的内容">2.1 更新sevret文件的内容</h3><pre><code> [root@master yaml]# kubectl edit secrets mysecret2 //修改保存文件的内容</code></pre><p><img src="/" class="lazyload" data-src="/posts/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C14.assets%5Cimage-20200217162834490.png"  alt="image-20200217162834490"></p><h4 id="查看一下-7">查看一下</h4><pre><code>[root@master secret]# kubectl exec -it mypod2 /bin/sh/ # echo $SECRET_USERNAMEroot/ # echo $SECRET_PASSWORD123.com</code></pre><p><em><strong>等待了一定时间后，可以看到这个数据并没有没有改变</strong></em></p><h2 id="总结">总结</h2><p><strong>如果引用secret数据的应用， 要求会随着secret资源对象内保存的数据的更新，而实时更新，那么应该使用volumes挂载的方式引用资源因为用环境变量的方式引用不会实时更新数据。</strong></p><h1>二、ConfigMap</h1><p><strong>和Secret资源类似，不同之处在于，secret 资源保存的是敏感信息，而Configmap保存的是以明文方式存放的数据。</strong></p><blockquote><p><strong>username：adam</strong></p><p><strong>age：18</strong></p></blockquote><h2 id="创建的四种方式">创建的四种方式</h2><h3 id="1、通过-from-literal-文字的">1、通过-- from- literal(文字的):</h3><pre><code>[root@master yaml]# kubectl create configmap myconfigmap1 --from-literal=username=adam --from-literal=age=18</code></pre><h4 id="查看一下-8">查看一下</h4><pre><code>[root@master yaml]# kubectl get cm</code></pre><p><img src="/" class="lazyload" data-src="/posts/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C14.assets%5Cimage-20200217103048235.png"  alt="image-20200217103048235"></p><pre><code>[root@master yaml]# kubectl describe cm</code></pre><p><img src="/" class="lazyload" data-src="/posts/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C14.assets%5Cimage-20200217103123130.png"  alt="image-20200217103123130"></p><h3 id="2、通过–from-file-文件">2、通过–from-file (文件) :</h3><pre><code>[root@master yaml]# echo adam &gt; username[root@master yaml]# echo 18 &gt; age</code></pre><h4 id="创建">创建</h4><pre><code>[root@master yaml]# kubectl create configmap myconfigmap2 --from-file=username --from-file=age </code></pre><h4 id="查看一下-9">查看一下</h4><pre><code>[root@master yaml]# kubectl describe cm</code></pre><p><img src="/" class="lazyload" data-src="/posts/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C14.assets%5Cimage-20200217103509006.png"  alt="image-20200217103509006"></p><h3 id="3、通过–from-env-file">3、通过–from- env-file:</h3><pre><code>[root@master yaml]# vim env.txt username=adamage=18</code></pre><h4 id="创建-2">创建</h4><pre><code>[root@master yaml]# kubectl create configmap  myconfigmap3 --from-env-file=env.txt</code></pre><h4 id="查看一下-10">查看一下</h4><pre><code>[root@master configmap]# kubectl describe cm</code></pre><p><img src="/" class="lazyload" data-src="/posts/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C14.assets%5Cimage-20200217165039190.png"  alt="image-20200217165039190"></p><h3 id="4、通过yaml配置文件-2">4、通过yaml配置文件:</h3><pre><code>[root@master yaml]# vim configmap.yamlapiVersion: v1kind: ConfigMapmetadata:  name: myconfigmap4data:  username: 'adam'  age: '18'</code></pre><h4 id="创建-3">创建</h4><pre><code>[root@master yaml]# kubectl apply -f configmap.yaml </code></pre><h4 id="查看一下-11">查看一下</h4><pre><code>[root@master yaml]# kubectl describe cm</code></pre><p><img src="/" class="lazyload" data-src="/posts/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C14.assets%5Cimage-20200217104428521.png"  alt="image-20200217104428521"></p><h2 id="如何来使用configmap资源">如何来使用configmap资源</h2><h3 id="1-以Volume挂载的方式-2">1. 以Volume挂载的方式</h3><pre><code>[root@master yaml]# vim v-pod.yaml apiVersion: v1kind: Podmetadata:  name: pod1spec:  containers:  - name: mypod    image: busybox    args:      - /bin/sh      - -c      - sleep 300000    volumeMounts:    - name: cmp-test      mountPath: "/etc/cmp-test"      readOnly: true  volumes:  - name: cmp-test    configMap:      name: myconfigmap1</code></pre><h4 id="执行一下-5">执行一下</h4><pre><code>[root@master configmap]# kubectl apply -f v-pod.yaml </code></pre><h4 id="查看一下-12">查看一下</h4><pre><code>[root@master configmap]# kubectl exec -it pod1 /bin/sh//进入容器查看一下 # cat /etc/cmp-test/age 18/  # cat /etc/cmp-test/username adam/ </code></pre><h3 id="1-1-自定义存放数据的文件名的yaml文件-2">1.1 自定义存放数据的文件名的yaml文件</h3><pre><code>[root@master configmap]# vim v-pod2.yaml apiVersion: v1kind: Podmetadata:  name: pod3spec:  containers:  - name: mypod    image: busybox    args:      - /bin/sh      - -c      - sleep 300000    volumeMounts:    - name: cmp-test      mountPath: "/etc/cmp-test"      readOnly: true  volumes:  - name: cmp-test    configMap:      name: myconfigmap1      items:      - key: username        path: my-group/my-username   #自定义的容器中的目录      - key: age        path: my-group/my-age   #自定义的容器中的目录 </code></pre><h4 id="执行一下-6">执行一下</h4><pre><code>[root@master configmap]# kubectl apply -f v-pod2.yaml</code></pre><h4 id="查看一下-13">查看一下</h4><pre><code>[root@master configmap]# kubectl exec -it pod3 /bin/sh//进入容器查看# cat /etc/cmp-test/my-group/my-username adam/ # cat /etc/cmp-test/my-group/my-age 18/ </code></pre><h3 id="1-2-如果，现在将secret资源内保存的数据进行更新，请问，使用此数据的应用内，数据是是否也会更新-2">1.2 如果，现在将secret资源内保存的数据进行更新，请问，使用此数据的应用内，数据是是否也会更新?</h3><pre><code>[root@master configmap]# kubectl edit cm myconfigmap1</code></pre><p><img src="/" class="lazyload" data-src="/posts/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C14.assets%5Cimage-20200217172107999.png"  alt="image-20200217172107999"></p><h4 id="查看一下-14">查看一下</h4><pre><code>[root@master configmap]# kubectl exec -it pod3 /bin/sh//进入容器查看# cat /etc/cmp-test/my-group/my-username adam/ # cat /etc/cmp-test/my-group/my-age 10</code></pre><p><em><strong>可以看到更新成功</strong></em></p><h3 id="2-以环境变量的方式">2.以环境变量的方式</h3><pre><code>[root@master configmap]# vim e-pod.yaml apiVersion: v1kind: Podmetadata:  name: pod2spec:  containers:  - name: mypod    image: busybox    args:      - /bin/sh      - -c      - sleep 300000    env:      - name: CONFIGMAP_NAME        valueFrom:          configMapKeyRef:            name: myconfigmap2            key: username      - name: CONFIGMAP_AGE        valueFrom:          configMapKeyRef:            name: myconfigmap2            key: age</code></pre><h4 id="执行一下-7">执行一下</h4><pre><code>[root@master configmap]# kubectl apply -f e-pod.yaml </code></pre><h4 id="查看一下-15">查看一下</h4><pre><code>[root@master configmap]# kubectl exec -it pod2 /bin/sh//进入容器查看一下 # echo $CONFIGMAP_NAMEadam # echo $CONFIGMAP_AGE18</code></pre><h3 id="2-1-更新sevret文件的内容-2">2.1 更新sevret文件的内容</h3><pre><code>[root@master configmap]# kubectl edit cm myconfigmap2 //修改保存文件的内容</code></pre><p><img src="/" class="lazyload" data-src="/posts/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C14.assets%5Cimage-20200217172701793.png"  alt="image-20200217172701793"></p><h4 id="查看一下-16">查看一下</h4><pre><code>[root@master configmap]# kubectl exec -it pod2 /bin/sh//进入容器查看一下 # echo $CONFIGMAP_NAMEadam # echo $CONFIGMAP_AGE18</code></pre><p><em><strong>等待了一定时间后，可以看到这个数据并没有没有改变</strong></em></p><p><strong>可以看出这个configmap和secret的更新效果基本没有区别。</strong></p><h2 id="总结configmap、与secret资源有什么相同和不同之处。"><strong>总结configmap、与secret资源有什么相同和不同之处。</strong></h2><h3 id="Secret-与-ConfigMap-对比">Secret 与 ConfigMap 对比</h3><p><strong>相同点：</strong></p><blockquote><p><strong>key/value的形式</strong></p><p><strong>属于某个特定的namespace</strong></p><p><strong>可以导出到环境变量</strong></p><p><strong>可以通过目录/文件形式挂载</strong></p><p><strong>通过 volume 挂载的配置信息均可热更新</strong></p></blockquote><p><strong>不同点：</strong></p><blockquote><p><strong>Secret 可以被 ServerAccount 关联</strong></p><p><strong>Secret 可以存储 docker register 的鉴权信息，用在 ImagePullSecret 参数中，用于拉取私有仓库的镜像</strong></p><p><strong>Secret 支持 Base64 加密</strong></p><p><strong>Secret 分为 <a href="http://kubernetes.io/service-account-token%E3%80%81kubernetes.io/dockerconfigjson%E3%80%81Opaque" target="_blank" rel="noopener">kubernetes.io/service-account-token、kubernetes.io/dockerconfigjson、Opaque</a> 三种类型，而 Configmap 不区分类型</strong></p></blockquote><h2 id="总结以volumes挂载、和环境变量方式引用资源的相同和不同之处。">总结以volumes挂载、和环境变量方式引用资源的相同和不同之处。</h2><p><strong>volumes挂载(可根据更改数据更新)：引用自己创建的secret（密文）或configmap（明文），挂载到容器中指定的目录下。查看保存的文件时，根据自己所填路径和secret或configmap创建的文件，进行查看。</strong></p><p><strong>环境变量(不因更改数据更新)：引用自己创建的secret（密文）或configmap（明文），挂载到容器中指定的目录下。查看保存的文件时，根据自己环境变量，进行查看。</strong></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>11 k8s持久化存储应用</title>
      <link href="/posts/5849.html"/>
      <url>/posts/5849.html</url>
      
        <content type="html"><![CDATA[<h1>k8s存储: (持久化)</h1><p><strong>docker容器是有生命周期的。</strong></p><p><strong>volume</strong></p><h2 id="1-emptyDir（空目录）：-类似docker-数据持久化的-docer-manager-volume">**1.emptyDir（空目录）：**类似docker 数据持久化的:docer manager volume</h2><p><strong>使用场景:在同一 个Pod里，不同的容器，共享数据卷。</strong></p><p><strong>如果容器被删除，数据仍然存在，如果Pod被 删除，数据也会被删除。</strong></p><blockquote><p><strong>测试编写一个yaml文件</strong></p><pre><code>[root@master yaml]# vim emptyDir.yamlapiVersion: v1kind: Podmetadata:  name: producer-consumerspec:  containers:  - image: busybox    name: producer    volumeMounts:    - mountPath: /producer_dir      name: shared-volume    args:    - /bin/sh    - -c    - echo "hello k8s" &gt; /producer_dir/hello; sleep 30000  - image: busybox    name: consumer    volumeMounts:    - mountPath: /consumer_dir      name: shared-volume    args:    - /bin/sh    - -c    - cat /consumer_dir/hello; sleep 30000  volumes:  - name: shared-volume    emptyDir: {}</code></pre><p><strong>执行一下</strong></p><pre><code>[root@master yaml]# kubectl apply -f emptyDir.yaml </code></pre><p><strong>查看一下</strong></p><pre><code>[root@master yaml]# kubectl get pod  </code></pre><p>![image-20200205095431565](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200205095431565.png)</p><p><strong>查看日志</strong></p><pre><code>[root@master yaml]# kubectl logs  producer-consumer producer[root@master yaml]# kubectl logs  producer-consumer consumer</code></pre><p>![image-20200205095543780](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200205095543780.png)</p><p><strong>查看挂载的目录</strong></p><p><strong>node节点查看容器名，并通过容器名查看挂载的目录</strong></p><pre><code>[root@node01 shared-volume]# docker ps </code></pre><p>![image-20200205102007328](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200205102007328.png)</p><pre><code>[root@node01 shared-volume]# docker inspect k8s_consumer_producer-consumer_default_9ec83f9e-e58b-4bf8-8e16-85b0f83febf9_0</code></pre><p>![image-20200205102048470](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200205102048470.png)</p><p><strong>进入挂载目录查看一下</strong></p><p>![image-20200205102128953](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200205102128953.png)</p></blockquote><h2 id="2-hostPath-Volume：类似docker-数据持久化的-bind-mount">2.hostPath Volume：类似docker 数据持久化的:bind mount</h2><p><strong>如果Pod被删除，数据会保留，相比较emptyDir要好一点。不过一旦host崩溃，hostPath也无法访问 了。</strong></p><p><strong>docker或者k8s集群本身的存储会采用hostPath这种方式。</strong></p><h2 id="3-Persistent-Volume-PV-持久卷-提前做好的，数据持久化的数据存放目录。">3.Persistent Volume| PV(持久卷) 提前做好的，数据持久化的数据存放目录。</h2><h3 id="Psesistent-Volume-Claim-PVC-持久卷使用声明-申请"><strong>Psesistent Volume Claim| PVC( 持久卷使用声明|申请)</strong></h3><h3 id="（1）基于nfs服务来做的PV和pvc"><strong>（1）基于nfs服务来做的PV和pvc</strong></h3><p><strong>下载nfs所需安装包</strong></p><pre><code>[root@node02 ~]# yum -y install nfs-utils  rpcbind</code></pre><p><strong>创建共享目录</strong></p><pre><code>[root@master ~]# mkdir /nfsdata</code></pre><p><strong>创建共享目录的权限</strong></p><pre><code>[root@master ~]# vim /etc/exports/nfsdata *(rw,sync,no_root_squash)</code></pre><p><strong>开启nfs和rpcbind</strong></p><pre><code>[root@master ~]# systemctl start nfs-server.service [root@master ~]# systemctl start rpcbind</code></pre><p><strong>测试一下</strong></p><pre><code>[root@master ~]# showmount -e</code></pre><p>![image-20200205105654925](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200205105654925.png)</p><h4 id="1-创建nfs-pv的yaml文件"><strong>&lt;1&gt;创建nfs-pv的yaml文件</strong></h4><pre><code>[root@master yaml]# cd yaml/[root@master yaml]# vim nfs-pv.yamlapiVersion: v1kind: PersistentVolumemetadata:  name: test-pvspec:  capacity:   #pv容量的大小    storage: 1Gi  accessModes:  #访问pv的模式    - ReadWriteOnce #能以读-写mount到单个的节点  persistentVolumeReclaimPolicy: Recycle  storageClassName: nfs  nfs:    path: /nfsdata/pv1    server: 192.168.1.21</code></pre><blockquote><pre><code>  accessModes:(PV支持的访问模式)    - ReadWriteOnce: 能以读-写mount到单个的节点    - ReadWriteMany: 能以读-写mount到多个的节点。- ReadOnlyMnce:  能以只读的方式mount到多个节点。</code></pre></blockquote><blockquote><pre><code>persistentVolumeReclaimPolicy : (PV存储空间的回收策略是什么)Recycle: 自动清除数据。Retain: 需要管理员手动回收。Delete： 云存储专用。</code></pre></blockquote><h4 id="2-执行一下"><strong>&lt;2&gt;执行一下</strong></h4><pre><code>[root@master yaml]# kubectl apply -f nfs-pv.yaml </code></pre><h4 id="3-查看一下">&lt;3&gt;查看一下</h4><pre><code>[root@master yaml]# kubectl get pv</code></pre><p>![image-20200205111307317](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200205111307317.png)</p><h4 id="1-创建nfs-pvc的yaml文件"><strong>&lt;1&gt;创建nfs-pvc的yaml文件</strong></h4><pre><code>[root@master yaml]# vim nfs-pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata:  name: test-pvcspec:  accessModes:    - ReadWriteOnce  resources:    requests:      storage: 1Gi  storageClassName: nfs</code></pre><h4 id="2-执行一下-2"><strong>&lt;2&gt;执行一下</strong></h4><pre><code>[root@master yaml]# kubectl apply -f nfs-pvc.yaml </code></pre><h4 id="3-查看一下-2">&lt;3&gt;查看一下</h4><pre><code>[root@master yaml]# kubectl get pvc</code></pre><p>![image-20200205113407860](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200205113407860.png)</p><pre><code>[root@master yaml]# kubectl get pv</code></pre><p>![image-20200205113512580](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200205113512580.png)</p><h3 id="（2）创建一个pod资源">（2）创建一个pod资源</h3><pre><code>[root@master yaml]# vim pod.yamlkind: PodapiVersion: v1metadata:  name: test-podspec:  containers:    - name: pod1      image: busybox      args:      - /bin/sh      - -c      - sleep 30000      volumeMounts:      - mountPath: "/mydata"        name: mydata  volumes:    - name: mydata      persistentVolumeClaim:        claimName: test-pvc</code></pre><h4 id="1-执行一下">&lt;1&gt; 执行一下</h4><pre><code>[root@master yaml]# kubectl apply -f pod.yaml </code></pre><h4 id="2-查看一下">&lt;2&gt;查看一下</h4><pre><code>[root@master yaml]# kubectl get pod -o wide</code></pre><p>![image-20200207100212328](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207100212328.png)</p><p><strong>可以看到现在没有开启成功</strong></p><h5 id="查看一下test-pod的信息看看是哪里的问题">查看一下test-pod的信息看看是哪里的问题</h5><pre><code>[root@master yaml]# kubectl describe pod test-pod </code></pre><p>![image-20200207123950227](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207123950227.png)</p><h5 id="那是因为pv的本地挂载目录没有创建好">那是因为pv的本地挂载目录没有创建好</h5><pre><code>[root@master yaml]# mkdir /nfsdata/pv1///要和nfs-pv.yaml的名字一样</code></pre><h5 id="重新创建一下pod">重新创建一下pod</h5><pre><code>[root@master yaml]# kubectl delete -f pod.yaml [root@master yaml]# kubectl apply -f pod.yaml [root@master yaml]# kubectl get pod -o wide</code></pre><p>![image-20200207102822785](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207102822785.png)</p><h3 id="（3）test-pod创建hello创建文件并添加内容">（3）test-pod创建hello创建文件并添加内容</h3><pre><code>[root@master yaml]# kubectl exec test-pod touch /mydata/hello</code></pre><p><strong>进入容器</strong></p><pre><code>[root@master yaml]# kubectl exec -it test-pod  /bin/sh/ # echo 123 &gt; /mydata/hello/ # exit</code></pre><p><strong>挂载目录查看一下</strong></p><pre><code>[root@master yaml]# cat  /nfsdata/pv1/hello </code></pre><p>![image-20200207104239153](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207104239153.png)</p><p><strong>和刚刚的一样</strong></p><h3 id="（4）测试回收策略">（4）测试回收策略</h3><h4 id="删除pod和pvc，pv">删除pod和pvc，pv</h4><pre><code>[root@master yaml]# kubectl delete pod test-pod [root@master yaml]# kubectl delete pvc test-pvc [root@master yaml]# kubectl delete pv test-pv </code></pre><h4 id="查看一下">查看一下</h4><pre><code>[root@master yaml]# kubectl get pv</code></pre><p>![image-20200207104454636](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207104454636.png)</p><pre><code>[root@master yaml]# cat  /nfsdata/pv1/hello</code></pre><p>![image-20200207104520048](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207104520048.png)</p><p><em><strong>文件已被回收</strong></em></p><h3 id="（5）修改pv的回收策略为手动">（5）修改pv的回收策略为手动</h3><h4 id="修改">修改</h4><pre><code>[root@master yaml]# vim nfs-pv.yaml apiVersion: v1kind: PersistentVolumemetadata:  name: test-pvspec :  capacity :    storage: 1Gi  accessModes:    - ReadWriteOnce  persistentVolumeReclaimPolicy: Retain   #修改  storageClassName: nfs  nfs:    path: /nfsdata/pv1    server: 192.168.1.21</code></pre><h4 id="执行一下">执行一下</h4><pre><code>[root@master yaml]# kubectl apply -f nfs-pv.yaml </code></pre><h4 id="创建pod">创建pod</h4><pre><code>[root@master yaml]# kubectl apply -f pod.yaml </code></pre><h4 id="查看一下-2">查看一下</h4><p>![image-20200207105203009](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207105203009.png)</p><pre><code>[root@master yaml]# kubectl describe pod test-pod </code></pre><p>![image-20200207105248025](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207105248025.png)</p><h4 id="创建pvc">创建pvc</h4><pre><code>[root@master yaml]# kubectl apply -f nfs-pvc.yaml </code></pre><h4 id="查看一下pod">查看一下pod</h4><pre><code>[root@master yaml]# kubectl get pod</code></pre><p>![image-20200207105402354](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207105402354.png)</p><h3 id="（6）test-pod创建hello创建文件并添加内容">（6）test-pod创建hello创建文件并添加内容</h3><pre><code>[root@master yaml]# kubectl exec test-pod touch /mydata/k8s</code></pre><h4 id="查看一下挂载目录">查看一下挂载目录</h4><pre><code>[root@master yaml]# ls /nfsdata/pv1/</code></pre><p>![image-20200207105618318](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207105618318.png)</p><h4 id="删除pod和pvc，pv，再次查看挂载目录">删除pod和pvc，pv，再次查看挂载目录</h4><pre><code>[root@master yaml]# kubectl delete pod test-pod [root@master yaml]# kubectl delete pvc test-pvc[root@master yaml]# kubectl delete pv test-pv </code></pre><h4 id="查看挂载目录">查看挂载目录</h4><pre><code>[root@master yaml]# ls /nfsdata/pv1/</code></pre><p>![image-20200207105757641](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207105757641.png)</p><p><em><strong>内容还在</strong></em></p><h2 id="4-mysql对数据持久化的应用">4.mysql对数据持久化的应用</h2><p><strong>最小化安装系统需要</strong></p><pre><code>yum -y install mariadb</code></pre><h2 id="（1）通过之前的yaml文件，创建pv和pvc">（1）通过之前的yaml文件，创建pv和pvc</h2><pre><code>[root@master yaml]# kubectl apply -f  nfs-pv.yaml [root@master yaml]# kubectl apply -f  nfs-pvc.yaml </code></pre><h3 id="查看一下-3">查看一下</h3><pre><code>[root@master yaml]# kubectl get pv</code></pre><p>![image-20200207110132199](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207110132199.png)</p><pre><code>[root@master yaml]# kubectl get pvc</code></pre><p>![image-20200207110140002](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207110140002.png)</p><h2 id="（2）编写一个mysql的yaml文件">（2）编写一个mysql的yaml文件</h2><pre><code>[root@master yaml]# vim mysql.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata:  name: test-mysqlspec:  selector:    matchLabels:    #支持等值的标签      app: mysqlapiVersion: extensions/v1beta1kind: Deploymentmetadata:  name: test-mysqlspec:  selector:    matchLabels:      app: mysql  template:    metadata:      labels:        app: mysql    spec:      containers:      - image: mysql:5.6        name: mysql        env:        - name: MYSQL_ROOT_PASSWORD          value: 123.com        volumeMounts:        - name: mysql-storage          mountPath: /var/lib/mysql      volumes:      - name: mysql-storage        persistentVolumeClaim:          claimName: test-pvc</code></pre><h3 id="执行一下-2">执行一下</h3><pre><code>[root@master yaml]# kubectl apply -f mysql.yaml </code></pre><h3 id="查看一下-4">查看一下</h3><pre><code>[root@master yaml]# kubectl get pod</code></pre><p>![image-20200207110741833](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207110741833.png)</p><h2 id="（3）进入mysql容器">（3）进入mysql容器</h2><pre><code>[root@master yaml]# kubectl exec -it test-mysql-569f8df4db-rkpwm  -- mysql -u root -p123.com </code></pre><h3 id="创建数据库">创建数据库</h3><pre><code>mysql&gt; create database yun33;</code></pre><h3 id="切换数据库">切换数据库</h3><pre><code>mysql&gt; use yun33;</code></pre><h3 id="创建表">创建表</h3><pre><code>mysql&gt; create table my_id( id int(4))；</code></pre><h3 id="在表中插入数据">在表中插入数据</h3><pre><code>mysql&gt; insert my_id values(9527);</code></pre><h3 id="查看表">查看表</h3><pre><code>mysql&gt; select * from my_id;</code></pre><p>![image-20200207113808540](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207113808540.png)</p><h2 id="（4）查看本地的挂载目录">（4）查看本地的挂载目录</h2><pre><code>[root@master yaml]# ls /nfsdata/pv1/</code></pre><p>![image-20200207113909796](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207113909796.png)</p><h3 id="查看一下pod-2">查看一下pod</h3><pre><code>[root@master yaml]# kubectl get pod -o wide -w</code></pre><p>![image-20200207114050117](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207114050117.png)</p><h3 id="挂起node01">挂起node01</h3><p>![image-20200207114607518](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207114607518.png)</p><h2 id="（5）查看node02上面数据是否和刚才一样（验证数据的一致性）">（5）查看node02上面数据是否和刚才一样（验证数据的一致性）</h2><h3 id="进入数据库">进入数据库</h3><pre><code>[root@master yaml]#  kubectl exec -it test-mysql-569f8df4db-nsdnz  -- mysql -u root -p123.com </code></pre><h3 id="查看数据库">查看数据库</h3><pre><code>mysql&gt; show databases;</code></pre><p>![image-20200207115253123](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207115253123.png)</p><h3 id="查看表-2">查看表</h3><pre><code>mysql&gt; show tables;</code></pre><p>![image-20200207115352727](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207115352727.png)</p><pre><code>mysql&gt; select * from my_id;</code></pre><p>![image-20200207113808540](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207113808540.png)</p><p><em><strong>可以看到数据还在</strong></em></p><h2 id="5-总结">5. 总结</h2><h4 id="PV的访问控制类型"><strong>PV的访问控制类型</strong></h4><p><strong>accessModes:(PV支持的访问模式)</strong></p><ul><li><strong>ReadWriteOnce: 能以读-写mount到单个的节点</strong></li><li><strong>ReadWriteMany: 能以读-写mount到多个的节点。</strong></li><li><strong>ReadOnlyOnce: 能以只读的方式mount到单个节点。</strong></li></ul><h4 id="PV的空间回收策略"><strong>PV的空间回收策略</strong></h4><p><strong>persistentVolumeReclaimPolicy : (PV存储空间的回收策略是什么)</strong></p><p>​    <strong>Recycle: 自动清除数据。</strong></p><p>​    <strong>Retain: 需要管理员手动回收。</strong></p><p>​    <strong>Delete： 云存储专用。</strong></p><h4 id="PV和PVC相互关联"><strong>PV和PVC相互关联</strong></h4><p><strong>是通过accessModes和storageClassName模块关联的</strong></p><h4 id="Pod不断的重启">Pod不断的重启:</h4><p><strong>1、swap,没有关闭，导致集群运行不正常。</strong><br><strong>2、内存不足，运行服务也会重后。</strong></p><p>kubectl describe<br>kubectl logs<br>/var/ log/messages<br>查看该节点的kubelet的日志。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>10 复习 </title>
      <link href="/posts/1b18.html"/>
      <url>/posts/1b18.html</url>
      
        <content type="html"><![CDATA[<h1>虚拟化</h1><p><strong>云计算的分类</strong>：</p><blockquote><p><strong>基础及服务：laas</strong><br><strong>平台及服务：paas</strong><br><strong>软件及服务：saas</strong></p></blockquote><p><strong>docker虚拟化的底层原理:</strong> Namespace + Cgroup</p><p><strong>Namespace六项隔离:</strong></p><blockquote><p><strong>IPC: 共享内存,消息列队</strong><br><strong>MNT: 挂载点 文件系统</strong><br><strong>NET: 网络栈</strong><br><strong>PID: 进程编号</strong><br><strong>USER: 用户 组</strong><br><strong>UTS: 主机名 域名</strong><br><strong>namespace 六项隔离 实现了容器与宿主机 容器与容器之间的隔离</strong></p></blockquote><p><strong>Cgroup 四项作用：</strong></p><blockquote><p>**1） 资源的限制：**cgroup可以对进程组使用的资源总额进行限制<br>**2） 优先级分配：**通过分配的cpu时间片数量以及硬盘IO带宽的大小，实际上相当于控制了进程运行的优先级别<br><strong>3） 资源统计：</strong> group可以统计系统资源使用量，比如gpu使用时间，内存使用量等，用于按量计费。同时还支持挂起动能，也就是说通过cgroup把所有 资源限制起来,对资源都不能使用，注意着并不是说我们的程序不能使用了,知识不能使用资源，处于等待状态。<br>**4） 进程控制：**可以对进程组执行挂起、恢复等操作。</p></blockquote><p><strong>镜像是容器运行的核心，容器是镜像运行的后的实例。</strong></p><p><strong>DockerHub| registry  ----&gt;  pull</strong><br><strong>image :     save &gt;   |  load &lt;</strong><br><strong>run    ----&gt;  Container    ----&gt;   commit</strong>*<br><strong>Dockerfile</strong></p><p><strong>Docker 三剑客。</strong></p><blockquote><p><strong>docker  machine :自动化部署多台dockerHost 。</strong></p><p>​        <strong>Docker-compose: 它可以同时控制多个容器。</strong></p><p>​        <strong>yaml。</strong></p><p><strong>Docker Swarm：</strong></p><p>​        <strong>从单个的服务向集群的形势发展。</strong></p><p>​         <strong>高可用、高性能、高并发 ：为了防止单点故障。</strong></p><p>​         <strong>Service：服务  ----&gt; 包括运行什么服务，需要多个                          rep1icas（副本）, 外网如何访问。</strong></p></blockquote><h3 id="k8s"><strong>k8s</strong></h3><p>关闭防火墙、禁用selinux、修改主机名并加入域名解析、关闭swap 、时间同步、免密登录、打开iptables桥接</p><blockquote><p><strong>对硬件的基本要求： CPU：2核   MEM：2G</strong></p><p><strong>主机名：master node01 node02</strong></p><p><strong>时间必须同步</strong></p></blockquote><p><strong>kubctl：k8s客户端      kubeadm：工具  kubelet：客户端代理</strong></p><p><strong>组件：</strong></p><p>​<strong>三层网络： DockerHost  &gt;   Pod  &gt; Service</strong></p><p>​        <strong>Deployment:        Service:</strong></p><h3 id="master组件">**master组件: **</h3><p><strong>kube- api( application interface) k8s的前端接口</strong></p><p>**Scheduler[集群分发调度器]**负责决定将Pod放在哪个Node上运行。在调度时，会充分考虑集群的拓扑结构，当前各个节点的负载情况，以及应对高可用、性能、数据亲和性和需求。</p><p><strong>Controller Manager[内部管理控制中心]</strong>：负责管理集群的各种资源，保证资源处于预期的状态。它由多种Controller组成，包括Replication Controller、Endpoints Controller、Namespace Controller、Serviceaccounts Controller等。</p><p>**Etcd：**负责保存k8s集群的配置信息和各种资源的状态信息。当数据发生变化时，etcd会快速的通知k8s相关组件。<a href="">（第三方组件）它有可替换方案。Consul、zookeeper</a></p><p>**Flanner：**是k8s集群网络，可以保证Pod的跨主机通信。也有替换方案。</p><h3 id="Node组件：">Node组件：</h3><p><strong>Kubelet[节点上的Pod管家]</strong>：它是Node的agent(代理)，当Scheduler确定某 个Node上运行Pod之后，会将Pod的具体配置信息发送给该节点的kubelet,kubelet会根据这些信息创建和运行容器，并向Master报告运行状态。</p><p>**kube-proxy[负载均衡、路由转发]:**负责将访问service的TCP/UDP数据流转发到后端的容器。如果有多个副本，kube-proxy会实现负载均衡。</p><h3 id="yaml文件的一级字段">yaml文件的一级字段:</h3><p>​<strong>VERSION:</strong><br>​<strong>KIND:</strong><br>​<strong>METADATA:</strong><br>​<strong>SPEC :</strong></p><pre><code>[root@master ~]# vim web.yamlkind: Deployment  #资源对象是控制器apiVersion: extensions/v1beta1   #api的版本metadata:      #描述kind（资源类型）  name: web   #定义控制器名称  namespace：  #名称空间spec:  replicas: 2   #副本数量  template:     #模板    metadata:          labels:   #标签        app: web_server    spec:      containers:   #指定容器      - name: nginx  #容器名称        image: nginx   #使用的镜像</code></pre><p>​<strong>Deployment（控制器)：</strong></p><p>​**ReplicationController：**用来确保由其管控的Pod对象副本数量，能够满足用户期望，多则删除，少则通过模本创建</p><p>​**RS（RpelicaSet）:**RS也是用于保证与label selector匹配的pod数量维持在期望状态</p><p>​<strong>Service：</strong></p><p>​<strong>type：默认Cluster IP</strong></p><p>​<strong>NodePort：  30000-32767</strong></p><p>​<strong>Deployment和Service关联：标签和标签选择器</strong></p><p>​<strong>Namespace：</strong></p><p>​<strong>Pod：最小单位</strong></p><p>​<strong>镜像的下载策略：</strong></p><blockquote><p>​**Always：**镜像标签为“laster”或镜像不存在时，总是从指定的仓库中获取镜像。</p><p>​**IfNotPresent：**仅当本地镜像不存在时才从目标仓库下载。</p><p>​**Never：**禁止从仓库中下载镜像，即只使用本地镜像。</p></blockquote><p>​<strong>默认的标签 为latest：always</strong></p><p>​<strong>Pod的重启策略：</strong></p><blockquote><p>​<strong>Always：</strong>（默认情况下使用）但凡Pod对象终止就将其重启；<br>​**OnFailure：**仅在Pod对象出现错误时才将其重启；<br>​**Never：**从不重启；</p></blockquote><p>​<strong>Pod的健康检查:</strong><br>​Liveness:   探测失败重启pod<br>​Readiness: 探测失败将pod设置为不可用<br>kubelet：控制pod</p><p>DaemonSet :会在每一个节点都会运行，并且只运行一个Pod</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>09 Job资源对象</title>
      <link href="/posts/e9be.html"/>
      <url>/posts/e9be.html</url>
      
        <content type="html"><![CDATA[<h1>Job资源对象</h1><blockquote><p>**服务类的Pod容器：**RC、RS、DS、Deployment</p><p>**工作类的Pod容器：**Job—&gt;执行一次，或者批量执行处理程序，完成之后退出容器。</p></blockquote><p><strong>注意： 如果容器内执行任务有误，会根据容器的重启策略操作容器，不过这里</strong><br><strong>的容器重启策略只能是: Never和 OnFailure。</strong></p><h1>概念</h1><p><strong>在有些场景下，是想要运行一些容器执行某种特定的任务，任务一旦执行完成，容器也就没有存在的必要了。在这种场景下，创建pod就显得不那么合适。于是就是了Job，Job指的就是那些一次性任务。通过Job运行一个容器，当其任务执行完以后，就自动退出，集群也不再重新将其唤醒。</strong></p><p><strong>从程序的运行形态上来区分，可以将Pod分为两类：长时运行服务（jboss、mysql等）和一次性任务（数据计算、测试）。RC创建的Pod都是长时运行的服务，Job多用于执行一次性任务、批处理工作等，执行完成后便会停止（status.phase变为Succeeded）。</strong></p><h1>一、kubernetes支持以下几种job</h1><blockquote><ul><li><strong>非并行job：通常创建一个pod直至其成功结束。</strong></li><li><strong>固定结束次数的job：设置spec.completions,创建多个pod，直到.spec.completions个pod成功结束。</strong></li><li><strong>带有工作队列的并行job：设置.spec.Parallelism但不设置.spec.completions,当所有pod结束并且至少一个成功时，job就认为是成功。</strong></li></ul></blockquote><h2 id="Job-Controller">Job Controller</h2><p><strong>Job Controller负责根据Job Spec创建pod，并持续监控pod的状态，直至其成功结束，如果失败，则根据restartPolicy（只支持OnFailure和Never，不支持Always）决定是否创建新的pod再次重试任务。</strong></p><h2 id="例子"><strong>例子</strong></h2><h3 id="（1）编写一个job的yaml文件">（1）编写一个job的yaml文件</h3><pre><code>[root@master yaml]# vim jop.yamlkind: JobapiVersion: batch/v1metadata:  name: test-jobspec:  template:    metadata:      name: test-job    spec:      containers:      - name: hello        image: busybox        command: ["echo","hello k8s job!"]      restartPolicy: Never</code></pre><h3 id="（2）执行一下">（2）执行一下</h3><pre><code>[root@master yaml]# kubectl apply -f jop.yaml </code></pre><h3 id="（3）查看一下">（3）查看一下</h3><pre><code>[root@master yaml]# kubectl get pod</code></pre><p>![image-20200115090831524](E:\软件\博客\Blog\blog\source_posts\09 Jop资源对象.assets\image-20200115090831524.png)</p><h4 id="查看日志">查看日志</h4><pre><code>[root@master yaml]# kubectl logs test-job-gs45w </code></pre><p>![image-20200115091213349](E:\软件\博客\Blog\blog\source_posts\09 Jop资源对象.assets\image-20200115091213349.png)</p><p><strong>我们可以看到job与其他资源对象不同，仅执行一次性任务，默认pod借宿运行后job即结束，状态为Completed。</strong></p><h3 id="（4）修改一下jop的yaml文件，把echo命令换成乱码">（4）修改一下jop的yaml文件，把echo命令换成乱码</h3><pre><code>[root@master yaml]# vim jop.yamlkind: JobapiVersion: batch/v1metadata:  name: test-jobspec:  template:    metadata:      name: test-job    spec:      containers:      - name: hello        image: busybox        command: ["asdasxsddwefew","hello k8s job!"] #修改      restartPolicy: Never</code></pre><h3 id="（5）先删除之前的pod">（5）先删除之前的pod</h3><pre><code>[root@master yaml]# kubectl delete jobs.batch test-job  </code></pre><h3 id="（6）执行一下">（6）执行一下</h3><pre><code>[root@master yaml]# kubectl apply -f jop.yaml </code></pre><h3 id="（7）查看一下">（7）查看一下</h3><pre><code>[root@master yaml]# kubectl get pod -w</code></pre><p>![image-20200115091647925](E:\软件\博客\Blog\blog\source_posts\09 Jop资源对象.assets\image-20200115091647925.png)</p><p><em><strong>它会一直创建pod直到完成命令。</strong></em></p><h3 id="（8）修改一下jop的yaml文件，修改重启策略">（8）修改一下jop的yaml文件，修改重启策略</h3><pre><code>[root@master yaml]# vim jop.yaml kind: JobapiVersion: batch/v1metadata:  name: test-jobspec:  template:    metadata:      name: test-job    spec:      containers:      - name: hello        image: busybox        command: ["asdasxsddwefew","hello k8s job!"]      restartPolicy: OnFailure</code></pre><h3 id="（9）先删除之前的pod">（9）先删除之前的pod</h3><pre><code>[root@master yaml]# kubectl delete jobs.batch test-job </code></pre><h3 id="（10）执行一下">（10）执行一下</h3><pre><code>[root@master yaml]# kubectl apply -f jop.yaml </code></pre><h3 id="（11）查看一下">（11）查看一下</h3><pre><code>[root@master yaml]# kubectl get pod -w</code></pre><p>![image-20200115092801882](E:\软件\博客\Blog\blog\source_posts\09 Job资源对象.assets\image-20200115092801882.png)</p><p><em><strong>它会一直重启pod完成命令，直到重启到一定次数就会删除job。</strong></em></p><h1>二、提高Job的执行效率</h1><h2 id="1-我们可以在Job-spec字段下加上parallelism选项。表示同时运行多少个Pod执行任务。">1. 我们可以在Job.spec字段下加上<a href="">parallelism</a>选项。表示同时运行多少个Pod执行任务。</h2><hr><h3 id="（1）编写一个job的yaml文件-2">（1）编写一个job的yaml文件</h3><pre><code>[root@master yaml]# vim jop.yamlkind: JobapiVersion: batch/v1metadata:  name: test-jobspec:  parallelism: 2    #同时启用几个pod  template:    metadata:      name: test-job    spec:      containers:      - name: hello        image: busybox        command: ["echo","hello k8s job!"]      restartPolicy: OnFailure</code></pre><h3 id="（3）执行一下">（3）执行一下</h3><pre><code>[root@master yaml]# kubectl apply -f jop.yaml </code></pre><h3 id="（4）查看一下">（4）查看一下</h3><pre><code>[root@master yaml]# kubectl get pod</code></pre><p>![image-20200115093854913](E:\软件\博客\Blog\blog\source_posts\09 Job资源对象.assets\image-20200115093854913.png)</p><h4 id="查看日志-2">查看日志</h4><p>![image-20200115094002236](E:\软件\博客\Blog\blog\source_posts\09 Job资源对象.assets\image-20200115094002236.png)</p><h2 id="2-我们可以在Job-spec字段下加上complations选项。表示总共需要完成Pod的数量">2. 我们可以在Job.spec字段下加上complations选项。表示总共需要完成Pod的数量</h2><h3 id="（1）编写一个job的yaml文件-3">（1）编写一个job的yaml文件</h3><pre><code>[root@master yaml]# vim jop.yamlkind: JobapiVersion: batch/v1metadata:  name: test-jobspec:  complations: 8            #运行pod的总数量8个  parallelism: 2            #同时运行2个pod  template:    metadata:      name: test-job    spec:      containers:      - name: hello        image: busybox        command: ["echo","hello k8s job!"]      restartPolicy: OnFailure</code></pre><p><strong>job 字段解释：</strong></p><blockquote><p><strong>标志Job结束需要成功运行的Pod个数，默认为1</strong><br><strong>parallelism：标志并行运行的Pod的个数，默认为1</strong><br><strong>activeDeadlineSeconds：标志失败Pod的重试最大时间，超过这个时间不会继续重试.</strong></p></blockquote><h3 id="（2）先删除之前的pod">（2）先删除之前的pod</h3><pre><code>[root@master yaml]# kubectl delete jobs.batch test-job </code></pre><h3 id="（3）执行一下-2">（3）执行一下</h3><pre><code>[root@master yaml]# kubectl apply -f jop.yaml </code></pre><h3 id="（4）查看一下-2">（4）查看一下</h3><pre><code>[root@master yaml]# kubectl get pod</code></pre><p>![image-20200115094519494](E:\软件\博客\Blog\blog\source_posts\09 Job资源对象.assets\image-20200115094519494.png)</p><p><strong>可以看到pod是两个两个的启动的。</strong></p><h2 id="3-如何定时执行Job">3. 如何定时执行Job</h2><h3 id="（1）编写一个cronjob的yaml文件">（1）编写一个cronjob的yaml文件</h3><pre><code>[root@master yaml]# vim cronjop.yamlkind: CronJobapiVersion: batch/v1beta1metadata:  name: hellospec:  schedule: "*/1 * * * *" #限定时间  jobTemplate:    spec:      template:        spec:          containers:          - name: hello            image: busybox            command: ["echo","hello","cronjob"]          restartPolicy: OnFailure</code></pre><h3 id="（2）先删除之前的pod-2">（2）先删除之前的pod</h3><pre><code>[root@master yaml]# kubectl delete jobs.batch test-job </code></pre><h3 id="（3）执行一下-3">（3）执行一下</h3><pre><code>[root@master yaml]# kubectl apply -f jop.yaml </code></pre><h3 id="（4）查看一下-3">（4）查看一下</h3><pre><code>[root@master yaml]# kubectl get pod</code></pre><p>![image-20200115095857428](E:\软件\博客\Blog\blog\source_posts\09 Job资源对象.assets\image-20200115095857428.png)</p><pre><code>[root@master yaml]# kubectl get cronjobs.batch </code></pre><p>![image-20200115095920740](E:\软件\博客\Blog\blog\source_posts\09 Job资源对象.assets\image-20200115095920740.png)</p><p><strong>此时查看Pod的状态，会发现，每分钟都会运行一个新的Pod来执行命令规定的任</strong><br><strong>务。</strong></p><h2 id="练习：规定2020-1-15-10-5分运行上面的crontab任务。">练习：规定2020.1.15.10.5分运行上面的crontab任务。</h2><h3 id="（1）编写一个cronjob的yaml文件-2">（1）编写一个cronjob的yaml文件</h3><pre><code>[root@master yaml]# vim cronjop.yamlkind: CronJobapiVersion: batch/v1beta1metadata:  name: hellospec:  schedule: "5 10 15 1 *" #限定时间  jobTemplate:    spec:      template:        spec:          containers:          - name: hello            image: busybox            command: ["echo","hello","cronjob"]          restartPolicy: OnFailure</code></pre><h3 id="（2）先删除之前的pod-3">（2）先删除之前的pod</h3><pre><code>[root@master yaml]# kubectl delete cronjobs.batch hello </code></pre><h3 id="（3）执行一下-4">（3）执行一下</h3><pre><code>[root@master yaml]# kubectl apply -f jop.yaml </code></pre><h3 id="（4）查看一下-4">（4）查看一下</h3><pre><code>[root@master yaml]# kubectl get pod</code></pre><p>![image-20200115100855819](E:\软件\博客\Blog\blog\source_posts\09 Job资源对象.assets\image-20200115100855819.png)</p><p><strong>这时会发现，如果规定具体时间，可能并不会执行任务。</strong></p><h3 id="（5）添加apiVersion库">（5）添加apiVersion库</h3><pre><code>[root@master yaml]# vim /etc/kubernetes/manifests/kube-apiserver.yaml spec:  containers:  - command:    - kube-apiserver    - --runtime-config=batch/v2alpha1=true    #添加</code></pre><p>![image-20200115104218361](E:\软件\博客\Blog\blog\source_posts\09 Job资源对象.assets\image-20200115104218361.png)</p><h3 id="（6）重启kubelet">（6）重启kubelet</h3><pre><code>[root@master yaml]# systemctl restart kubelet.service </code></pre><h3 id="（7）查看api版本">（7）查看api版本</h3><pre><code>[root@master yaml]# kubectl api-versions </code></pre><p>![image-20200115104521662](E:\软件\博客\Blog\blog\source_posts\09 Job资源对象.assets\image-20200115104521662.png)</p><h3 id="（8）编写一个cronjob的yaml文件">（8）编写一个cronjob的yaml文件</h3><pre><code>[root@master yaml]# vim cronjop.yamlkind: CronJobapiVersion: batch/v1beta1metadata:  name: hellospec:  schedule: "47 10 15 1 *" #限定时间  jobTemplate:    spec:      template:        spec:          containers:          - name: hello            image: busybox            command: ["echo","hello","cronjob"]          restartPolicy: OnFailure</code></pre><h3 id="（9）执行一下">（9）执行一下</h3><pre><code>[root@master yaml]# kubectl apply -f jop.yaml </code></pre><h3 id="（4）查看一下-5">（4）查看一下</h3><pre><code>[root@master yaml]# kubectl get pod -w</code></pre><p>![image-20200115100855819](E:\软件\博客\Blog\blog\source_posts\09 Job资源对象.assets\image-20200115100855819.png)</p><p><strong>注意：此时仍然不能正常运行指定时间的Job，这是因为K8s官方在cronjob这个资源对象的支持中还没有完善此功能，还待开发。</strong></p><p><strong>跟Job资源一样在cronjob.spec.jobTemplate.spec 下同样支持并发Job参数:</strong><br><strong>parallelism，也支持完成Pod的总数参数: completionsr</strong></p><h1>总结</h1><p><strong>Job 作为 Kubernetes 中用于处理任务的资源，与其他的资源没有太多的区别，它也使用 Kubernetes 中常见的控制器模式，监听 Informer 中的事件并运行 <code>syncHandler</code> 同步任务</strong></p><p><strong>而 CronJob 由于其功能的特殊性，每隔 10s 会从 apiserver 中取出资源并进行检查是否应该触发调度创建新的资源，需要注意的是 CronJob 并不能保证在准确的目标时间执行，执行会有一定程度的滞后。</strong></p><p><strong>两个控制器的实现都比较清晰，只是边界条件比较多，分析其实现原理时一定要多注意。</strong></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>08 ReplicaSet、DaemonSet</title>
      <link href="/posts/7772.html"/>
      <url>/posts/7772.html</url>
      
        <content type="html"><![CDATA[<h1>ReplicaSet简单介绍</h1><h2 id="1-RC：ReplicationController（老一代的pod控制器）">1. RC：ReplicationController（老一代的pod控制器）</h2><p><strong>用来确保由其管控的Pod对象副本数量，能够满足用户期望，多则删除，少则通过模本创建</strong></p><h3 id="特点：">特点：</h3><ul><li>​<strong>确保Pod资源对象的数量精准。</strong></li><li>​<strong>确保pod健康运行。</strong></li><li>​<strong>弹性伸缩</strong></li></ul><p><strong>同样，它也可以通过yaml或json格式的资源清单来创建。其中spec字段一般嵌套以下字段：</strong></p><ul><li>​<strong>replicas：期望的Pod对象副本数量。</strong></li><li>​<strong>selector：当前控制器匹配Pod对此项副本的标签选择器</strong></li><li>​<strong>template：pod副本的模板</strong></li></ul><p><strong>与RC相比而言，RS不仅支持<em>基于等值</em>的标签选择器，而且还支持<em>基于集合</em>的标签选择器。</strong></p><h2 id="2-标签：解决同类型的资源对象，为了更好的管理，按照标签分组。">2. 标签：解决同类型的资源对象，为了更好的管理，按照标签分组。</h2><h3 id="常用的标签分类：">常用的标签分类：</h3><ul><li>​<strong>release（版本）：stable（稳定版）、canary（金丝雀版本）、beta（测试版本）</strong></li><li>​<strong>environment（环境变量）：dev（开发）、qa（测试）、production（生产）</strong></li><li>​<strong>application（应用）：ui、as（application software应用软件）、pc、sc</strong></li><li>​<strong>tier（架构层级）：frontend（前端）、backend（后端）、cache（缓存）</strong></li><li>​<strong>partition（分区）：customerA（客户A）、customerB（客户B）</strong></li><li>​<strong>track（品控级别）：daily（每天）、weekly（每周）</strong></li></ul><p><strong>标签要做到：见名知意。</strong></p><h2 id="3-测试">3.测试</h2><h3 id="（1）编写一个pod的yaml文件">（1）编写一个pod的yaml文件</h3><pre><code>[root@master ~]# vim label.yaml kind: PodapiVersion: v1metadata:  name: labels  labels:    env: qa    tier: frontendspec:  containers:  - name: myapp    image: httpd</code></pre><h4 id="1-执行一下">&lt;1&gt;执行一下</h4><pre><code>[root@master ~]# kubectl apply -f label.yaml  --record </code></pre><h4 id="2-查看一下">&lt;2&gt;查看一下</h4><pre><code>[root@master ~]# kubectl get pod  --show-labels //通过--show-labels显示资源对象的</code></pre><p>![image-20200114095943595](E:\软件\博客\Blog\blog\source_posts\08 ReplicaS儿童、DaemonSet.assets\image-20200114095943595.png)</p><pre><code>[root@master ~]# kubectl get po -L env,tier//显示某个键对应的值</code></pre><p>![image-20200114100043922](E:\软件\博客\Blog\blog\source_posts\08 ReplicaS儿童、DaemonSet.assets\image-20200114100043922.png)</p><pre><code>[root@master ~]# kubectl get po -l env,tier//通过-l 查看仅包含某个标签的资源。</code></pre><p>![image-20200114100200895](E:\软件\博客\Blog\blog\source_posts\08 ReplicaS儿童、DaemonSet.assets\image-20200114100200895.png)</p><h3 id="（2）添加标签">（2）添加标签</h3><pre><code>[root@master ~]# kubectl label pod  labels app=pc//给pod资源添加标签</code></pre><h3 id="（3）修改标签">（3）修改标签</h3><pre><code>[root@master ~]# kubectl label pod labels env=dev --overwrite//修改标签</code></pre><pre><code>[root@master ~]# kubectl get pod -l tier --show-labels //查看标签</code></pre><p>![image-20200114100607585](E:\软件\博客\Blog\blog\source_posts\08 ReplicaS儿童、DaemonSet.assets\image-20200114100607585.png)</p><h3 id="（4）编写一个service的yaml文件">（4）编写一个service的yaml文件</h3><pre><code>[root@master ~]# vim service.yamlkind: ServiceapiVersion: v1metadata:  name: servicespec:  type: NodePort  selector:    env: qa  ports:  - protocol: TCP    port: 90    targetPort: 80    nodePort: 30123</code></pre><h4 id="1-执行一下-2">&lt;1&gt;执行一下</h4><pre><code>[root@master ~]# kubectl apply -f service.yaml </code></pre><h4 id="2-查看一下-2">&lt;2&gt;查看一下</h4><pre><code>[root@master ~]# kubectl describe svc</code></pre><p>![image-20200114101837151](E:\软件\博客\Blog\blog\source_posts\08 ReplicaSet、DaemonSet.assets\image-20200114101837151.png)</p><h4 id="3-访问一下">&lt;3&gt;访问一下</h4><pre><code>[root@master ~]# curl 127.0.0.1:30123</code></pre><p>![image-20200114101915248](E:\软件\博客\Blog\blog\source_posts\08 ReplicaSet、DaemonSet.assets\image-20200114101915248.png)</p><p><strong>如果标签有多个，标签选择器选择其中一个，也可以关联成功。相反，如果选择器有多个，那么标签必须完全满足条件，才可以关联成功。</strong></p><h2 id="4-标签选择器：标签的查询过滤条件。">4. 标签选择器：标签的查询过滤条件。</h2><p><strong><a href="">基于等值关系的（equality-based）</a>：“=”，“==”，“！ =”前面两个都是相等，最后一个是不等于。</strong></p><p><strong><a href="">基于集合关系（set-based）</a>:in、notin、exists三种。选择器列表间为“逻辑与”关系，使用ln或者NotIn操作时，其valuas不强制要求为非空的字符串列表，而使用Exists或DostNotExist时，其values必须为空</strong></p><h4 id="使用标签选择器的逻辑：">使用标签选择器的逻辑：</h4><ul><li><strong>同时指定的多个选择器之间的逻辑关系为“与”操作。</strong></li><li><strong>使用空值的标签选择器意味着每个资源对象都将把选中。</strong></li><li><strong>空的标签选择器无法选中任何资源。</strong></li></ul><h3 id="（1）例子">（1）例子</h3><p>![image-20200114110334223](E:\软件\博客\Blog\blog\source_posts\08 ReplicaSet、DaemonSet.assets\image-20200114110334223.png)</p><h4 id="编写一个selector的yaml’文件">编写一个selector的yaml’文件</h4><pre class=" language-language-yaml"><code class="language-language-yaml">[root@master ~]# vim selector.yamlselector:  matchLabels:    app: nginx  mathExpressions:    - {key: name,operator: In,values: [zhangsan,lisi]}    - {key: age,operator: Exists,values:}</code></pre><ul><li><strong>selector：当前控制器匹配Pod对此项副本的标签选择器</strong></li><li><strong>matchLabels: 指定键值对表示的标签选择器。</strong></li><li><strong>mathExpressions:：基于表达式来指定的标签选择器。</strong></li></ul><h1>DaemonSet</h1><p><em><strong>它也是一种pod控制器。</strong></em></p><p><em><strong>RC，RS , deployment , daemonset.都是pod控制器。statfukSet，RBAC</strong></em></p><h3 id="1-使用场景：">1. 使用场景：</h3><p><strong>如果必须将pod运行在固定的某个或某几个节点，且要优先于其他的pod的启动。通常情况下，默认会将每一个节点都运行，并且只能运行一个pod。这种情况推荐使用DeamonSet资源对象。</strong></p><ul><li><strong>监控程序；</strong></li><li><strong>日志收集程序；</strong></li><li><strong>集群存储程序；</strong></li></ul><pre><code>[root@master ~]# kubectl get ds -n kube-system //查看一下DaemonSet</code></pre><h3 id="2-DaemonSet-与-Deployment-的区别">2. DaemonSet 与 Deployment 的区别</h3><ul><li><strong>Deployment 部署的副本 Pod 会分布在各个 Node 上，每个 Node 都可能运行好几个副本。</strong></li><li><strong>DaemonSet 的不同之处在于：每个 Node 上最多只能运行一个副本。</strong></li></ul><h3 id="3-运行一个web服务，在每一个节点运行一个pod。">3. 运行一个web服务，在每一个节点运行一个pod。</h3><pre><code>[root@master ~]# vim daemonset.yamlkind: DaemonSetapiVersion: extensions/v1beta1metadata:  name: test-dsspec:  template:    metadata:      labels:        name: test-ds    spec:      containers:      - name: test-ds        image: httpd</code></pre><h4 id="1-执行一下-3">&lt;1&gt;执行一下</h4><pre><code>[root@master ~]# kubectl apply -f daemonset.yaml </code></pre><h4 id="2-查看一下-3">&lt;2&gt;查看一下</h4><pre><code>[root@master ~]# kubectl get ds</code></pre><p>![image-20200114112936161](E:\软件\博客\Blog\blog\source_posts\08 ReplicaSet、DaemonSet.assets\image-20200114112936161.png)</p><h1>总结</h1><h2 id="1）总结RC、RS、Deplyment、DaemonSet控制器的特点及使用场景。"><strong>1）总结RC、RS、Deplyment、DaemonSet控制器的特点及使用场景。</strong></h2><h3 id="1-Replication-Controller（RC）">&lt;1&gt;Replication Controller（RC）</h3><h4 id="介绍及使用场景">介绍及使用场景</h4><p><strong><code>Replication Controller</code>简称<code>RC</code>，<code>RC</code>是<code>Kubernetes</code>系统中的核心概念之一，简单来说，<code>RC</code>可以保证在任意时间运行<code>Pod</code>的副本数量，能够保证<code>Pod</code>总是可用的。如果实际<code>Pod</code>数量比指定的多那就结束掉多余的，如果实际数量比指定的少就新启动一些<code>Pod</code>，当<code>Pod</code>失败、被删除或者挂掉后，<code>RC</code>都会去自动创建新的<code>Pod</code>来保证副本数量，所以即使只有一个<code>Pod</code>，我们也应该使用<code>RC</code>来管理我们的<code>Pod</code>。</strong></p><h4 id="主要功能">主要功能</h4><ul><li><strong>确保pod数量：RC用来管理正常运行Pod数量，一个RC可以由一个或多个Pod组成，在RC被创建后，系统会根据定义好的副本数来创建Pod数量。在运行过程中，如果Pod数量小于定义的，就会重启停止的或重新分配Pod，反之则杀死多余的。</strong></li><li><strong>确保pod健康：当pod不健康，运行出错或者无法提供服务时，RC也会杀死不健康的pod，重新创建新的。</strong></li><li><strong>弹性伸缩 ：在业务高峰或者低峰期的时候，可以通过RC动态的调整pod的数量来提高资源的利用率。同时，配置相应的监控功能（Hroizontal Pod Autoscaler），会定时自动从监控平台获取RC关联pod的整体资源使用情况，做到自动伸缩。</strong></li><li><strong>滚动升级：滚动升级为一种平滑的升级方式，通过逐步替换的策略，保证整体系统的稳定，在初始化升级的时候就可以及时发现和解决问题，避免问题不断扩大。</strong></li></ul><h3 id="2-Replication-Set（RS）">&lt;2&gt;Replication Set（RS）</h3><p><strong>被认为 是“升级版”的RC。RS也是用于保证与label selector匹配的pod数量维持在期望状态。</strong></p><blockquote><p><strong>实际上<code>RS</code>和<code>RC</code>的功能基本一致，目前唯一的一个区别就是<code>RC</code>只支持基于等式的<code>selector</code>（env=dev或app=nginx），但<code>RS</code>还支持基于集合的<code>selector</code>（version in (v1, v2)），这对复杂的运维管理就非常方便了。</strong></p><p><strong><code>kubectl</code>命令行工具中关于<code>RC</code>的大部分命令同样适用于我们的<code>RS</code>资源对象。不过我们也很少会去单独使用<code>RS</code>，它主要被<code>Deployment</code>这个更加高层的资源对象使用，除非用户需要自定义升级功能或根本不需要升级<code>Pod</code>，在一般情况下，我们推荐使用<code>Deployment</code>而不直接使用<code>Replica Set</code>。</strong></p></blockquote><h4 id="区别在于">区别在于</h4><p><strong>1、RC只支持基于等式的selector（env=dev或environment!=qa），但RS还支持新的，基于集合的selector（version in (v1.0, v2.0)或env notin (dev, qa)），这对复杂的运维管理很方便。</strong></p><p><strong>2、升级方式</strong></p><ul><li><strong>RS不能使用kubectlrolling-update进行升级</strong></li><li><strong>kubectl rolling-update专用于rc</strong></li><li><strong>RS升级使用deployment或者kubectl replace命令</strong></li><li><strong>社区引入这一API的初衷是用于取代vl中的RC，也就是说当v1版本被废弃时，RC就完成了它的历史使命，而由RS来接管其工作</strong></li></ul><h3 id="3-DaemonSet">&lt;3&gt;DaemonSet</h3><h4 id="1-特点：">1. 特点：</h4><p><strong>如果必须将pod运行在固定的某个或某几个节点，且要优先于其他的pod的启动。通常情况下，默认会将每一个节点都运行，并且只能运行一个pod。这种情况推荐使用DeamonSet资源对象。</strong></p><p><strong>一个DaemonSet对象能确保其创建的Pod在集群中的每一台（或指定）Node上都运行一个副本。如果集群中动态加入了新的Node，DaemonSet中的Pod也会被添加在新加入Node上运行。删除一个DaemonSet也会级联删除所有其创建的Pod。</strong></p><h4 id="2-使用环境"><strong>2. 使用环境</strong></h4><ul><li><strong>监控程序；</strong></li><li><strong>日志收集程序；</strong></li><li><strong>集群存储程序；</strong></li></ul><h3 id="4-Deployment">&lt;4&gt;Deployment</h3><h4 id="1-什么是Deployment">1. 什么是Deployment</h4><p><strong>Kubernetes Deployment提供了官方的用于更新Pod和Replica Set（下一代的Replication Controller）的方法，您可以在Deployment对象中只描述您所期望的理想状态（预期的运行状态），Deployment控制器为您将现在的实际状态转换成您期望的状态，例如，您想将所有的webapp:v1.0.9升级成webapp:v1.1.0，您只需创建一个Deployment，Kubernetes会按照Deployment自动进行升级。现在，您可以通过Deployment来创建新的资源（pod，rs，rc），替换已经存在的资源等。</strong></p><p><strong>你只需要在Deployment中描述你想要的目标状态是什么，Deployment controller就会帮你将Pod和Replica Set的实际状态改变到你的目标状态。你可以定义一个全新的Deployment，也可以创建一个新的替换旧的Deployment。</strong></p><h4 id="2-典型的用例">2. 典型的用例</h4><ul><li><strong>使用Deployment来创建ReplicaSet。ReplicaSet在后台创建pod。检查启动状态，看它是成功还是失败。</strong></li><li><strong>然后，通过更新Deployment的PodTemplateSpec字段来声明Pod的新状态。这会创建一个新的ReplicaSet，Deployment会按照控制的速率将pod从旧的ReplicaSet移动到新的ReplicaSet中。</strong></li><li><strong>如果当前状态不稳定，回滚到之前的Deployment revision。每次回滚都会更新Deployment的revision。</strong></li><li><strong>扩容Deployment以满足更高的负载。</strong></li><li><strong>暂停Deployment来应用PodTemplateSpec的多个修复，然后恢复上线。</strong></li><li><strong>根据Deployment 的状态判断上线是否hang住了。</strong></li><li><strong>清除旧的不必要的ReplicaSet。</strong></li></ul><h4 id="3-使用环境">3. 使用环境</h4><p><strong>Deployment集成了上线部署、滚动升级、创建副本、暂停上线任务，恢复上线任务，回滚到以前某一版本（成功/稳定）的Deployment等功能，在某种程度上，Deployment可以帮我们实现无人值守的上线，大大降低我们的上线过程的复杂沟通、操作风险。</strong></p><ul><li><strong>定义Deployment来创建Pod和ReplicaSet</strong></li><li><strong>滚动升级和回滚应用</strong></li><li><strong>扩容和缩容</strong></li><li><strong>暂停和继续Deployment</strong></li></ul><h4 id="3-DaemonSet-与-Deployment-的区别">3. DaemonSet 与 Deployment 的区别</h4><ul><li><strong>Deployment 部署的副本 Pod 会分布在各个 Node 上，每个 Node 都可能运行好几个副本。</strong></li><li><strong>DaemonSet 的不同之处在于：每个 Node 上最多只能运行一个副本。</strong></li></ul><h2 id="2）使用DaemonSet控制器运行httpd服务，要求名称以自己的名称命名。标签为：tier-backend-env-dev"><strong>2）使用DaemonSet控制器运行httpd服务，要求名称以自己的名称命名。标签为：tier=backend,env=dev.</strong></h2><pre><code>[root@master ~]# vim daemonset.yaml kind: DaemonSetapiVersion: extensions/v1beta1metadata:  name: xgp-dsspec:  template:    metadata:      labels:        tier: backend        env: dev    spec:      containers:      - name: xgp-ds        image: httpd</code></pre><h3 id="查看一下">查看一下</h3><pre><code>[root@master ~]# kubectl get pod  --show-labels </code></pre><p>![image-20200114100043922](E:\软件\博客\Blog\blog\source_posts\08 ReplicaS儿童、DaemonSet.assets\image-20200114100043922.png)</p><pre><code>[root@master ~]# kubectl get pod -L env,tier</code></pre><p>![image-20200114095943595](E:\软件\博客\Blog\blog\source_posts\08 ReplicaSet、DaemonSet.assets\image-20200114095943595.png)</p><h2 id="3-创建service资源对象与上述资源进行关联，要有验证。"><strong>3) 创建service资源对象与上述资源进行关联，要有验证。</strong></h2><pre><code>[root@master ~]# vim service.yaml kind: ServiceapiVersion: v1metadata:  name: servicespec:   type: NodePort  selector:     env: dev  ports:      - protocol: TCP    port: 90     targetPort: 80    nodePort: 30123 </code></pre><h3 id="执行一下">执行一下</h3><pre><code>[root@master ~]# kubectl apply -f service.yaml </code></pre><h3 id="查看一下-2">查看一下</h3><pre><code>[root@master ~]# kubectl describe svc</code></pre><p>![image-20200114120345596](E:\软件\博客\Blog\blog\source_posts\08 ReplicaSet、DaemonSet.assets\image-20200114120345596.png)</p><h3 id="访问一下">访问一下</h3><pre><code>[root@master ~]# curl 127.0.0.1:30123</code></pre><p>![image-20200114120444524](E:\软件\博客\Blog\blog\source_posts\08 ReplicaSet、DaemonSet.assets\image-20200114120444524.png)</p><h2 id="4）整理关于标签和标签选择器都有什么作用？"><strong>4）整理关于标签和标签选择器都有什么作用？</strong></h2><p><strong>&lt;1&gt;标签：解决同类型的资源对象，为了更好的管理，按照标签分组。</strong></p><p><strong>&lt;2&gt;标签选择器：标签的查询过滤条件。</strong></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>06 pod资源对象</title>
      <link href="/posts/cf38.html"/>
      <url>/posts/cf38.html</url>
      
        <content type="html"><![CDATA[<h1>一，k8s的资源对象</h1><p><em><strong>Deployment、Service、Pod是k8s最核心的3个资源对象</strong></em></p><blockquote><p>**Deployment：**最常见的无状态应用的控制器，支持应用的扩缩容、滚动升级等操作。</p><p>**Service：**为弹性变动且存在生命周期的Pod对象提供了一个固定的访问接口，用于服务发现和服务访问。</p><p>**Pod：**是运行容器以及调度的最小单位。同一个pod可以同时运行多个容器，这些容器共享net、UTS、IPC，除此之外还有USER、PID、MOUNT。</p><p>**ReplicationController：**用于确保每个Pod副本在任意时刻都能满足目标数量，简单来说，它用于每个容器或容器组总是运行并且可以访问的：老一代无状态的Pod应用控制器。</p><p>**RwplicatSet：**新一代的无状态的Pod应用控制器，它与RC的不同之处在于支持的标签选择器不同，RC只支持等值选择器（键值对），RS还额外支持基于集合的选择器。</p><p>**StatefulSet：**用于管理有状态的持久化应用，如database服务程序，它与Deployment不同之处在于，它会为每一个pod创建一个独有的持久性标识符，并确保每个pod之间的顺序性。</p><p>**DaemonSet：**用于确保每一个节点都运行了某个pod的一个副本，新增的节点一样会被添加到此类pod，在节点移除时，此pod会被回收。</p><p>**Job：**用于管理运行完成后即可终止的应用，例如批量处理做作业任务；</p><p>**volume：**pv pvc<br><strong>ConfigMap：</strong><br><strong>Secret：</strong><br><strong>Role：</strong><br><strong>ClusterRole：</strong><br><strong>RoleBinding：</strong><br><strong>cluster RoleBinding：</strong><br><strong>service account：</strong><br><strong>Helm：</strong></p></blockquote><h2 id="Pod的生命周期被定义为以下几个阶段。">Pod的生命周期被定义为以下几个阶段。</h2><blockquote><ul><li><strong>Pending：Pod已经被创建，但是一个或者多个容器还未创建，这包括Pod调度阶段，以及容器镜像的下载过程。</strong></li><li><strong>Running：Pod已经被调度到Node，所有容器已经创建，并且至少一个容器在运行或者正在重启。</strong></li><li><strong>Succeeded：Pod中所有容器正常退出。</strong></li><li><strong>Failed：Pod中所有容器退出，至少有一个容器是一次退出的。</strong></li></ul></blockquote><h1>环境介绍</h1><table><thead><tr><th>主机</th><th>IP地址</th><th>服务</th></tr></thead><tbody><tr><td>master</td><td>192.168.1.21</td><td>k8s</td></tr><tr><td>node01</td><td>192.168.1.22</td><td>k8s</td></tr><tr><td>node02</td><td>192.168.1.23</td><td>k8s</td></tr></tbody></table><h1>二，Namespace：名称空间</h1><p><strong>默认的名称空间：</strong></p><blockquote><p><strong>Namespace（命名空间）是kubernetes系统中的另一个重要的概念，通过将系统内部的对象“分配”到不同的Namespace中，形成逻辑上分组的不同项目、小组或用户组，便于不同的分组在共享使用整个集群的资源的同时还能被分别管理。</strong></p><p><strong>Kubernetes集群在启动后，会创建一个名为“default”的Namespace，如果不特别指明Namespace，则用户创建的Pod、RC、Service都被系统创建到“default”的Namespace中。</strong></p></blockquote><h2 id="1-查看名称空间">1.查看名称空间</h2><pre><code>[root@master ~]# kubectl get namespaces</code></pre><p>![image-20200109094700728](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109094700728.png)</p><h2 id="2-查看名称空间详细信息">2.查看名称空间详细信息</h2><pre><code>[root@master ~]# kubectl describe ns default</code></pre><p>![image-20200109095006067](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109095006067.png)</p><h2 id="3-创建名称空间">3.创建名称空间</h2><pre><code>[root@master ~]# kubectl create ns bdqn</code></pre><h3 id="查看一下">查看一下</h3><pre><code>[root@master ~]# kubectl get namespaces</code></pre><p>![image-20200109095153448](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109095153448.png)</p><h2 id="4-创建namespace的yaml文件">4.创建namespace的yaml文件</h2><h3 id="（1）查看格式">（1）查看格式</h3><pre><code>[root@master ~]# kubectl explain ns//查看nasespace的yaml文件的格式</code></pre><h3 id="（2）创建namespace的yaml文件">（2）创建namespace的yaml文件</h3><pre><code>[root@master ~]# vim test-ns.yamlapiVersion: v1kind: Namespacemetadata:  name: test</code></pre><h3 id="（3）运行namespace的yaml文件">（3）运行namespace的yaml文件</h3><pre><code>[root@master ~]# kubectl apply -f test-ns.yaml </code></pre><h3 id="（4）查看一下">（4）查看一下</h3><pre><code>[root@master ~]# kubectl get ns</code></pre><p>![image-20200109095808777](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109095808777.png)</p><h2 id="4-删除名称空间">4.删除名称空间</h2><pre><code>[root@master ~]# kubectl delete ns test [root@master ~]# kubectl delete -f test-ns.yaml </code></pre><p><strong>注意：namespace资源对象进用于资源对象的隔离，并不能隔绝不同名称空间的Pod之间的通信。那是网络策略资源的功能。</strong></p><h2 id="5-查看指定名称空间">5.查看指定名称空间</h2><p><strong>可使用–namespace或-n选项</strong></p><pre><code>[root@master ~]# kubectl get pod -n kube-system [root@master ~]# kubectl get pod --namespace kube-system </code></pre><h1>三，Pod</h1><h2 id="1-编写一个pod的yaml文件">1.编写一个pod的yaml文件</h2><pre><code>[root@master ~]# vim pod.yamlkind: PodapiVersion: v1metadata:  name: test-podspec:  containers:  - name: test-app    image: 192.168.1.21:5000/web:v1</code></pre><p><em><strong>pod的yaml文件不支持replicas字段</strong></em></p><h3 id="（1）运行一下">（1）运行一下</h3><pre><code>[root@master ~]# kubectl apply -f pod.yaml </code></pre><h3 id="（2）查看一下">（2）查看一下</h3><pre><code>[root@master ~]# kubectl get pod</code></pre><p>![image-20200109100836911](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109100836911.png)</p><p><em><strong>ps：这个pod因为是自己创建的，所以删除之后k8s并不会自动生成，相当于docker中创建</strong></em></p><h2 id="2-指定pod的namespace名称空间">2.指定pod的namespace名称空间</h2><h3 id="（1）修改pod的yaml文件">（1）修改pod的yaml文件</h3><pre><code>[root@master ~]# vim pod.yamlkind: Pod        #资源类型apiVersion: v1   #api版本metadata:  name: test-pod    #指定控制器名称  namespace: bdqn   #指定namespace（名称空间）spec:  containers:      #容器  - name: test-app  #容器名称    image: 192.168.1.21:5000/web:v1  #镜像</code></pre><h5 id="执行一下">执行一下</h5><pre><code>[root@master ~]# kubectl apply -f pod.yaml </code></pre><h3 id="（2）查看一下-2">（2）查看一下</h3><pre><code>[root@master ~]#  kubectl get pod -n bdqn //根据namespace名称查看</code></pre><p>![image-20200109101521992](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109101521992.png)</p><h2 id="3-pod中镜像获取策略">3.pod中镜像获取策略</h2><blockquote><p>**Always：**镜像标签为“laster”或镜像不存在时，总是从指定的仓库中获取镜像。</p><p>**IfNotPresent：**仅当本地镜像不存在时才从目标仓库下载。</p><p>**Never：**禁止从仓库中下载镜像，即只使用本地镜像。</p></blockquote><p><em><strong>注意：对于标签为“laster”或者标签不存在，其默认的镜像下载策略为“Always”，而对于其他的标签镜像，默认策略为“IfNotPresent”。</strong></em></p><h2 id="4-观察pod和service的不同并关联">4.观察pod和service的不同并关联</h2><h3 id="（1）pod的yaml文件（指定端口）">（1）pod的yaml文件（指定端口）</h3><pre><code>[root@master ~]# vim pod.yaml kind: Pod          #资源类型apiVersion: v1      #api版本metadata:  name: test-pod       #指定控制器名称  namespace: bdqn   #指定namespace（名称空间）spec:  containers:                          #容器  - name: test-app                    #容器名称    image: 192.168.1.21:5000/web:v1   #镜像    imagePullPolicy: IfNotPresent   #获取的策略    ports:    - protocol: TCP      containerPort: 80  </code></pre><h4 id="1-删除之前的pod">&lt;1&gt;删除之前的pod</h4><pre><code>[root@master ~]# kubectl delete pod -n bdqn test-pod </code></pre><h4 id="2-执行一下">&lt;2&gt;执行一下</h4><pre><code>[root@master ~]# kubectl apply -f pod.yaml </code></pre><h4 id="3-查看一下">&lt;3&gt;查看一下</h4><pre><code>[root@master ~]# kubectl get pod -n bdqn </code></pre><p>![image-20200109110215669](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109110215669.png)</p><h3 id="（2）pod的yaml文件（修改端口）">（2）pod的yaml文件（修改端口）</h3><pre><code>[root@master ~]# vim pod.yaml kind: PodapiVersion: v1metadata:  name: test-pod  namespace: bdqnspec:  containers:  - name: test-app    image: 192.168.1.21:5000/web:v1    imagePullPolicy: IfNotPresent    ports:    - protocol: TCP      containerPort: 90   #改一下端口</code></pre><h4 id="1-删除之前的pod-2">&lt;1&gt;删除之前的pod</h4><pre><code>[root@master ~]# kubectl delete pod -n bdqn test-pod </code></pre><h4 id="2-执行一下-2">&lt;2&gt;执行一下</h4><pre><code>[root@master ~]# kubectl apply -f pod.yaml </code></pre><h4 id="3-查看一下-2">&lt;3&gt;查看一下</h4><pre><code>[root@master ~]# kubectl get pod -n bdqn -o wide</code></pre><p>![image-20200109110409584](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109110409584.png)</p><h4 id="4-访问一下">&lt;4&gt;访问一下</h4><p>![image-20200109110430334](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109110430334.png)</p><p><strong>会发现修改的90端口并不生效，他只是一个提示字段并不生效。</strong></p><h3 id="（3）pod的yaml文件（添加标签）">（3）pod的yaml文件（添加标签）</h3><pre><code>[root@master ~]# vim pod.yaml kind: PodapiVersion: v1metadata:  name: test-pod  namespace: bdqn  labels:                 #标签    app: test-web          #标签名称spec:  containers:  - name: test-app    image: 192.168.1.21:5000/web:v1    imagePullPolicy: IfNotPresent    ports:    - protocol: TCP      containerPort: 90   #改一下端口</code></pre><h4 id="pod">--------------------------------------pod---------------------------------------------</h4><h2 id="（4）编写一个service的yaml文件">（4）编写一个service的yaml文件</h2><pre><code>[root@master ~]# vim test-svc.yaml apiVersion: v1      #api版本kind: Service          #资源类型metadata:  name: test-svc       #指定控制器名称  namespace: bdqn   #指定namespace（名称空间）spec:  selector:          #标签    app: test-web    #标签名称（须和pod的标签名称一致）  ports:                - port: 80          #宿主机端口    targetPort: 80    #容器端口</code></pre><p><em><strong>会发现添加的80端口生效了，所以不能乱改。</strong></em></p><h4 id="1-执行一下">&lt;1&gt;执行一下</h4><pre><code>[root@master ~]# kubectl apply -f test-svc.yaml</code></pre><h4 id="2-查看一下">&lt;2&gt;查看一下</h4><pre><code>[root@master ~]# kubectl get svc -n bdqn </code></pre><p>![image-20200109121106859](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109121106859.png)</p><pre><code>[root@master ~]# kubectl describe svc -n bdqn test-svc </code></pre><p>![image-20200109121139399](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109121139399.png)</p><h4 id="4-访问一下-2">&lt;4&gt;访问一下</h4><pre><code>[root@master ~]# curl 10.98.57.97 </code></pre><p>![image-20200109121205607](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109121205607.png)</p><h4 id="service">--------------------------------------service---------------------------------------------</h4><h1>四，容器的重启策略</h1><p><strong>Pod的重启策略（RestartPolicy）应用与Pod内所有容器，并且仅在Pod所处的Node上由kubelet进行判断和重启操作。当某个容器异常退出或者健康检查失败时，kubelet将根据RestartPolicy的设置来进行相应的操作。</strong></p><blockquote><p><strong>Always：</strong>（默认情况下使用）但凡Pod对象终止就将其重启；<br>**OnFailure：**仅在Pod对象出现错误时才将其重启；<br>**Never：**从不重启；</p></blockquote><h1>五，pod的默认健康检查</h1><p><strong>每个容器启动时都会执行一个进程，此进程由 Dockerfile 的 CMD 或 ENTRYPOINT 指定。如果进程退出时返回码非零，则认为容器发生故障，Kubernetes 就会根据 <code>restartPolicy</code> 重启容器。</strong></p><h2 id="（1）编写健康检查的yaml文件">（1）编写健康检查的yaml文件</h2><p><strong>下面我们模拟一个容器发生故障的场景，Pod 配置文件如下：</strong></p><pre><code>[root@master ~]# vim healcheck.yaml apiVersion: v1kind: Podmetadata:  labels:    test: healcheck  name:  healcheckspec:  restartPolicy: OnFailure  #指定重启策略  containers:  - name:  healcheck    image: busybox:latest    args:                   #生成pod时运行的命令    - /bin/sh    - -c    - sleep 20; exit 1 </code></pre><h3 id="1-执行一下-2">&lt;1&gt;执行一下</h3><pre><code>[root@master ~]# kubectl apply -f  healcheck.yaml</code></pre><h3 id="2-查看一下-2">&lt;2&gt;查看一下</h3><pre><code>[root@master ~]# kubectl get pod -o wide</code></pre><p>![image-20200109121809350](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109121809350.png)</p><pre><code>[root@master ~]# kubectl get pod -w | grep healcheck</code></pre><p>![image-20200109121817775](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109121817775.png)</p><p><strong>在上面的例子中，容器进程返回值非零，Kubernetes 则认为容器发生故障，需要重启。但有不少情况是发生了故障，但进程并不会退出。</strong></p><h1>六，小实验</h1><h2 id="1）以自己的名称创建一个k8s名称空间，以下所有操作都在此名称空间中。">1）以自己的名称创建一个k8s名称空间，以下所有操作都在此名称空间中。</h2><h3 id="（1）创建名称空间">（1）创建名称空间</h3><pre><code>[root@master ~]# kubectl create ns xgp</code></pre><h3 id="（2）查看一下-3">（2）查看一下</h3><pre><code>[root@master ~]# kubectl get ns xgp </code></pre><p>![image-20200109133106300](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109133106300.png)</p><h2 id="2）创建一个Pod资源对象，使用的是私有仓库中私有镜像，其镜像的下载策略为：NEVER。-Pod的重启策略为：-Never">2）创建一个Pod资源对象，使用的是私有仓库中私有镜像，其镜像的下载策略为：NEVER。 Pod的重启策略为： Never.</h2><pre><code>[root@master ~]# vim pod.yamlkind: PodapiVersion: v1metadata:  name: test-pod  namespace: xgp  labels:    app: test-webspec:  restartPolicy: Never  containers:  - name: www    image: 192.168.1.21:5000/web:v1    imagePullPolicy: Never    args:                       - /bin/sh    - -c    - sleep 90; exit 1    ports:    - protocol: TCP      containerPort: 80</code></pre><h2 id="3）创建出容器之后，执行非正常退出，查看Pod的最终状态。">3）创建出容器之后，执行非正常退出，查看Pod的最终状态。</h2><h3 id="（1）执行一下上面pod的yaml文件">（1）执行一下上面pod的yaml文件</h3><pre><code>[root@master ~]# kubectl apply -f pod.yaml </code></pre><h3 id="（2）动态查看ns中test-pod的信息">（2）动态查看ns中test-pod的信息</h3><pre><code>[root@master ~]# kubectl get pod -n xgp  -w | grep test-pod</code></pre><p>![image-20200109135543482](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109135543482.png)</p><blockquote><p><strong>删除test-pod</strong></p><pre><code>[root@master ~]# kubectl delete pod -n xgp test-pod </code></pre></blockquote><h2 id="4-创建一个Service资源对象，与上述Pod对象关联，验证他们的关联性。">4) 创建一个Service资源对象，与上述Pod对象关联，验证他们的关联性。</h2><h3 id="（1）修改pod的yaml文件-2">（1）修改pod的yaml文件</h3><pre><code>[root@master ~]# vim pod.yamlkind: PodapiVersion: v1metadata:  name: test-pod  namespace: xgp  labels:    app: test-webspec:  restartPolicy: Never  containers:  - name: www    image: 192.168.1.21:5000/web:v1    imagePullPolicy: Never    ports:    - protocol: TCP      containerPort: 80</code></pre><h3 id="（1）编写service的yaml文件">（1）编写service的yaml文件</h3><pre><code>[root@master ~]# vim svc.yaml apiVersion: v1kind: Servicemetadata:  name: test-svc  namespace: xgpspec:  selector:    app: test-web  ports:  - port: 80    targetPort: 80</code></pre><h3 id="（2）执行一下">（2）执行一下</h3><pre><code>[root@master ~]# kubectl apply -f svc.yaml </code></pre><h3 id="（3）查看一下">（3）查看一下</h3><pre><code>[root@master ~]# kubectl get  pod -o wide -n xgp </code></pre><p>![image-20200109141712910](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109141712910.png)</p><h3 id="（4）访问一下">（4）访问一下</h3><pre><code>[root@master ~]# curl 10.244.1.21</code></pre><p>![image-20200109141749352](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109141749352.png)</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>05 Delpoyment、service</title>
      <link href="/posts/936d.html"/>
      <url>/posts/936d.html</url>
      
        <content type="html"><![CDATA[<h1>Deployment介绍</h1><p><strong>Deployment是kubernetes 1.2引入的概念，用来解决Pod的编排问题。Deployment可以理解为RC的升级版（RC+Reolicat Set）。特点在于可以随时知道Pod的部署进度，即对Pod的创建、调度、绑定节点、启动容器完整过程的进度展示。</strong></p><h2 id="使用场景">使用场景</h2><blockquote><p>创建一个Deployment对象来生成对应的Replica Set并完成Pod副本的创建过程。<br>检查Deployment的状态来确认部署动作是否完成（Pod副本的数量是否达到预期值）。<br>更新Deployment以创建新的Pod(例如镜像升级的场景)。<br>如果当前Deployment不稳定，回退到上一个Deployment版本。<br>挂起或恢复一个Deployment。</p></blockquote><h1>Service介绍</h1><p><img src="/" class="lazyload" data-src="https://img-blog.csdn.net/20170809212910268?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaHV3aF8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center"  alt="img"></p><p><strong>Service定义了一个服务的访问入口地址，前端应用通过这个入口地址访问其背后的一组由Pod副本组成的集群实例，Service与其后端的Pod副本集群之间是通过Label Selector来实现“无缝对接”。RC保证Service的Pod副本实例数目保持预期水平。</strong></p><h2 id="外部系统访问Service的问题">外部系统访问Service的问题</h2><table><thead><tr><th style="text-align:left">IP类型</th><th style="text-align:left">说明</th></tr></thead><tbody><tr><td style="text-align:left">Node IP</td><td style="text-align:left">Node节点的IP地址</td></tr><tr><td style="text-align:left">Pod IP</td><td style="text-align:left">Pod的IP地址</td></tr><tr><td style="text-align:left">Cluster IP</td><td style="text-align:left">Service的IP地址</td></tr></tbody></table><h1>环境介绍</h1><table><thead><tr><th>主机</th><th>IP地址</th><th>服务</th></tr></thead><tbody><tr><td><strong>master</strong></td><td><strong>192.168.1.21</strong></td><td><strong>k8s</strong></td></tr><tr><td><strong>node01</strong></td><td><strong>192.168.1.22</strong></td><td><strong>k8s</strong></td></tr><tr><td><strong>node02</strong></td><td><strong>192.168.1.23</strong></td><td><strong>k8s</strong></td></tr></tbody></table><h1>一，Delpoyment和service的简单使用</h1><h2 id="1-练习写一个yaml文件，要求使用自己的私有镜像，要求副本数量为三个。">1.练习写一个yaml文件，要求使用自己的私有镜像，要求副本数量为三个。</h2><pre><code>[root@master ~]# vim xgp.yamlkind: DeploymentapiVersion: extensions/v1beta1metadata:  name: xgp-webspec:  replicas: 3  template:    metadata:      labels:        app: xgp-server    spec:      containers:      - name: web        image: 192.168.1.21:5000/web:v1</code></pre><h3 id="（1）执行一下">（1）执行一下</h3><pre><code>[root@master ~]# kubectl apply -f xgp.yaml  --recore</code></pre><h3 id="（2）查看一下">（2）查看一下</h3><pre><code>[root@master ~]# kubectl get pod</code></pre><p>![image-20200108090638488](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108090638488.png)</p><h3 id="（3）访问一下">（3）访问一下</h3><pre><code>[root@master ~]# curl 10.244.2.16</code></pre><p>![image-20200108090817058](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108090817058.png)</p><h3 id="（4）更新一下yaml文件，副本加一">（4）更新一下yaml文件，副本加一</h3><pre><code>[root@master ~]# vim xgp.yamlkind: DeploymentapiVersion: extensions/v1beta1metadata:  name: xgp-webspec:  replicas: 4  template:    metadata:      labels:        app: xgp-server    spec:      containers:      - name: web        image: 192.168.1.21:5000/web:v1</code></pre><h4 id="1-执行一下">&lt;1&gt;执行一下</h4><pre><code>[root@master ~]# kubectl apply -f xgp.yaml --recore</code></pre><h4 id="2-查看一下">&lt;2&gt;查看一下</h4><pre><code>[root@master ~]# kubectl get pod</code></pre><p>![image-20200108091104534](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108091104534.png)</p><p><em><strong>副本数量加一，如果yaml文件的副本为0，则副本数量还是之前的状态，并不会更新。</strong></em></p><h2 id="2-练习写一个service文件">2.练习写一个service文件</h2><pre><code>[root@master ~]# vim xgp-svc.yamlkind: ServiceapiVersion: v1metadata:  name: xgp-svcspec:  selector:    app: xgp-server  ports:    - protocol: TCP      port: 80      targetPort: 80</code></pre><h3 id="（1）执行一下-2">（1）执行一下</h3><pre><code>[root@master ~]# kubectl apply -f xgp-svc.yaml </code></pre><h3 id="（2）查看一下-2">（2）查看一下</h3><pre><code>[root@master ~]# kubectl get svc</code></pre><p>![image-20200108091909396](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108091909396.png)</p><h3 id="（3）访问一下-2">（3）访问一下</h3><pre><code>[root@master ~]# curl 10.107.119.49</code></pre><p>![image-20200108092011164](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108092011164.png)</p><h2 id="3-修改yaml文件">3.修改yaml文件</h2><pre><code>[root@master ~]# vim xgp.yaml kind: DeploymentapiVersion: extensions/v1beta1metadata:  name: xgp-webspec:  replicas: 3  template:    metadata:      labels:        app: xgp-server    spec:      containers:      - name: web        image: 192.168.1.21:5000/web:v1        ports:          - containerPort: 80  #提示端口</code></pre><p><em><strong>注意：在Delpoyment资源对象中，可以添加Port字段，但此字段仅供用户查看，并不实际生效</strong></em></p><h3 id="执行一下">执行一下</h3><pre><code>[root@master ~]# kubectl apply -f xgp.yaml --recore</code></pre><h2 id="4-service文件映射端口">4.service文件映射端口</h2><pre><code>[root@master ~]# vim xgp-svc.yaml kind: ServiceapiVersion: v1metadata:  name: xgp-svcspec:  type: NodePort  selector:    app: xgp-server  ports:    - protocol: TCP      port: 80      targetPort: 80      nodePort: 30123</code></pre><h3 id="执行一下-2">执行一下</h3><pre><code>[root@master ~]# kubectl apply -f xgp-svc.yaml </code></pre><h3 id="查看一下">查看一下</h3><pre><code>[root@master ~]# kubectl get svc</code></pre><p>![image-20200108094404773](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108094404773.png)</p><h3 id="访问一下">访问一下</h3><pre><code>[root@master ~]# curl 127.0.0.1:30123</code></pre><p>![image-20200108094439682](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108094439682.png)</p><p>![image-20200108094501253](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108094501253.png)</p><h2 id="5-修改三个pod页面内容">5.修改三个pod页面内容</h2><h3 id="（1）查看一下pod信息">（1）查看一下pod信息</h3><pre><code>[root@master ~]# kubectl get pod -o wide</code></pre><p>![image-20200108094953119](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108094953119.png)</p><h3 id="（2）修改POD页面内容（三台不一样）">（2）修改POD页面内容（三台不一样）</h3><pre><code>[root@master ~]# kubectl exec -it xgp-web-8d5f9656f-8z7d9 /bin/bash//根据pod名称进入pod之中</code></pre><h3 id="进入容器后修改页面内容">进入容器后修改页面内容</h3><pre><code>root@xgp-web-8d5f9656f-8z7d9:/usr/local/apache2# echo xgp-v1 &gt; htdocs/index.html root@xgp-web-8d5f9656f-8z7d9:/usr/local/apache2# exit</code></pre><h3 id="访问一下-2">访问一下</h3><pre><code>[root@master ~]# curl 127.0.0.1:30123</code></pre><p>![image-20200108095626532](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108095626532.png)</p><h1>二.分析一下k8s负载均衡原理</h1><h3 id="（1）查看service的暴露IP">（1）查看service的暴露IP</h3><pre><code>[root@master ~]# kubectl get svc</code></pre><p>![image-20200108101539835](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108101539835.png)</p><h3 id="（2）查看一下iptabes规则">（2）查看一下iptabes规则</h3><pre><code>[root@master ~]# iptables-save //查看已配置的规则</code></pre><blockquote><p>SNAT：Source NAT（源地址转换）</p><p>DNAT：Destination NAT（目标地址转换）</p><p>MASQ：动态的源地址转换</p></blockquote><h3 id="（3）根据service的暴露IP，查看对应的iptabes规则">（3）根据service的暴露IP，查看对应的iptabes规则</h3><pre><code>[root@master ~]# iptables-save | grep 10.107.119.49</code></pre><p>![image-20200108101726315](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108101726315.png)</p><pre><code>[root@master ~]# iptables-save | grep KUBE-SVC-ESI7C72YHAUGMG5S</code></pre><p>![image-20200108102003596](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108102003596.png)</p><h3 id="（4）对应一下IP是否一致">（4）对应一下IP是否一致</h3><pre><code>[root@master ~]# iptables-save | grep KUBE-SEP-ZHDQ73ZKUBMELLJB</code></pre><p>![image-20200108102137062](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108102137062.png)</p><pre><code>[root@master ~]# kubectl get pod -o wide</code></pre><p>![image-20200108102203144](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108102203144.png)</p><p><strong>Service实现的负载均衡：默认使用的是iptables规则。IPVS</strong></p><h1>三.回滚到指定版本</h1><h3 id="（1）删除之前创建的delpoy和service">（1）删除之前创建的delpoy和service</h3><pre><code>[root@master ~]# kubectl  delete -f xgp.yaml [root@master ~]# kubectl  delete -f xgp-svc.yaml </code></pre><h3 id="（2）准备三个版本所使用的私有镜像，来模拟每次升级不同的镜像">（2）准备三个版本所使用的私有镜像，来模拟每次升级不同的镜像</h3><pre><code>[root@master ~]# vim xgp1.yaml  （三个文件名不相同）kind: DeploymentapiVersion: extensions/v1beta1metadata:  name: xgp-webspec:  revisionHistoryLimit: 10  replicas: 3  template:    metadata:      labels:        app: xgp-server    spec:      containers:      - name: web        image: 192.168.1.21:5000/web:v1  （三台版本不同）        ports:          - containerPort: 80</code></pre><p>此处3个yaml文件 指定不同版本的镜像</p><h3 id="（3）运行三个服务，并记录三个版本信息">（3）运行三个服务，并记录三个版本信息</h3><pre><code>[root@master ~]# kubectl apply -f xgp-1.yaml --record [root@master ~]# kubectl apply -f xgp-2.yaml --record [root@master ~]# kubectl apply -f xgp-3.yaml --record </code></pre><h3 id="（4）查看有哪些版本信息">（4）查看有哪些版本信息</h3><pre><code>[root@master ~]# kubectl rollout history deployment xgp-web </code></pre><p>![image-20200108105842447](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108105842447.png)</p><h3 id="（5）运行之前的service文件">（5）运行之前的service文件</h3><pre><code>[root@master ~]# kubectl apply -f xgp-svc.yaml</code></pre><h3 id="（6）查看service暴露端口">（6）查看service暴露端口</h3><pre><code>[root@master ~]# kubectl get svc</code></pre><p>![image-20200108110014614](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108110014614.png)</p><h3 id="（7）测试访问">（7）测试访问</h3><pre><code>[root@master ~]# curl 127.0.0.1:30123</code></pre><p>![image-20200108110049396](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108110049396.png)</p><h3 id="（8）回滚到指定版本">（8）回滚到指定版本</h3><pre><code>[root@master ~]# kubectl rollout undo deployment xgp-web --to-revision=1//这里指定的是版本信息的编号</code></pre><h4 id="1-访问一下">&lt;1&gt;访问一下</h4><pre><code>[root@master ~]# curl 127.0.0.1:30123</code></pre><p>![image-20200108110337266](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108110337266.png)</p><h4 id="2-查看有哪些版本信息">&lt;2&gt;查看有哪些版本信息</h4><pre><code>[root@master ~]# kubectl rollout history deployment xgp-web </code></pre><p>![image-20200108110443558](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108110443558.png)</p><p><em><strong>编号1已经被编号2替代，从而生的是一个新的编号4</strong></em></p><h1>四.用label控制pod的位置</h1><blockquote><p>默认情况下，scheduler会将pod调度到所有可用的Node，不过有些情况我们希望将 Pod 部署到指定的 Node，比如将有大量磁盘 I/O 的 Pod 部署到配置了 SSD 的 Node；或者 Pod 需要 GPU，需要运行在配置了 GPU 的节点上。</p><p>kubernetes通过label来实现这个功能</p><p>label 是 key-value 对，各种资源都可以设置 label，灵活添加各种<strong>自定义属性</strong>。比如执行如下命令标注 k8s-node1 是配置了 SSD 的节点</p></blockquote><h4 id="首先我们给node1节点打上一个ssd的标签">首先我们给node1节点打上一个ssd的标签</h4><pre><code>[root@master ~]# kubectl label nodes node02 disk=ssd</code></pre><h3 id="（1）查看标签">（1）查看标签</h3><pre><code>[root@master ~]# kubectl get nodes --show-labels | grep node02</code></pre><p>![image-20200108111354832](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108111354832.png)</p><h3 id="（2）删除副本一">（2）删除副本一</h3><pre><code>[root@master ~]# kubectl delete -f xgp-1.yaml deployment.extensions "xgp-web" deleted[root@master ~]# kubectl delete svc xgp-svc </code></pre><h3 id="（3）修改副本一的yaml文件">（3）修改副本一的yaml文件</h3><pre><code>[root@master ~]# vim xgp-1.yaml kind: DeploymentapiVersion: extensions/v1beta1metadata:  name: xgp-webspec:  revisionHistoryLimit: 10  replicas: 3  template:    metadata:      labels:        app: xgp-server    spec:      containers:      - name: web        image: 192.168.1.21:5000/web:v1        ports:          - containerPort: 80      nodeSelector:    #添加节点选择器        disk: ssd      #和标签内容一致</code></pre><h3 id="（4）执行一下">（4）执行一下</h3><pre><code>[root@master ~]# kubectl apply -f xgp-1.yaml </code></pre><h4 id="查看一下-2">查看一下</h4><pre><code>[root@master ~]# kubectl get pod -o wide</code></pre><p>![image-20200108112059395](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108112059395.png)</p><p><em><strong>现在pod都在node02上运行</strong></em></p><h3 id="（5）删除标签">（5）删除标签</h3><pre><code>[root@master ~]# kubectl  label nodes node02 disk-</code></pre><h4 id="查看一下-3">查看一下</h4><pre><code>[root@master ~]# kubectl get nodes --show-labels | grep node02</code></pre><p>![image-20200108112245347](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108112245347.png)</p><p><em><strong>没有disk标签了</strong></em></p><h1>五，小实验</h1><h3 id="1）使用私有镜像v1版本部署一个Deployment资源对象，要求副本Pod数量为3个，并创建一个Service资源对象相互关联，指定要求3个副本Pod全部运行在node01节点上，记录一个版本。"><strong>1）使用私有镜像v1版本部署一个Deployment资源对象，要求副本Pod数量为3个，并创建一个Service资源对象相互关联，指定要求3个副本Pod全部运行在node01节点上，记录一个版本。</strong></h3><h4 id="（1）用label控制pod的位置">（1）用label控制pod的位置</h4><pre><code>[root@master ~]# kubectl label nodes node01 disk=ssd</code></pre><h4 id="（2）编写源yaml文件">（2）编写源yaml文件</h4><pre><code>[root@master ~]# vim xgp.yamlkind: DeploymentapiVersion: extensions/v1beta1metadata:  name: xgp-webspec:  replicas: 3  template:    metadata:      labels:        app: xgp-server    spec:      containers:      - name: web        image: 192.168.1.21:5000/web:v1        ports:          - containerPort: 80      nodeSelector:            disk: ssd  </code></pre><h4 id="（3）编写源service文件">（3）编写源service文件</h4><pre><code>[root@master ~]# vim xgp-svc.yamlkind: ServiceapiVersion: v1metadata:  name: xgp-svcspec:  type: NodePort  selector:    app: xgp-server  ports:    - protocol: TCP      port: 80      targetPort: 80      nodePort: 30123</code></pre><h4 id="（4）执行yaml文件，创建控制器。执行service文件创建映射端口">（4）执行yaml文件，创建控制器。执行service文件创建映射端口</h4><pre><code>[root@master ~]# kubectl apply -f  xgp.yaml [root@master ~]# kubectl apply -f xgp-svc.yaml </code></pre><h4 id="（5）查看一下pod节点">（5）查看一下pod节点</h4><pre><code>[root@master ~]# kubectl get pod -o wide</code></pre><p>![image-20200108122424654](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108122424654.png)</p><h4 id="（6）记录一个版本">（6）记录一个版本</h4><pre><code>[root@master ~]# kubectl rollout history deployment xgp-web &gt; pod.txt</code></pre><p>![image-20200108142016701](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108142016701.png)</p><h4 id="（7）访问一下">（7）访问一下</h4><p>![image-20200108122518278](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108122518278.png)</p><p>![image-20200108122534683](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108122534683.png)</p><h3 id="2）根据上述Deployment，升级为v2版本，记录一个版本。"><strong>2）根据上述Deployment，升级为v2版本，记录一个版本。</strong></h3><h4 id="（1）修改yaml文件镜像版本">（1）修改yaml文件镜像版本</h4><pre><code>[root@master ~]# vim xgp.yaml kind: DeploymentapiVersion: extensions/v1beta1metadata:  name: xgp-webspec:  replicas: 3  template:    metadata:      labels:        app: xgp-server    spec:      containers:      - name: web        image: 192.168.1.21:5000/web:v2    #修改版本为二        ports:          - containerPort: 80      nodeSelector:        disk: ssd</code></pre><h4 id="（2）刷新一下yaml文件">（2）刷新一下yaml文件</h4><pre><code>[root@master ~]# kubectl apply -f xgp.yaml --recore</code></pre><h4 id="（3）访问一下-3">（3）访问一下</h4><p>![image-20200108141825924](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108141825924.png)</p><h4 id="（4）记录一个版本">（4）记录一个版本</h4><pre><code>[root@master ~]# kubectl rollout history deployment xgp-web &gt; pod.txt</code></pre><p>![image-20200108142030157](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108142030157.png)</p><h3 id="3）最后升级到v3版本，这时，查看Service关联，并且分析访问流量的负载均衡详细情况。"><strong>3）最后升级到v3版本，这时，查看Service关联，并且分析访问流量的负载均衡详细情况。</strong></h3><h4 id="1）修改yaml文件镜像版本">1）修改yaml文件镜像版本</h4><pre><code>[root@master ~]# vim xgp.yaml kind: DeploymentapiVersion: extensions/v1beta1metadata:  name: xgp-webspec:  replicas: 3  template:    metadata:      labels:        app: xgp-server    spec:      containers:      - name: web        image: 192.168.1.21:5000/web:v3   #修改版本为二        ports:          - containerPort: 80      nodeSelector:        disk: ssd</code></pre><h4 id="（2）刷新一下yaml文件-2">（2）刷新一下yaml文件</h4><pre><code>[root@master ~]# kubectl apply -f xgp.yaml --recore</code></pre><h4 id="（3）访问一下-4">（3）访问一下</h4><p>![image-20200108142329749](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108142329749.png)</p><h4 id="（5）分析访问流量的负载均衡详细情况">（5）分析访问流量的负载均衡详细情况</h4><h5 id="1-查看一下service映射端口">&lt;1&gt;查看一下service映射端口</h5><p>![image-20200108142504637](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108142504637.png)</p><h5 id="2-以ip为起点，分析访问流量的负载均衡详细情况">&lt;2&gt;以ip为起点，分析访问流量的负载均衡详细情况</h5><p><strong>Service实现的负载均衡：默认使用的是iptables规则。IPVS</strong></p><pre><code>[root@master ~]# iptables-save | grep 10.107.27.229//根据service的暴露IP，查看对应的iptabes规则</code></pre><p>![image-20200108143052433](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108143052433.png)</p><pre><code>[root@master ~]# iptables-save | grep KUBE-SVC-ESI7C72YHAUGMG5S</code></pre><p>![image-20200108143359463](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108143359463.png)</p><p><em><strong>这里显示了各节点的负载比例</strong></em></p><h5 id="3-对应一下IP是否一致">&lt;3&gt;对应一下IP是否一致</h5><pre><code>[root@master ~]# iptables-save | grep KUBE-SEP-VDKW5WQIWOLZMJ6G</code></pre><p>![image-20200108143547946](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108143547946.png)</p><pre><code>[root@master ~]# kubectl get pod -o wide</code></pre><p>![image-20200108143608942](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108143608942.png)</p><h3 id="4）回滚到指定版本v1，并作验证。"><strong>4）回滚到指定版本v1，并作验证。</strong></h3><h4 id="1-回滚到指定版本">&lt;1&gt;回滚到指定版本</h4><pre><code>[root@master ~]# kubectl rollout undo deployment xgp-web --to-revision=1//这里指定的是版本信息的编号</code></pre><h4 id="2-访问一下">&lt;2&gt;访问一下</h4><pre><code>[root@master ~]# curl 127.0.0.1:30123</code></pre><p>![image-20200108110337266](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108110337266.png)</p><blockquote><p><strong>排错思路</strong></p><pre><code>[root@master ~]# less /var/log/messages  | grep kubelet[root@master ~]# kubectl  logs -n  kube-system kube-scheduler-master [root@master ~]# kubectl describe pod xgp-web-7d478f5bb7-bd4bj </code></pre></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>04 配置清单</title>
      <link href="/posts/748b.html"/>
      <url>/posts/748b.html</url>
      
        <content type="html"><![CDATA[<h1>一，两种创建资源的方法</h1><h2 id="1-基于命令的方式：">1. 基于命令的方式：</h2><ol><li><strong>简单直观快捷，上手快。</strong></li><li><strong>适合临时测试或实验。</strong></li></ol><h2 id="2-基于配置清单的方式：">2. 基于配置清单的方式：</h2><ol><li><strong>配置文件描述了 <code>What</code>，即应用最终要达到的状态。</strong></li><li><strong>配置文件提供了创建资源的模板，能够重复部署。</strong></li><li><strong>可以像管理代码一样管理部署。</strong></li><li><strong>适合正式的、跨环境的、规模化部署。</strong></li><li><strong>这种方式要求熟悉配置文件的语法，有一定难度。</strong></li></ol><h2 id="环境介绍">环境介绍</h2><table><thead><tr><th>主机</th><th>IP地址</th><th>服务</th></tr></thead><tbody><tr><td>master</td><td>192.168.1.21</td><td>k8s</td></tr><tr><td>node01</td><td>192.168.1.22</td><td>k8s</td></tr><tr><td>node02</td><td>192.168.1.23</td><td>k8s</td></tr></tbody></table><h1>二. 配置清单（yam，yaml）</h1><p><strong>在k8s中，一般使用yaml格式的文件来创建符合我们预期期望的pod，这样的yaml文件我们一般称为资源清单</strong></p><blockquote><p><strong>/etc/kubernetes/manifests/</strong>    k8s存放（yam、yaml）文件的地方</p><p>**kubectl explain deployment（通过explain参数加上资源类别就能看到该资源应该怎么定义）</p><p><strong>kubectl explain deployment.metadata</strong> 通过资源类别加上带有Object标记的字段，我们就可以看到一级字段下二级字段的内容有那些怎么去定义等</p><p><strong>kubectl explain deployment.metadata.ownerReferences</strong> 通过加上不同级别的字段名称来看下字段下的内容，而且前面的[]号代表对象列表</p></blockquote><h2 id="1-常见yaml文件写法，以及字段的作用">1.常见yaml文件写法，以及字段的作用</h2><p><strong>(1) apiVersion：api版本信息</strong></p><p><em><strong>（用来定义当前属于哪个组和那个版本，这个直接关系到最终提供使用的是那个版本）</strong></em></p><pre><code>[root@master manifests]# kubectl api-versions//查看到当前所有api的版本</code></pre><p><strong>(2) kind: 资源对象的类别</strong></p><p><em><strong>(用来定义创建的对象是属于什么类别，是pod，service，还是deployment等对象，可以按照其固定的语法格式来自定义。)</strong></em><br><strong>(3) metadata: 元数据 名称字段（必写）</strong></p><blockquote><p><strong>提供以下几个字段：</strong><br>　　<strong>creationTimestamp: "2019-06-24T12:18:48Z"</strong><br>　　<strong>generateName: myweb-5b59c8b9d-</strong><br>　　<strong>labels: （对象标签）</strong><br>　　　　<strong>pod-template-hash: 5b59c8b9d</strong><br>　　　　<strong>run: myweb</strong><br>　　<strong>name: myweb-5b59c8b9d-gwzz5 （pods对象的名称，同一个类别当中的pod对象名称是唯一的，不能重复）</strong><br>　　<strong>namespace: default （对象所属的名称空间，同一名称空间内可以重复，这个名称空间也是k8s级别的名称空间，不和容器的名称空间混淆）</strong><br>　　<strong>ownerReferences:</strong></p><p>- <strong>apiVersion: apps/v1</strong><br>　　　　<strong>blockOwnerDeletion: true</strong><br>　　　　<strong>controller: true</strong><br>　　　　<strong>kind: ReplicaSet</strong><br>　　　　<strong>name: myweb-5b59c8b9d</strong><br>　　　　<strong>uid: 37f38f64-967a-11e9-8b4b-000c291028e5</strong><br>　　<strong>resourceVersion: "943"</strong><br>　　<strong>selfLink: /api/v1/namespaces/default/pods/myweb-5b59c8b9d-gwzz5</strong><br>　　<strong>uid: 37f653a6-967a-11e9-8b4b-000c291028e5</strong><br>　　<strong>annotations（资源注解，这个需要提前定义，默认是没有的）</strong><br><strong>通过这些标识定义了每个资源引用的path：即/api/group/version/namespaces/名称空间/资源类别/对象名称</strong></p></blockquote><p><strong>(4) spec： 用户期望的状态</strong></p><p><em><strong>（这个字段最重要，因为spec是用来定义目标状态的‘disired state’，而且资源不通导致spec所嵌套的字段也各不相同，也就因为spec重要且字段不相同，k8s在内部自建了一个spec的说明用于查询）</strong></em></p><p><strong>(5) status：资源现在处于什么样的状态</strong></p><p><em><strong>（当前状态，’current state‘，这个字段有k8s集群来生成和维护，不能自定义，属于一个只读字段）</strong></em></p><h2 id="2-编写一个yaml文件">2.编写一个yaml文件</h2><pre><code>[root@master ~]# vim web.yamlkind: Deployment  #资源对象是控制器apiVersion: extensions/v1beta1   #api的版本metadata:      #描述kind（资源类型）  name: web   #定义控制器名称spec:  replicas: 2   #副本数量  template:     #模板    metadata:          labels:   #标签        app: web_server    spec:      containers:   #指定容器      - name: nginx  #容器名称        image: nginx   #使用的镜像</code></pre><h3 id="执行一下">执行一下</h3><pre><code>[root@master ~]# kubectl apply -f web.yaml </code></pre><h3 id="查看一下">查看一下</h3><pre><code>[root@master ~]# kubectl get deployments.  -o wide//查看控制器信息</code></pre><p><img src="/" class="lazyload" data-src="/posts/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C04%E9%85%8D%E7%BD%AE%E6%B8%85%E5%8D%95.assets%5Cimage-20200107100450262.png"  alt="image-20200107100450262"></p><pre><code>[root@master ~]# kubectl get pod -o wide//查看pod节点信息</code></pre><p><img src="/" class="lazyload" data-src="/posts/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C04%E9%85%8D%E7%BD%AE%E6%B8%85%E5%8D%95.assets%5Cimage-20200107101803209.png"  alt="image-20200107101803209"></p><h2 id="3-编写一个service-yaml文件">3.编写一个service.yaml文件</h2><pre><code>[root@master ~]# vim web-svc.yamlkind: Service  #资源对象是副本apiVersion: v1   #api的版本metadata:  name: web-svcspec:  selector:     #标签选择器    app: web-server  #须和web.yaml的标签一致  ports:              #端口  - protocol: TCP    port: 80            #宿主机的端口    targetPort: 80      #容器的端口</code></pre><blockquote><p><strong>使用相同标签和标签选择器内容，使两个资源对象相互关联。</strong></p><p><strong>创建的service资源对象，默认的type为ClusterIP，意味着集群内任意节点都可访问。它的作用是为后端真正服务的pod提供一个统一的接口。如果想要外网能够访问服务，应该把type改为NodePort</strong></p></blockquote><h3 id="（1）执行一下">（1）执行一下</h3><pre><code>[root@master ~]# kubectl apply -f web-svc.yaml </code></pre><h3 id="（2）查看一下">（2）查看一下</h3><pre><code>[root@master ~]# kubectl get svc//查看控制器信息</code></pre><p><img src="/" class="lazyload" data-src="/posts/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C04%E9%85%8D%E7%BD%AE%E6%B8%85%E5%8D%95.assets%5Cimage-20200107110717972.png"  alt="image-20200107110717972"></p><h3 id="（3）访问一下">（3）访问一下</h3><pre><code>[root@master ~]# curl 10.111.193.168</code></pre><p><img src="/" class="lazyload" data-src="/posts/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C04%E9%85%8D%E7%BD%AE%E6%B8%85%E5%8D%95.assets%5Cimage-20200107110837353.png"  alt="image-20200107110837353"></p><h2 id="4-外网能够访问服务">4.外网能够访问服务</h2><h3 id="（1）修改web-svc-yaml文件">（1）修改web-svc.yaml文件</h3><pre><code>kind: Service  #资源对象是副本apiVersion: v1   #api的版本metadata:  name: web-svcspec:  type: NodePort    #添加 更改网络类型  selector:     #标签选择器    app: web_server  #须和web.yaml的标签一致  ports:              #端口  - protocol: TCP    port: 80            #宿主机的端口    targetPort: 80      #容器的端口    nodePort: 30086     #指定群集映射端口，范围是30000-32767</code></pre><h3 id="（2）刷新一下">（2）刷新一下</h3><pre><code>[root@master ~]#  kubectl apply -f web-svc.yaml </code></pre><h3 id="（3）查看一下">（3）查看一下</h3><pre><code>[root@master ~]# kubectl get svc</code></pre><p><img src="/" class="lazyload" data-src="/posts/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C04%E9%85%8D%E7%BD%AE%E6%B8%85%E5%8D%95.assets%5Cimage-20200107111338940.png"  alt="image-20200107111338940"></p><h3 id="（4）浏览器测试">（4）浏览器测试</h3><p><img src="/" class="lazyload" data-src="/posts/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C04%E9%85%8D%E7%BD%AE%E6%B8%85%E5%8D%95.assets%5Cimage-20200107111451952.png"  alt="image-20200107111451952"></p><h1>三、小实验</h1><blockquote><p><strong>基于上一篇博客实验继续进行</strong></p></blockquote><h3 id="1-使用yaml文件的方式创建一个Deployment资源对象，要求镜像使用个人私有镜像v1版本。replicas为3个。">1.使用yaml文件的方式创建一个Deployment资源对象，要求镜像使用个人私有镜像v1版本。replicas为3个。</h3><h3 id="编写yaml文件">编写yaml文件</h3><pre><code>[root@master ~]# vim www.yamlkind: DeploymentapiVersion: extensions/v1beta1metadata:  name: xgpspec:  replicas: 3  template:    metadata:      labels:        app: www_server    spec:      containers:      - name: web        image: 192.168.1.21:5000/web:v1   </code></pre><h4 id="（1）执行一下-2">（1）执行一下</h4><pre><code>[root@master ~]# kubectl apply -f web-svc.yaml </code></pre><h4 id="（2）查看一下-2">（2）查看一下</h4><pre><code>[root@master ~]# kubectl get deployments. -o wide//查看控制器信息</code></pre><p><img src="/" class="lazyload" data-src="/posts/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C04%E9%85%8D%E7%BD%AE%E6%B8%85%E5%8D%95.assets%5Cimage-20200107120901208.png"  alt="image-20200107120901208"></p><pre><code>[root@master ~]# kubectl get pod -o wide//查看pod节点信息</code></pre><p><img src="/" class="lazyload" data-src="/posts/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C04%E9%85%8D%E7%BD%AE%E6%B8%85%E5%8D%95.assets%5Cimage-20200107121002152.png"  alt="image-20200107121002152"></p><h4 id="（3）访问一下-2">（3）访问一下</h4><p><img src="/" class="lazyload" data-src="/posts/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C04%E9%85%8D%E7%BD%AE%E6%B8%85%E5%8D%95.assets%5Cimage-20200107121147669.png"  alt="image-20200107121147669"></p><h3 id="2-使用yaml文件的方式创建一个Service资源对象，要与上述Deployment资源对象关联，type类型为：-NodePort，端口为-30123"><strong>2.</strong>  使用yaml文件的方式创建一个Service资源对象，要与上述Deployment资源对象关联，type类型为： NodePort，端口为:30123.</h3><h4 id="编写service文件">编写service文件</h4><pre><code>[root@master ~]# vim www-svc.yamlkind: ServiceapiVersion: v1metadata:  name: www-svcspec:  type: NodePort  selector:    app: www_server  ports:  - protocol: TCP    port: 80    targetPort: 80    nodePort: 30123</code></pre><h4 id="执行一下-2">执行一下</h4><pre><code>[root@master ~]# kubectl apply -f www-svc.yaml </code></pre><h4 id="查看一下-2">查看一下</h4><pre><code>[root@master ~]# kubectl get svc</code></pre><p><img src="/" class="lazyload" data-src="/posts/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C04%E9%85%8D%E7%BD%AE%E6%B8%85%E5%8D%95.assets%5Cimage-20200107121929525.png"  alt="image-20200107121929525"></p><h4 id="访问一下">访问一下</h4><p><img src="/" class="lazyload" data-src="/posts/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C04%E9%85%8D%E7%BD%AE%E6%B8%85%E5%8D%95.assets%5Cimage-20200107122015559.png"  alt="image-20200107122015559"></p><h1>四. 总结</h1><h2 id="1-Pod的作用"><strong>1. Pod的作用</strong></h2><blockquote><p>在k8s中pod是最小的管理单位，在一个pod中通常会包含一个或多个容器。大多数情况下，一个Pod内只有一个Container容器。<br>在每一个Pod中都有一个特殊的Pause容器和一个或多个业务容器，Pause来源于pause-amd64镜像,Pause容器在Pod中具有非常重要的作用：</p><ul><li>Pause容器作为Pod容器的根容器，其本地于业务容器无关，它的状态代表了整个pod的状态。</li><li>Pod里的多个业务容器共享Pause容器的IP，每个Pod被分配一个独立的IP地址，Pod中的每个容器共享网络命名空间，包括IP地址和网络端口。Pod内的容器可以使用localhost相互通信。k8s支持底层网络集群内任意两个Pod之间进行通信。</li><li>Pod中的所有容器都可以访问共享volumes，允许这些容器共享数据。volumes还用于Pod中的数据持久化，以防其中一个容器需要重新启动而丢失数据。</li></ul></blockquote><h2 id="2-Service的作用"><strong>2. Service的作用</strong></h2><p><strong>Service 是后端真实服务的抽象，一个 Service 可以代表多个相同的后端服务</strong></p><p><strong>Service 为 POD 控制器控制的 POD 集群提供一个固定的访问端点，Service 的工作还依赖于 K8s 中的一个附件，就是 CoreDNS ，它将 Service 地址提供一个域名解析。</strong></p><h3 id="NodePort-类型的-service">NodePort 类型的 service</h3><blockquote><p>**clusterIP：**指定 Service 处于 service 网络的哪个 IP，默认为动态分配</p><p><strong>NodePort 是在 ClusterIP 类型上增加了一个暴露在了 node 的网络命名空间上的一个 nodePort，所以用户可以从集群外部访问到集群了，因而用户的请求流程是：Client -&gt; NodeIP:NodePort -&gt; ClusterIP:ServicePort -&gt; PodIP:ContainerPort。</strong></p><p><strong>可以理解为 NodePort 增强了 ClusterIP 的功能，让客户端可以在每个集群外部访问任意一个 nodeip 从而访问到 clusterIP，再由 clusterIP 进行负载均衡至 POD。</strong></p></blockquote><h2 id="3-流量走向">3.流量走向</h2><p><strong>我们在创建完成一个服务之后，用户首先应该访问的是nginx反向代理的ip，然后通过nginx访问到后端的k8s服务器（master节点）的“NodePort暴露IP 及 映射的端口“，master的apiserver接受到客户端发送来的访问指令，将访问指令通知Controller Manager控制器，Scheduler执行调度任务，将访问指令分发到各节点之上，通过”master节点“的“ip+映射端口”访问到后端k8s节点的信息，节点的Kubelet（pod代理）当Scheduler确定让那个节点返回访问信息之后，kube-proxy将访问信息负载均衡到该节点的容器上，各容器返回信息，并向Master报告运行状态</strong></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>03 创建资源的两种方式</title>
      <link href="/posts/6989.html"/>
      <url>/posts/6989.html</url>
      
        <content type="html"><![CDATA[<table><thead><tr><th>主机</th><th>IP地址</th><th></th></tr></thead><tbody><tr><td>master</td><td>192.168.1.21</td><td></td></tr><tr><td>node01</td><td>192.168.1.22</td><td></td></tr><tr><td>node02</td><td>192.168.1.23</td><td></td></tr></tbody></table><h1>两种创建资源的方法</h1><h2 id="基于命令的方式：">基于命令的方式：</h2><ol><li><strong>简单直观快捷，上手快。</strong></li><li><strong>适合临时测试或实验。</strong></li></ol><h2 id="基于配置文件的方式：">基于配置文件的方式：</h2><ol><li><strong>配置文件描述了 <code>What</code>，即应用最终要达到的状态。</strong></li><li><strong>配置文件提供了创建资源的模板，能够重复部署。</strong></li><li><strong>可以像管理代码一样管理部署。</strong></li><li><strong>适合正式的、跨环境的、规模化部署。</strong></li><li><strong>这种方式要求熟悉配置文件的语法，有一定难度。</strong></li></ol><h1>一，用命令行的方式创建资源</h1><h3 id="仅接受json格式"><em>仅接受json格式</em></h3><h2 id="配置清单（yml、yaml）">配置清单（yml、yaml）</h2><pre><code>[root@master ~]# cd /etc/kubernetes/manifests///k8s的yml、yaml文件</code></pre><h2 id="1-node01和node02下载nginx镜像">1.node01和node02下载nginx镜像</h2><pre><code>docker pull nginx//下载nginx镜像</code></pre><h2 id="2-master创建Pod控制器（test-web），deployment">2.master创建Pod控制器（test-web），deployment</h2><pre><code>[root@master ~]# kubectl run test-web --image=nginx --replicas=5//创建Pod控制器，deployment</code></pre><h2 id="3-查看控制器情况">3.查看控制器情况</h2><h3 id="（1）">（1）</h3><pre><code>[root@master ~]# kubectl get deployments.//查看控制器情况</code></pre><p>![image-20200106093615852](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106093615852.png)</p><pre><code>[root@master ~]# kubectl get pod --all-namespaces -o wide//显示pod的节点信息</code></pre><p>![image-20200106093922849](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106093922849.png)</p><h3 id="（2）">（2）</h3><pre><code>[root@master ~]# kubectl get namespaces //查看k8s名称空间</code></pre><p>![image-20200106093850247](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106093850247.png)</p><pre><code>[root@master ~]# kubectl describe deployments. test-web//查看资源详细信息</code></pre><p>![image-20200106093723330](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106093723330.png)</p><p><em><strong>查看某种资源对象，没有指定名称空间，默认是在default名称空间。可以加上-n选项，查看指定名称空间的资源。</strong></em></p><pre><code>[root@master ~]# kubectl get pod -n kube-system </code></pre><p>![image-20200106094343401](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106094343401.png)</p><h2 id="3-删除test-web控制器">3.删除test-web控制器</h2><pre><code>[root@master ~]# kubectl delete deployments. test-web </code></pre><h2 id="4-master创建Pod控制器（web），deployment">4.master创建Pod控制器（web），deployment</h2><pre><code>[root@master ~]# kubectl run web --image=nginx --replicas=5</code></pre><h3 id="查看一下pod信息">查看一下pod信息</h3><pre><code>[root@master ~]# kubectl get pod -o wide//查看一下pod的节点信息</code></pre><p>![image-20200106095722353](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106095722353.png)</p><pre><code>[root@master ~]# kubectl describe deployments. web //查看资源详细信息</code></pre><p>![image-20200106100606861](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106100606861.png)</p><p><em><strong>注意：直接运行创建的deployment资源对象，是经常使用的一个控制器资源类型，除了deployment，还有rc、rs等等pod控制器，deployment是一个高级的pod控制器。</strong></em></p><h3 id="本机测试访问nginx">本机测试访问nginx</h3><pre><code>[root@master ~]# curl 10.244.1.7</code></pre><p>![image-20200106100827131](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106100827131.png)</p><h2 id="5-创建service资源类型">5.创建service资源类型</h2><pre><code>[root@master ~]# kubectl expose deployment web --name=web-xgp --port=80 --type=NodePort//创建service资源类型，这里我们设置了映射端口</code></pre><p><em><strong>如果想要外网能够访问服务，可以暴露deployment资源，得到service资源，但svc资源的类型必须为NodePort。</strong></em></p><p><strong>映射端口范围：30000-32767</strong></p><h3 id="查看service信息">查看service信息</h3><pre><code>[root@master ~]# kubectl get svc</code></pre><p>![image-20200106101443348](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106101443348.png)</p><h3 id="浏览器测试访问http-192-168-1-21-30493">浏览器测试访问http://192.168.1.21:30493/</h3><p>![image-20200106101624954](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106101624954.png)</p><h1>二、服务的扩容与缩容</h1><h2 id="1-查看控制器信息">1. 查看控制器信息</h2><pre><code>[root@master ~]# kubectl get deployments. -o wide</code></pre><p>![image-20200106104638757](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106104638757.png)</p><h2 id="2-扩容">2.扩容</h2><pre><code>[root@master ~]# kubectl scale deployment web --replicas=8</code></pre><h3 id="查看一下">查看一下</h3><pre><code>[root@master ~]# kubectl get deployments. -o wide</code></pre><p>![image-20200106104757123](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106104757123.png)</p><h2 id="3-缩容">3.缩容</h2><pre><code>[root@master ~]# kubectl scale deployment web --replicas=4</code></pre><h3 id="查看一下-2">查看一下</h3><pre><code>[root@master ~]# kubectl get deployments. -o wide</code></pre><p>![image-20200106105536316](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106105536316.png)</p><h2 id="3-通过修改web的yaml文件进行扩容缩容">3.通过修改web的yaml文件进行扩容缩容</h2><h3 id="备份web的yaml文件">备份web的yaml文件</h3><pre><code>[root@master ~]# kubectl get deployments. -o yaml &gt; web.yaml</code></pre><h3 id="使用edit修改web的yaml文件">使用edit修改web的yaml文件</h3><pre><code>[root@master ~]# kubectl edit deployments. web </code></pre><p>![image-20200106105924531](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106105924531.png)</p><h3 id="查看一下-3">查看一下</h3><pre><code>[root@master ~]# kubectl get deployments. -o wide</code></pre><p>![image-20200106105816339](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106105816339.png)</p><h1>三、服务的升级与回滚</h1><h2 id="node01和node02下载1-15版本的nginx">node01和node02下载1.15版本的nginx</h2><pre><code>[root@master ~]# docker pull nginx:1.15</code></pre><h2 id="1-master设置服务升级">1.master设置服务升级</h2><pre><code>[root@master ~]#  kubectl set image deployment web web=nginx:1.15</code></pre><h3 id="查看一下-4">查看一下</h3><p>![image-20200106111227960](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106111227960.png)</p><h2 id="2-master设置服务回滚">2.master设置服务回滚</h2><h3 id="（1）修改配置文件回滚">（1）修改配置文件回滚</h3><h3 id="使用edit修改web的yaml文件-2">使用edit修改web的yaml文件</h3><pre><code>[root@master ~]# kubectl edit deployments. web </code></pre><p>![image-20200106111523148](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106111523148.png)</p><h3 id="查看一下-5">查看一下</h3><pre><code>[root@master ~]# kubectl get deployments. -o wide</code></pre><p>![image-20200106111319699](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106111319699.png)</p><h3 id="（2）命令回滚">（2）命令回滚</h3><pre><code>[root@master ~]# kubectl rollout undo deployment web </code></pre><p>![image-20200106111733617](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106111733617.png)</p><p><em><strong>注意:只能回滚到上一次操作的状态</strong></em></p><h1>四、实验环境</h1><table><thead><tr><th>主机</th><th>IP地址</th><th>服务</th></tr></thead><tbody><tr><td>master</td><td>192.168.1.21</td><td>registry+Deployment</td></tr><tr><td>node01</td><td>192.168.1.22</td><td></td></tr><tr><td>node02</td><td>192.168.1.23</td><td></td></tr></tbody></table><h2 id="1-master-基于httpd制作自己的镜像，需要3个版本，v1-v2-v3-并且对应的版本镜像，访问的主目录内容不一样">1.master 基于httpd制作自己的镜像，需要3个版本，v1,v2,v3.并且对应的版本镜像，访问的主目录内容不一样</h2><h3 id="（1）master下载httpd镜像">（1）master下载httpd镜像</h3><pre><code>[root@master ~]# docker pull httpd</code></pre><h3 id="（2）编写Dockerfile">（2）编写Dockerfile</h3><pre><code>[root@master xgp]# vim DockerfileFROM httpdCOPY index.html /usr/local/apache2/htdocs/index.html</code></pre><h3 id="（3）创建测试网页v1">（3）创建测试网页v1</h3><pre><code>[root@master xgp]#echo "&lt;h1&gt;xgp | test-web | httpd:v1&lt;h1&gt;" &gt; index.html</code></pre><h3 id="（4）基于Dockerfile创建镜像-web1">（4）基于Dockerfile创建镜像 web1</h3><pre><code>[root@master xgp]# docker build -t web1 .</code></pre><h3 id="（5）创建测试网页v2">（5）创建测试网页v2</h3><pre><code>[root@master xgp]#echo "&lt;h1&gt;xgp | test-web | httpd:v1&lt;h1&gt;" &gt; index.html</code></pre><h3 id="（6）基于Dockerfile创建镜像-web2">（6）基于Dockerfile创建镜像 web2</h3><pre><code>[root@master xgp]# docker build -t web2 .</code></pre><h3 id="（7）创建测试网页v3">（7）创建测试网页v3</h3><pre><code>[root@master xgp]# echo "&lt;h1&gt;xgp | test-web | httpd:v3&lt;h1&gt;" &gt; index.html</code></pre><h3 id="（8）基于Dockerfile创建镜像-web3">（8）基于Dockerfile创建镜像 web3</h3><pre><code>[root@master xgp]# docker build -t web3 .</code></pre><h2 id="2-master部署私有仓库">2.master部署私有仓库</h2><h3 id="（1）master下载registry镜像">（1）master下载registry镜像</h3><pre><code>[root@master ~]# docker pull registry</code></pre><h3 id="（2）启动registry">（2）启动registry</h3><pre><code>[root@master xgp]# docker run -itd --name registry -p 5000:5000 --restart=always registry:latest </code></pre><h3 id="（3）修改docker配置文件，加入私有仓库（三台）">（3）修改docker配置文件，加入私有仓库（三台）</h3><pre><code>[root@master xgp]# vim /usr/lib/systemd/system/docker.serviceExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.21:5000</code></pre><p>![image-20200106120848869](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106120848869.png)</p><h3 id="（4）重启docker（三台）">（4）重启docker（三台）</h3><pre><code>[root@master xgp]# systemctl daemon-reload [root@master xgp]# systemctl restart docker</code></pre><h2 id="3-上传之前创建的三个web镜像到私有仓库">3.上传之前创建的三个web镜像到私有仓库</h2><h3 id="（1）修改镜像标签">（1）修改镜像标签</h3><pre><code>[root@master xgp]# docker tag web1:latest 192.168.1.21:5000/web1:latest[root@master xgp]# docker tag web2:latest 192.168.1.21:5000/web2:latest[root@master xgp]# docker tag web3:latest 192.168.1.21:5000/web3:latest</code></pre><h3 id="（2）将三个web镜像上传到私有仓库">（2）将三个web镜像上传到私有仓库</h3><pre><code>[root@master xgp]# docker push  192.168.1.21:5000/web1:latest [root@master xgp]# docker push  192.168.1.21:5000/web2:latest[root@master xgp]# docker push  192.168.1.21:5000/web3:latest </code></pre><h2 id="4-部署一个Deployment资源对象，要求镜像使用上述私有镜像v1版本。6个副本Pod。">4.部署一个Deployment资源对象，要求镜像使用上述私有镜像v1版本。6个副本Pod。</h2><pre><code>[root@master xgp]# kubectl run www1 --image=192.168.1.21:5000/web1:latest --replicas=6</code></pre><h3 id="查看一下-6">查看一下</h3><pre><code>[root@master xgp]# kubectl get pod</code></pre><p>![image-20200106122026271](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106122026271.png)</p><h3 id="本地访问一下">本地访问一下</h3><p>![image-20200106122426308](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106122426308.png)</p><h3 id="5-将上述Deployment暴露一个service资源对象，使外网能否访问服务。">5.将上述Deployment暴露一个service资源对象，使外网能否访问服务。</h3><pre><code>[root@master xgp]#  kubectl expose deployment www1 --name=web-xgp --port=80 --type=NodePort</code></pre><h3 id="查看一下-7">查看一下</h3><pre><code>[root@master xgp]# kubectl get svc</code></pre><p>![image-20200106122313996](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106122313996.png)</p><h3 id="浏览器访问一下">浏览器访问一下</h3><p>![image-20200106122340747](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106122340747.png)</p><h2 id="6-将上述Deployment进行扩容和缩容操作，扩容为8个副本Pod，然后缩容为4个副本Pod。">6.将上述Deployment进行扩容和缩容操作，扩容为8个副本Pod，然后缩容为4个副本Pod。</h2><h2 id="（1）扩容">（1）扩容</h2><pre><code>[root@master xgp]# kubectl scale deployment www1 --replicas=8</code></pre><h3 id="查看一下-8">查看一下</h3><pre><code>[root@master xgp]# kubectl get deployments. -o wide</code></pre><p>![image-20200106122722977](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106122722977.png)</p><h2 id="（2）缩容">（2）缩容</h2><h3 id="修改k8s配置文件">修改k8s配置文件</h3><h3 id="备份web的yaml文件-2">备份web的yaml文件</h3><pre><code>[root@master ~]# kubectl get deployments. -o yaml &gt; www1.yaml</code></pre><h3 id="使用edit修改web的yaml文件-3">使用edit修改web的yaml文件</h3><pre><code>[root@master ~]# kubectl edit deployments. www1</code></pre><p>![image-20200106105924531](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106105924531.png)</p><h3 id="查看一下-9">查看一下</h3><pre><code>[root@master xgp]# kubectl get deployments. -o wide</code></pre><p>![image-20200106122953397](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106122953397.png)</p><h2 id="7-将上述Deployment进行升级与回滚操作，将v1版本，升级到v2版本。">7.将上述Deployment进行升级与回滚操作，将v1版本，升级到v2版本。</h2><h2 id="（1）升级版本为web2">（1）升级版本为web2</h2><pre><code>[root@master ~]# kubectl set image deployment www1 www1=192.168.1.21:5000/web2</code></pre><h3 id="本机测试访问">本机测试访问</h3><pre><code>[root@master ~]# curl 127.0.0.1:30996&lt;h1&gt;xgp | test-web | httpd:v2&lt;h1&gt;</code></pre><p>![image-20200106125722931](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106125722931.png)</p><h3 id="浏览器测试访问">浏览器测试访问</h3><p>![image-20200106125750021](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106125750021.png)</p><h2 id="（2）回滚版本到web1">（2）回滚版本到web1</h2><h3 id="1-修改配置文件回滚">&lt;1&gt;修改配置文件回滚</h3><h3 id="使用edit修改web的yaml文件-4">使用edit修改web的yaml文件</h3><pre><code>[root@master ~]# kubectl edit deployments. www1</code></pre><p>![image-20200106130010344](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106130010344.png)</p><h3 id="查看一下-10">查看一下</h3><pre><code>[root@master ~]# kubectl get deployments. -o wide</code></pre><p>![image-20200106130304423](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106130304423.png)</p><h3 id="访问一下">访问一下</h3><p>![image-20200106130435212](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106130435212.png)</p><p>![image-20200106130447693](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106130447693.png)</p><h3 id="2-命令回滚">&lt;2&gt;命令回滚</h3><pre><code>[root@master ~]# kubectl rollout undo deployment www1</code></pre><p>![image-20200106130317956](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106130317956.png)</p><p><em><strong>注意:只能回滚到上一次操作的状态</strong></em></p><h3 id="访问一下-2">访问一下</h3><p>![image-20200106130357339](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106130357339.png)</p><p>![image-20200106130414060](E:\软件\博客\Blog\blog\source_posts\03 创建资源的两种方式.assets\image-20200106130414060.png)</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>02 k8s架构，基本概念</title>
      <link href="/posts/cd85.html"/>
      <url>/posts/cd85.html</url>
      
        <content type="html"><![CDATA[<table><thead><tr><th>主机名</th><th>IP地址</th><th>服务</th></tr></thead><tbody><tr><td>master</td><td>192.168.1.21</td><td></td></tr><tr><td>node01</td><td>192.168.1.22</td><td></td></tr><tr><td>node02</td><td>192.168.1.23</td><td></td></tr></tbody></table><h1>kubernetes架构</h1><p><img src="/" class="lazyload" data-src="https://upload-images.jianshu.io/upload_images/2032456-3447c966ff470446.png?imageMogr2/auto-orient/strip%7CimageView2/2/format/webp"  alt="img"></p><p>**kubectl：**k8s是命令行端，用来发送客户的操作指令。</p><h2 id="master节点">master节点</h2><p><strong>1. API server[资源操作入口]</strong>：是k8s集群的前端接口，各种各样客户端工具以及k8s的其他组件可以通过它管理k8s集群的各种资源。它提供了HTTP/HTTPS RESTful API,即K8S API。</p><blockquote><ul><li>提供了资源对象的唯一操作入口，其他所有组件都必须通过它提供的API来操作资源数据，只有API Server与存储通信，其他模块通过API Server访问集群状态。</li></ul><p>第一，是为了保证集群状态访问的安全。</p><p>第二，是为了隔离集群状态访问的方式和后端存储实现的方式：API Server是状态访问的方式，不会因为后端存储技术etcd的改变而改变。</p><ul><li>作为kubernetes系统的入口，封装了核心对象的增删改查操作，以<a href="https://www.centos.bz/tag/restful/" target="_blank" rel="noopener">RESTFul</a>接口方式提供给外部客户和内部组件调用。对相关的资源数据“全量查询”+“变化监听”，实时完成相关的业务功能。</li></ul></blockquote><p><strong>2. Scheduler[集群分发调度器]</strong>：负责决定将Pod放在哪个Node上运行。在调度时，会充分考虑集群的拓扑结构，当前各个节点的负载情况，以及应对高可用、性能、数据亲和性和需求。</p><blockquote><p>1.Scheduler收集和分析当前Kubernetes集群中所有Minion节点的资源(内存、CPU)负载情况，然后依此分发新建的Pod到Kubernetes集群中可用的节点。</p><p>2.实时监测Kubernetes集群中未分发和已分发的所有运行的Pod。</p><p>3.Scheduler也监测Minion节点信息，由于会频繁查找Minion节点，Scheduler会缓存一份最新的信息在本地。</p><p>4.最后，Scheduler在分发Pod到指定的Minion节点后，会把Pod相关的信息Binding写回API Server。</p></blockquote><p><strong>4. Controller Manager[内部管理控制中心]</strong>：负责管理集群的各种资源，保证资源处于预期的状态。它由多种Controller组成，包括Replication Controller、Endpoints Controller、Namespace Controller、Serviceaccounts Controller等。</p><blockquote><p>实现集群故障检测和恢复的自动化工作，负责执行各种控制器，主要有：</p><p>1.endpoint-controller：定期关联<a href="https://www.centos.bz/tag/service/" target="_blank" rel="noopener">service</a>和pod(关联信息由endpoint对象维护)，保证service到pod的映射总是最新的。</p><p>2.replication-controller：定期关联replicationController和pod，保证replicationController定义的复制数量与实际运行pod的数量总是一致的。</p></blockquote><p>**5. Etcd：**负责保存k8s集群的配置信息和各种资源的状态信息。当数据发生变化时，etcd会快速的通知k8s相关组件。<a href="">（第三方组件）它有可替换方案。Consul、zookeeper</a></p><p><strong>6. Pod:</strong> k8s集群的最小组成单位。一个Pod内，可以运行一个或多个容器。大多数情况下，一个Pod内只有一个Container容器。</p><p>**7. Flanner：**是k8s集群网络，可以保证Pod的跨主机通信。也有替换方案。</p><pre><code>[root@master ~]# kubectl get pod --all-namespaces//查看pod信息</code></pre><p><img src="/" class="lazyload" data-src="/posts/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5Cpost-name%5Cimage-20200104100759831.png"  alt="image-20200104100759831"></p><pre><code>[root@master ~]# kubectl get pod --all-namespaces -o wide//显示pod的节点信息</code></pre><p><img src="/" class="lazyload" data-src="/posts/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5Cpost-name%5Cimage-20200104101023909.png"  alt="image-20200104101023909"></p><h2 id="Node节点">Node节点</h2><p><strong>Kubelet[节点上的Pod管家]</strong>：它是Node的agent(代理)，当Scheduler确定某 个Node上运行Pod之后，会将Pod的具体配置信息发送给该节点的kubelet,kubelet会根据这些信息创建和运行容器，并向Master报告运行状态。</p><blockquote><ul><li>负责Node节点上pod的创建、修改、监控、删除等全生命周期的管理</li><li>定时上报本Node的状态信息给API Server。</li><li>kubelet是Master API Server和Minion之间的桥梁，接收Master API Server分配给它的commands和work，与持久性键值存储etcd、file、server和http进行交互，读取配置信息。</li><li>具体的工作如下：</li></ul><p>设置容器的环境变量、给容器绑定<a href="https://www.centos.bz/tag/volume/" target="_blank" rel="noopener">Volume</a>、给容器绑定Port、根据指定的Pod运行一个单一容器、给指定的Pod创建network 容器。</p><p>同步Pod的状态、同步Pod的状态、从<a href="https://www.centos.bz/tag/cadvisor/" target="_blank" rel="noopener">cAdvisor</a>获取<a href="https://www.centos.bz/tag/container/" target="_blank" rel="noopener">Container</a> info、 pod info、 root info、 <a href="https://www.centos.bz/tag/machine/" target="_blank" rel="noopener">machine</a> info。</p><p>在容器中运行命令、杀死容器、删除Pod的所有容器。</p></blockquote><p>**kube-proxy[负载均衡、路由转发]:**负责将访问service的TCP/UDP数据流转发到后端的容器。如果有多个副本，kube-proxy会实现负载均衡。</p><blockquote><ul><li>Proxy是为了解决外部网络能够访问跨机器集群中容器提供的应用服务而设计的，运行在每个Node上。Proxy提供TCP/UDP sockets的proxy，每创建一种Service，Proxy主要从etcd获取Services和Endpoints的配置信息（也可以从file获取），然后根据配置信息在Minion上启动一个Proxy的进程并监听相应的服务端口，当外部请求发生时，Proxy会根据Load Balancer将请求分发到后端正确的容器处理。</li><li>Proxy不但解决了同一主宿机相同服务端口冲突的问题，还提供了Service转发服务端口对外提供服务的能力，Proxy后端使用了随机、轮循负载均衡算法。</li></ul></blockquote><h2 id="范例">范例</h2><blockquote><h3 id="分析各个组件的作用以及架构工作流程">分析各个组件的作用以及架构工作流程:</h3><p><strong>1) kubectl发送部署 请求到API server</strong><br><strong>2) APIserver通知Controller Manager创建一个Deployment资源。</strong><br><strong>3) Scheduler执行调度任务,将两个副本Pod分发到node01和node02. 上。</strong><br><strong>4) node01和node02, 上的kubelet在各自节点上创建并运行Pod。</strong></p><h3 id="补充">补充</h3><p><strong>1.应用的配置和当前的状态信息保存在etcd中，执行kubectl get pod时API server会从etcd中读取这些数据。</strong></p><p><strong>2.flannel会为每个Pod分配一个IP。 但此时没有创建Service资源，目前kube-proxy还没有参与进来。</strong></p></blockquote><h3 id="运行一个例子（创建一个deployment资源对象-pod控制器-）">运行一个例子（创建一个deployment资源对象&lt;pod控制器&gt;）</h3><pre><code>[root@master ~]# kubectl run test-web --image=httpd --replicas=2//创建一个deployment资源对象。</code></pre><p><em><strong>运行完成之后，如果有镜像可直接开启，没有的话需要等待一会儿，node节点要在docker hup上下载</strong></em></p><h4 id="查看一下">查看一下</h4><pre><code>[root@master ~]# kubectl get  deployments.或 kubectl get  deploy</code></pre><p><img src="/" class="lazyload" data-src="/posts/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5Cpost-name%5Cimage-20200104110812772.png"  alt="image-20200104110812772"></p><pre><code>[root@master ~]# kubectl get pod</code></pre><p><img src="/" class="lazyload" data-src="/posts/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5Cpost-name%5Cimage-20200104110954406.png"  alt="image-20200104110954406"></p><pre><code>[root@master ~]# kubectl get pod  -o wide//显示pod的节点信息</code></pre><p><img src="/" class="lazyload" data-src="/posts/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5Cpost-name%5Cimage-20200104111128779.png"  alt="image-20200104111128779"></p><p><em><strong>如果，node节点没有运行test-web服务，需要在节点上重启一下<systemctl restart="" kubelet=""></systemctl></strong></em></p><h3 id="如果删除一个pod">如果删除一个pod</h3><pre><code>[root@master ~]# kubectl delete pod test-web-5b56bdff65-2njqf </code></pre><h4 id="查看一下-2">查看一下</h4><pre><code>[root@master ~]# kubectl get pod -o wide</code></pre><p><img src="/" class="lazyload" data-src="/posts/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5Cpost-name%5Cimage-20200104112418012.png"  alt="image-20200104112418012"></p><p><em><strong>现在发现容器还存在，因为控制器会自动发现，一旦与之前执行的命令有误差，他会自动补全。</strong></em></p><p><a href="https://blog.csdn.net/gongxsh00/article/details/79932136" target="_blank" rel="noopener">https://blog.csdn.net/gongxsh00/article/details/79932136</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>部署k8s集群</title>
      <link href="/posts/2cda.html"/>
      <url>/posts/2cda.html</url>
      
        <content type="html"><![CDATA[<h2 id="k8s">k8s</h2><h4 id="最基本的硬件要求"><strong>最基本的硬件要求</strong></h4><p><strong>CPU: 双核</strong><br><strong>Mem: 2G</strong><br><strong>3台dockerhost</strong><br><strong>时间必须同步</strong></p><h1>实验环境</h1><table><thead><tr><th>主机名</th><th>IP地址</th><th>服务</th></tr></thead><tbody><tr><td>master</td><td>192.168.1.21</td><td>dockerhost</td></tr><tr><td>node01</td><td>192.168.1.22</td><td>dockerhost</td></tr><tr><td>node02</td><td>192.168.1.23</td><td>dockerhost</td></tr></tbody></table><h2 id="环境准备">环境准备</h2><p><strong>分别将3台虚拟机命名，设置好对应IP，并将其写入域名解析/etc/hosts中，关闭防火墙，iptables，禁用selinux。还有要做到，时间必须一致。全部禁用swap</strong></p><h3 id="1-给三台docker命名">1.给三台docker命名</h3><p><strong>k8.1</strong></p><pre><code>[root@localhost ~]# hostnamectl set-hostname master[root@localhost ~]# su -</code></pre><p><strong>k8.2</strong></p><pre><code>[root@localhost ~]# hostnamectl set-hostname node01[root@localhost ~]# su -</code></pre><p><strong>k8.3</strong></p><pre><code>[root@localhost ~]# hostnamectl set-hostname node02[root@localhost ~]# su -</code></pre><p>验证docker是否能使用及版本是否一样</p><pre><code>[root@master ~]# docker -v</code></pre><p><img src="/" class="lazyload" data-src="/posts/image-20200102093813472.png"  alt="image-20200102093813472"></p><h3 id="2-关闭防火墙及禁用selinux">2.关闭防火墙及禁用selinux</h3><pre><code>[root@master ~]# systemctl stop firewalld[root@master ~]# systemctl disable firewalld [root@master ~]# vim /etc/selinux/config</code></pre><p><img src="/" class="lazyload" data-src="/posts/picture_path"  alt="description" title="image-20200102115453524.jpg"></p><p><img src="/" class="lazyload" data-src="/posts/image-20200102115453524.jpg"  alt="image-20200501001702921"></p><h3 id="3-禁用swap（三台）">3.  禁用swap（三台）</h3><pre><code>[root@master ~]# swapoff -a//临时禁用swap[root@master ~]# free -h[root@master ~]# vim /etc/fstab </code></pre><img src="/"  lazyloadclass="" title="This is an test image" data-src="/posts/2cda/image-20200102094039749.png"><h3 id="4-添加域名解析（三台）">4.添加域名解析（三台）</h3><pre><code>[root@master ~]# echo 192.168.1.21 master &gt;&gt; /etc/hosts[root@master ~]# echo 192.168.1.22 node01 &gt;&gt; /etc/hosts[root@master ~]# echo 192.168.1.23 node02 &gt;&gt; /etc/hosts</code></pre><h3 id="5-做免密登陆（三台）">5.做免密登陆（三台）</h3><pre><code>[root@master ~]# ssh-keygen -t rsa//生成密钥</code></pre><p><strong>复制密钥到其他主机</strong></p><pre><code>   54  ssh-copy-id node01   55  ssh-copy-id node02</code></pre><h4 id="把域名解析复制到其他主机"><strong>把域名解析复制到其他主机</strong></h4><pre><code>   63  scp /etc/hosts node01:/etc   64  scp /etc/hosts node02:/etc</code></pre><h3 id="6-打开路由转发和iptables桥接功能（三台）">6.打开路由转发和iptables桥接功能（三台）</h3><pre><code>[root@master ~]# vim /etc/sysctl.d/k8s.conf//开启iptables桥接功能net.bridge.bridge-nf-call-iptables = 1net.bridge.bridge-nf-call-ip6tables = 1[root@master ~]# echo net.ipv4.ip_forward = 1 &gt;&gt; /etc/sysctl.conf //**打开路由转发[root@master ~]# sysctl -p /etc/sysctl.d/k8s.conf [root@master ~]# sysctl -p //刷新一下</code></pre><p><strong>如果以上命令执行失败可能是缺少模块，可执行以下命令</strong></p><pre><code>[root@master ~]# modprobe br_netfiler</code></pre><p><strong>把路由转发和iptables桥接复制到其他主机</strong></p><pre><code>[root@master ~]# scp /etc/sysctl.d/k8s.conf  node01:/etc/sysctl.d/[root@master ~]# scp /etc/sysctl.d/k8s.conf  node02:/etc/sysctl.d/[root@master ~]# scp /etc/sysctl.conf  node02:/etc/[root@master ~]# scp /etc/sysctl.conf  node01:/etc/</code></pre><p><strong>记得node01和node02也要执行以下命令</strong></p><pre><code>[root@master ~]# sysctl -p /etc/sysctl.d/k8s.conf [root@master ~]# sysctl -p </code></pre><h1>master节点安装部署k8s</h1><h2 id="指定yum安装kubernetes的yum源（三台）">指定yum安装kubernetes的yum源（三台）</h2><pre><code>cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF</code></pre><p><strong>下载完成之后，查看一下仓库是否可用</strong></p><pre><code>[root@master ~]# yum repolist </code></pre><p>![image-20200102095945934](./01 部署k8s集群/image-20200102095945934.png)</p><p><strong>创建本地缓存（三台）</strong></p><pre><code>[root@master ~]# yum makecache fast</code></pre><h2 id="各节点安装所需安装包">各节点安装所需安装包</h2><h3 id="master下载"><strong>master下载</strong></h3><pre><code>[root@master ~]# yum -y install kubeadm-1.15.0-0 kubelet-1.15.0-0 kubectl-1.15.0-0</code></pre><h3 id="node01和node02下载"><strong>node01和node02下载</strong></h3><pre><code>[root@node01 ~]# yum -y install kubeadm-1.15.0-0 kubelet-1.15.0-0</code></pre><h3 id="三台主机把-kubelet加入开机自启"><strong>三台主机把 kubelet加入开机自启</strong></h3><pre><code>[root@master ~]# systemctl enable kubelet</code></pre><h2 id="master导入，之前准备好的镜像"><strong>master导入，之前准备好的镜像</strong></h2><pre><code>[root@master ~]# mkdir images[root@master ~]# cd images/[root@master images]# ls</code></pre><p>![image-20200102101531123](./01 部署k8s集群/image-20200102101531123.png)</p><h3 id="创建一个导入镜像的脚本"><strong>创建一个导入镜像的脚本</strong></h3><pre><code>[root@master images]# cat &gt; image.sh &lt;&lt;EOF&gt; #!/bin/bash&gt; for i in /root/images/*&gt; do&gt; docker load &lt; $i &gt; done&gt; EOF[root@master images]# chmod +x image.sh </code></pre><h3 id="导入镜像">导入镜像</h3><pre><code>[root@master images]# sh image.sh </code></pre><h3 id="初始化Kubernetes集群">初始化Kubernetes集群</h3><pre><code>[root@master ~]#  kubeadm init --kubernetes-version=v1.15.0 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 --ignore-preflight-errors=Swap</code></pre><p><strong>如果以上的命令报错，找出问题后先重置一下（下面的命令），然后再执行以上命令</strong></p><pre><code>[root@master ~]# kubeadm reset//重置kubeadm</code></pre><p>![image-20200102122213788](./01 部署k8s集群/image-20200102122213788.png)</p><pre><code>[root@master images]# kubectl get node//查看当前节点信息</code></pre><p>![image-20200102110808239](./01 部署k8s集群/image-20200102110808239.png)</p><p><strong>可以看出master的状态是未就绪（NotReady），之所以是这种状态是因为还缺少一个附件flannel，没有网络各Pod是无法通信的</strong></p><h3 id="也可以通过检查组件的健康状态"><strong>也可以通过检查组件的健康状态</strong></h3><pre><code>[root@master images]# kubectl get cs</code></pre><p>![image-20200102122413443](./01 部署k8s集群/image-20200102122413443.png)</p><h3 id="添加网络组件（flannel）">添加网络组件（flannel）</h3><p><strong>组件flannel可以通过https://github.com/coreos/flannel中获取</strong></p><pre><code>[root@master ~]# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</code></pre><p>以上只是方式之一，在网络状况良好的情况下建议使用上述方法（调用远端文件执行一下），<strong>若网速较差，建议使用以下方法：</strong></p><pre><code>[root@master images]# wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml#将github官网指定的.yml配置文件下载到本地[root@master images]# ls | grep flannel.yml   #确定下载到了当前目录kube-flannel.yml[root@master images]# kubectl apply -f kube-flannel.yml  #指定下载的.yml文件执行相应命令</code></pre><p><em><strong>上述方法，二选一进行配置即可。</strong></em></p><p><strong>看到很多东西被创建是还不够的，还需要查看flannel是否处于正常启动并运行的状态，才算正在的部署完成</strong></p><pre><code>[root@master images]# kubectl get pods --all-namespaces//查看所有的名称空间的pod（可以看到flannel网络运行正常）</code></pre><p>![image-20200102122732587](./01 部署k8s集群/image-20200102122732587.png)</p><pre><code>[root@master images]# kubectl get pod -n kube-system//查看名称空间为kube-system的pod</code></pre><p>![image-20200102122826221](./01 部署k8s集群/image-20200102122826221.png)</p><p>查看当前节点信息</p><pre><code>kubectl get node//查看当前节点信息（已经准备好了）</code></pre><p>![image-20200102111853910](./01 部署k8s集群/image-20200102111853910.png)</p><h1>node两台节点，导入镜像并加入群集</h1><h2 id="导入镜像-2">导入镜像</h2><p>上传所需镜像包，也可以使用docker pull下载</p><p>![image-20200102133744555](./01 部署k8s集群/image-20200102133744555.png)</p><pre><code>[root@node01 images]# docker load &lt; kube-proxy-1-15.tar &amp;&amp; docker load -i myflannel-11-0.tar  &amp;&amp; docker load -i pause-3-1.tar</code></pre><pre><code>[root@node01 images]# docker images//查看本地镜像</code></pre><p>![image-20200102134006724](./01 部署k8s集群/image-20200102134006724.png)</p><h2 id="node01和node02加入群集">node01和node02加入群集</h2><p><strong>这时使用的命令是初始化群集之后生成的令牌（只有24小时的时效）</strong></p><p>![image-20200102134336922](./01 部署k8s集群/image-20200102134336922.png)</p><pre><code>[root@node01 ~]# kubeadm join 192.168.1.21:6443 --token z0vknh.s6ib4eu4f8bre2nu     --discovery-token-ca-cert-hash sha256:8da72cc83f45d1247f42ce888658129b43726fe2af4ffc0c4e79faedb4050359</code></pre><h2 id="加入群集之后查看一下">加入群集之后查看一下</h2><pre><code>[root@master images]# kubectl get node</code></pre><p>![image-20200102114628989](./01 部署k8s集群/image-20200102114628989.png)</p><h1>各节点优化一下</h1><h2 id="设置table键的默认间距；">设置table键的默认间距；</h2><pre><code>[root@master ~]# vim .vimrcset tabstop=2[root@master ~]# source .vimrc </code></pre><h2 id="设置kubectl命令自动补全">设置kubectl命令自动补全</h2><pre><code>[root@master ~]# yum  -y install bash-completion[root@master ~]#  source /usr/share/bash-completion/bash_completion [root@master ~]# source &lt;(kubectl completion bash)[root@master ~]# echo "source &lt;(kubectl completion bash)" &gt;&gt; ~/.bashrc</code></pre><h2 id="确认k8s群集没有问题，并设置为开机自启">确认k8s群集没有问题，并设置为开机自启</h2><h3 id="master主机操作如下："><strong>master主机操作如下：</strong></h3><pre><code>[root@master ~]# kubectl get pod -n kube-system   #查看pod资源，类似于docker中的容器，确保返回的信息都是running#“-n kube-system”：是k8s的名称空间</code></pre><p>![image-20200102142028971](./01 部署k8s集群/image-20200102142028971.png)</p><h3 id="master和node节点上都需要进行以下操作，以便设置为开机自启："><strong>master和node节点上都需要进行以下操作，以便设置为开机自启：</strong></h3><pre><code>[root@master ~]# systemctl enable kubelet[root@master ~]# systemctl enable docker </code></pre><p><strong>设置为开机自启后，k8s群集的配置基本完成了，现在可以重启一下这三台服务器，如果重启后，执行下面的命令，状态都还是running，则表示绝对没有问题了。</strong></p><pre><code>[root@master ~]# kubectl get pod -n kube-system    #重启后验证状态是否还都是running</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>HEXO分类和标签的使用</title>
      <link href="/posts/bf18.html"/>
      <url>/posts/bf18.html</url>
      
        <content type="html"><![CDATA[<h1>Hexo使用攻略-添加分类及标签</h1><h2 id="1、创建“分类”选项">1、创建“分类”选项</h2><h3 id="1-1-生成“分类”页并添加tpye属性">1.1 生成“分类”页并添加tpye属性</h3><p><strong>打开命令行，进入博客所在文件夹。执行命令</strong></p><pre class=" language-language-cpp"><code class="language-language-cpp">hexo new page categories</code></pre><p><strong>成功后会提示：</strong></p><pre class=" language-language-bash"><code class="language-language-bash">INFO  Created: ~/Documents/blog/source/categories/index.md</code></pre><p><strong>根据上面的路径，找到<code>index.md</code>这个文件，打开后默认内容是这样的：</strong></p><pre class=" language-language-css"><code class="language-language-css">---title: 文章分类date: 2017-05-27 13:47:40---</code></pre><p><strong>添加<code>type: "categories"</code>到内容中，添加后是这样的：</strong></p><pre class=" language-language-bash"><code class="language-language-bash">---title: 文章分类date: 2017-05-27 13:47:40type: "categories"---</code></pre><p><strong>保存并关闭文件。</strong></p><h4 id="1-2-给文章添加“categories”属性">1.2 给文章添加“categories”属性</h4><p><strong>打开需要添加分类的文章，为其添加categories属性。下方的<code>categories: web前端</code>表示添加这篇文章到“web前端”这个分类。注意：hexo一篇文章只能属于一个分类，也就是说如果在“- web前端”下方添加“-xxx”，hexo不会产生两个分类，而是把分类嵌套（即该文章属于 “- web前端”下的 “-xxx ”分类）。</strong></p><pre class=" language-language-css"><code class="language-language-css">---title: jQuery对表单的操作及更多应用  #这个网页上不会显示，用于自己分辨date: 2017-05-26 12:12:57categories: - web前端                         #这个是分类名称---</code></pre><p><strong>至此，成功给文章添加分类，点击首页的“分类”可以看到该分类下的所有文章。当然，只有添加了<code>categories: xxx</code>的文章才会被收录到首页的“分类”中。</strong></p><h3 id="2、创建“标签”选项">2、创建“标签”选项</h3><h4 id="2-1-生成“标签”页并添加tpye属性">2.1 生成“标签”页并添加tpye属性</h4><p><strong>打开命令行，进入博客所在文件夹。执行命令</strong></p><pre class=" language-language-cpp"><code class="language-language-cpp"> hexo new page tags</code></pre><p><strong>成功后会提示：</strong></p><pre class=" language-language-bash"><code class="language-language-bash">INFO  Created: ~/Documents/blog/source/tags/index.md</code></pre><p><strong>根据上面的路径，找到<code>index.md</code>这个文件，打开后默认内容是这样的：</strong></p><pre class=" language-language-css"><code class="language-language-css">---title: 标签date: 2017-05-27 14:22:08---</code></pre><p><strong>添加<code>type: "tags"</code>到内容中，添加后是这样的：</strong></p><pre class=" language-language-bash"><code class="language-language-bash">---title: 文章分类date: 2017-05-27 13:47:40type: "tags"---</code></pre><p><strong>保存并关闭文件。</strong></p><h4 id="2-2-给文章添加“tags”属性">2.2 给文章添加“tags”属性</h4><p><strong>打开需要添加标签的文章，为其添加tags属性。下方的<code>tags:</code>下方的<code>- jQuery</code> <code>- 表格</code><br><code>- 表单验证</code>就是这篇文章的标签了</strong></p><pre class=" language-language-css"><code class="language-language-css">---title: jQuery对表单的操作及更多应用date: 2017-05-26 12:12:57categories: - web前端tags:- jQuery               #标签名称---</code></pre><p><strong>至此，成功给文章添加分类，点击首页的“标签”可以看到该标签下的所有文章。当然，只有添加了<code>tags: xxx</code>的文章才会被收录到首页的“标签”中。</strong></p><p><strong>细心的朋友可能已经发现，这两个的设置几乎一模一样！是的，没错，思路都是一样的。所以我们可以打开scaffolds/post.md文件，在tages:上面加入categories:,保存后，之后执行<code>hexo new 文章名</code>命令生成的文件，页面里就有<code>categories:</code>项了。</strong></p><p><strong>scaffolds目录下，是新建页面的模板，执行新建命令时，是根据这里的模板页来完成的，所以可以在这里根据你自己的需求添加一些默认值。</strong></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> HEXO的使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HEXO </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Nginx优化与防盗链</title>
      <link href="/posts/cfe.html"/>
      <url>/posts/cfe.html</url>
      
        <content type="html"><![CDATA[<p>Nginx 是俄罗斯人编写的十分轻量级的 HTTP 服务器,Nginx， 它的发音为“engine X”， 是一个高<br>性能的 HTTP 和反向代理服务器，同时也是一个 IMAP/POP3/SMTP 代理服务器． Nginx 是由<br>俄罗斯人 Igor Sysoev 为俄罗斯访问量第二的 <a href="http://Rambler.ru" target="_blank" rel="noopener">Rambler.ru</a> 站点开发.<br>Nginx 以事件驱动（epoll） 的方式编写，所以有非常好的性能，同时也是一个非常高效的反<br>向代理、负载平衡。 但是 Nginx 并不支持 cgi 方式运行，原因是可以减少因此带来的一些程<br>序上的漏洞。所以必须使用 FastCGI 方式来执行 PHP 程序。<br>由于 Nginx 本身的一些优点， 轻量，开源，易用，越来越多的公司使用 nginx 作为自己公司<br>的 web 应用服务器，本文详细介绍 nginx 源码安装的同时并对 nginx 进行优化配置。<br>一、 Nginx 的优化<br>1、 编译安装前优化<br>编译前的优化主要是用来修改程序名等等， 目的更改源码隐藏软件名称和版本号<br>安装 zlib-devel、 pcre-devel 等依赖包</p><pre><code>[root@bogon ~]# [root@bogon ~]# tar zxf nginx-1.14.0.tar.gz [root@bogon ~]# cd nginx-1.14.0/[root@bogon nginx-1.14.0]# yum -y install openssl-devel pcre-devel</code></pre><p>2.编译安装nginx</p><pre><code>./configure --prefix=/usr/local/nginx1.14 --with-http_dav_module --with-http_stub_status_module --with-http_addition_module  --with-http_sub_module --with-http_flv_module --with-http_mp4_module --with-pcre --with-http_ssl_module --with-http_gzip_static_module --user=nginx --group=nginxmake  &amp;&amp;  make install</code></pre><p>3.创建所需的用户<br>[root@bogon nginx-1.14.0]# useradd nginx -s /sbin/nologin -M<br>4.链接命令目录<br>[root@bogon nginx-1.14.0]#ln -s /usr/local/nginx1.14/sbin/nginx /usr/local/sbin/<br>5.开启nginx，查看端口<br>nginx<br>[root@bogon nginx-1.14.0]# netstat -anpt | grep 80<br>tcp        0      0 0.0.0.0:80              0.0.0.0:*           LISTEN 5671/nginx: master<br>6.修改/usr/local/nginx1.14/conf/nginx.conf<br><a href="https://s1.51cto.com/images/blog/201910/24/1aaadc6aa6cc737acd8a424b97504f4a.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=" target="_blank" rel="noopener">https://s1.51cto.com/images/blog/201910/24/1aaadc6aa6cc737acd8a424b97504f4a.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=</a><br>7.修改/etc/security/limits.conf<br><a href="https://s1.51cto.com/images/blog/201910/24/a508ad0b699d78707ff1ad65efbb0697.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=" target="_blank" rel="noopener">https://s1.51cto.com/images/blog/201910/24/a508ad0b699d78707ff1ad65efbb0697.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=</a><br>8.查看现在是否没变<br><a href="https://s1.51cto.com/images/blog/201910/24/648c8e34c917753ba68c9bc52f9bca4b.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=" target="_blank" rel="noopener">https://s1.51cto.com/images/blog/201910/24/648c8e34c917753ba68c9bc52f9bca4b.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=</a><br>9.注销一下，重新查看一下，是否变化<br><a href="https://s1.51cto.com/images/blog/201910/24/13371a17405ce555d9c2e7b69c5f8a73.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=" target="_blank" rel="noopener">https://s1.51cto.com/images/blog/201910/24/13371a17405ce555d9c2e7b69c5f8a73.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=</a><br>10.开启nginx，查看nginx变化<br><a href="https://s1.51cto.com/images/blog/201910/24/f2ad409eac8bec625c5f0e6e4242a203.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=" target="_blank" rel="noopener">https://s1.51cto.com/images/blog/201910/24/f2ad409eac8bec625c5f0e6e4242a203.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=</a></p><p><a href="https://s1.51cto.com/images/blog/201910/24/460c4a55096b042947f2c144b0d2d403.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=" target="_blank" rel="noopener">https://s1.51cto.com/images/blog/201910/24/460c4a55096b042947f2c144b0d2d403.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=</a>)<br><a href="https://s1.51cto.com/images/blog/201910/24/ec3da9517fee664957794f6508a714dd.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=" target="_blank" rel="noopener">https://s1.51cto.com/images/blog/201910/24/ec3da9517fee664957794f6508a714dd.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=</a><br>11.页面访问一下nginx<br><img src="/" class="lazyload" data-src="https://s1.51cto.com/images/blog/201910/24/b044fd450fbb18a8ed1b24e5e6954dee.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk="  alt=""></p><p>二、部署 LNMP （部署环境需求：nginx和php在一台，再开启一台apache做防盗链）<br>1.安装php<br>首先安装libmcrypt 和依赖包<br><img src="/" class="lazyload" data-src="/posts/lor_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk="  alt=""><br><img src="/" class="lazyload" data-src="https://s1.51cto.com/images/blog/201910/24/7de79265df81e71f3639ea6bc663bb82.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk="  alt=""></p><p>编译安装php</p><pre><code>./configure  --prefix=/usr/local/php5.6  --with-mysql=mysqlnd   --with-pdo-mysql=mysqlnd --with-mysqli=mysqlnd   --with-openssl --enable-fpm --enable-sockets  --enable-sysvshm  --enable-mbstring --with-freetype-dir --with-jpeg-dir --with-png-dir --with-zlib  --with-libxml-dir=/usr --enable-xml  --with-mhash  --with-mcrypt=/usr/local/libmcrypt  --with-config-file-path=/etc --with-config-file-scan-dir=/etc/php.d --with-bz2 --enable-maintainer-ztsmake &amp;&amp; make intall</code></pre><p>提供 php 配置文件<br>[root@bogon php-5.6.27]# cp php.ini-production  /etc/php.ini</p><p>为 php-fpm 提供脚本<br><img src="/" class="lazyload" data-src="https://s1.51cto.com/images/blog/201910/24/d397ef1d67c0b73e88fdd43f75c04250.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk="  alt=""></p><p>提供 php-fpm 配置文件并编辑<br><img src="/" class="lazyload" data-src="https://s1.51cto.com/images/blog/201910/24/55501fc53895eb8988bb32024b97b79b.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk="  alt=""><br>修改/usr/local/php5.6/etc/php-fpm.conf</p><p>启动php 查看9000端口</p><pre><code>/etc/init.d/php-fpm  start[root@bogon etc]# netstat -anpt | grep 9000tcp        0      0 0.0.0.0:9000            0.0.0.0:*               LISTEN</code></pre><p>修改/usr/local/nginx1.14/conf/nginx.conf<br><img src="/" class="lazyload" data-src="https://s1.51cto.com/images/blog/201910/24/69dc6a47aebff7e1aba5d77f36661635.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk="  alt=""><br><img src="/" class="lazyload" data-src="https://s1.51cto.com/images/blog/201910/24/a93aec7853a978374ff02d8e6858dfe8.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk="  alt=""></p><p>创建/usr/local/nginx1.14/html/index.php<br><img src="/" class="lazyload" data-src="https://s1.51cto.com/images/blog/201910/24/89c17abe068b68a6ee991a6ab6348b03.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk="  alt=""><br><img src="/" class="lazyload" data-src="https://s1.51cto.com/images/blog/201910/24/a02e66d9f5d5cc8695781c6b93bf2ac7.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk="  alt=""></p><p>测试<br><img src="/" class="lazyload" data-src="https://s1.51cto.com/images/blog/201910/24/d8afb06d3b51ff455eeebacc37ca5282.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk="  alt=""></p><pre><code>内核参数优化 添加 /etc//sysctl.conffs.file-max = 999999net.ipv4.ip_forward = 0net.ipv4.conf.default.rp_filter = 1net.ipv4.conf.default.accept_source_route = 0kernel.sysrq = 0kernel.core_uses_pid = 1net.ipv4.tcp_syncookies = 1kernel.msgmnb = 65536kernel.msgmax = 65536kernel.shmmax = 68719476736kernel.shmall = 4294967296net.ipv4.tcp_max_tw_buckets = 6000net.ipv4.tcp_sack = 1net.ipv4.tcp_window_scaling = 1net.ipv4.tcp_rmem = 10240 87380 12582912net.ipv4.tcp_wmem = 10240 87380 12582912net.core.wmem_default = 8388608net.core.rmem_default = 8388608net.core.rmem_max = 16777216net.core.wmem_max = 16777216net.core.netdev_max_backlog = 262144net.core.somaxconn = 40960net.ipv4.tcp_max_orphans = 3276800net.ipv4.tcp_max_syn_backlog = 262144net.ipv4.tcp_timestamps = 0net.ipv4.tcp_synack_retries = 1net.ipv4.tcp_syn_retries = 1net.ipv4.tcp_tw_recycle = 1net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_mem = 94500000 915000000 927000000net.ipv4.tcp_fin_timeout = 1net.ipv4.tcp_keepalive_time = 30net.ipv4.ip_local_port_range = 1024 65000执行 sysctl -p 使内核修改生效</code></pre><p><img src="/" class="lazyload" data-src="https://s1.51cto.com/images/blog/201910/24/713c229373aea83e02c0a8e4d20bcdab.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk="  alt=""><br>查看更新值<br><img src="/" class="lazyload" data-src="https://s1.51cto.com/images/blog/201910/24/c0bbf25a5e53f9b1afac5af2ff94e034.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk="  alt=""></p><p>修改/usr/local/nginx1.14/conf/nginx.conf<br><img src="/" class="lazyload" data-src="https://s1.51cto.com/images/blog/201910/24/955e6fc97af123b4317de1ac734d9b82.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk="  alt=""></p><pre><code>sendfile on;tcp_nopush on;keepalive_timeout 65;tcp_nodelay on;client_header_buffer_size 4k;open_file_cache max=102400 inactive=20s;open_file_cache_valid 30s;open_file_cache_min_uses 1;client_header_timeout 15;client_body_timeout 15;reset_timedout_connection on;send_timeout 15;server_tokens off;client_max_body_size 10m;fastcgi_connect_timeout 600;fastcgi_send_timeout 600;fastcgi_read_timeout 600;fastcgi_buffer_size 64k;fastcgi_buffers 4 64k;fastcgi_busy_buffers_size 128k;fastcgi_temp_file_write_size 128k;fastcgi_temp_path /usr/local/nginx1.10/nginx_tmp;fastcgi_intercept_errors on;fastcgi_cache_path /usr/local/nginx1.10/fastcgi_cache levels=1:2keys_zone=cache_fastcgi:128m inactive=1d max_size=10g;gzip on;gzip_min_length 2k;gzip_buffers 4 32k;gzip_http_version 1.1;gzip_comp_level 6;gzip_types text/plain text/css text/javascript application/json application/javascriptapplication/x-javascript application/xml;gzip_vary on;gzip_proxied anylocation ~* ^.+\.(jpg|gif|png|swf|flv|wma|wmv|asf|mp3|mmf|zip|rar)$ {                valid_referers none blocked 192.168.1.50；                if ($invalid_referer) {                #return 302 http://www.benet.com/img/nolink.jpg;                return 404;                break;                }                access_log off;        }location / {            root   html;            index  index.php  index.html index.htm;        }location /status {stub_status on;}把之前php测试页注释掉</code></pre><p><img src="/" class="lazyload" data-src="https://s1.51cto.com/images/blog/201910/24/67ccc5fd235260f2cf030e4f604168a5.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk="  alt=""></p><pre><code>location ~ .*\.(php|php5)?$ {root html;fastcgi_pass 127.0.0.1:9000;fastcgi_index index.php;include fastcgi.conf;fastcgi_cache cache_fastcgi;fastcgi_cache_valid 200 302 1h;fastcgi_cache_valid 301 1d;fastcgi_cache_valid any 1m;fastcgi_cache_min_uses 1;fastcgi_cache_use_stale error timeout invalid_header http_500;fastcgi_cache_key http://$host$request_uri;}</code></pre><p>重启php<br>nginx   -s  reload</p><p>测试</p><p><img src="/" class="lazyload" data-src="https://s1.51cto.com/images/blog/201910/24/77bacba9f977ef69165d77bb52ca576b.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk="  alt=""></p><p>三、验证、压力测试<br>查看ab的安装包<br>yum provides ab</p><p>安装httpd-tools<br>yum -y install httpd-tools</p><p>压力测试<br>ab -c 2000 -n 10000 192.168.1.50/index.html<br><img src="/" class="lazyload" data-src="https://s1.51cto.com/images/blog/201910/24/441c6635cdcedaa3d7c38bdd66b9f0bd.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk="  alt=""></p><p>ab -c 2000 -n 10000 192.168.1.50/index.php<br><img src="/" class="lazyload" data-src="https://s1.51cto.com/images/blog/201910/24/de29a59b7a5a96260bf978504da2599b.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk="  alt=""><br>两次压力测试做对比</p><p>验证防盗链<br>使用 apache 做为一个测试站点，域名为 <a href="http://www.test.com" target="_blank" rel="noopener">www.test.com</a>，在测试页上做一个超链接，链接 nginx</p><p>在apache主机上创建vim /var/www/html/index.html<br><img src="/" class="lazyload" data-src="https://s1.51cto.com/images/blog/201910/24/ed574cbe8e06795ca0a54e01ffb1430b.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk="  alt=""><br>开启httpd<br>systemctl  start httpd</p><p>测试</p><p><img src="/" class="lazyload" data-src="https://s1.51cto.com/images/blog/201910/24/c57c74b59eba91b5faf2c3c3d739143c.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk="  alt=""><br><img src="/" class="lazyload" data-src="https://s1.51cto.com/images/blog/201910/24/1b699af52241e6050626b9ed6800bd31.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk="  alt=""></p><p>论坛<br>Php服务器解压 压缩包<br>unzip Discuz_7.0.0_FULL_SC_UTF8.zip<br>cd Discuz_7.0.0_FULL_SC_UTF8/<br>ls<br>ls upload/<br><img src="/" class="lazyload" data-src="https://s1.51cto.com/images/blog/201910/24/abbfa3c66befd53d7187a49e35d42ee1.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk="  alt=""><br>修改/etc/php.ini<br><img src="/" class="lazyload" data-src="https://s1.51cto.com/images/blog/201910/24/51b84e704f1eaf27bbfcd82918b42668.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk="  alt=""></p><p>Mysql服务器</p><p>进入MySQL<br>mysql -u root  -p123</p><p>创建数据库<br>create database bbs;</p><p>grant all on <em>.</em>  to  root@‘192.168.1.%’ identified by ‘<a href="http://123.com" target="_blank" rel="noopener">123.com</a>’;</p><p>测试 浏览器输入  <a href="http://xn--IP-im8ckc388bqo2btxq/bbs/install" target="_blank" rel="noopener">http://自己的IP地址/bbs/install</a><br><img src="/" class="lazyload" data-src="https://s1.51cto.com/101?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk="  alt=""><a href="http://xn--ip-im8ckc388bqo2btxq/bbs/install" target="_blank" rel="noopener">http://自己的ip地址/bbs/install</a>)</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 优化 </tag>
            
            <tag> 防盗链 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
    
    
    <entry>
      <title>contact</title>
      <link href="/contact/index.html"/>
      <url>/contact/index.html</url>
      
        <content type="html"><![CDATA[<script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>tags</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      
        <content type="html"><![CDATA[<script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>categories</title>
      <link href="/categories/index.html"/>
      <url>/categories/index.html</url>
      
        <content type="html"><![CDATA[<script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/lib/canvas-nest/canvas-nest-nomobile.min.js"/>
      <url>/lib/canvas-nest/canvas-nest-nomobile.min.js</url>
      
        <content type="html"><![CDATA[!function(){var userAgentInfo = navigator.userAgent;    var Agents = ["iPad", "iPhone", "Android",           "SymbianOS", "Windows Phone",           "iPod", "webOS", "BlackBerry", "IEMobile"];    for (var v = 0; v &lt; Agents.length; v++) {      if (userAgentInfo.indexOf(Agents[v]) &gt; 0) {        return;      }    }function o(w,v,i){return w.getAttribute(v)||i}function j(i){return document.getElementsByTagName(i)}function l(){var i=j("script"),w=i.length,v=i[w-1];return{l:w,z:o(v,"zIndex",-1),o:o(v,"opacity",0.5),c:o(v,"color","0,0,0"),n:o(v,"count",99)}}function k(){r=u.width=window.innerWidth||document.documentElement.clientWidth||document.body.clientWidth,n=u.height=window.innerHeight||document.documentElement.clientHeight||document.body.clientHeight}function b(){e.clearRect(0,0,r,n);var w=[f].concat(t);var x,v,A,B,z,y;t.forEach(function(i){i.x+=i.xa,i.y+=i.ya,i.xa*=i.x&gt;r||i.x&lt;0?-1:1,i.ya*=i.y&gt;n||i.y&lt;0?-1:1,e.fillRect(i.x-0.5,i.y-0.5,1,1);for(v=0;v<w.length;v++){x=w[v];if(i!==x&&null!==x.x&&null!==x.y){b=i.x-x.x,z=i.y-x.y,y=b*b+z*z;y<x.max&&(x===f&&y>=x.max/2&amp;&amp;(i.x-=0.03*B,i.y-=0.03*z),A=(x.max-y)/x.max,e.beginPath(),e.lineWidth=A/2,e.strokeStyle="rgba("+s.c+","+(A+0.2)+")",e.moveTo(i.x,i.y),e.lineTo(x.x,x.y),e.stroke())}}w.splice(w.indexOf(i),1)}),m(b)}var u=document.createElement("canvas"),s=l(),c="c_n"+s.l,e=u.getContext("2d"),r,n,m=window.requestAnimationFrame||window.webkitRequestAnimationFrame||window.mozRequestAnimationFrame||window.oRequestAnimationFrame||window.msRequestAnimationFrame||function(i){window.setTimeout(i,1000/45)},a=Math.random,f={x:null,y:null,max:20000};u.id=c;u.style.cssText="position:fixed;top:0;left:0;z-index:"+s.z+";opacity:"+s.o;j("body")[0].appendChild(u);k(),window.onresize=k;window.onmousemove=function(i){i=i||window.event,f.x=i.clientX,f.y=i.clientY},window.onmouseout=function(){f.x=null,f.y=null};for(var t=[],p=0;s.n&gt;p;p++){var h=a()*r,g=a()*n,q=2*a()-1,d=2*a()-1;t.push({x:h,y:g,xa:q,ya:d,max:6000})}setTimeout(function(){b()},100)}();<!--0?-1:1,e.fillRect(i.x-0.5,i.y-0.5,1,1);for(v=0;v<w.length;v++){x=w[v];if(i!==x&&null!==x.x&&null!==x.y){B=i.x-x.x,z=i.y-x.y,y=B*B+z*z;y<x.max&&(x===f&&y--><!--0?-1:1,i.ya*=i.y--></w.length;v++){x=w[v];if(i!==x&&null!==x.x&&null!==x.y){b=i.x-x.x,z=i.y-x.y,y=b*b+z*z;y<x.max&&(x===f&&y><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/lib/canvas-nest/README.html"/>
      <url>/lib/canvas-nest/README.html</url>
      
        <content type="html"><![CDATA[<h1>Theme NexT Canvas Nest</h1><p><img src="/" class="lazyload" data-src="https://img.shields.io/badge/NexT-v7.3.0+-blue?style=flat-square"  alt="Theme Version"></p><p><a href="https://github.com/hustcc/canvas-nest.js" target="_blank" rel="noopener">canvas-nest.js</a> for <a href="https://github.com/theme-next" target="_blank" rel="noopener">NexT</a>.</p><h2 id="Install">Install</h2><h3 id="Step-1-→-Go-to-Hexo-dir">Step 1 → Go to Hexo dir</h3><p>Change dir to <strong>Hexo</strong> directory. There must be <code>scaffolds</code>, <code>source</code>, <code>themes</code> and other directories:</p><pre class=" language-language-sh"><code class="language-language-sh">$ cd hexo$ lsscaffolds  source  themes  _config.yml  package.json</code></pre><h3 id="Step-2-→-Create-footer-swig">Step 2 → Create <code>footer.swig</code></h3><p>Create a file named <code>footer.swig</code> in <code>hexo/source/_data</code> directory (create <code>_data</code> directory if it does not exist).</p><p>Edit this file and add the following content:</p><pre class=" language-language-xml"><code class="language-language-xml"><script color="0,0,255" opacity="0.5" zIndex="-1" count="99" src="https://cdn.jsdelivr.net/npm/canvas-nest.js@1/dist/canvas-nest.js"></script></code></pre><p>You can customize these options.</p><h3 id="Step-3-→-Set-it-up">Step 3 → Set it up</h3><p>In the NexT <code>_config.yml</code>, uncomment <code>footer</code> under the <code>custom_file_path</code> section.</p><pre class=" language-language-yml"><code class="language-language-yml"># Define custom file paths.# Create your custom files in site directory `source/_data` and uncomment needed files below.custom_file_path:  #head: source/_data/head.swig  #header: source/_data/header.swig  #sidebar: source/_data/sidebar.swig  #postMeta: source/_data/post-meta.swig  #postBodyEnd: source/_data/post-body-end.swig  footer: source/_data/footer.swig  #bodyEnd: source/_data/body-end.swig  #variable: source/_data/variables.styl  #mixin: source/_data/mixins.styl  #style: source/_data/styles.styl</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/lib/canvas-nest/canvas-nest.min.js"/>
      <url>/lib/canvas-nest/canvas-nest.min.js</url>
      
        <content type="html"><![CDATA[!function(){function o(w,v,i){return w.getAttribute(v)||i}function j(i){return document.getElementsByTagName(i)}function l(){var i=j("script"),w=i.length,v=i[w-1];return{l:w,z:o(v,"zIndex",-1),o:o(v,"opacity",0.5),c:o(v,"color","0,0,0"),n:o(v,"count",99)}}function k(){r=u.width=window.innerWidth||document.documentElement.clientWidth||document.body.clientWidth,n=u.height=window.innerHeight||document.documentElement.clientHeight||document.body.clientHeight}function b(){e.clearRect(0,0,r,n);var w=[f].concat(t);var x,v,A,B,z,y;t.forEach(function(i){i.x+=i.xa,i.y+=i.ya,i.xa*=i.x&gt;r||i.x&lt;0?-1:1,i.ya*=i.y&gt;n||i.y&lt;0?-1:1,e.fillRect(i.x-0.5,i.y-0.5,1,1);for(v=0;v<w.length;v++){x=w[v];if(i!==x&&null!==x.x&&null!==x.y){b=i.x-x.x,z=i.y-x.y,y=b*b+z*z;y<x.max&&(x===f&&y>=x.max/2&amp;&amp;(i.x-=0.03*B,i.y-=0.03*z),A=(x.max-y)/x.max,e.beginPath(),e.lineWidth=A/2,e.strokeStyle="rgba("+s.c+","+(A+0.2)+")",e.moveTo(i.x,i.y),e.lineTo(x.x,x.y),e.stroke())}}w.splice(w.indexOf(i),1)}),m(b)}var u=document.createElement("canvas"),s=l(),c="c_n"+s.l,e=u.getContext("2d"),r,n,m=window.requestAnimationFrame||window.webkitRequestAnimationFrame||window.mozRequestAnimationFrame||window.oRequestAnimationFrame||window.msRequestAnimationFrame||function(i){window.setTimeout(i,1000/45)},a=Math.random,f={x:null,y:null,max:20000};u.id=c;u.style.cssText="position:fixed;top:0;left:0;z-index:"+s.z+";opacity:"+s.o;j("body")[0].appendChild(u);k(),window.onresize=k;window.onmousemove=function(i){i=i||window.event,f.x=i.clientX,f.y=i.clientY},window.onmouseout=function(){f.x=null,f.y=null};for(var t=[],p=0;s.n&gt;p;p++){var h=a()*r,g=a()*n,q=2*a()-1,d=2*a()-1;t.push({x:h,y:g,xa:q,ya:d,max:6000})}setTimeout(function(){b()},100)}();<!--0?-1:1,e.fillRect(i.x-0.5,i.y-0.5,1,1);for(v=0;v<w.length;v++){x=w[v];if(i!==x&&null!==x.x&&null!==x.y){B=i.x-x.x,z=i.y-x.y,y=B*B+z*z;y<x.max&&(x===f&&y--><!--0?-1:1,i.ya*=i.y--></w.length;v++){x=w[v];if(i!==x&&null!==x.x&&null!==x.y){b=i.x-x.x,z=i.y-x.y,y=b*b+z*z;y<x.max&&(x===f&&y><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
    </entry>
    
    
  
</search>
