<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Small steel gun article"><meta name="keywords" content=""><meta name="author" content="Wu Shao Dong"><meta name="copyright" content="Wu Shao Dong"><title>Today is still beautiful | Xgp &amp; Blog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.7.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.7.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
} </script><meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="Xgp & Blog" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="false"><div class="author-info"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">Wu Shao Dong</div><div class="author-info__description text-center">Small steel gun article</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">16</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">3</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">2</span></a></div></div></div><nav class="no-bg" id="nav"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Xgp &amp; Blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"></span></div><div id="site-info"><div id="site-title">Xgp &amp; Blog</div><div id="site-sub-title">Today is still beautiful</div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/posts/595e.html">post_name</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-04-25</time><div class="content"><script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script></div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/posts/c224.html">20 k8s的helm模板</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-04-25</time><div class="content"><h1 id="自定义helm模板"><a href="#自定义helm模板" class="headerlink" title="自定义helm模板"></a>自定义helm模板</h1><p><a href="https://hub.helm.sh/" target="_blank" rel="noopener">https://hub.helm.sh/</a></p>
<h2 id="1、开发自己的chare包"><a href="#1、开发自己的chare包" class="headerlink" title="1、开发自己的chare包"></a>1、开发自己的chare包</h2><pre><code>[root@master ~]# helm create mychare
//创建一个名为mychare的chare包
[root@master ~]# tree -C mychare/
//以树状图查看一下chare包
mychare/
├── charts
├── Chart.yaml
├── templates
│   ├── deployment.yaml
│   ├── _helpers.tpl
│   ├── ingress.yaml
│   ├── NOTES.txt
│   ├── service.yaml
│   └── tests
│       └── test-connection.yaml
└── values.yaml</code></pre><h2 id="2、调试chart"><a href="#2、调试chart" class="headerlink" title="2、调试chart"></a>2、调试chart</h2><pre><code>[root@master mychare]# cd
[root@master ~]# helm install --dry-run --debug mychare
//检查这个mychare是否有问题</code></pre><h2 id="3、安装chart"><a href="#3、安装chart" class="headerlink" title="3、安装chart"></a>3、安装chart</h2><pre><code>[root@node02 ~]# docker pull nginx:stable</code></pre><h3 id="（1）通过仓库安装"><a href="#（1）通过仓库安装" class="headerlink" title="（1）通过仓库安装"></a>（1）通过仓库安装</h3><pre><code>[root@master mychare]# helm search redis
//搜索chare包</code></pre><pre><code>[root@master mychare]# helm repo list
//查看是否有能访问仓库</code></pre><pre><code>[root@master mychare]# helm install stable/redis
//安装</code></pre><h3 id="（2）通过tar包安装"><a href="#（2）通过tar包安装" class="headerlink" title="（2）通过tar包安装"></a>（2）通过tar包安装</h3><pre><code>[root@master ~]# helm fetch stable/redis
//直接下载chare包
[root@master ~]# tar -zxf redis-1.1.15.tgz
//解压下载的chare包
[root@master ~]# tree -C redis
redis
├── Chart.yaml
├── README.md
├── templates
│   ├── deployment.yaml
│   ├── _helpers.tpl
│   ├── networkpolicy.yaml
│   ├── NOTES.txt
│   ├── pvc.yaml
│   ├── secrets.yaml
│   └── svc.yaml
└── values.yaml</code></pre><h3 id="（3）通过chare本地目录安装"><a href="#（3）通过chare本地目录安装" class="headerlink" title="（3）通过chare本地目录安装"></a>（3）通过chare本地目录安装</h3><pre><code>[root@master ~]# helm fetch stable/redis
//直接下载chare包
[root@master ~]# tar -zxf redis-1.1.15.tgz
//解压下载的chare包
[root@master ~]# helm install redis</code></pre><h3 id="（4）通过URL安装"><a href="#（4）通过URL安装" class="headerlink" title="（4）通过URL安装"></a>（4）通过URL安装</h3><pre><code>[root@master ~]# helm install https://example.com/charts/foo-1.2.3.tgz</code></pre><p>使用本地目录安装：</p>
<pre><code>[root@master ~]# cd mychare/
[root@master mychare]# vim values.yaml </code></pre><p>![image-20200304094840738](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304094840738.png)</p>
<pre><code>[root@master mychare]# cd templates/
[root@master templates]# vim service.yaml </code></pre><p>![image-20200304095647172](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304095647172.png)</p>
<pre><code>[root@master templates]# cd ..
[root@master mychare]# helm install -n test ../mychare/
[root@master ~]# helm upgrade test mychare/ -f  mychare/values.yaml </code></pre><h2 id="4、例子"><a href="#4、例子" class="headerlink" title="4、例子"></a>4、例子</h2><p><strong>使用mychart部署一个实例: xgp。使用镜像为私有镜像v1 版本。</strong></p>
<p><strong>完成之后，镜像版本。</strong></p>
<p><strong>全部成功之后，将实例做一个升级，将镜像改为v2版本。</strong></p>
<h3 id="更改镜像为私有镜像"><a href="#更改镜像为私有镜像" class="headerlink" title="更改镜像为私有镜像"></a>更改镜像为私有镜像</h3><pre><code>[root@master ~]# vim mychare/values.yaml</code></pre><p>![image-20200304104416415](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304104416415.png)</p>
<pre><code>[root@master ~]#  helm install -n xgp mychare/ -f mychare/values.yaml
[root@master ~]# kubectl get deployments. -o wide</code></pre><p>![image-20200304104645260](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304104645260.png)</p>
<pre><code>[root@master ~]# vim mychare/values.yaml</code></pre><p>![image-20200304105120894](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304105120894.png)</p>
<pre><code>[root@master ~]# helm upgrade  xgp mychare/  -f mychare/values.yaml 
[root@master ~]# kubectl get deployments. -o wide</code></pre><p>![image-20200304105211506](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304105211506.png)</p>
<pre><code>[root@master ~]# kubectl edit deployments. xgp-mychare</code></pre><p>![image-20200304105334541](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304105334541.png)</p>
<pre><code>[root@master ~]# kubectl get deployments. -o wide</code></pre><p>![image-20200304105359184](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304105359184.png)</p>
<h1 id="创建自己的Repo仓库"><a href="#创建自己的Repo仓库" class="headerlink" title="创建自己的Repo仓库"></a>创建自己的Repo仓库</h1><h2 id="1、node01启动一个httpd的容器"><a href="#1、node01启动一个httpd的容器" class="headerlink" title="1、node01启动一个httpd的容器"></a>1、node01启动一个httpd的容器</h2><pre><code>[root@node01 ~]# mkdir /var/xgp
//创建一个目录
[root@node01 ~]# docker pull httpd
//下载httpd镜像
[root@node02 ~]# docker run -d -p 8080:80 -v /var/xgp:/usr/local/apache2/htdocs httpd
//启动一个httpd的容器</code></pre><h2 id="2、master节点上，将mychart目录打包。"><a href="#2、master节点上，将mychart目录打包。" class="headerlink" title="2、master节点上，将mychart目录打包。"></a>2、master节点上，将mychart目录打包。</h2><pre><code>[root@master ~]# helm package mychare/
Successfully packaged chart and saved it to: /root/mychare-0.1.0.tgz</code></pre><h2 id="3、生成仓库的index文件。"><a href="#3、生成仓库的index文件。" class="headerlink" title="3、生成仓库的index文件。"></a>3、生成仓库的index文件。</h2><pre><code>[root@master ~]# mkdir myrepo
//创建一个目录存放打包的chare
[root@master ~]# mv mychare-0.1.0.tgz myrepo/
//移动打包好的文件
[root@master ~]# helm repo index myrepo/ --url http://192.168.1.22:8080/charts
//生成仓库的index文件
[root@master ~]# ls myrepo/
index.yaml  mychare-0.1.0.tgz</code></pre><h2 id="4、将生成的tar包和index-yaml上传到node01的-var-www-charts目录下"><a href="#4、将生成的tar包和index-yaml上传到node01的-var-www-charts目录下" class="headerlink" title="4、将生成的tar包和index.yaml上传到node01的/var/www/charts目录下."></a>4、将生成的tar包和index.yaml上传到node01的/var/www/charts目录下.</h2><h3 id="node01创建目录"><a href="#node01创建目录" class="headerlink" title="node01创建目录"></a>node01创建目录</h3><pre><code>[root@node01 ~]# mkdir /var/xgp/charts</code></pre><h3 id="master移动动到"><a href="#master移动动到" class="headerlink" title="master移动动到"></a>master移动动到</h3><pre><code>[root@master ~]# scp myrepo/* node01:/var/xgp/charts/</code></pre><h3 id="node01查看一下"><a href="#node01查看一下" class="headerlink" title="node01查看一下"></a>node01查看一下</h3><pre><code>[root@node01 ~]# ls /var/xgp/charts/
index.yaml  mychare-0.1.0.tgz</code></pre><h2 id="5、添加新的repo仓库。"><a href="#5、添加新的repo仓库。" class="headerlink" title="5、添加新的repo仓库。"></a>5、添加新的repo仓库。</h2><pre><code>[root@master ~]# helm repo add newrepo http://192.168.1.22:8080/charts</code></pre><pre><code>[root@master ~]# helm repo list</code></pre><p>![image-20200304112410286](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304112410286.png)</p>
<pre><code>[root@master ~]# helm search mychare</code></pre><p>![image-20200304112443931](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304112443931.png)</p>
<h2 id="6、我们就可以直接使用新的repo仓库部署实例了。"><a href="#6、我们就可以直接使用新的repo仓库部署实例了。" class="headerlink" title="6、我们就可以直接使用新的repo仓库部署实例了。"></a>6、我们就可以直接使用新的repo仓库部署实例了。</h2><pre><code>[root@master ~]# helm install newrepo/mychare -n wsd</code></pre><pre><code>[root@master ~]# helm list </code></pre><p>![image-20200304112515084](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304112515084.png)</p>
<h2 id="7-如果以后仓库中新添加了chart包-需要用helm-repo-update命玲更新本地的index文件。"><a href="#7-如果以后仓库中新添加了chart包-需要用helm-repo-update命玲更新本地的index文件。" class="headerlink" title="7.如果以后仓库中新添加了chart包,需要用helm repo update命玲更新本地的index文件。"></a>7.如果以后仓库中新添加了chart包,需要用helm repo update命玲更新本地的index文件。</h2><p> 练习：<br>    新创建一个bdqn.的chart包。然后将chart包上传到上述repo源中。</p>
<pre><code>[root@master ~]# helm create bdqn
[root@master ~]# helm package bdqn/
[root@master ~]# mv bdqn-0.1.0.tgz myrepo/
[root@master ~]#  helm repo index myrepo/ --url http://192.168.1.22:8080/charts
[root@master myrepo]# scp bdqn-0.1.0.tgz index.yaml  node01:/var/xgp/charts
[root@master myrepo]# helm repo update
[root@master myrepo]# helm search bdqn
[root@master myrepo]# helm install http://192.168.1.22:8080/charts/bdqn-0.1.0.tgz
</code></pre><h2 id="1）创建helm的私有仓库，以自己的名字命名。"><a href="#1）创建helm的私有仓库，以自己的名字命名。" class="headerlink" title="1）创建helm的私有仓库，以自己的名字命名。"></a>1）创建helm的私有仓库，以自己的名字命名。</h2><h3 id="1、node01启动一个httpd的容器-1"><a href="#1、node01启动一个httpd的容器-1" class="headerlink" title="1、node01启动一个httpd的容器"></a>1、node01启动一个httpd的容器</h3><pre><code>[root@node01 ~]# mkdir /var/xgp
//创建一个目录
[root@node01 ~]# docker pull httpd
//下载httpd镜像
[root@node02 ~]# docker run -d -p 8080:80 -v /var/xgp:/usr/local/apache2/htdocs httpd
//启动一个httpd的容器</code></pre><h3 id="3、生成仓库的index文件。-1"><a href="#3、生成仓库的index文件。-1" class="headerlink" title="3、生成仓库的index文件。"></a>3、生成仓库的index文件。</h3><pre><code>[root@master ~]# mkdir xgprepo
//创建一个目录存放打包的chare
[root@master ~]# helm repo index xgprepo/ --url http://192.168.1.22:8080/charts
//生成仓库的index文件</code></pre><h3 id="4、将生成的index-yaml上传到node01的-var-www-charts目录下"><a href="#4、将生成的index-yaml上传到node01的-var-www-charts目录下" class="headerlink" title="4、将生成的index.yaml上传到node01的/var/www/charts目录下."></a>4、将生成的index.yaml上传到node01的/var/www/charts目录下.</h3><h4 id="node01创建目录-1"><a href="#node01创建目录-1" class="headerlink" title="node01创建目录"></a>node01创建目录</h4><pre><code>[root@node01 ~]# mkdir /var/xgp/charts</code></pre><h4 id="master移动动到-1"><a href="#master移动动到-1" class="headerlink" title="master移动动到"></a>master移动动到</h4><pre><code>[root@master ~]# scp xgprepo/* node01:/var/xgp/charts/</code></pre><h4 id="node01查看一下-1"><a href="#node01查看一下-1" class="headerlink" title="node01查看一下"></a>node01查看一下</h4><pre><code>[root@node01 ~]# ls /var/xgp/charts/
index.yaml  </code></pre><h3 id="5、添加新的repo仓库"><a href="#5、添加新的repo仓库" class="headerlink" title="5、添加新的repo仓库"></a>5、添加新的repo仓库</h3><pre><code>[root@master ~]# helm repo add xgp http://192.168.1.22:8080/charts
[root@master ~]# helm repo list </code></pre><p>![image-20200304132528938](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304132528938.png)</p>
<h2 id="2）-自定义一个chart包，要求这个包运行一个httpd的服务，使用私有镜像v1版本。3个副本Pod，service类型更改为NodePort，端口指定为-30000"><a href="#2）-自定义一个chart包，要求这个包运行一个httpd的服务，使用私有镜像v1版本。3个副本Pod，service类型更改为NodePort，端口指定为-30000" class="headerlink" title="2） 自定义一个chart包，要求这个包运行一个httpd的服务，使用私有镜像v1版本。3个副本Pod，service类型更改为NodePort，端口指定为:30000"></a>2） 自定义一个chart包，要求这个包运行一个httpd的服务，使用私有镜像v1版本。3个副本Pod，service类型更改为NodePort，端口指定为:30000</h2><h4 id="自定义一个chart包"><a href="#自定义一个chart包" class="headerlink" title="自定义一个chart包"></a>自定义一个chart包</h4><pre><code>[root@master ~]# helm create wsd
//创建一个名为wsd的chares包</code></pre><h4 id="按照要求修改配置文件"><a href="#按照要求修改配置文件" class="headerlink" title="按照要求修改配置文件"></a>按照要求修改配置文件</h4><pre><code>[root@master ~]# cd wsd/
//进入这个chart包
[root@master wsd]# vim values.yaml
//修改wsd的配置文件
replicaCount: 3                         #三个副本

image:
  repository: 192.168.1.21:5000/web      #更改镜像为私有镜像
  tag: v1                                #镜像标签v1
  pullPolicy: IfNotPresent              

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

service:
  type: NodePort              #修改模式为映射端口
  port: 80
  nodePort: 30000             #添加端口

[root@master wsd]# vim templates/service.yaml 

apiVersion: v1
kind: Service
metadata:
  name: {{ include "wsd.fullname" . }}
  labels:
{{ include "wsd.labels" . | indent 4 }}
spec:
  type: {{ .Values.service.type }}
  ports:
    - port: {{ .Values.service.port }}
      targetPort: http
      protocol: TCP
      name: http
      nodePort: {{ .Values.service.nodePort }}    #“添加”能让服务识别到nodePort的端口
  selector:
    app.kubernetes.io/name: {{ include "wsd.name" . }}
    app.kubernetes.io/instance: {{ .Release.Name }}</code></pre><h4 id="测试一下"><a href="#测试一下" class="headerlink" title="测试一下"></a>测试一下</h4><pre><code>[root@master ~]# helm install -n wsd  wsd/ -f wsd/values.yaml </code></pre><p>![image-20200304134959273](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304134959273.png)</p>
<h4 id="查看一下镜像版本"><a href="#查看一下镜像版本" class="headerlink" title="查看一下镜像版本"></a>查看一下镜像版本</h4><pre><code>[root@master ~]# kubectl get deployments. -o wide</code></pre><p>![image-20200304135106081](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304135106081.png)</p>
<h4 id="访问一下"><a href="#访问一下" class="headerlink" title="访问一下"></a>访问一下</h4><pre><code>[root@master ~]# curl 127.0.0.1:30000</code></pre><p>![image-20200304150609552](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304150609552.png)</p>
<h2 id="3-将实例进行更新，要求镜像生产v2版本。"><a href="#3-将实例进行更新，要求镜像生产v2版本。" class="headerlink" title="3)  将实例进行更新，要求镜像生产v2版本。"></a>3)  将实例进行更新，要求镜像生产v2版本。</h2><p><strong>私有镜像和官方镜像升级有所不同，官方的只需通过 （helm upgrade –set imageTag=“标签” 服务名称 charts包名 ）进行更改标签即可，而私有镜像需通过更改values.yaml中的标签才行比较麻烦一点。</strong></p>
<h3 id="1、修改values-yaml"><a href="#1、修改values-yaml" class="headerlink" title="1、修改values.yaml"></a>1、修改values.yaml</h3><pre><code>[root@master ~]# vim wsd/values.yaml 

# Default values for wsd.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 3

image:
  repository: 192.168.1.21:5000/web
  tag: v2                            #修改标签为v2
  pullPolicy: IfNotPresent
[root@master ~]# helm upgrade wsd wsd/ -f wsd/values.yaml
//基于配置文件刷新一下wsd服务</code></pre><h4 id="查看一下"><a href="#查看一下" class="headerlink" title="查看一下"></a>查看一下</h4><pre><code>[root@master ~]# kubectl get deployments. -o wide</code></pre><p>![image-20200304140054269](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304140054269.png)</p>
<h4 id="访问一下-1"><a href="#访问一下-1" class="headerlink" title="访问一下"></a>访问一下</h4><pre><code>[root@master ~]# curl 127.0.0.1:30000</code></pre><p>![image-20200304150742815](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304150742815.png)</p>
<h3 id="2、使用edit进行版本更新"><a href="#2、使用edit进行版本更新" class="headerlink" title="2、使用edit进行版本更新"></a>2、使用edit进行版本更新</h3><p><strong><em>确定wsd这个服务开启</em></strong></p>
<pre><code>[root@master ~]# kubectl edit deployments. wsd</code></pre><p>![](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304140425336.png)</p>
<h4 id="查看一下-1"><a href="#查看一下-1" class="headerlink" title="查看一下"></a>查看一下</h4><pre><code>[root@master ~]# kubectl get deployments. -o wide</code></pre><p>![image-20200304140520342](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304140520342.png)</p>
<h4 id="访问一下-2"><a href="#访问一下-2" class="headerlink" title="访问一下"></a>访问一下</h4><pre><code>[root@master ~]# curl 127.0.0.1:30000</code></pre><p>![image-20200304150839440](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304150839440.png)</p>
<h2 id="4）重新定义一个chart包，名称为-new-test-将这个包上传到上述私有仓库中。"><a href="#4）重新定义一个chart包，名称为-new-test-将这个包上传到上述私有仓库中。" class="headerlink" title="4）重新定义一个chart包，名称为: new-test,将这个包上传到上述私有仓库中。"></a>4）重新定义一个chart包，名称为: new-test,将这个包上传到上述私有仓库中。</h2><pre><code>[root@master ~]# helm repo list </code></pre><p>![image-20200304142059023](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304142059023.png)</p>
<pre><code>[root@master ~]# helm create xgp-wsd
//创建一个名为xgp-wsd的charts包

[root@master ~]# helm package xgp-wsd/
//将xgp-wsd打包在当前目录

[root@master ~]# mv xgp-wsd-0.1.0.tgz xgprepo/
//把打包文件放到仓库目录

[root@master ~]# helm repo index xgprepo/ --url http://192.168.1.22:8080/charts
//把仓库目录新加入的charts包信息记录在index.yaml中，使得其他加入的主机可以识别到，仓库的charts包

[root@master ~]# scp xgprepo/* node01:/var/xgp/charts
//将仓库目录的文件移动到httpd服务上，使各个主机可以访问，下载仓库的charts包

[root@master ~]# helm repo update 
//更新一下chart存储库</code></pre><h3 id="查看一下-2"><a href="#查看一下-2" class="headerlink" title="查看一下"></a>查看一下</h3><pre><code>[root@master ~]# helm search xgp-wsd</code></pre><p>![image-20200304142009776](E:\软件\博客\Blog\blog\source_posts\20 k8s的helm模板.assets\image-20200304142009776.png)</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script></div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/posts/2643.html">18 k8s的HPA自动扩容与缩容</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-04-25</time><div class="content"><h1 id="HPA"><a href="#HPA" class="headerlink" title="HPA"></a>HPA</h1><p><strong>可以根据当前Pod资源的使用率，比如说CPU、磁盘、内存等进行副本Pod的动态的扩容与缩容。</strong></p>
<p><strong>前提条件:系统应该能否获取到当前Pod的资源使用情况 (意思是可以执行kubectl top pod命令,并且能够得到反馈信息)。</strong></p>
<p><strong>heapster：这个组件之前是集成在k8s集群的,不过在1.12版本之后被移除了。如果还想使用此功能，应该部署metricServer, 这个k8s集群资源使用情况的聚合器。</strong></p>
<p><strong>这里，我们使用一个测试镜像， 这个镜像基于php-apache制作的docker镜像，包含了一些可以运行cpu密集计算任务的代码。</strong></p>
<h2 id="1、创建一个deployment控制器"><a href="#1、创建一个deployment控制器" class="headerlink" title="1、创建一个deployment控制器"></a>1、创建一个deployment控制器</h2><pre><code>[root@master ~]#docker pull mirrorgooglecontainers/hpa-example:latest
//下载hpa-example镜像

[root@master ~]# kubectl run php-apache --image=mirrorgooglecontainers/hpa-example --requests=cpu=200m --expose  --port=80
//基于hpa-example镜像，运行一个deployment控制器，请求CPU的资源为200m，暴露一个80端口</code></pre><h3 id="查看一下"><a href="#查看一下" class="headerlink" title="查看一下"></a>查看一下</h3><pre><code>[root@master ~]# kubectl get deployments.</code></pre><p>![image-20200228102643352](E:\软件\博客\Blog\blog\source_posts\18 HPA自动容与蒲容.assets\image-20200228102643352.png)</p>
<h2 id="2、创建HPA控制器"><a href="#2、创建HPA控制器" class="headerlink" title="2、创建HPA控制器"></a>2、创建HPA控制器</h2><pre><code>[root@master ~]# kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10
//当deployment资源对象的CPU使用率达到50%时，就进行扩容，最多可以扩容到10个</code></pre><h3 id="查看一下-1"><a href="#查看一下-1" class="headerlink" title="查看一下"></a>查看一下</h3><pre><code>[root@master ~]# kubectl get hpa</code></pre><p>![image-20200228101908398](E:\软件\博客\Blog\blog\source_posts\18 HPA自动容与蒲容.assets\image-20200228101908398.png)</p>
<h2 id="3、测试（master开启三个端口）"><a href="#3、测试（master开启三个端口）" class="headerlink" title="3、测试（master开启三个端口）"></a>3、测试（master开启三个端口）</h2><p><strong>新开启多个终端，对pod进行死循环请求php-apache的pod</strong></p>
<h3 id="端口一"><a href="#端口一" class="headerlink" title="端口一"></a>端口一</h3><h4 id="（1）创建一个应用，用来不停的访问我们刚刚创建的php-apache的svc资源。"><a href="#（1）创建一个应用，用来不停的访问我们刚刚创建的php-apache的svc资源。" class="headerlink" title="（1）创建一个应用，用来不停的访问我们刚刚创建的php-apache的svc资源。"></a>（1）创建一个应用，用来不停的访问我们刚刚创建的php-apache的svc资源。</h4><pre><code>[root@master ~]# kubectl run -i --tty load-generator --image=busybox /bin/sh</code></pre><h4 id="（2）进入Pod内，执行以下这条命令-用来模拟访问php-apache的svc资源。"><a href="#（2）进入Pod内，执行以下这条命令-用来模拟访问php-apache的svc资源。" class="headerlink" title="（2）进入Pod内，执行以下这条命令.用来模拟访问php-apache的svc资源。"></a>（2）进入Pod内，执行以下这条命令.用来模拟访问php-apache的svc资源。</h4><pre><code>[root@master ~]# while true; do wget -q -O- http://php-apache.default.svc.cluster.local ; done
//不停地向php-apache的svc资源，发送ok</code></pre><h3 id="端口二"><a href="#端口二" class="headerlink" title="端口二"></a>端口二</h3><pre><code>[root@master ~]# kubectl get hpa -w
//实时查看pod的cpu状态</code></pre><p>![image-20200228133816724](E:\软件\博客\Blog\blog\source_posts\18 k8s的HPA自动容与缩容.assets\image-20200228133816724.png)</p>
<p><strong>可以看到php-apache的cpu使用情况已经超过了50%</strong></p>
<h3 id="端口三"><a href="#端口三" class="headerlink" title="端口三"></a>端口三</h3><pre><code>[root@master images]# kubectl get pod -w
//实时查看pod的状态</code></pre><p>![image-20200228134105507](E:\软件\博客\Blog\blog\source_posts\18 k8s的HPA自动容与缩容.assets\image-20200228134105507.png)</p>
<p><strong>可以看到当php-apache的cpu使用情况超过50%后，就会不断生成新的php-apache来进行负载均衡（目前设置的上线时10个），当然，如果cpu使用情况下降到50%，master就会陆续地删除php-apache，这样的使用可以减少不必要的资源浪费、资源分配不均等情况。</strong></p>
<h1 id="二、资源限制"><a href="#二、资源限制" class="headerlink" title="二、资源限制"></a>二、资源限制</h1><h2 id="1、基于Pod"><a href="#1、基于Pod" class="headerlink" title="1、基于Pod"></a>1、基于Pod</h2><p><strong>Kubernetes对资源的限制实际上是通过cgroup来控制的，cgroup 是容器的一组用来控制内核如何运行进程的相关属性集合。针对内存、CPU 和各种设备都有对应的cgroup</strong></p>
<p><strong>默认情况下，Pod运行没有CPU和内存的限额。这意味着系统中的任何 Pod将能够像执行该Pod所在的节点一样，消耗足够多的CPU和内存。一般会针对某些应用的pod资源进行资源限制，这个资源限制是通过</strong></p>
<p><strong>resources的requests和limits来实现</strong></p>
<pre><code>[root@master ~]# vim cgroup-pod.yaml</code></pre><p>![image-20200228153809932](E:\软件\博客\Blog\blog\source_posts\18 k8s的HPA自动容与缩容.assets\image-20200228153809932.png)</p>
<p><strong>requests: 要分配的资源，limits为最高请求的资源值。可以简单的理解为初始值和最大值。</strong></p>
<h2 id="2、基于名称空间"><a href="#2、基于名称空间" class="headerlink" title="2、基于名称空间"></a><strong>2、基于名称空间</strong></h2><h3 id="1）-计算资源配额"><a href="#1）-计算资源配额" class="headerlink" title="1） 计算资源配额"></a>1） 计算资源配额</h3><pre><code>[root@master ~]# vim compute-resources.yaml</code></pre><p> ![image-20200228153818288](E:\软件\博客\Blog\blog\source_posts\18 k8s的HPA自动容与缩容.assets\image-20200228153818288.png)</p>
<h3 id="2）配置对象数量配额限制"><a href="#2）配置对象数量配额限制" class="headerlink" title="2）配置对象数量配额限制"></a>2）配置对象数量配额限制</h3><pre><code>[root@master ~]# vim object-counts.yaml</code></pre><p> ![image-20200228153828002](E:\软件\博客\Blog\blog\source_posts\18 k8s的HPA自动容与缩容.assets\image-20200228153828002.png)</p>
<h3 id="3）-配置CPU和内存的LimitRange"><a href="#3）-配置CPU和内存的LimitRange" class="headerlink" title="3） 配置CPU和内存的LimitRange"></a>3） 配置CPU和内存的LimitRange</h3><pre><code>[root@master ~]# vim limitRange.yaml</code></pre><p> ![image-20200228153834705](E:\软件\博客\Blog\blog\source_posts\18 k8s的HPA自动容与缩容.assets\image-20200228153834705.png)</p>
<p><strong>default 即 limit的值。</strong></p>
<p><strong>defaultRequest 即 request的值。</strong></p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script></div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/posts/a50d.html">14 k8s的Secret（密文）和configmap（明文）的使用教程</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-04-25</time><div class="content"><h1 id="一、Secret"><a href="#一、Secret" class="headerlink" title="一、Secret"></a>一、Secret</h1><p><strong><em>Secret :用来保存一些敏感信息，比如数据库的用户名密码或者秘钥。</em></strong></p>
<h2 id="举例-保存数据库的用户名和密码"><a href="#举例-保存数据库的用户名和密码" class="headerlink" title="举例:保存数据库的用户名和密码"></a>举例:保存数据库的用户名和密码</h2><blockquote>
<p><strong>用户名：</strong>        <strong>root</strong><br><strong>密码：</strong>           <strong>123.com</strong></p>
</blockquote>
<h3 id="1、通过–from-literal（文字的）"><a href="#1、通过–from-literal（文字的）" class="headerlink" title="1、通过–from-literal（文字的）"></a>1、通过–from-literal（文字的）</h3><pre><code>[root@master secret]# kubectl create secret generic mysecret1 --from-literal=username=root --from-literal=password=123.com</code></pre><blockquote>
<p><strong>generic：通用的，一般的加密方式</strong></p>
</blockquote>
<h4 id="查看一下"><a href="#查看一下" class="headerlink" title="查看一下"></a>查看一下</h4><pre><code>[root@master secret]# kubectl get secrets </code></pre><p><img src="/posts/a50d.htm/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C14.assets%5Cimage-20200214100419966.png" alt=""></p>
<p><strong>类型是Opaque（不透明的）</strong></p>
<h3 id="2、通过from-file（文件）"><a href="#2、通过from-file（文件）" class="headerlink" title="2、通过from-file（文件）"></a>2、通过from-file（文件）</h3><h4 id="新建两个文件并分别写入用户名和密码"><a href="#新建两个文件并分别写入用户名和密码" class="headerlink" title="新建两个文件并分别写入用户名和密码"></a>新建两个文件并分别写入用户名和密码</h4><pre><code>[root@master secret]# echo root &gt; username
[root@master secret]# echo 123.com  &gt; password</code></pre><h4 id="创建一个secret"><a href="#创建一个secret" class="headerlink" title="创建一个secret"></a>创建一个secret</h4><pre><code>[root@master secret]#  kubectl create secret generic mysecret2 --from-file=username --from-file=password </code></pre><h4 id="查看一下-1"><a href="#查看一下-1" class="headerlink" title="查看一下"></a>查看一下</h4><pre><code>[root@master secret]# kubectl get secrets</code></pre><p><img src="/posts/a50d.htm/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C14.assets%5Cimage-20200214103506842.png" alt="image-20200214103506842"></p>
<h3 id="3、通过–-from-env-file"><a href="#3、通过–-from-env-file" class="headerlink" title="3、通过– from- env-file:"></a>3、通过– from- env-file:</h3><h4 id="创建一个文件写入用户名和密码"><a href="#创建一个文件写入用户名和密码" class="headerlink" title="创建一个文件写入用户名和密码"></a>创建一个文件写入用户名和密码</h4><pre><code>[root@master secret]#vim env.txt 
username=root
password=123.com</code></pre><h4 id="创建一个secret-1"><a href="#创建一个secret-1" class="headerlink" title="创建一个secret"></a>创建一个secret</h4><pre><code>[root@master secret]# kubectl create secret generic mysecret3 --from-env-file=env.txt </code></pre><h4 id="查看一下-2"><a href="#查看一下-2" class="headerlink" title="查看一下"></a>查看一下</h4><pre><code>[root@master secret]# kubectl get secrets </code></pre><p><img src="/posts/a50d.htm/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C14.assets%5Cimage-20200214103905956.png" alt="image-20200214103905956"></p>
<h3 id="4、通过yaml配置文件"><a href="#4、通过yaml配置文件" class="headerlink" title="4、通过yaml配置文件"></a>4、通过yaml配置文件</h3><h4 id="（1）把需要保存的数据加密（”base64“的方式）"><a href="#（1）把需要保存的数据加密（”base64“的方式）" class="headerlink" title="（1）把需要保存的数据加密（”base64“的方式）"></a>（1）把需要保存的数据加密（”base64“的方式）</h4><pre><code>[root@master secret]# echo root | base64
cm9vdAo=
[root@master secret]# echo 123.com | base64
MTIzLmNvbQo=</code></pre><blockquote>
<p><strong>解码：</strong></p>
<pre><code>[root@master secret]# echo -n cm9vdAo | base64 --decode 
root
[root@master secret]# echo -n MTIzLmNvbQo | base64 --decode 
123.com</code></pre></blockquote>
<h4 id="（2）编写secre4的yaml文件"><a href="#（2）编写secre4的yaml文件" class="headerlink" title="（2）编写secre4的yaml文件"></a>（2）编写secre4的yaml文件</h4><pre><code>[root@master secret]# vim secret4.yaml
apiVersion: v1
kind: Secret
metadata:
  name: mysecret4
data:
  username: cm9vdAo=
  password: MTIzLmNvbQo=</code></pre><h5 id="执行一下"><a href="#执行一下" class="headerlink" title="执行一下"></a>执行一下</h5><pre><code>[root@master secret]# kubectl apply -f secret4.yaml </code></pre><h4 id="（3）查看一下"><a href="#（3）查看一下" class="headerlink" title="（3）查看一下"></a>（3）查看一下</h4><pre><code>[root@master secret]# kubectl get secrets </code></pre><p><img src="/posts/a50d.htm/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C14.assets%5Cimage-20200214104544899.png" alt="image-20200214104544899"></p>
<h2 id="如果来使用Secret资源"><a href="#如果来使用Secret资源" class="headerlink" title="如果来使用Secret资源"></a>如果来使用Secret资源</h2><h3 id="1-以Volume挂载的方式"><a href="#1-以Volume挂载的方式" class="headerlink" title="1. 以Volume挂载的方式"></a>1. 以Volume挂载的方式</h3><h4 id="编写pod的yaml文件"><a href="#编写pod的yaml文件" class="headerlink" title="编写pod的yaml文件"></a><strong>编写pod的yaml文件</strong></h4><pre><code>[root@master secret]# vim pod.yaml 

apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: mypod
    image: busybox
    args:
      - /bin/sh
      - -c
      - sleep 300000
    volumeMounts:
    - name: secret-test
      mountPath: "/etc/secret-test"  #pod中的路径
      readOnly: true                 #是否只读
  volumes:
  - name: secret-test
    secret:
      secretName: mysecret1</code></pre><p><strong>还可以自定义存放数据的文件名</strong></p>
<h4 id="执行一下-1"><a href="#执行一下-1" class="headerlink" title="执行一下"></a>执行一下</h4><pre><code>[root@master secret]# kubectl apply -f pod.yaml </code></pre><h4 id="进入容器查看保存的数据"><a href="#进入容器查看保存的数据" class="headerlink" title="进入容器查看保存的数据"></a>进入容器查看保存的数据</h4><pre><code>[root@master secret]# kubectl exec -it mypod /bin/sh
/ # cd /etc/secret-test/
/etc/secret-test # ls
pasword   username</code></pre><pre><code>/etc/secret-test # cat username 
root
/etc/secret-test # cat pasword 
123.com</code></pre><h4 id="测试是否有只读权限"><a href="#测试是否有只读权限" class="headerlink" title="测试是否有只读权限"></a>测试是否有只读权限</h4><pre><code>123.com/etc/secret-test # echo admin &gt; username
/bin/sh: can't create username: Read-only file system</code></pre><h3 id="1-1-自定义存放数据的文件名的yaml文件"><a href="#1-1-自定义存放数据的文件名的yaml文件" class="headerlink" title="1.1 自定义存放数据的文件名的yaml文件"></a>1.1 自定义存放数据的文件名的yaml文件</h3><pre><code>[root@master yaml]#  vim pod.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: mypod
    image: busybox
    args:
      - /bin/sh
      - -c
      - sleep 300000
    volumeMounts:
    - name: secret-test
      mountPath: "/etc/secret-test"  #pod中的路径
      readOnly: true                 #是否只读
  volumes:
  - name: secret-test
    secret:
      secretName: mysecret1
      items:
      - key: username
        path: my-group/my-username   #自定义的容器中的目录
      - key: password
        path: my-group/my-password   #自定义的容器中的目录</code></pre><h4 id="执行一下-2"><a href="#执行一下-2" class="headerlink" title="执行一下"></a>执行一下</h4><pre><code>[root@master yaml]# kubectl apply -f pod.yaml</code></pre><h4 id="查看一下-3"><a href="#查看一下-3" class="headerlink" title="查看一下"></a>查看一下</h4><pre><code>[root@master secret]# kubectl exec -it mypod /bin/sh
//进入容器查看
 # cat /etc/secret-test/my-group/my-password 
123.com 
 # cat /etc/secret-test/my-group/my-username 
root</code></pre><h3 id="1-2-如果，现在将secret资源内保存的数据进行更新，请问，使用此数据的应用内，数据是是否也会更新"><a href="#1-2-如果，现在将secret资源内保存的数据进行更新，请问，使用此数据的应用内，数据是是否也会更新" class="headerlink" title="1.2 如果，现在将secret资源内保存的数据进行更新，请问，使用此数据的应用内，数据是是否也会更新?"></a>1.2 如果，现在将secret资源内保存的数据进行更新，请问，使用此数据的应用内，数据是是否也会更新?</h3><p><strong>会实时更新(这里引用数据，是以volumes挂 载使用数据的方式)。</strong></p>
<p><strong>更新mysecret1的数据:   password  —&gt;  admin   YWRtaW4K (base64)</strong> </p>
<p><strong>可以通过edit 命令，直接修改。</strong></p>
<pre><code>[root@master secret]# kubectl edit secrets mysecret1</code></pre><p><img src="/posts/a50d.htm/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C14.assets%5Cimage-20200217162834490.png" alt="image-20200217162834490"></p>
<h4 id="查看一下-4"><a href="#查看一下-4" class="headerlink" title="查看一下"></a>查看一下</h4><pre><code>[root@master secret]# kubectl exec -it mypod /bin/sh
//进入容器查看
 # cat /etc/secret-test/my-group/my-password 
admin
 # cat /etc/secret-test/my-group/my-username 
root</code></pre><p><strong><em>数据已经成功更新了</em></strong></p>
<h3 id="2、以环境变量的方式"><a href="#2、以环境变量的方式" class="headerlink" title="2、以环境变量的方式"></a>2、以环境变量的方式</h3><p><strong>编写pod的yaml文件</strong></p>
<pre><code>[root@master secret]# vim pod-env.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: mypod2
spec:
  containers:
  - name: mypod
    image: busybox
    args:
      - /bin/sh
      - -c
      - sleep 300000
    env:
      - name: SECRET_USERNAME
        valueFrom:
          secretKeyRef:
            name: mysecret2
            key: username
      - name: SECRET_PASSWORD
        valueFrom:
          secretKeyRef:
            name: mysecret2
            key: password</code></pre><h4 id="执行一下-3"><a href="#执行一下-3" class="headerlink" title="执行一下"></a>执行一下</h4><pre><code>[root@master secret]# kubectl apply -f pod-env.yaml </code></pre><h4 id="查看一下-5"><a href="#查看一下-5" class="headerlink" title="查看一下"></a>查看一下</h4><pre><code>[root@master secret]# kubectl get pod</code></pre><p><img src="/posts/a50d.htm/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C14.assets%5Cimage-20200214111931566.png" alt="image-20200214111931566"></p>
<h4 id="进入容器查看保存的数据-1"><a href="#进入容器查看保存的数据-1" class="headerlink" title="进入容器查看保存的数据"></a>进入容器查看保存的数据</h4><pre><code>[root@master secret]# kubectl exec -it mypod2 /bin/sh
/ # echo $SECRET_USERNAME
root
/ # echo $SECRET_PASSWORD
123.com</code></pre><h3 id="2-1-更新sevret文件的内容"><a href="#2-1-更新sevret文件的内容" class="headerlink" title="2.1 更新sevret文件的内容"></a>2.1 更新sevret文件的内容</h3><pre><code> [root@master yaml]# kubectl edit secrets mysecret2
 //修改保存文件的内容</code></pre><p><img src="/posts/a50d.htm/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C14.assets%5Cimage-20200217162834490.png" alt="image-20200217162834490"></p>
<h4 id="查看一下-6"><a href="#查看一下-6" class="headerlink" title="查看一下"></a>查看一下</h4><pre><code>[root@master secret]# kubectl exec -it mypod2 /bin/sh
/ # echo $SECRET_USERNAME
root
/ # echo $SECRET_PASSWORD
123.com</code></pre><p><strong><em>等待了一定时间后，可以看到这个数据并没有没有改变</em></strong></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><strong>如果引用secret数据的应用， 要求会随着secret资源对象内保存的数据的更新，而实时更新，那么应该使用volumes挂载的方式引用资源因为用环境变量的方式引用不会实时更新数据。</strong></p>
<h1 id="二、ConfigMap"><a href="#二、ConfigMap" class="headerlink" title="二、ConfigMap"></a>二、ConfigMap</h1><p><strong>和Secret资源类似，不同之处在于，secret 资源保存的是敏感信息，而Configmap保存的是以明文方式存放的数据。</strong></p>
<blockquote>
<p><strong>username：adam</strong></p>
<p><strong>age：18</strong></p>
</blockquote>
<h2 id="创建的四种方式"><a href="#创建的四种方式" class="headerlink" title="创建的四种方式"></a>创建的四种方式</h2><h3 id="1、通过–-from-literal-文字的"><a href="#1、通过–-from-literal-文字的" class="headerlink" title="1、通过– from- literal(文字的):"></a>1、通过– from- literal(文字的):</h3><pre><code>[root@master yaml]# kubectl create configmap myconfigmap1 --from-literal=username=adam --from-literal=age=18</code></pre><h4 id="查看一下-7"><a href="#查看一下-7" class="headerlink" title="查看一下"></a>查看一下</h4><pre><code>[root@master yaml]# kubectl get cm</code></pre><p><img src="/posts/a50d.htm/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C14.assets%5Cimage-20200217103048235.png" alt="image-20200217103048235"></p>
<pre><code>[root@master yaml]# kubectl describe cm</code></pre><p><img src="/posts/a50d.htm/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C14.assets%5Cimage-20200217103123130.png" alt="image-20200217103123130"></p>
<h3 id="2、通过–from-file-文件"><a href="#2、通过–from-file-文件" class="headerlink" title="2、通过–from-file (文件) :"></a>2、通过–from-file (文件) :</h3><pre><code>[root@master yaml]# echo adam &gt; username
[root@master yaml]# echo 18 &gt; age</code></pre><h4 id="创建"><a href="#创建" class="headerlink" title="创建"></a>创建</h4><pre><code>[root@master yaml]# kubectl create configmap myconfigmap2 --from-file=username --from-file=age </code></pre><h4 id="查看一下-8"><a href="#查看一下-8" class="headerlink" title="查看一下"></a>查看一下</h4><pre><code>[root@master yaml]# kubectl describe cm</code></pre><p><img src="/posts/a50d.htm/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C14.assets%5Cimage-20200217103509006.png" alt="image-20200217103509006"></p>
<h3 id="3、通过–from-env-file"><a href="#3、通过–from-env-file" class="headerlink" title="3、通过–from- env-file:"></a>3、通过–from- env-file:</h3><pre><code>[root@master yaml]# vim env.txt 
username=adam
age=18</code></pre><h4 id="创建-1"><a href="#创建-1" class="headerlink" title="创建"></a>创建</h4><pre><code>[root@master yaml]# kubectl create configmap  myconfigmap3 --from-env-file=env.txt</code></pre><h4 id="查看一下-9"><a href="#查看一下-9" class="headerlink" title="查看一下"></a>查看一下</h4><pre><code>[root@master configmap]# kubectl describe cm</code></pre><p><img src="/posts/a50d.htm/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C14.assets%5Cimage-20200217165039190.png" alt="image-20200217165039190"></p>
<h3 id="4、通过yaml配置文件-1"><a href="#4、通过yaml配置文件-1" class="headerlink" title="4、通过yaml配置文件:"></a>4、通过yaml配置文件:</h3><pre><code>[root@master yaml]# vim configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myconfigmap4
data:
  username: 'adam'
  age: '18'</code></pre><h4 id="创建-2"><a href="#创建-2" class="headerlink" title="创建"></a>创建</h4><pre><code>[root@master yaml]# kubectl apply -f configmap.yaml </code></pre><h4 id="查看一下-10"><a href="#查看一下-10" class="headerlink" title="查看一下"></a>查看一下</h4><pre><code>[root@master yaml]# kubectl describe cm</code></pre><p><img src="/posts/a50d.htm/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C14.assets%5Cimage-20200217104428521.png" alt="image-20200217104428521"></p>
<h2 id="如何来使用configmap资源"><a href="#如何来使用configmap资源" class="headerlink" title="如何来使用configmap资源"></a>如何来使用configmap资源</h2><h3 id="1-以Volume挂载的方式-1"><a href="#1-以Volume挂载的方式-1" class="headerlink" title="1. 以Volume挂载的方式"></a>1. 以Volume挂载的方式</h3><pre><code>[root@master yaml]# vim v-pod.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  containers:
  - name: mypod
    image: busybox
    args:
      - /bin/sh
      - -c
      - sleep 300000
    volumeMounts:
    - name: cmp-test
      mountPath: "/etc/cmp-test"
      readOnly: true
  volumes:
  - name: cmp-test
    configMap:
      name: myconfigmap1</code></pre><h4 id="执行一下-4"><a href="#执行一下-4" class="headerlink" title="执行一下"></a>执行一下</h4><pre><code>[root@master configmap]# kubectl apply -f v-pod.yaml </code></pre><h4 id="查看一下-11"><a href="#查看一下-11" class="headerlink" title="查看一下"></a>查看一下</h4><pre><code>[root@master configmap]# kubectl exec -it pod1 /bin/sh
//进入容器查看一下
 # cat /etc/cmp-test/age 
18/ 
 # cat /etc/cmp-test/username 
adam/ </code></pre><h3 id="1-1-自定义存放数据的文件名的yaml文件-1"><a href="#1-1-自定义存放数据的文件名的yaml文件-1" class="headerlink" title="1.1 自定义存放数据的文件名的yaml文件"></a>1.1 自定义存放数据的文件名的yaml文件</h3><pre><code>[root@master configmap]# vim v-pod2.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: pod3
spec:
  containers:
  - name: mypod
    image: busybox
    args:
      - /bin/sh
      - -c
      - sleep 300000
    volumeMounts:
    - name: cmp-test
      mountPath: "/etc/cmp-test"
      readOnly: true
  volumes:
  - name: cmp-test
    configMap:
      name: myconfigmap1
      items:
      - key: username
        path: my-group/my-username   #自定义的容器中的目录
      - key: age
        path: my-group/my-age   #自定义的容器中的目录 </code></pre><h4 id="执行一下-5"><a href="#执行一下-5" class="headerlink" title="执行一下"></a>执行一下</h4><pre><code>[root@master configmap]# kubectl apply -f v-pod2.yaml</code></pre><h4 id="查看一下-12"><a href="#查看一下-12" class="headerlink" title="查看一下"></a>查看一下</h4><pre><code>[root@master configmap]# kubectl exec -it pod3 /bin/sh
//进入容器查看
# cat /etc/cmp-test/my-group/my-username 
adam/ 
# cat /etc/cmp-test/my-group/my-age 
18/ </code></pre><h3 id="1-2-如果，现在将secret资源内保存的数据进行更新，请问，使用此数据的应用内，数据是是否也会更新-1"><a href="#1-2-如果，现在将secret资源内保存的数据进行更新，请问，使用此数据的应用内，数据是是否也会更新-1" class="headerlink" title="1.2 如果，现在将secret资源内保存的数据进行更新，请问，使用此数据的应用内，数据是是否也会更新?"></a>1.2 如果，现在将secret资源内保存的数据进行更新，请问，使用此数据的应用内，数据是是否也会更新?</h3><pre><code>[root@master configmap]# kubectl edit cm myconfigmap1</code></pre><p><img src="/posts/a50d.htm/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C14.assets%5Cimage-20200217172107999.png" alt="image-20200217172107999"></p>
<h4 id="查看一下-13"><a href="#查看一下-13" class="headerlink" title="查看一下"></a>查看一下</h4><pre><code>[root@master configmap]# kubectl exec -it pod3 /bin/sh
//进入容器查看
# cat /etc/cmp-test/my-group/my-username 
adam/ 
# cat /etc/cmp-test/my-group/my-age 
10</code></pre><p><strong><em>可以看到更新成功</em></strong></p>
<h3 id="2-以环境变量的方式"><a href="#2-以环境变量的方式" class="headerlink" title="2.以环境变量的方式"></a>2.以环境变量的方式</h3><pre><code>[root@master configmap]# vim e-pod.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: pod2
spec:
  containers:
  - name: mypod
    image: busybox
    args:
      - /bin/sh
      - -c
      - sleep 300000
    env:
      - name: CONFIGMAP_NAME
        valueFrom:
          configMapKeyRef:
            name: myconfigmap2
            key: username
      - name: CONFIGMAP_AGE
        valueFrom:
          configMapKeyRef:
            name: myconfigmap2
            key: age</code></pre><h4 id="执行一下-6"><a href="#执行一下-6" class="headerlink" title="执行一下"></a>执行一下</h4><pre><code>[root@master configmap]# kubectl apply -f e-pod.yaml </code></pre><h4 id="查看一下-14"><a href="#查看一下-14" class="headerlink" title="查看一下"></a>查看一下</h4><pre><code>[root@master configmap]# kubectl exec -it pod2 /bin/sh
//进入容器查看一下
 # echo $CONFIGMAP_NAME
adam
 # echo $CONFIGMAP_AGE
18</code></pre><h3 id="2-1-更新sevret文件的内容-1"><a href="#2-1-更新sevret文件的内容-1" class="headerlink" title="2.1 更新sevret文件的内容"></a>2.1 更新sevret文件的内容</h3><pre><code>[root@master configmap]# kubectl edit cm myconfigmap2
 //修改保存文件的内容</code></pre><p><img src="/posts/a50d.htm/E:%5C%E8%BD%AF%E4%BB%B6%5C%E5%8D%9A%E5%AE%A2%5CBlog%5Cblog%5Csource_posts%5C14.assets%5Cimage-20200217172701793.png" alt="image-20200217172701793"></p>
<h4 id="查看一下-15"><a href="#查看一下-15" class="headerlink" title="查看一下"></a>查看一下</h4><pre><code>[root@master configmap]# kubectl exec -it pod2 /bin/sh
//进入容器查看一下
 # echo $CONFIGMAP_NAME
adam
 # echo $CONFIGMAP_AGE
18</code></pre><p><strong><em>等待了一定时间后，可以看到这个数据并没有没有改变</em></strong></p>
<p><strong>可以看出这个configmap和secret的更新效果基本没有区别。</strong></p>
<h2 id="总结configmap、与secret资源有什么相同和不同之处。"><a href="#总结configmap、与secret资源有什么相同和不同之处。" class="headerlink" title="总结configmap、与secret资源有什么相同和不同之处。"></a><strong>总结configmap、与secret资源有什么相同和不同之处。</strong></h2><h3 id="Secret-与-ConfigMap-对比"><a href="#Secret-与-ConfigMap-对比" class="headerlink" title="Secret 与 ConfigMap 对比"></a>Secret 与 ConfigMap 对比</h3><p><strong>相同点：</strong></p>
<blockquote>
<p><strong>key/value的形式</strong></p>
<p><strong>属于某个特定的namespace</strong></p>
<p><strong>可以导出到环境变量</strong></p>
<p><strong>可以通过目录/文件形式挂载</strong></p>
<p><strong>通过 volume 挂载的配置信息均可热更新</strong></p>
</blockquote>
<p><strong>不同点：</strong></p>
<blockquote>
<p><strong>Secret 可以被 ServerAccount 关联</strong></p>
<p><strong>Secret 可以存储 docker register 的鉴权信息，用在 ImagePullSecret 参数中，用于拉取私有仓库的镜像</strong></p>
<p><strong>Secret 支持 Base64 加密</strong></p>
<p><strong>Secret 分为 kubernetes.io/service-account-token、kubernetes.io/dockerconfigjson、Opaque 三种类型，而 Configmap 不区分类型</strong></p>
</blockquote>
<h2 id="总结以volumes挂载、和环境变量方式引用资源的相同和不同之处。"><a href="#总结以volumes挂载、和环境变量方式引用资源的相同和不同之处。" class="headerlink" title="总结以volumes挂载、和环境变量方式引用资源的相同和不同之处。"></a>总结以volumes挂载、和环境变量方式引用资源的相同和不同之处。</h2><p><strong>volumes挂载(可根据更改数据更新)：引用自己创建的secret（密文）或configmap（明文），挂载到容器中指定的目录下。查看保存的文件时，根据自己所填路径和secret或configmap创建的文件，进行查看。</strong></p>
<p><strong>环境变量(不因更改数据更新)：引用自己创建的secret（密文）或configmap（明文），挂载到容器中指定的目录下。查看保存的文件时，根据自己环境变量，进行查看。</strong></p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script></div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/posts/5849.html">11 k8s持久化存储应用</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-04-25</time><div class="content"><h1 id="k8s存储-持久化"><a href="#k8s存储-持久化" class="headerlink" title="k8s存储: (持久化)"></a>k8s存储: (持久化)</h1><p><strong>docker容器是有生命周期的。</strong></p>
<p><strong>volume</strong></p>
<h2 id="1-emptyDir（空目录）：类似docker-数据持久化的-docer-manager-volume"><a href="#1-emptyDir（空目录）：类似docker-数据持久化的-docer-manager-volume" class="headerlink" title="1.emptyDir（空目录）：类似docker 数据持久化的:docer manager volume"></a><strong>1.emptyDir（空目录）：</strong>类似docker 数据持久化的:docer manager volume</h2><p><strong>使用场景:在同一 个Pod里，不同的容器，共享数据卷。</strong></p>
<p><strong>如果容器被删除，数据仍然存在，如果Pod被 删除，数据也会被删除。</strong></p>
<blockquote>
<p><strong>测试编写一个yaml文件</strong></p>
<pre><code>[root@master yaml]# vim emptyDir.yaml
apiVersion: v1
kind: Pod
metadata:
  name: producer-consumer
spec:
  containers:
  - image: busybox
    name: producer
    volumeMounts:
    - mountPath: /producer_dir
      name: shared-volume
    args:
    - /bin/sh
    - -c
    - echo "hello k8s" &gt; /producer_dir/hello; sleep 30000
  - image: busybox
    name: consumer
    volumeMounts:
    - mountPath: /consumer_dir
      name: shared-volume
    args:
    - /bin/sh
    - -c
    - cat /consumer_dir/hello; sleep 30000
  volumes:
  - name: shared-volume
    emptyDir: {}</code></pre><p><strong>执行一下</strong></p>
<pre><code>[root@master yaml]# kubectl apply -f emptyDir.yaml </code></pre><p><strong>查看一下</strong></p>
<pre><code>[root@master yaml]# kubectl get pod  </code></pre><p>![image-20200205095431565](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200205095431565.png)</p>
<p><strong>查看日志</strong></p>
<pre><code>[root@master yaml]# kubectl logs  producer-consumer producer
[root@master yaml]# kubectl logs  producer-consumer consumer</code></pre><p>![image-20200205095543780](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200205095543780.png)</p>
<p><strong>查看挂载的目录</strong></p>
<p><strong>node节点查看容器名，并通过容器名查看挂载的目录</strong></p>
<pre><code>[root@node01 shared-volume]# docker ps </code></pre><p>![image-20200205102007328](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200205102007328.png)</p>
<pre><code>[root@node01 shared-volume]# docker inspect k8s_consumer_producer-consumer_default_9ec83f9e-e58b-4bf8-8e16-85b0f83febf9_0</code></pre><p>![image-20200205102048470](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200205102048470.png)</p>
<p><strong>进入挂载目录查看一下</strong></p>
<p>![image-20200205102128953](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200205102128953.png)</p>
</blockquote>
<h2 id="2-hostPath-Volume：类似docker-数据持久化的-bind-mount"><a href="#2-hostPath-Volume：类似docker-数据持久化的-bind-mount" class="headerlink" title="2.hostPath Volume：类似docker 数据持久化的:bind mount"></a>2.hostPath Volume：类似docker 数据持久化的:bind mount</h2><p><strong>如果Pod被删除，数据会保留，相比较emptyDir要好一点。不过一旦host崩溃，hostPath也无法访问 了。</strong></p>
<p><strong>docker或者k8s集群本身的存储会采用hostPath这种方式。</strong></p>
<h2 id="3-Persistent-Volume-PV-持久卷-提前做好的，数据持久化的数据存放目录。"><a href="#3-Persistent-Volume-PV-持久卷-提前做好的，数据持久化的数据存放目录。" class="headerlink" title="3.Persistent Volume| PV(持久卷) 提前做好的，数据持久化的数据存放目录。"></a>3.Persistent Volume| PV(持久卷) 提前做好的，数据持久化的数据存放目录。</h2><h3 id="Psesistent-Volume-Claim-PVC-持久卷使用声明-申请"><a href="#Psesistent-Volume-Claim-PVC-持久卷使用声明-申请" class="headerlink" title="Psesistent Volume Claim| PVC( 持久卷使用声明|申请)"></a><strong>Psesistent Volume Claim| PVC( 持久卷使用声明|申请)</strong></h3><h3 id="（1）基于nfs服务来做的PV和pvc"><a href="#（1）基于nfs服务来做的PV和pvc" class="headerlink" title="（1）基于nfs服务来做的PV和pvc"></a><strong>（1）基于nfs服务来做的PV和pvc</strong></h3><p><strong>下载nfs所需安装包</strong></p>
<pre><code>[root@node02 ~]# yum -y install nfs-utils  rpcbind</code></pre><p><strong>创建共享目录</strong></p>
<pre><code>[root@master ~]# mkdir /nfsdata</code></pre><p><strong>创建共享目录的权限</strong></p>
<pre><code>[root@master ~]# vim /etc/exports
/nfsdata *(rw,sync,no_root_squash)</code></pre><p><strong>开启nfs和rpcbind</strong></p>
<pre><code>[root@master ~]# systemctl start nfs-server.service 
[root@master ~]# systemctl start rpcbind</code></pre><p><strong>测试一下</strong></p>
<pre><code>[root@master ~]# showmount -e</code></pre><p>![image-20200205105654925](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200205105654925.png)</p>
<h4 id="lt-1-gt-创建nfs-pv的yaml文件"><a href="#lt-1-gt-创建nfs-pv的yaml文件" class="headerlink" title="<1>创建nfs-pv的yaml文件"></a><strong>&lt;1&gt;创建nfs-pv的yaml文件</strong></h4><pre><code>[root@master yaml]# cd yaml/
[root@master yaml]# vim nfs-pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: test-pv
spec:
  capacity:   #pv容量的大小
    storage: 1Gi
  accessModes:  #访问pv的模式
    - ReadWriteOnce #能以读-写mount到单个的节点
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: nfs
  nfs:
    path: /nfsdata/pv1
    server: 192.168.1.21</code></pre><blockquote>
<pre><code>  accessModes:(PV支持的访问模式)
    - ReadWriteOnce: 能以读-写mount到单个的节点
    - ReadWriteMany: 能以读-写mount到多个的节点。
    - ReadOnlyMnce:  能以只读的方式mount到多个节点。</code></pre></blockquote>
<blockquote>
<pre><code>persistentVolumeReclaimPolicy : (PV存储空间的回收策略是什么)
    Recycle: 自动清除数据。
    Retain: 需要管理员手动回收。
    Delete： 云存储专用。</code></pre></blockquote>
<h4 id="lt-2-gt-执行一下"><a href="#lt-2-gt-执行一下" class="headerlink" title="<2>执行一下"></a><strong>&lt;2&gt;执行一下</strong></h4><pre><code>[root@master yaml]# kubectl apply -f nfs-pv.yaml </code></pre><h4 id="lt-3-gt-查看一下"><a href="#lt-3-gt-查看一下" class="headerlink" title="<3>查看一下"></a>&lt;3&gt;查看一下</h4><pre><code>[root@master yaml]# kubectl get pv</code></pre><p>![image-20200205111307317](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200205111307317.png)</p>
<h4 id="lt-1-gt-创建nfs-pvc的yaml文件"><a href="#lt-1-gt-创建nfs-pvc的yaml文件" class="headerlink" title="<1>创建nfs-pvc的yaml文件"></a><strong>&lt;1&gt;创建nfs-pvc的yaml文件</strong></h4><pre><code>[root@master yaml]# vim nfs-pvc.yaml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: nfs</code></pre><h4 id="lt-2-gt-执行一下-1"><a href="#lt-2-gt-执行一下-1" class="headerlink" title="<2>执行一下"></a><strong>&lt;2&gt;执行一下</strong></h4><pre><code>[root@master yaml]# kubectl apply -f nfs-pvc.yaml </code></pre><h4 id="lt-3-gt-查看一下-1"><a href="#lt-3-gt-查看一下-1" class="headerlink" title="<3>查看一下"></a>&lt;3&gt;查看一下</h4><pre><code>[root@master yaml]# kubectl get pvc</code></pre><p>![image-20200205113407860](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200205113407860.png)</p>
<pre><code>[root@master yaml]# kubectl get pv</code></pre><p>![image-20200205113512580](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200205113512580.png)</p>
<h3 id="（2）创建一个pod资源"><a href="#（2）创建一个pod资源" class="headerlink" title="（2）创建一个pod资源"></a>（2）创建一个pod资源</h3><pre><code>[root@master yaml]# vim pod.yaml
kind: Pod
apiVersion: v1
metadata:
  name: test-pod
spec:
  containers:
    - name: pod1
      image: busybox
      args:
      - /bin/sh
      - -c
      - sleep 30000
      volumeMounts:
      - mountPath: "/mydata"
        name: mydata
  volumes:
    - name: mydata
      persistentVolumeClaim:
        claimName: test-pvc</code></pre><h4 id="lt-1-gt-执行一下"><a href="#lt-1-gt-执行一下" class="headerlink" title="<1> 执行一下"></a>&lt;1&gt; 执行一下</h4><pre><code>[root@master yaml]# kubectl apply -f pod.yaml </code></pre><h4 id="lt-2-gt-查看一下"><a href="#lt-2-gt-查看一下" class="headerlink" title="<2>查看一下"></a>&lt;2&gt;查看一下</h4><pre><code>[root@master yaml]# kubectl get pod -o wide</code></pre><p>![image-20200207100212328](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207100212328.png)</p>
<p><strong>可以看到现在没有开启成功</strong></p>
<h5 id="查看一下test-pod的信息看看是哪里的问题"><a href="#查看一下test-pod的信息看看是哪里的问题" class="headerlink" title="查看一下test-pod的信息看看是哪里的问题"></a>查看一下test-pod的信息看看是哪里的问题</h5><pre><code>[root@master yaml]# kubectl describe pod test-pod </code></pre><p>![image-20200207123950227](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207123950227.png)</p>
<h5 id="那是因为pv的本地挂载目录没有创建好"><a href="#那是因为pv的本地挂载目录没有创建好" class="headerlink" title="那是因为pv的本地挂载目录没有创建好"></a>那是因为pv的本地挂载目录没有创建好</h5><pre><code>[root@master yaml]# mkdir /nfsdata/pv1/
//要和nfs-pv.yaml的名字一样</code></pre><h5 id="重新创建一下pod"><a href="#重新创建一下pod" class="headerlink" title="重新创建一下pod"></a>重新创建一下pod</h5><pre><code>[root@master yaml]# kubectl delete -f pod.yaml 
[root@master yaml]# kubectl apply -f pod.yaml 
[root@master yaml]# kubectl get pod -o wide</code></pre><p>![image-20200207102822785](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207102822785.png)</p>
<h3 id="（3）test-pod创建hello创建文件并添加内容"><a href="#（3）test-pod创建hello创建文件并添加内容" class="headerlink" title="（3）test-pod创建hello创建文件并添加内容"></a>（3）test-pod创建hello创建文件并添加内容</h3><pre><code>[root@master yaml]# kubectl exec test-pod touch /mydata/hello</code></pre><p><strong>进入容器</strong></p>
<pre><code>[root@master yaml]# kubectl exec -it test-pod  /bin/sh
/ # echo 123 &gt; /mydata/hello
/ # exit</code></pre><p><strong>挂载目录查看一下</strong></p>
<pre><code>[root@master yaml]# cat  /nfsdata/pv1/hello </code></pre><p>![image-20200207104239153](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207104239153.png)</p>
<p><strong>和刚刚的一样</strong></p>
<h3 id="（4）测试回收策略"><a href="#（4）测试回收策略" class="headerlink" title="（4）测试回收策略"></a>（4）测试回收策略</h3><h4 id="删除pod和pvc，pv"><a href="#删除pod和pvc，pv" class="headerlink" title="删除pod和pvc，pv"></a>删除pod和pvc，pv</h4><pre><code>[root@master yaml]# kubectl delete pod test-pod 
[root@master yaml]# kubectl delete pvc test-pvc 
[root@master yaml]# kubectl delete pv test-pv </code></pre><h4 id="查看一下"><a href="#查看一下" class="headerlink" title="查看一下"></a>查看一下</h4><pre><code>[root@master yaml]# kubectl get pv</code></pre><p>![image-20200207104454636](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207104454636.png)</p>
<pre><code>[root@master yaml]# cat  /nfsdata/pv1/hello</code></pre><p>![image-20200207104520048](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207104520048.png)</p>
<p><strong><em>文件已被回收</em></strong></p>
<h3 id="（5）修改pv的回收策略为手动"><a href="#（5）修改pv的回收策略为手动" class="headerlink" title="（5）修改pv的回收策略为手动"></a>（5）修改pv的回收策略为手动</h3><h4 id="修改"><a href="#修改" class="headerlink" title="修改"></a>修改</h4><pre><code>[root@master yaml]# vim nfs-pv.yaml 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: test-pv
spec :
  capacity :
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain   #修改
  storageClassName: nfs
  nfs:
    path: /nfsdata/pv1
    server: 192.168.1.21</code></pre><h4 id="执行一下"><a href="#执行一下" class="headerlink" title="执行一下"></a>执行一下</h4><pre><code>[root@master yaml]# kubectl apply -f nfs-pv.yaml </code></pre><h4 id="创建pod"><a href="#创建pod" class="headerlink" title="创建pod"></a>创建pod</h4><pre><code>[root@master yaml]# kubectl apply -f pod.yaml </code></pre><h4 id="查看一下-1"><a href="#查看一下-1" class="headerlink" title="查看一下"></a>查看一下</h4><p>![image-20200207105203009](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207105203009.png)</p>
<pre><code>[root@master yaml]# kubectl describe pod test-pod </code></pre><p>![image-20200207105248025](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207105248025.png)</p>
<h4 id="创建pvc"><a href="#创建pvc" class="headerlink" title="创建pvc"></a>创建pvc</h4><pre><code>[root@master yaml]# kubectl apply -f nfs-pvc.yaml </code></pre><h4 id="查看一下pod"><a href="#查看一下pod" class="headerlink" title="查看一下pod"></a>查看一下pod</h4><pre><code>[root@master yaml]# kubectl get pod</code></pre><p>![image-20200207105402354](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207105402354.png)</p>
<h3 id="（6）test-pod创建hello创建文件并添加内容"><a href="#（6）test-pod创建hello创建文件并添加内容" class="headerlink" title="（6）test-pod创建hello创建文件并添加内容"></a>（6）test-pod创建hello创建文件并添加内容</h3><pre><code>[root@master yaml]# kubectl exec test-pod touch /mydata/k8s</code></pre><h4 id="查看一下挂载目录"><a href="#查看一下挂载目录" class="headerlink" title="查看一下挂载目录"></a>查看一下挂载目录</h4><pre><code>[root@master yaml]# ls /nfsdata/pv1/</code></pre><p>![image-20200207105618318](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207105618318.png)</p>
<h4 id="删除pod和pvc，pv，再次查看挂载目录"><a href="#删除pod和pvc，pv，再次查看挂载目录" class="headerlink" title="删除pod和pvc，pv，再次查看挂载目录"></a>删除pod和pvc，pv，再次查看挂载目录</h4><pre><code>[root@master yaml]# kubectl delete pod test-pod 
[root@master yaml]# kubectl delete pvc test-pvc
[root@master yaml]# kubectl delete pv test-pv </code></pre><h4 id="查看挂载目录"><a href="#查看挂载目录" class="headerlink" title="查看挂载目录"></a>查看挂载目录</h4><pre><code>[root@master yaml]# ls /nfsdata/pv1/</code></pre><p>![image-20200207105757641](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207105757641.png)</p>
<p><strong><em>内容还在</em></strong></p>
<h2 id="4-mysql对数据持久化的应用"><a href="#4-mysql对数据持久化的应用" class="headerlink" title="4.mysql对数据持久化的应用"></a>4.mysql对数据持久化的应用</h2><p><strong>最小化安装系统需要</strong></p>
<pre><code>yum -y install mariadb</code></pre><h2 id="（1）通过之前的yaml文件，创建pv和pvc"><a href="#（1）通过之前的yaml文件，创建pv和pvc" class="headerlink" title="（1）通过之前的yaml文件，创建pv和pvc"></a>（1）通过之前的yaml文件，创建pv和pvc</h2><pre><code>[root@master yaml]# kubectl apply -f  nfs-pv.yaml 
[root@master yaml]# kubectl apply -f  nfs-pvc.yaml </code></pre><h3 id="查看一下-2"><a href="#查看一下-2" class="headerlink" title="查看一下"></a>查看一下</h3><pre><code>[root@master yaml]# kubectl get pv</code></pre><p>![image-20200207110132199](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207110132199.png)</p>
<pre><code>[root@master yaml]# kubectl get pvc</code></pre><p>![image-20200207110140002](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207110140002.png)</p>
<h2 id="（2）编写一个mysql的yaml文件"><a href="#（2）编写一个mysql的yaml文件" class="headerlink" title="（2）编写一个mysql的yaml文件"></a>（2）编写一个mysql的yaml文件</h2><pre><code>[root@master yaml]# vim mysql.yaml

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: test-mysql
spec:
  selector:
    matchLabels:    #支持等值的标签
      app: mysql
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: test-mysql
spec:
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - image: mysql:5.6
        name: mysql
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: 123.com
        volumeMounts:
        - name: mysql-storage
          mountPath: /var/lib/mysql
      volumes:
      - name: mysql-storage
        persistentVolumeClaim:
          claimName: test-pvc</code></pre><h3 id="执行一下-1"><a href="#执行一下-1" class="headerlink" title="执行一下"></a>执行一下</h3><pre><code>[root@master yaml]# kubectl apply -f mysql.yaml </code></pre><h3 id="查看一下-3"><a href="#查看一下-3" class="headerlink" title="查看一下"></a>查看一下</h3><pre><code>[root@master yaml]# kubectl get pod</code></pre><p>![image-20200207110741833](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207110741833.png)</p>
<h2 id="（3）进入mysql容器"><a href="#（3）进入mysql容器" class="headerlink" title="（3）进入mysql容器"></a>（3）进入mysql容器</h2><pre><code>[root@master yaml]# kubectl exec -it test-mysql-569f8df4db-rkpwm  -- mysql -u root -p123.com </code></pre><h3 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h3><pre><code>mysql&gt; create database yun33;</code></pre><h3 id="切换数据库"><a href="#切换数据库" class="headerlink" title="切换数据库"></a>切换数据库</h3><pre><code>mysql&gt; use yun33;</code></pre><h3 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h3><pre><code>mysql&gt; create table my_id( id int(4))；</code></pre><h3 id="在表中插入数据"><a href="#在表中插入数据" class="headerlink" title="在表中插入数据"></a>在表中插入数据</h3><pre><code>mysql&gt; insert my_id values(9527);</code></pre><h3 id="查看表"><a href="#查看表" class="headerlink" title="查看表"></a>查看表</h3><pre><code>mysql&gt; select * from my_id;</code></pre><p>![image-20200207113808540](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207113808540.png)</p>
<h2 id="（4）查看本地的挂载目录"><a href="#（4）查看本地的挂载目录" class="headerlink" title="（4）查看本地的挂载目录"></a>（4）查看本地的挂载目录</h2><pre><code>[root@master yaml]# ls /nfsdata/pv1/</code></pre><p>![image-20200207113909796](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207113909796.png)</p>
<h3 id="查看一下pod-1"><a href="#查看一下pod-1" class="headerlink" title="查看一下pod"></a>查看一下pod</h3><pre><code>[root@master yaml]# kubectl get pod -o wide -w</code></pre><p>![image-20200207114050117](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207114050117.png)</p>
<h3 id="挂起node01"><a href="#挂起node01" class="headerlink" title="挂起node01"></a>挂起node01</h3><p>![image-20200207114607518](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207114607518.png)</p>
<h2 id="（5）查看node02上面数据是否和刚才一样（验证数据的一致性）"><a href="#（5）查看node02上面数据是否和刚才一样（验证数据的一致性）" class="headerlink" title="（5）查看node02上面数据是否和刚才一样（验证数据的一致性）"></a>（5）查看node02上面数据是否和刚才一样（验证数据的一致性）</h2><h3 id="进入数据库"><a href="#进入数据库" class="headerlink" title="进入数据库"></a>进入数据库</h3><pre><code>[root@master yaml]#  kubectl exec -it test-mysql-569f8df4db-nsdnz  -- mysql -u root -p123.com </code></pre><h3 id="查看数据库"><a href="#查看数据库" class="headerlink" title="查看数据库"></a>查看数据库</h3><pre><code>mysql&gt; show databases;</code></pre><p>![image-20200207115253123](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207115253123.png)</p>
<h3 id="查看表-1"><a href="#查看表-1" class="headerlink" title="查看表"></a>查看表</h3><pre><code>mysql&gt; show tables;</code></pre><p>![image-20200207115352727](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207115352727.png)</p>
<pre><code>mysql&gt; select * from my_id;</code></pre><p>![image-20200207113808540](E:\软件\博客\Blog\blog\source_posts\11 k8s的存储.assets\image-20200207113808540.png)</p>
<p><strong><em>可以看到数据还在</em></strong></p>
<h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2><h4 id="PV的访问控制类型"><a href="#PV的访问控制类型" class="headerlink" title="PV的访问控制类型"></a><strong>PV的访问控制类型</strong></h4><p><strong>accessModes:(PV支持的访问模式)</strong></p>
<ul>
<li><strong>ReadWriteOnce: 能以读-写mount到单个的节点</strong></li>
<li><strong>ReadWriteMany: 能以读-写mount到多个的节点。</strong></li>
<li><strong>ReadOnlyOnce: 能以只读的方式mount到单个节点。</strong></li>
</ul>
<h4 id="PV的空间回收策略"><a href="#PV的空间回收策略" class="headerlink" title="PV的空间回收策略"></a><strong>PV的空间回收策略</strong></h4><p><strong>persistentVolumeReclaimPolicy : (PV存储空间的回收策略是什么)</strong></p>
<p>​    <strong>Recycle: 自动清除数据。</strong></p>
<p>​    <strong>Retain: 需要管理员手动回收。</strong></p>
<p>​    <strong>Delete： 云存储专用。</strong></p>
<h4 id="PV和PVC相互关联"><a href="#PV和PVC相互关联" class="headerlink" title="PV和PVC相互关联"></a><strong>PV和PVC相互关联</strong></h4><p><strong>是通过accessModes和storageClassName模块关联的</strong></p>
<h4 id="Pod不断的重启"><a href="#Pod不断的重启" class="headerlink" title="Pod不断的重启:"></a>Pod不断的重启:</h4><p><strong>1、swap,没有关闭，导致集群运行不正常。</strong><br><strong>2、内存不足，运行服务也会重后。</strong></p>
<p>kubectl describe<br>kubectl logs<br>/var/ log/messages<br>查看该节点的kubelet的日志。</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script></div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/posts/1b18.html">10 复习 </a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-04-25</time><div class="content"><h1 id="虚拟化"><a href="#虚拟化" class="headerlink" title="虚拟化"></a>虚拟化</h1><p><strong>云计算的分类</strong>：</p>
<blockquote>
<p><strong>基础及服务：laas</strong><br><strong>平台及服务：paas</strong><br><strong>软件及服务：saas</strong></p>
</blockquote>
<p><strong>docker虚拟化的底层原理:</strong> Namespace + Cgroup </p>
<p><strong>Namespace六项隔离:</strong></p>
<blockquote>
<p><strong>IPC: 共享内存,消息列队</strong><br><strong>MNT: 挂载点 文件系统</strong><br><strong>NET: 网络栈</strong><br><strong>PID: 进程编号</strong><br><strong>USER: 用户 组</strong><br><strong>UTS: 主机名 域名</strong><br><strong>namespace 六项隔离 实现了容器与宿主机 容器与容器之间的隔离</strong></p>
</blockquote>
<p><strong>Cgroup 四项作用：</strong></p>
<blockquote>
<p><strong>1） 资源的限制：</strong>cgroup可以对进程组使用的资源总额进行限制<br><strong>2） 优先级分配：</strong>通过分配的cpu时间片数量以及硬盘IO带宽的大小，实际上相当于控制了进程运行的优先级别<br><strong>3） 资源统计：</strong> group可以统计系统资源使用量，比如gpu使用时间，内存使用量等，用于按量计费。同时还支持挂起动能，也就是说通过cgroup把所有 资源限制起来,对资源都不能使用，注意着并不是说我们的程序不能使用了,知识不能使用资源，处于等待状态。<br><strong>4） 进程控制：</strong>可以对进程组执行挂起、恢复等操作。</p>
</blockquote>
<p><strong>镜像是容器运行的核心，容器是镜像运行的后的实例。</strong></p>
<p><strong>DockerHub| registry  —-&gt;  pull</strong><br>       <strong>image :     save &gt;   |  load &lt;</strong><br>       <strong>run    —-&gt;  Container    —-&gt;   commit*</strong><br>                                                           <strong>Dockerfile</strong></p>
<p><strong>Docker 三剑客。</strong></p>
<blockquote>
<p><strong>docker  machine :自动化部署多台dockerHost 。</strong></p>
<p>​        <strong>Docker-compose: 它可以同时控制多个容器。</strong></p>
<p>​        <strong>yaml。</strong></p>
<p><strong>Docker Swarm：</strong></p>
<p>​        <strong>从单个的服务向集群的形势发展。</strong></p>
<p>​         <strong>高可用、高性能、高并发 ：为了防止单点故障。</strong></p>
<p>​         <strong>Service：服务  —-&gt; 包括运行什么服务，需要多个                          rep1icas（副本）, 外网如何访问。</strong></p>
</blockquote>
<h3 id="k8s"><a href="#k8s" class="headerlink" title="k8s"></a><strong>k8s</strong></h3><p>关闭防火墙、禁用selinux、修改主机名并加入域名解析、关闭swap 、时间同步、免密登录、打开iptables桥接</p>
<blockquote>
<p><strong>对硬件的基本要求： CPU：2核   MEM：2G</strong></p>
<p><strong>主机名：master node01 node02</strong></p>
<p><strong>时间必须同步</strong></p>
</blockquote>
<p><strong>kubctl：k8s客户端      kubeadm：工具  kubelet：客户端代理</strong></p>
<p><strong>组件：</strong></p>
<p>​        <strong>三层网络： DockerHost  &gt;   Pod  &gt; Service</strong></p>
<p>​        <strong>Deployment:        Service:</strong></p>
<h3 id="master组件"><a href="#master组件" class="headerlink" title="*master组件: *"></a>*<em>master组件: *</em></h3><p><strong>kube- api( application interface) k8s的前端接口</strong></p>
<p><strong>Scheduler[集群分发调度器]</strong>负责决定将Pod放在哪个Node上运行。在调度时，会充分考虑集群的拓扑结构，当前各个节点的负载情况，以及应对高可用、性能、数据亲和性和需求。</p>
<p><strong>Controller Manager[内部管理控制中心]</strong>：负责管理集群的各种资源，保证资源处于预期的状态。它由多种Controller组成，包括Replication Controller、Endpoints Controller、Namespace Controller、Serviceaccounts Controller等。</p>
<p><strong>Etcd：</strong>负责保存k8s集群的配置信息和各种资源的状态信息。当数据发生变化时，etcd会快速的通知k8s相关组件。<a href="">（第三方组件）它有可替换方案。Consul、zookeeper</a></p>
<p><strong>Flanner：</strong>是k8s集群网络，可以保证Pod的跨主机通信。也有替换方案。</p>
<h3 id="Node组件："><a href="#Node组件：" class="headerlink" title="Node组件："></a>Node组件：</h3><p><strong>Kubelet[节点上的Pod管家]</strong>：它是Node的agent(代理)，当Scheduler确定某 个Node上运行Pod之后，会将Pod的具体配置信息发送给该节点的kubelet,kubelet会根据这些信息创建和运行容器，并向Master报告运行状态。</p>
<p><strong>kube-proxy[负载均衡、路由转发]:</strong>负责将访问service的TCP/UDP数据流转发到后端的容器。如果有多个副本，kube-proxy会实现负载均衡。</p>
<h3 id="yaml文件的一级字段"><a href="#yaml文件的一级字段" class="headerlink" title="yaml文件的一级字段:"></a>yaml文件的一级字段:</h3><p>​        <strong>VERSION:</strong><br>​        <strong>KIND:</strong><br>​        <strong>METADATA:</strong><br>​        <strong>SPEC :</strong></p>
<pre><code>[root@master ~]# vim web.yaml
kind: Deployment  #资源对象是控制器
apiVersion: extensions/v1beta1   #api的版本
metadata:      #描述kind（资源类型）
  name: web   #定义控制器名称
  namespace：  #名称空间
spec:
  replicas: 2   #副本数量
  template:     #模板
    metadata:    
      labels:   #标签
        app: web_server
    spec:
      containers:   #指定容器
      - name: nginx  #容器名称
        image: nginx   #使用的镜像</code></pre><p>​        <strong>Deployment（控制器)：</strong></p>
<p>​                <strong>ReplicationController：</strong>用来确保由其管控的Pod对象副本数量，能够满足用户期望，多则删除，少则通过模本创建</p>
<p>​                <strong>RS（RpelicaSet）:</strong>RS也是用于保证与label selector匹配的pod数量维持在期望状态</p>
<p>​        <strong>Service：</strong></p>
<p>​                <strong>type：默认Cluster IP</strong></p>
<p>​                            <strong>NodePort：  30000-32767</strong></p>
<p>​        <strong>Deployment和Service关联：标签和标签选择器</strong></p>
<p>​        <strong>Namespace：</strong></p>
<p>​        <strong>Pod：最小单位</strong></p>
<p>​                    <strong>镜像的下载策略：</strong></p>
<blockquote>
<p>​                        <strong>Always：</strong>镜像标签为“laster”或镜像不存在时，总是从指定的仓库中获取镜像。</p>
<p>​                        <strong>IfNotPresent：</strong>仅当本地镜像不存在时才从目标仓库下载。</p>
<p>​                        <strong>Never：</strong>禁止从仓库中下载镜像，即只使用本地镜像。</p>
</blockquote>
<p>​                    <strong>默认的标签 为latest：always</strong></p>
<p>​                    <strong>Pod的重启策略：</strong></p>
<blockquote>
<p>​                    <strong>Always：</strong>（默认情况下使用）但凡Pod对象终止就将其重启；<br>​                    <strong>OnFailure：</strong>仅在Pod对象出现错误时才将其重启；<br>​                    <strong>Never：</strong>从不重启；</p>
</blockquote>
<p>​                <strong>Pod的健康检查:</strong><br>​                    Liveness:   探测失败重启pod<br>​                    Readiness: 探测失败将pod设置为不可用<br>kubelet：控制pod</p>
<p>DaemonSet :会在每一个节点都会运行，并且只运行一个Pod</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script></div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/posts/e9be.html">09 Job资源对象</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-04-25</time><div class="content"><h1 id="Job资源对象"><a href="#Job资源对象" class="headerlink" title="Job资源对象"></a>Job资源对象</h1><blockquote>
<p><strong>服务类的Pod容器：</strong>RC、RS、DS、Deployment</p>
<p><strong>工作类的Pod容器：</strong>Job—&gt;执行一次，或者批量执行处理程序，完成之后退出容器。</p>
</blockquote>
<p><strong>注意： 如果容器内执行任务有误，会根据容器的重启策略操作容器，不过这里</strong><br><strong>的容器重启策略只能是: Never和 OnFailure。</strong></p>
<h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><p><strong>在有些场景下，是想要运行一些容器执行某种特定的任务，任务一旦执行完成，容器也就没有存在的必要了。在这种场景下，创建pod就显得不那么合适。于是就是了Job，Job指的就是那些一次性任务。通过Job运行一个容器，当其任务执行完以后，就自动退出，集群也不再重新将其唤醒。</strong></p>
<p><strong>从程序的运行形态上来区分，可以将Pod分为两类：长时运行服务（jboss、mysql等）和一次性任务（数据计算、测试）。RC创建的Pod都是长时运行的服务，Job多用于执行一次性任务、批处理工作等，执行完成后便会停止（status.phase变为Succeeded）。</strong></p>
<h1 id="一、kubernetes支持以下几种job"><a href="#一、kubernetes支持以下几种job" class="headerlink" title="一、kubernetes支持以下几种job"></a>一、kubernetes支持以下几种job</h1><blockquote>
<ul>
<li><strong>非并行job：通常创建一个pod直至其成功结束。</strong></li>
<li><strong>固定结束次数的job：设置spec.completions,创建多个pod，直到.spec.completions个pod成功结束。</strong></li>
<li><strong>带有工作队列的并行job：设置.spec.Parallelism但不设置.spec.completions,当所有pod结束并且至少一个成功时，job就认为是成功。</strong></li>
</ul>
</blockquote>
<h2 id="Job-Controller"><a href="#Job-Controller" class="headerlink" title="Job Controller"></a>Job Controller</h2><p><strong>Job Controller负责根据Job Spec创建pod，并持续监控pod的状态，直至其成功结束，如果失败，则根据restartPolicy（只支持OnFailure和Never，不支持Always）决定是否创建新的pod再次重试任务。</strong></p>
<h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a><strong>例子</strong></h2><h3 id="（1）编写一个job的yaml文件"><a href="#（1）编写一个job的yaml文件" class="headerlink" title="（1）编写一个job的yaml文件"></a>（1）编写一个job的yaml文件</h3><pre><code>[root@master yaml]# vim jop.yaml
kind: Job
apiVersion: batch/v1
metadata:
  name: test-job
spec:
  template:
    metadata:
      name: test-job
    spec:
      containers:
      - name: hello
        image: busybox
        command: ["echo","hello k8s job!"]
      restartPolicy: Never</code></pre><h3 id="（2）执行一下"><a href="#（2）执行一下" class="headerlink" title="（2）执行一下"></a>（2）执行一下</h3><pre><code>[root@master yaml]# kubectl apply -f jop.yaml </code></pre><h3 id="（3）查看一下"><a href="#（3）查看一下" class="headerlink" title="（3）查看一下"></a>（3）查看一下</h3><pre><code>[root@master yaml]# kubectl get pod</code></pre><p>![image-20200115090831524](E:\软件\博客\Blog\blog\source_posts\09 Jop资源对象.assets\image-20200115090831524.png)</p>
<h4 id="查看日志"><a href="#查看日志" class="headerlink" title="查看日志"></a>查看日志</h4><pre><code>[root@master yaml]# kubectl logs test-job-gs45w </code></pre><p>![image-20200115091213349](E:\软件\博客\Blog\blog\source_posts\09 Jop资源对象.assets\image-20200115091213349.png)</p>
<p><strong>我们可以看到job与其他资源对象不同，仅执行一次性任务，默认pod借宿运行后job即结束，状态为Completed。</strong></p>
<h3 id="（4）修改一下jop的yaml文件，把echo命令换成乱码"><a href="#（4）修改一下jop的yaml文件，把echo命令换成乱码" class="headerlink" title="（4）修改一下jop的yaml文件，把echo命令换成乱码"></a>（4）修改一下jop的yaml文件，把echo命令换成乱码</h3><pre><code>[root@master yaml]# vim jop.yaml
kind: Job
apiVersion: batch/v1
metadata:
  name: test-job
spec:
  template:
    metadata:
      name: test-job
    spec:
      containers:
      - name: hello
        image: busybox
        command: ["asdasxsddwefew","hello k8s job!"] #修改
      restartPolicy: Never</code></pre><h3 id="（5）先删除之前的pod"><a href="#（5）先删除之前的pod" class="headerlink" title="（5）先删除之前的pod"></a>（5）先删除之前的pod</h3><pre><code>[root@master yaml]# kubectl delete jobs.batch test-job  </code></pre><h3 id="（6）执行一下"><a href="#（6）执行一下" class="headerlink" title="（6）执行一下"></a>（6）执行一下</h3><pre><code>[root@master yaml]# kubectl apply -f jop.yaml </code></pre><h3 id="（7）查看一下"><a href="#（7）查看一下" class="headerlink" title="（7）查看一下"></a>（7）查看一下</h3><pre><code>[root@master yaml]# kubectl get pod -w</code></pre><p>![image-20200115091647925](E:\软件\博客\Blog\blog\source_posts\09 Jop资源对象.assets\image-20200115091647925.png)</p>
<p><strong><em>它会一直创建pod直到完成命令。</em></strong></p>
<h3 id="（8）修改一下jop的yaml文件，修改重启策略"><a href="#（8）修改一下jop的yaml文件，修改重启策略" class="headerlink" title="（8）修改一下jop的yaml文件，修改重启策略"></a>（8）修改一下jop的yaml文件，修改重启策略</h3><pre><code>[root@master yaml]# vim jop.yaml 
kind: Job
apiVersion: batch/v1
metadata:
  name: test-job
spec:
  template:
    metadata:
      name: test-job
    spec:
      containers:
      - name: hello
        image: busybox
        command: ["asdasxsddwefew","hello k8s job!"]
      restartPolicy: OnFailure</code></pre><h3 id="（9）先删除之前的pod"><a href="#（9）先删除之前的pod" class="headerlink" title="（9）先删除之前的pod"></a>（9）先删除之前的pod</h3><pre><code>[root@master yaml]# kubectl delete jobs.batch test-job </code></pre><h3 id="（10）执行一下"><a href="#（10）执行一下" class="headerlink" title="（10）执行一下"></a>（10）执行一下</h3><pre><code>[root@master yaml]# kubectl apply -f jop.yaml </code></pre><h3 id="（11）查看一下"><a href="#（11）查看一下" class="headerlink" title="（11）查看一下"></a>（11）查看一下</h3><pre><code>[root@master yaml]# kubectl get pod -w</code></pre><p>![image-20200115092801882](E:\软件\博客\Blog\blog\source_posts\09 Job资源对象.assets\image-20200115092801882.png)</p>
<p><strong><em>它会一直重启pod完成命令，直到重启到一定次数就会删除job。</em></strong></p>
<h1 id="二、提高Job的执行效率"><a href="#二、提高Job的执行效率" class="headerlink" title="二、提高Job的执行效率"></a>二、提高Job的执行效率</h1><h2 id="1-我们可以在Job-spec字段下加上parallelism选项。表示同时运行多少个Pod执行任务。"><a href="#1-我们可以在Job-spec字段下加上parallelism选项。表示同时运行多少个Pod执行任务。" class="headerlink" title="1. 我们可以在Job.spec字段下加上parallelism选项。表示同时运行多少个Pod执行任务。"></a>1. 我们可以在Job.spec字段下加上<a href="">parallelism</a>选项。表示同时运行多少个Pod执行任务。</h2><hr>
<h3 id="（1）编写一个job的yaml文件-1"><a href="#（1）编写一个job的yaml文件-1" class="headerlink" title="（1）编写一个job的yaml文件"></a>（1）编写一个job的yaml文件</h3><pre><code>[root@master yaml]# vim jop.yaml
kind: Job
apiVersion: batch/v1
metadata:
  name: test-job
spec:
  parallelism: 2    #同时启用几个pod
  template:
    metadata:
      name: test-job
    spec:
      containers:
      - name: hello
        image: busybox
        command: ["echo","hello k8s job!"]
      restartPolicy: OnFailure</code></pre><h3 id="（3）执行一下"><a href="#（3）执行一下" class="headerlink" title="（3）执行一下"></a>（3）执行一下</h3><pre><code>[root@master yaml]# kubectl apply -f jop.yaml </code></pre><h3 id="（4）查看一下"><a href="#（4）查看一下" class="headerlink" title="（4）查看一下"></a>（4）查看一下</h3><pre><code>[root@master yaml]# kubectl get pod</code></pre><p>![image-20200115093854913](E:\软件\博客\Blog\blog\source_posts\09 Job资源对象.assets\image-20200115093854913.png)</p>
<h4 id="查看日志-1"><a href="#查看日志-1" class="headerlink" title="查看日志"></a>查看日志</h4><p>![image-20200115094002236](E:\软件\博客\Blog\blog\source_posts\09 Job资源对象.assets\image-20200115094002236.png)</p>
<h2 id="2-我们可以在Job-spec字段下加上complations选项。表示总共需要完成Pod的数量"><a href="#2-我们可以在Job-spec字段下加上complations选项。表示总共需要完成Pod的数量" class="headerlink" title="2. 我们可以在Job.spec字段下加上complations选项。表示总共需要完成Pod的数量"></a>2. 我们可以在Job.spec字段下加上complations选项。表示总共需要完成Pod的数量</h2><h3 id="（1）编写一个job的yaml文件-2"><a href="#（1）编写一个job的yaml文件-2" class="headerlink" title="（1）编写一个job的yaml文件"></a>（1）编写一个job的yaml文件</h3><pre><code>[root@master yaml]# vim jop.yaml
kind: Job
apiVersion: batch/v1
metadata:
  name: test-job
spec:
  complations: 8            #运行pod的总数量8个
  parallelism: 2            #同时运行2个pod
  template:
    metadata:
      name: test-job
    spec:
      containers:
      - name: hello
        image: busybox
        command: ["echo","hello k8s job!"]
      restartPolicy: OnFailure</code></pre><p><strong>job 字段解释：</strong></p>
<blockquote>
<p><strong>标志Job结束需要成功运行的Pod个数，默认为1</strong><br><strong>parallelism：标志并行运行的Pod的个数，默认为1</strong><br><strong>activeDeadlineSeconds：标志失败Pod的重试最大时间，超过这个时间不会继续重试.</strong></p>
</blockquote>
<h3 id="（2）先删除之前的pod"><a href="#（2）先删除之前的pod" class="headerlink" title="（2）先删除之前的pod"></a>（2）先删除之前的pod</h3><pre><code>[root@master yaml]# kubectl delete jobs.batch test-job </code></pre><h3 id="（3）执行一下-1"><a href="#（3）执行一下-1" class="headerlink" title="（3）执行一下"></a>（3）执行一下</h3><pre><code>[root@master yaml]# kubectl apply -f jop.yaml </code></pre><h3 id="（4）查看一下-1"><a href="#（4）查看一下-1" class="headerlink" title="（4）查看一下"></a>（4）查看一下</h3><pre><code>[root@master yaml]# kubectl get pod</code></pre><p>![image-20200115094519494](E:\软件\博客\Blog\blog\source_posts\09 Job资源对象.assets\image-20200115094519494.png)</p>
<p><strong>可以看到pod是两个两个的启动的。</strong></p>
<h2 id="3-如何定时执行Job"><a href="#3-如何定时执行Job" class="headerlink" title="3. 如何定时执行Job"></a>3. 如何定时执行Job</h2><h3 id="（1）编写一个cronjob的yaml文件"><a href="#（1）编写一个cronjob的yaml文件" class="headerlink" title="（1）编写一个cronjob的yaml文件"></a>（1）编写一个cronjob的yaml文件</h3><pre><code>[root@master yaml]# vim cronjop.yaml
kind: CronJob
apiVersion: batch/v1beta1
metadata:
  name: hello
spec:
  schedule: "*/1 * * * *" #限定时间
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            command: ["echo","hello","cronjob"]
          restartPolicy: OnFailure</code></pre><h3 id="（2）先删除之前的pod-1"><a href="#（2）先删除之前的pod-1" class="headerlink" title="（2）先删除之前的pod"></a>（2）先删除之前的pod</h3><pre><code>[root@master yaml]# kubectl delete jobs.batch test-job </code></pre><h3 id="（3）执行一下-2"><a href="#（3）执行一下-2" class="headerlink" title="（3）执行一下"></a>（3）执行一下</h3><pre><code>[root@master yaml]# kubectl apply -f jop.yaml </code></pre><h3 id="（4）查看一下-2"><a href="#（4）查看一下-2" class="headerlink" title="（4）查看一下"></a>（4）查看一下</h3><pre><code>[root@master yaml]# kubectl get pod</code></pre><p>![image-20200115095857428](E:\软件\博客\Blog\blog\source_posts\09 Job资源对象.assets\image-20200115095857428.png)</p>
<pre><code>[root@master yaml]# kubectl get cronjobs.batch </code></pre><p>![image-20200115095920740](E:\软件\博客\Blog\blog\source_posts\09 Job资源对象.assets\image-20200115095920740.png)</p>
<p><strong>此时查看Pod的状态，会发现，每分钟都会运行一个新的Pod来执行命令规定的任</strong><br><strong>务。</strong></p>
<h2 id="练习：规定2020-1-15-10-5分运行上面的crontab任务。"><a href="#练习：规定2020-1-15-10-5分运行上面的crontab任务。" class="headerlink" title="练习：规定2020.1.15.10.5分运行上面的crontab任务。"></a>练习：规定2020.1.15.10.5分运行上面的crontab任务。</h2><h3 id="（1）编写一个cronjob的yaml文件-1"><a href="#（1）编写一个cronjob的yaml文件-1" class="headerlink" title="（1）编写一个cronjob的yaml文件"></a>（1）编写一个cronjob的yaml文件</h3><pre><code>[root@master yaml]# vim cronjop.yaml
kind: CronJob
apiVersion: batch/v1beta1
metadata:
  name: hello
spec:
  schedule: "5 10 15 1 *" #限定时间
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            command: ["echo","hello","cronjob"]
          restartPolicy: OnFailure</code></pre><h3 id="（2）先删除之前的pod-2"><a href="#（2）先删除之前的pod-2" class="headerlink" title="（2）先删除之前的pod"></a>（2）先删除之前的pod</h3><pre><code>[root@master yaml]# kubectl delete cronjobs.batch hello </code></pre><h3 id="（3）执行一下-3"><a href="#（3）执行一下-3" class="headerlink" title="（3）执行一下"></a>（3）执行一下</h3><pre><code>[root@master yaml]# kubectl apply -f jop.yaml </code></pre><h3 id="（4）查看一下-3"><a href="#（4）查看一下-3" class="headerlink" title="（4）查看一下"></a>（4）查看一下</h3><pre><code>[root@master yaml]# kubectl get pod</code></pre><p>![image-20200115100855819](E:\软件\博客\Blog\blog\source_posts\09 Job资源对象.assets\image-20200115100855819.png)</p>
<p><strong>这时会发现，如果规定具体时间，可能并不会执行任务。</strong></p>
<h3 id="（5）添加apiVersion库"><a href="#（5）添加apiVersion库" class="headerlink" title="（5）添加apiVersion库"></a>（5）添加apiVersion库</h3><pre><code>[root@master yaml]# vim /etc/kubernetes/manifests/kube-apiserver.yaml 
spec:
  containers:
  - command:
    - kube-apiserver
    - --runtime-config=batch/v2alpha1=true    #添加</code></pre><p>![image-20200115104218361](E:\软件\博客\Blog\blog\source_posts\09 Job资源对象.assets\image-20200115104218361.png)</p>
<h3 id="（6）重启kubelet"><a href="#（6）重启kubelet" class="headerlink" title="（6）重启kubelet"></a>（6）重启kubelet</h3><pre><code>[root@master yaml]# systemctl restart kubelet.service </code></pre><h3 id="（7）查看api版本"><a href="#（7）查看api版本" class="headerlink" title="（7）查看api版本"></a>（7）查看api版本</h3><pre><code>[root@master yaml]# kubectl api-versions </code></pre><p>![image-20200115104521662](E:\软件\博客\Blog\blog\source_posts\09 Job资源对象.assets\image-20200115104521662.png)</p>
<h3 id="（8）编写一个cronjob的yaml文件"><a href="#（8）编写一个cronjob的yaml文件" class="headerlink" title="（8）编写一个cronjob的yaml文件"></a>（8）编写一个cronjob的yaml文件</h3><pre><code>[root@master yaml]# vim cronjop.yaml
kind: CronJob
apiVersion: batch/v1beta1
metadata:
  name: hello
spec:
  schedule: "47 10 15 1 *" #限定时间
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            command: ["echo","hello","cronjob"]
          restartPolicy: OnFailure</code></pre><h3 id="（9）执行一下"><a href="#（9）执行一下" class="headerlink" title="（9）执行一下"></a>（9）执行一下</h3><pre><code>[root@master yaml]# kubectl apply -f jop.yaml </code></pre><h3 id="（4）查看一下-4"><a href="#（4）查看一下-4" class="headerlink" title="（4）查看一下"></a>（4）查看一下</h3><pre><code>[root@master yaml]# kubectl get pod -w</code></pre><p>![image-20200115100855819](E:\软件\博客\Blog\blog\source_posts\09 Job资源对象.assets\image-20200115100855819.png)</p>
<p><strong>注意：此时仍然不能正常运行指定时间的Job，这是因为K8s官方在cronjob这个资源对象的支持中还没有完善此功能，还待开发。</strong></p>
<p><strong>跟Job资源一样在cronjob.spec.jobTemplate.spec 下同样支持并发Job参数:</strong><br><strong>parallelism，也支持完成Pod的总数参数: completionsr</strong></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p><strong>Job 作为 Kubernetes 中用于处理任务的资源，与其他的资源没有太多的区别，它也使用 Kubernetes 中常见的控制器模式，监听 Informer 中的事件并运行 <code>syncHandler</code> 同步任务</strong></p>
<p><strong>而 CronJob 由于其功能的特殊性，每隔 10s 会从 apiserver 中取出资源并进行检查是否应该触发调度创建新的资源，需要注意的是 CronJob 并不能保证在准确的目标时间执行，执行会有一定程度的滞后。</strong></p>
<p><strong>两个控制器的实现都比较清晰，只是边界条件比较多，分析其实现原理时一定要多注意。</strong></p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script></div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/posts/7772.html">08 ReplicaSet、DaemonSet</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-04-25</time><div class="content"><h1 id="ReplicaSet简单介绍"><a href="#ReplicaSet简单介绍" class="headerlink" title="ReplicaSet简单介绍"></a>ReplicaSet简单介绍</h1><h2 id="1-RC：ReplicationController（老一代的pod控制器）"><a href="#1-RC：ReplicationController（老一代的pod控制器）" class="headerlink" title="1. RC：ReplicationController（老一代的pod控制器）"></a>1. RC：ReplicationController（老一代的pod控制器）</h2><p><strong>用来确保由其管控的Pod对象副本数量，能够满足用户期望，多则删除，少则通过模本创建</strong></p>
<h3 id="特点："><a href="#特点：" class="headerlink" title="特点："></a>特点：</h3><ul>
<li>​        <strong>确保Pod资源对象的数量精准。</strong></li>
<li>​        <strong>确保pod健康运行。</strong></li>
<li>​        <strong>弹性伸缩</strong></li>
</ul>
<p><strong>同样，它也可以通过yaml或json格式的资源清单来创建。其中spec字段一般嵌套以下字段：</strong></p>
<ul>
<li>​        <strong>replicas：期望的Pod对象副本数量。</strong></li>
<li>​        <strong>selector：当前控制器匹配Pod对此项副本的标签选择器</strong>    </li>
<li>​        <strong>template：pod副本的模板</strong></li>
</ul>
<p><strong>与RC相比而言，RS不仅支持<em>基于等值</em>的标签选择器，而且还支持<em>基于集合</em>的标签选择器。</strong></p>
<h2 id="2-标签：解决同类型的资源对象，为了更好的管理，按照标签分组。"><a href="#2-标签：解决同类型的资源对象，为了更好的管理，按照标签分组。" class="headerlink" title="2. 标签：解决同类型的资源对象，为了更好的管理，按照标签分组。"></a>2. 标签：解决同类型的资源对象，为了更好的管理，按照标签分组。</h2><h3 id="常用的标签分类："><a href="#常用的标签分类：" class="headerlink" title="常用的标签分类："></a>常用的标签分类：</h3><ul>
<li>​        <strong>release（版本）：stable（稳定版）、canary（金丝雀版本）、beta（测试版本）</strong></li>
<li>​        <strong>environment（环境变量）：dev（开发）、qa（测试）、production（生产）</strong></li>
<li>​        <strong>application（应用）：ui、as（application software应用软件）、pc、sc</strong></li>
<li>​        <strong>tier（架构层级）：frontend（前端）、backend（后端）、cache（缓存）</strong></li>
<li>​        <strong>partition（分区）：customerA（客户A）、customerB（客户B）</strong></li>
<li>​        <strong>track（品控级别）：daily（每天）、weekly（每周）</strong></li>
</ul>
<p><strong>标签要做到：见名知意。</strong></p>
<h2 id="3-测试"><a href="#3-测试" class="headerlink" title="3.测试"></a>3.测试</h2><h3 id="（1）编写一个pod的yaml文件"><a href="#（1）编写一个pod的yaml文件" class="headerlink" title="（1）编写一个pod的yaml文件"></a>（1）编写一个pod的yaml文件</h3><pre><code>[root@master ~]# vim label.yaml 

kind: Pod
apiVersion: v1
metadata:
  name: labels
  labels:
    env: qa
    tier: frontend
spec:
  containers:
  - name: myapp
    image: httpd</code></pre><h4 id="lt-1-gt-执行一下"><a href="#lt-1-gt-执行一下" class="headerlink" title="<1>执行一下"></a>&lt;1&gt;执行一下</h4><pre><code>[root@master ~]# kubectl apply -f label.yaml  --record </code></pre><h4 id="lt-2-gt-查看一下"><a href="#lt-2-gt-查看一下" class="headerlink" title="<2>查看一下"></a>&lt;2&gt;查看一下</h4><pre><code>[root@master ~]# kubectl get pod  --show-labels 
//通过--show-labels显示资源对象的</code></pre><p>![image-20200114095943595](E:\软件\博客\Blog\blog\source_posts\08 ReplicaS儿童、DaemonSet.assets\image-20200114095943595.png)</p>
<pre><code>[root@master ~]# kubectl get po -L env,tier
//显示某个键对应的值</code></pre><p>![image-20200114100043922](E:\软件\博客\Blog\blog\source_posts\08 ReplicaS儿童、DaemonSet.assets\image-20200114100043922.png)</p>
<pre><code>[root@master ~]# kubectl get po -l env,tier
//通过-l 查看仅包含某个标签的资源。</code></pre><p>![image-20200114100200895](E:\软件\博客\Blog\blog\source_posts\08 ReplicaS儿童、DaemonSet.assets\image-20200114100200895.png)</p>
<h3 id="（2）添加标签"><a href="#（2）添加标签" class="headerlink" title="（2）添加标签"></a>（2）添加标签</h3><pre><code>[root@master ~]# kubectl label pod  labels app=pc
//给pod资源添加标签</code></pre><h3 id="（3）修改标签"><a href="#（3）修改标签" class="headerlink" title="（3）修改标签"></a>（3）修改标签</h3><pre><code>[root@master ~]# kubectl label pod labels env=dev --overwrite
//修改标签</code></pre><pre><code>[root@master ~]# kubectl get pod -l tier --show-labels 
//查看标签</code></pre><p>![image-20200114100607585](E:\软件\博客\Blog\blog\source_posts\08 ReplicaS儿童、DaemonSet.assets\image-20200114100607585.png)</p>
<h3 id="（4）编写一个service的yaml文件"><a href="#（4）编写一个service的yaml文件" class="headerlink" title="（4）编写一个service的yaml文件"></a>（4）编写一个service的yaml文件</h3><pre><code>[root@master ~]# vim service.yaml
kind: Service
apiVersion: v1
metadata:
  name: service
spec:
  type: NodePort
  selector:
    env: qa
  ports:
  - protocol: TCP
    port: 90
    targetPort: 80
    nodePort: 30123</code></pre><h4 id="lt-1-gt-执行一下-1"><a href="#lt-1-gt-执行一下-1" class="headerlink" title="<1>执行一下"></a>&lt;1&gt;执行一下</h4><pre><code>[root@master ~]# kubectl apply -f service.yaml </code></pre><h4 id="lt-2-gt-查看一下-1"><a href="#lt-2-gt-查看一下-1" class="headerlink" title="<2>查看一下"></a>&lt;2&gt;查看一下</h4><pre><code>[root@master ~]# kubectl describe svc</code></pre><p>![image-20200114101837151](E:\软件\博客\Blog\blog\source_posts\08 ReplicaSet、DaemonSet.assets\image-20200114101837151.png)</p>
<h4 id="lt-3-gt-访问一下"><a href="#lt-3-gt-访问一下" class="headerlink" title="<3>访问一下"></a>&lt;3&gt;访问一下</h4><pre><code>[root@master ~]# curl 127.0.0.1:30123</code></pre><p>![image-20200114101915248](E:\软件\博客\Blog\blog\source_posts\08 ReplicaSet、DaemonSet.assets\image-20200114101915248.png)</p>
<p><strong>如果标签有多个，标签选择器选择其中一个，也可以关联成功。相反，如果选择器有多个，那么标签必须完全满足条件，才可以关联成功。</strong></p>
<h2 id="4-标签选择器：标签的查询过滤条件。"><a href="#4-标签选择器：标签的查询过滤条件。" class="headerlink" title="4. 标签选择器：标签的查询过滤条件。"></a>4. 标签选择器：标签的查询过滤条件。</h2><p><strong><a href="">基于等值关系的（equality-based）</a>：“=”，“==”，“！ =”前面两个都是相等，最后一个是不等于。</strong></p>
<p><strong><a href="">基于集合关系（set-based）</a>:in、notin、exists三种。选择器列表间为“逻辑与”关系，使用ln或者NotIn操作时，其valuas不强制要求为非空的字符串列表，而使用Exists或DostNotExist时，其values必须为空</strong></p>
<h4 id="使用标签选择器的逻辑："><a href="#使用标签选择器的逻辑：" class="headerlink" title="使用标签选择器的逻辑："></a>使用标签选择器的逻辑：</h4><ul>
<li><strong>同时指定的多个选择器之间的逻辑关系为“与”操作。</strong></li>
<li><strong>使用空值的标签选择器意味着每个资源对象都将把选中。</strong></li>
<li><strong>空的标签选择器无法选中任何资源。</strong></li>
</ul>
<h3 id="（1）例子"><a href="#（1）例子" class="headerlink" title="（1）例子"></a>（1）例子</h3><p>![image-20200114110334223](E:\软件\博客\Blog\blog\source_posts\08 ReplicaSet、DaemonSet.assets\image-20200114110334223.png)</p>
<h4 id="编写一个selector的yaml’文件"><a href="#编写一个selector的yaml’文件" class="headerlink" title="编写一个selector的yaml’文件"></a>编写一个selector的yaml’文件</h4><pre class=" language-yaml"><code class="language-yaml"><span class="token punctuation">[</span>root@master ~<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># vim selector.yaml</span>
<span class="token key atrule">selector</span><span class="token punctuation">:</span>
  <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>
    <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx
  <span class="token key atrule">mathExpressions</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token punctuation">{</span><span class="token key atrule">key</span><span class="token punctuation">:</span> name<span class="token punctuation">,</span><span class="token key atrule">operator</span><span class="token punctuation">:</span> In<span class="token punctuation">,</span><span class="token key atrule">values</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>zhangsan<span class="token punctuation">,</span>lisi<span class="token punctuation">]</span><span class="token punctuation">}</span>
    <span class="token punctuation">-</span> <span class="token punctuation">{</span><span class="token key atrule">key</span><span class="token punctuation">:</span> age<span class="token punctuation">,</span><span class="token key atrule">operator</span><span class="token punctuation">:</span> Exists<span class="token punctuation">,</span>values<span class="token punctuation">:</span><span class="token punctuation">}</span></code></pre>
<ul>
<li><strong>selector：当前控制器匹配Pod对此项副本的标签选择器</strong></li>
<li><strong>matchLabels: 指定键值对表示的标签选择器。</strong></li>
<li><strong>mathExpressions:：基于表达式来指定的标签选择器。</strong></li>
</ul>
<h1 id="DaemonSet"><a href="#DaemonSet" class="headerlink" title="DaemonSet"></a>DaemonSet</h1><p><strong><em>它也是一种pod控制器。</em></strong></p>
<p><strong><em>RC，RS , deployment , daemonset.都是pod控制器。statfukSet，RBAC</em></strong></p>
<h3 id="1-使用场景："><a href="#1-使用场景：" class="headerlink" title="1. 使用场景："></a>1. 使用场景：</h3><p><strong>如果必须将pod运行在固定的某个或某几个节点，且要优先于其他的pod的启动。通常情况下，默认会将每一个节点都运行，并且只能运行一个pod。这种情况推荐使用DeamonSet资源对象。</strong></p>
<ul>
<li><strong>监控程序；</strong></li>
<li><strong>日志收集程序；</strong></li>
<li><strong>集群存储程序；</strong></li>
</ul>
<pre><code>[root@master ~]# kubectl get ds -n kube-system 
//查看一下DaemonSet</code></pre><h3 id="2-DaemonSet-与-Deployment-的区别"><a href="#2-DaemonSet-与-Deployment-的区别" class="headerlink" title="2. DaemonSet 与 Deployment 的区别"></a>2. DaemonSet 与 Deployment 的区别</h3><ul>
<li><strong>Deployment 部署的副本 Pod 会分布在各个 Node 上，每个 Node 都可能运行好几个副本。</strong></li>
<li><strong>DaemonSet 的不同之处在于：每个 Node 上最多只能运行一个副本。</strong></li>
</ul>
<h3 id="3-运行一个web服务，在每一个节点运行一个pod。"><a href="#3-运行一个web服务，在每一个节点运行一个pod。" class="headerlink" title="3. 运行一个web服务，在每一个节点运行一个pod。"></a>3. 运行一个web服务，在每一个节点运行一个pod。</h3><pre><code>[root@master ~]# vim daemonset.yaml

kind: DaemonSet
apiVersion: extensions/v1beta1
metadata:
  name: test-ds
spec:
  template:
    metadata:
      labels:
        name: test-ds
    spec:
      containers:
      - name: test-ds
        image: httpd</code></pre><h4 id="lt-1-gt-执行一下-2"><a href="#lt-1-gt-执行一下-2" class="headerlink" title="<1>执行一下"></a>&lt;1&gt;执行一下</h4><pre><code>[root@master ~]# kubectl apply -f daemonset.yaml </code></pre><h4 id="lt-2-gt-查看一下-2"><a href="#lt-2-gt-查看一下-2" class="headerlink" title="<2>查看一下"></a>&lt;2&gt;查看一下</h4><pre><code>[root@master ~]# kubectl get ds</code></pre><p>![image-20200114112936161](E:\软件\博客\Blog\blog\source_posts\08 ReplicaSet、DaemonSet.assets\image-20200114112936161.png)</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><h2 id="1）总结RC、RS、Deplyment、DaemonSet控制器的特点及使用场景。"><a href="#1）总结RC、RS、Deplyment、DaemonSet控制器的特点及使用场景。" class="headerlink" title="1）总结RC、RS、Deplyment、DaemonSet控制器的特点及使用场景。"></a><strong>1）总结RC、RS、Deplyment、DaemonSet控制器的特点及使用场景。</strong></h2><h3 id="lt-1-gt-Replication-Controller（RC）"><a href="#lt-1-gt-Replication-Controller（RC）" class="headerlink" title="<1>Replication Controller（RC）"></a>&lt;1&gt;Replication Controller（RC）</h3><h4 id="介绍及使用场景"><a href="#介绍及使用场景" class="headerlink" title="介绍及使用场景"></a>介绍及使用场景</h4><p><strong><code>Replication Controller</code>简称<code>RC</code>，<code>RC</code>是<code>Kubernetes</code>系统中的核心概念之一，简单来说，<code>RC</code>可以保证在任意时间运行<code>Pod</code>的副本数量，能够保证<code>Pod</code>总是可用的。如果实际<code>Pod</code>数量比指定的多那就结束掉多余的，如果实际数量比指定的少就新启动一些<code>Pod</code>，当<code>Pod</code>失败、被删除或者挂掉后，<code>RC</code>都会去自动创建新的<code>Pod</code>来保证副本数量，所以即使只有一个<code>Pod</code>，我们也应该使用<code>RC</code>来管理我们的<code>Pod</code>。</strong></p>
<h4 id="主要功能"><a href="#主要功能" class="headerlink" title="主要功能"></a>主要功能</h4><ul>
<li><strong>确保pod数量：RC用来管理正常运行Pod数量，一个RC可以由一个或多个Pod组成，在RC被创建后，系统会根据定义好的副本数来创建Pod数量。在运行过程中，如果Pod数量小于定义的，就会重启停止的或重新分配Pod，反之则杀死多余的。</strong></li>
<li><strong>确保pod健康：当pod不健康，运行出错或者无法提供服务时，RC也会杀死不健康的pod，重新创建新的。</strong></li>
<li><strong>弹性伸缩 ：在业务高峰或者低峰期的时候，可以通过RC动态的调整pod的数量来提高资源的利用率。同时，配置相应的监控功能（Hroizontal Pod Autoscaler），会定时自动从监控平台获取RC关联pod的整体资源使用情况，做到自动伸缩。</strong></li>
<li><strong>滚动升级：滚动升级为一种平滑的升级方式，通过逐步替换的策略，保证整体系统的稳定，在初始化升级的时候就可以及时发现和解决问题，避免问题不断扩大。</strong></li>
</ul>
<h3 id="lt-2-gt-Replication-Set（RS）"><a href="#lt-2-gt-Replication-Set（RS）" class="headerlink" title="<2>Replication Set（RS）"></a>&lt;2&gt;Replication Set（RS）</h3><p><strong>被认为 是“升级版”的RC。RS也是用于保证与label selector匹配的pod数量维持在期望状态。</strong></p>
<blockquote>
<p><strong>实际上<code>RS</code>和<code>RC</code>的功能基本一致，目前唯一的一个区别就是<code>RC</code>只支持基于等式的<code>selector</code>（env=dev或app=nginx），但<code>RS</code>还支持基于集合的<code>selector</code>（version in (v1, v2)），这对复杂的运维管理就非常方便了。</strong></p>
<p><strong><code>kubectl</code>命令行工具中关于<code>RC</code>的大部分命令同样适用于我们的<code>RS</code>资源对象。不过我们也很少会去单独使用<code>RS</code>，它主要被<code>Deployment</code>这个更加高层的资源对象使用，除非用户需要自定义升级功能或根本不需要升级<code>Pod</code>，在一般情况下，我们推荐使用<code>Deployment</code>而不直接使用<code>Replica Set</code>。</strong></p>
</blockquote>
<h4 id="区别在于"><a href="#区别在于" class="headerlink" title="区别在于"></a>区别在于</h4><p><strong>1、RC只支持基于等式的selector（env=dev或environment!=qa），但RS还支持新的，基于集合的selector（version in (v1.0, v2.0)或env notin (dev, qa)），这对复杂的运维管理很方便。</strong></p>
<p><strong>2、升级方式</strong></p>
<ul>
<li><strong>RS不能使用kubectlrolling-update进行升级</strong></li>
<li><strong>kubectl rolling-update专用于rc</strong></li>
<li><strong>RS升级使用deployment或者kubectl replace命令</strong></li>
<li><strong>社区引入这一API的初衷是用于取代vl中的RC，也就是说当v1版本被废弃时，RC就完成了它的历史使命，而由RS来接管其工作</strong></li>
</ul>
<h3 id="lt-3-gt-DaemonSet"><a href="#lt-3-gt-DaemonSet" class="headerlink" title="<3>DaemonSet"></a>&lt;3&gt;DaemonSet</h3><h4 id="1-特点："><a href="#1-特点：" class="headerlink" title="1. 特点："></a>1. 特点：</h4><p><strong>如果必须将pod运行在固定的某个或某几个节点，且要优先于其他的pod的启动。通常情况下，默认会将每一个节点都运行，并且只能运行一个pod。这种情况推荐使用DeamonSet资源对象。</strong></p>
<p><strong>一个DaemonSet对象能确保其创建的Pod在集群中的每一台（或指定）Node上都运行一个副本。如果集群中动态加入了新的Node，DaemonSet中的Pod也会被添加在新加入Node上运行。删除一个DaemonSet也会级联删除所有其创建的Pod。</strong></p>
<h4 id="2-使用环境"><a href="#2-使用环境" class="headerlink" title="2. 使用环境"></a><strong>2. 使用环境</strong></h4><ul>
<li><strong>监控程序；</strong></li>
<li><strong>日志收集程序；</strong></li>
<li><strong>集群存储程序；</strong></li>
</ul>
<h3 id="lt-4-gt-Deployment"><a href="#lt-4-gt-Deployment" class="headerlink" title="<4>Deployment"></a>&lt;4&gt;Deployment</h3><h4 id="1-什么是Deployment"><a href="#1-什么是Deployment" class="headerlink" title="1. 什么是Deployment"></a>1. 什么是Deployment</h4><p><strong>Kubernetes Deployment提供了官方的用于更新Pod和Replica Set（下一代的Replication Controller）的方法，您可以在Deployment对象中只描述您所期望的理想状态（预期的运行状态），Deployment控制器为您将现在的实际状态转换成您期望的状态，例如，您想将所有的webapp:v1.0.9升级成webapp:v1.1.0，您只需创建一个Deployment，Kubernetes会按照Deployment自动进行升级。现在，您可以通过Deployment来创建新的资源（pod，rs，rc），替换已经存在的资源等。</strong></p>
<p><strong>你只需要在Deployment中描述你想要的目标状态是什么，Deployment controller就会帮你将Pod和Replica Set的实际状态改变到你的目标状态。你可以定义一个全新的Deployment，也可以创建一个新的替换旧的Deployment。</strong></p>
<h4 id="2-典型的用例"><a href="#2-典型的用例" class="headerlink" title="2. 典型的用例"></a>2. 典型的用例</h4><ul>
<li><strong>使用Deployment来创建ReplicaSet。ReplicaSet在后台创建pod。检查启动状态，看它是成功还是失败。</strong></li>
<li><strong>然后，通过更新Deployment的PodTemplateSpec字段来声明Pod的新状态。这会创建一个新的ReplicaSet，Deployment会按照控制的速率将pod从旧的ReplicaSet移动到新的ReplicaSet中。</strong></li>
<li><strong>如果当前状态不稳定，回滚到之前的Deployment revision。每次回滚都会更新Deployment的revision。</strong></li>
<li><strong>扩容Deployment以满足更高的负载。</strong></li>
<li><strong>暂停Deployment来应用PodTemplateSpec的多个修复，然后恢复上线。</strong></li>
<li><strong>根据Deployment 的状态判断上线是否hang住了。</strong></li>
<li><strong>清除旧的不必要的ReplicaSet。</strong></li>
</ul>
<h4 id="3-使用环境"><a href="#3-使用环境" class="headerlink" title="3. 使用环境"></a>3. 使用环境</h4><p><strong>Deployment集成了上线部署、滚动升级、创建副本、暂停上线任务，恢复上线任务，回滚到以前某一版本（成功/稳定）的Deployment等功能，在某种程度上，Deployment可以帮我们实现无人值守的上线，大大降低我们的上线过程的复杂沟通、操作风险。</strong></p>
<ul>
<li><strong>定义Deployment来创建Pod和ReplicaSet</strong></li>
<li><strong>滚动升级和回滚应用</strong></li>
<li><strong>扩容和缩容</strong></li>
<li><strong>暂停和继续Deployment</strong></li>
</ul>
<h4 id="3-DaemonSet-与-Deployment-的区别"><a href="#3-DaemonSet-与-Deployment-的区别" class="headerlink" title="3. DaemonSet 与 Deployment 的区别"></a>3. DaemonSet 与 Deployment 的区别</h4><ul>
<li><strong>Deployment 部署的副本 Pod 会分布在各个 Node 上，每个 Node 都可能运行好几个副本。</strong></li>
<li><strong>DaemonSet 的不同之处在于：每个 Node 上最多只能运行一个副本。</strong></li>
</ul>
<h2 id="2）使用DaemonSet控制器运行httpd服务，要求名称以自己的名称命名。标签为：tier-backend-env-dev"><a href="#2）使用DaemonSet控制器运行httpd服务，要求名称以自己的名称命名。标签为：tier-backend-env-dev" class="headerlink" title="2）使用DaemonSet控制器运行httpd服务，要求名称以自己的名称命名。标签为：tier=backend,env=dev."></a><strong>2）使用DaemonSet控制器运行httpd服务，要求名称以自己的名称命名。标签为：tier=backend,env=dev.</strong></h2><pre><code>[root@master ~]# vim daemonset.yaml 
kind: DaemonSet
apiVersion: extensions/v1beta1
metadata:
  name: xgp-ds
spec:
  template:
    metadata:
      labels:
        tier: backend
        env: dev
    spec:
      containers:
      - name: xgp-ds
        image: httpd</code></pre><h3 id="查看一下"><a href="#查看一下" class="headerlink" title="查看一下"></a>查看一下</h3><pre><code>[root@master ~]# kubectl get pod  --show-labels </code></pre><p>![image-20200114100043922](E:\软件\博客\Blog\blog\source_posts\08 ReplicaS儿童、DaemonSet.assets\image-20200114100043922.png)</p>
<pre><code>[root@master ~]# kubectl get pod -L env,tier</code></pre><p>![image-20200114095943595](E:\软件\博客\Blog\blog\source_posts\08 ReplicaSet、DaemonSet.assets\image-20200114095943595.png)</p>
<h2 id="3-创建service资源对象与上述资源进行关联，要有验证。"><a href="#3-创建service资源对象与上述资源进行关联，要有验证。" class="headerlink" title="3) 创建service资源对象与上述资源进行关联，要有验证。"></a><strong>3) 创建service资源对象与上述资源进行关联，要有验证。</strong></h2><pre><code>[root@master ~]# vim service.yaml 
kind: Service
apiVersion: v1
metadata:
  name: service
spec: 
  type: NodePort
  selector: 
    env: dev
  ports:    
  - protocol: TCP
    port: 90 
    targetPort: 80
    nodePort: 30123 </code></pre><h3 id="执行一下"><a href="#执行一下" class="headerlink" title="执行一下"></a>执行一下</h3><pre><code>[root@master ~]# kubectl apply -f service.yaml </code></pre><h3 id="查看一下-1"><a href="#查看一下-1" class="headerlink" title="查看一下"></a>查看一下</h3><pre><code>[root@master ~]# kubectl describe svc</code></pre><p>![image-20200114120345596](E:\软件\博客\Blog\blog\source_posts\08 ReplicaSet、DaemonSet.assets\image-20200114120345596.png)</p>
<h3 id="访问一下"><a href="#访问一下" class="headerlink" title="访问一下"></a>访问一下</h3><pre><code>[root@master ~]# curl 127.0.0.1:30123</code></pre><p>![image-20200114120444524](E:\软件\博客\Blog\blog\source_posts\08 ReplicaSet、DaemonSet.assets\image-20200114120444524.png)</p>
<h2 id="4）整理关于标签和标签选择器都有什么作用？"><a href="#4）整理关于标签和标签选择器都有什么作用？" class="headerlink" title="4）整理关于标签和标签选择器都有什么作用？"></a><strong>4）整理关于标签和标签选择器都有什么作用？</strong></h2><p><strong>&lt;1&gt;标签：解决同类型的资源对象，为了更好的管理，按照标签分组。</strong></p>
<p><strong>&lt;2&gt;标签选择器：标签的查询过滤条件。</strong></p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script></div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/posts/cf38.html">06 pod资源对象</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-04-25</time><div class="content"><h1 id="一，k8s的资源对象"><a href="#一，k8s的资源对象" class="headerlink" title="一，k8s的资源对象"></a>一，k8s的资源对象</h1><p><strong><em>Deployment、Service、Pod是k8s最核心的3个资源对象</em></strong></p>
<blockquote>
<p><strong>Deployment：</strong>最常见的无状态应用的控制器，支持应用的扩缩容、滚动升级等操作。</p>
<p><strong>Service：</strong>为弹性变动且存在生命周期的Pod对象提供了一个固定的访问接口，用于服务发现和服务访问。</p>
<p><strong>Pod：</strong>是运行容器以及调度的最小单位。同一个pod可以同时运行多个容器，这些容器共享net、UTS、IPC，除此之外还有USER、PID、MOUNT。</p>
<p><strong>ReplicationController：</strong>用于确保每个Pod副本在任意时刻都能满足目标数量，简单来说，它用于每个容器或容器组总是运行并且可以访问的：老一代无状态的Pod应用控制器。</p>
<p><strong>RwplicatSet：</strong>新一代的无状态的Pod应用控制器，它与RC的不同之处在于支持的标签选择器不同，RC只支持等值选择器（键值对），RS还额外支持基于集合的选择器。</p>
<p><strong>StatefulSet：</strong>用于管理有状态的持久化应用，如database服务程序，它与Deployment不同之处在于，它会为每一个pod创建一个独有的持久性标识符，并确保每个pod之间的顺序性。</p>
<p><strong>DaemonSet：</strong>用于确保每一个节点都运行了某个pod的一个副本，新增的节点一样会被添加到此类pod，在节点移除时，此pod会被回收。</p>
<p><strong>Job：</strong>用于管理运行完成后即可终止的应用，例如批量处理做作业任务；</p>
<p><strong>volume：</strong>pv pvc<br><strong>ConfigMap：</strong><br><strong>Secret：</strong><br><strong>Role：</strong><br><strong>ClusterRole：</strong><br><strong>RoleBinding：</strong><br><strong>cluster RoleBinding：</strong><br><strong>service account：</strong><br><strong>Helm：</strong></p>
</blockquote>
<h2 id="Pod的生命周期被定义为以下几个阶段。"><a href="#Pod的生命周期被定义为以下几个阶段。" class="headerlink" title="Pod的生命周期被定义为以下几个阶段。"></a>Pod的生命周期被定义为以下几个阶段。</h2><blockquote>
<ul>
<li><strong>Pending：Pod已经被创建，但是一个或者多个容器还未创建，这包括Pod调度阶段，以及容器镜像的下载过程。</strong></li>
<li><strong>Running：Pod已经被调度到Node，所有容器已经创建，并且至少一个容器在运行或者正在重启。</strong></li>
<li><strong>Succeeded：Pod中所有容器正常退出。</strong></li>
<li><strong>Failed：Pod中所有容器退出，至少有一个容器是一次退出的。</strong></li>
</ul>
</blockquote>
<h1 id="环境介绍"><a href="#环境介绍" class="headerlink" title="环境介绍"></a>环境介绍</h1><table>
<thead>
<tr>
<th>主机</th>
<th>IP地址</th>
<th>服务</th>
</tr>
</thead>
<tbody><tr>
<td>master</td>
<td>192.168.1.21</td>
<td>k8s</td>
</tr>
<tr>
<td>node01</td>
<td>192.168.1.22</td>
<td>k8s</td>
</tr>
<tr>
<td>node02</td>
<td>192.168.1.23</td>
<td>k8s</td>
</tr>
</tbody></table>
<h1 id="二，Namespace：名称空间"><a href="#二，Namespace：名称空间" class="headerlink" title="二，Namespace：名称空间"></a>二，Namespace：名称空间</h1><p><strong>默认的名称空间：</strong></p>
<blockquote>
<p><strong>Namespace（命名空间）是kubernetes系统中的另一个重要的概念，通过将系统内部的对象“分配”到不同的Namespace中，形成逻辑上分组的不同项目、小组或用户组，便于不同的分组在共享使用整个集群的资源的同时还能被分别管理。</strong></p>
<p>   <strong>Kubernetes集群在启动后，会创建一个名为“default”的Namespace，如果不特别指明Namespace，则用户创建的Pod、RC、Service都被系统创建到“default”的Namespace中。</strong></p>
</blockquote>
<h2 id="1-查看名称空间"><a href="#1-查看名称空间" class="headerlink" title="1.查看名称空间"></a>1.查看名称空间</h2><pre><code>[root@master ~]# kubectl get namespaces</code></pre><p>![image-20200109094700728](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109094700728.png)</p>
<h2 id="2-查看名称空间详细信息"><a href="#2-查看名称空间详细信息" class="headerlink" title="2.查看名称空间详细信息"></a>2.查看名称空间详细信息</h2><pre><code>[root@master ~]# kubectl describe ns default</code></pre><p>![image-20200109095006067](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109095006067.png)</p>
<h2 id="3-创建名称空间"><a href="#3-创建名称空间" class="headerlink" title="3.创建名称空间"></a>3.创建名称空间</h2><pre><code>[root@master ~]# kubectl create ns bdqn</code></pre><h3 id="查看一下"><a href="#查看一下" class="headerlink" title="查看一下"></a>查看一下</h3><pre><code>[root@master ~]# kubectl get namespaces</code></pre><p>![image-20200109095153448](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109095153448.png)</p>
<h2 id="4-创建namespace的yaml文件"><a href="#4-创建namespace的yaml文件" class="headerlink" title="4.创建namespace的yaml文件"></a>4.创建namespace的yaml文件</h2><h3 id="（1）查看格式"><a href="#（1）查看格式" class="headerlink" title="（1）查看格式"></a>（1）查看格式</h3><pre><code>[root@master ~]# kubectl explain ns
//查看nasespace的yaml文件的格式</code></pre><h3 id="（2）创建namespace的yaml文件"><a href="#（2）创建namespace的yaml文件" class="headerlink" title="（2）创建namespace的yaml文件"></a>（2）创建namespace的yaml文件</h3><pre><code>[root@master ~]# vim test-ns.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: test</code></pre><h3 id="（3）运行namespace的yaml文件"><a href="#（3）运行namespace的yaml文件" class="headerlink" title="（3）运行namespace的yaml文件"></a>（3）运行namespace的yaml文件</h3><pre><code>[root@master ~]# kubectl apply -f test-ns.yaml </code></pre><h3 id="（4）查看一下"><a href="#（4）查看一下" class="headerlink" title="（4）查看一下"></a>（4）查看一下</h3><pre><code>[root@master ~]# kubectl get ns</code></pre><p>![image-20200109095808777](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109095808777.png)</p>
<h2 id="4-删除名称空间"><a href="#4-删除名称空间" class="headerlink" title="4.删除名称空间"></a>4.删除名称空间</h2><pre><code>[root@master ~]# kubectl delete ns test 
[root@master ~]# kubectl delete -f test-ns.yaml </code></pre><p><strong>注意：namespace资源对象进用于资源对象的隔离，并不能隔绝不同名称空间的Pod之间的通信。那是网络策略资源的功能。</strong></p>
<h2 id="5-查看指定名称空间"><a href="#5-查看指定名称空间" class="headerlink" title="5.查看指定名称空间"></a>5.查看指定名称空间</h2><p><strong>可使用–namespace或-n选项</strong></p>
<pre><code>[root@master ~]# kubectl get pod -n kube-system 
[root@master ~]# kubectl get pod --namespace kube-system </code></pre><h1 id="三，Pod"><a href="#三，Pod" class="headerlink" title="三，Pod"></a>三，Pod</h1><h2 id="1-编写一个pod的yaml文件"><a href="#1-编写一个pod的yaml文件" class="headerlink" title="1.编写一个pod的yaml文件"></a>1.编写一个pod的yaml文件</h2><pre><code>[root@master ~]# vim pod.yaml
kind: Pod
apiVersion: v1
metadata:
  name: test-pod
spec:
  containers:
  - name: test-app
    image: 192.168.1.21:5000/web:v1</code></pre><p><strong><em>pod的yaml文件不支持replicas字段</em></strong></p>
<h3 id="（1）运行一下"><a href="#（1）运行一下" class="headerlink" title="（1）运行一下"></a>（1）运行一下</h3><pre><code>[root@master ~]# kubectl apply -f pod.yaml </code></pre><h3 id="（2）查看一下"><a href="#（2）查看一下" class="headerlink" title="（2）查看一下"></a>（2）查看一下</h3><pre><code>[root@master ~]# kubectl get pod</code></pre><p>![image-20200109100836911](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109100836911.png)</p>
<p><strong><em>ps：这个pod因为是自己创建的，所以删除之后k8s并不会自动生成，相当于docker中创建</em></strong></p>
<h2 id="2-指定pod的namespace名称空间"><a href="#2-指定pod的namespace名称空间" class="headerlink" title="2.指定pod的namespace名称空间"></a>2.指定pod的namespace名称空间</h2><h3 id="（1）修改pod的yaml文件"><a href="#（1）修改pod的yaml文件" class="headerlink" title="（1）修改pod的yaml文件"></a>（1）修改pod的yaml文件</h3><pre><code>[root@master ~]# vim pod.yaml
kind: Pod        #资源类型
apiVersion: v1   #api版本
metadata:
  name: test-pod    #指定控制器名称
  namespace: bdqn   #指定namespace（名称空间）
spec:
  containers:      #容器
  - name: test-app  #容器名称
    image: 192.168.1.21:5000/web:v1  #镜像</code></pre><h5 id="执行一下"><a href="#执行一下" class="headerlink" title="执行一下"></a>执行一下</h5><pre><code>[root@master ~]# kubectl apply -f pod.yaml </code></pre><h3 id="（2）查看一下-1"><a href="#（2）查看一下-1" class="headerlink" title="（2）查看一下"></a>（2）查看一下</h3><pre><code>[root@master ~]#  kubectl get pod -n bdqn 
//根据namespace名称查看</code></pre><p>![image-20200109101521992](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109101521992.png)</p>
<h2 id="3-pod中镜像获取策略"><a href="#3-pod中镜像获取策略" class="headerlink" title="3.pod中镜像获取策略"></a>3.pod中镜像获取策略</h2><blockquote>
<p><strong>Always：</strong>镜像标签为“laster”或镜像不存在时，总是从指定的仓库中获取镜像。</p>
<p><strong>IfNotPresent：</strong>仅当本地镜像不存在时才从目标仓库下载。</p>
<p><strong>Never：</strong>禁止从仓库中下载镜像，即只使用本地镜像。</p>
</blockquote>
<p><strong><em>注意：对于标签为“laster”或者标签不存在，其默认的镜像下载策略为“Always”，而对于其他的标签镜像，默认策略为“IfNotPresent”。</em></strong></p>
<h2 id="4-观察pod和service的不同并关联"><a href="#4-观察pod和service的不同并关联" class="headerlink" title="4.观察pod和service的不同并关联"></a>4.观察pod和service的不同并关联</h2><h3 id="（1）pod的yaml文件（指定端口）"><a href="#（1）pod的yaml文件（指定端口）" class="headerlink" title="（1）pod的yaml文件（指定端口）"></a>（1）pod的yaml文件（指定端口）</h3><pre><code>[root@master ~]# vim pod.yaml 
kind: Pod          #资源类型
apiVersion: v1      #api版本
metadata:
  name: test-pod       #指定控制器名称
  namespace: bdqn   #指定namespace（名称空间）
spec:
  containers:                          #容器
  - name: test-app                    #容器名称
    image: 192.168.1.21:5000/web:v1   #镜像
    imagePullPolicy: IfNotPresent   #获取的策略
    ports:
    - protocol: TCP
      containerPort: 80  </code></pre><h4 id="lt-1-gt-删除之前的pod"><a href="#lt-1-gt-删除之前的pod" class="headerlink" title="<1>删除之前的pod"></a>&lt;1&gt;删除之前的pod</h4><pre><code>[root@master ~]# kubectl delete pod -n bdqn test-pod </code></pre><h4 id="lt-2-gt-执行一下"><a href="#lt-2-gt-执行一下" class="headerlink" title="<2>执行一下"></a>&lt;2&gt;执行一下</h4><pre><code>[root@master ~]# kubectl apply -f pod.yaml </code></pre><h4 id="lt-3-gt-查看一下"><a href="#lt-3-gt-查看一下" class="headerlink" title="<3>查看一下"></a>&lt;3&gt;查看一下</h4><pre><code>[root@master ~]# kubectl get pod -n bdqn </code></pre><p>![image-20200109110215669](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109110215669.png)</p>
<h3 id="（2）pod的yaml文件（修改端口）"><a href="#（2）pod的yaml文件（修改端口）" class="headerlink" title="（2）pod的yaml文件（修改端口）"></a>（2）pod的yaml文件（修改端口）</h3><pre><code>[root@master ~]# vim pod.yaml 
kind: Pod
apiVersion: v1
metadata:
  name: test-pod
  namespace: bdqn
spec:
  containers:
  - name: test-app
    image: 192.168.1.21:5000/web:v1
    imagePullPolicy: IfNotPresent
    ports:
    - protocol: TCP
      containerPort: 90   #改一下端口</code></pre><h4 id="lt-1-gt-删除之前的pod-1"><a href="#lt-1-gt-删除之前的pod-1" class="headerlink" title="<1>删除之前的pod"></a>&lt;1&gt;删除之前的pod</h4><pre><code>[root@master ~]# kubectl delete pod -n bdqn test-pod </code></pre><h4 id="lt-2-gt-执行一下-1"><a href="#lt-2-gt-执行一下-1" class="headerlink" title="<2>执行一下"></a>&lt;2&gt;执行一下</h4><pre><code>[root@master ~]# kubectl apply -f pod.yaml </code></pre><h4 id="lt-3-gt-查看一下-1"><a href="#lt-3-gt-查看一下-1" class="headerlink" title="<3>查看一下"></a>&lt;3&gt;查看一下</h4><pre><code>[root@master ~]# kubectl get pod -n bdqn -o wide</code></pre><p>![image-20200109110409584](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109110409584.png)</p>
<h4 id="lt-4-gt-访问一下"><a href="#lt-4-gt-访问一下" class="headerlink" title="<4>访问一下"></a>&lt;4&gt;访问一下</h4><p>![image-20200109110430334](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109110430334.png)</p>
<p><strong>会发现修改的90端口并不生效，他只是一个提示字段并不生效。</strong></p>
<h3 id="（3）pod的yaml文件（添加标签）"><a href="#（3）pod的yaml文件（添加标签）" class="headerlink" title="（3）pod的yaml文件（添加标签）"></a>（3）pod的yaml文件（添加标签）</h3><pre><code>[root@master ~]# vim pod.yaml 
kind: Pod
apiVersion: v1
metadata:
  name: test-pod
  namespace: bdqn
  labels:                 #标签
    app: test-web          #标签名称
spec:
  containers:
  - name: test-app
    image: 192.168.1.21:5000/web:v1
    imagePullPolicy: IfNotPresent
    ports:
    - protocol: TCP
      containerPort: 90   #改一下端口</code></pre><h4 id="————————————–pod———————————————"><a href="#————————————–pod———————————————" class="headerlink" title="————————————–pod———————————————"></a>————————————–pod———————————————</h4><h2 id="（4）编写一个service的yaml文件"><a href="#（4）编写一个service的yaml文件" class="headerlink" title="（4）编写一个service的yaml文件"></a>（4）编写一个service的yaml文件</h2><pre><code>[root@master ~]# vim test-svc.yaml 
apiVersion: v1      #api版本
kind: Service          #资源类型
metadata:
  name: test-svc       #指定控制器名称
  namespace: bdqn   #指定namespace（名称空间）
spec:
  selector:          #标签
    app: test-web    #标签名称（须和pod的标签名称一致）
  ports:              
  - port: 80          #宿主机端口
    targetPort: 80    #容器端口</code></pre><p><strong><em>会发现添加的80端口生效了，所以不能乱改。</em></strong></p>
<h4 id="lt-1-gt-执行一下"><a href="#lt-1-gt-执行一下" class="headerlink" title="<1>执行一下"></a>&lt;1&gt;执行一下</h4><pre><code>[root@master ~]# kubectl apply -f test-svc.yaml</code></pre><h4 id="lt-2-gt-查看一下"><a href="#lt-2-gt-查看一下" class="headerlink" title="<2>查看一下"></a>&lt;2&gt;查看一下</h4><pre><code>[root@master ~]# kubectl get svc -n bdqn </code></pre><p>![image-20200109121106859](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109121106859.png)</p>
<pre><code>[root@master ~]# kubectl describe svc -n bdqn test-svc </code></pre><p>![image-20200109121139399](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109121139399.png)</p>
<h4 id="lt-4-gt-访问一下-1"><a href="#lt-4-gt-访问一下-1" class="headerlink" title="<4>访问一下"></a>&lt;4&gt;访问一下</h4><pre><code>[root@master ~]# curl 10.98.57.97 </code></pre><p>![image-20200109121205607](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109121205607.png)</p>
<h4 id="————————————–service———————————————"><a href="#————————————–service———————————————" class="headerlink" title="————————————–service———————————————"></a>————————————–service———————————————</h4><h1 id="四，容器的重启策略"><a href="#四，容器的重启策略" class="headerlink" title="四，容器的重启策略"></a>四，容器的重启策略</h1><p><strong>Pod的重启策略（RestartPolicy）应用与Pod内所有容器，并且仅在Pod所处的Node上由kubelet进行判断和重启操作。当某个容器异常退出或者健康检查失败时，kubelet将根据RestartPolicy的设置来进行相应的操作。</strong></p>
<blockquote>
<p><strong>Always：</strong>（默认情况下使用）但凡Pod对象终止就将其重启；<br><strong>OnFailure：</strong>仅在Pod对象出现错误时才将其重启；<br><strong>Never：</strong>从不重启；</p>
</blockquote>
<h1 id="五，pod的默认健康检查"><a href="#五，pod的默认健康检查" class="headerlink" title="五，pod的默认健康检查"></a>五，pod的默认健康检查</h1><p><strong>每个容器启动时都会执行一个进程，此进程由 Dockerfile 的 CMD 或 ENTRYPOINT 指定。如果进程退出时返回码非零，则认为容器发生故障，Kubernetes 就会根据 <code>restartPolicy</code> 重启容器。</strong></p>
<h2 id="（1）编写健康检查的yaml文件"><a href="#（1）编写健康检查的yaml文件" class="headerlink" title="（1）编写健康检查的yaml文件"></a>（1）编写健康检查的yaml文件</h2><p><strong>下面我们模拟一个容器发生故障的场景，Pod 配置文件如下：</strong></p>
<pre><code>[root@master ~]# vim healcheck.yaml 
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: healcheck
  name:  healcheck
spec:
  restartPolicy: OnFailure  #指定重启策略
  containers:
  - name:  healcheck
    image: busybox:latest
    args:                   #生成pod时运行的命令
    - /bin/sh
    - -c
    - sleep 20; exit 1 </code></pre><h3 id="lt-1-gt-执行一下-1"><a href="#lt-1-gt-执行一下-1" class="headerlink" title="<1>执行一下"></a>&lt;1&gt;执行一下</h3><pre><code>[root@master ~]# kubectl apply -f  healcheck.yaml</code></pre><h3 id="lt-2-gt-查看一下-1"><a href="#lt-2-gt-查看一下-1" class="headerlink" title="<2>查看一下"></a>&lt;2&gt;查看一下</h3><pre><code>[root@master ~]# kubectl get pod -o wide</code></pre><p>![image-20200109121809350](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109121809350.png)</p>
<pre><code>[root@master ~]# kubectl get pod -w | grep healcheck</code></pre><p>![image-20200109121817775](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109121817775.png)</p>
<p><strong>在上面的例子中，容器进程返回值非零，Kubernetes 则认为容器发生故障，需要重启。但有不少情况是发生了故障，但进程并不会退出。</strong></p>
<h1 id="六，小实验"><a href="#六，小实验" class="headerlink" title="六，小实验"></a>六，小实验</h1><h2 id="1）以自己的名称创建一个k8s名称空间，以下所有操作都在此名称空间中。"><a href="#1）以自己的名称创建一个k8s名称空间，以下所有操作都在此名称空间中。" class="headerlink" title="1）以自己的名称创建一个k8s名称空间，以下所有操作都在此名称空间中。"></a>1）以自己的名称创建一个k8s名称空间，以下所有操作都在此名称空间中。</h2><h3 id="（1）创建名称空间"><a href="#（1）创建名称空间" class="headerlink" title="（1）创建名称空间"></a>（1）创建名称空间</h3><pre><code>[root@master ~]# kubectl create ns xgp</code></pre><h3 id="（2）查看一下-2"><a href="#（2）查看一下-2" class="headerlink" title="（2）查看一下"></a>（2）查看一下</h3><pre><code>[root@master ~]# kubectl get ns xgp </code></pre><p>![image-20200109133106300](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109133106300.png)</p>
<h2 id="2）创建一个Pod资源对象，使用的是私有仓库中私有镜像，其镜像的下载策略为：NEVER。-Pod的重启策略为：-Never"><a href="#2）创建一个Pod资源对象，使用的是私有仓库中私有镜像，其镜像的下载策略为：NEVER。-Pod的重启策略为：-Never" class="headerlink" title="2）创建一个Pod资源对象，使用的是私有仓库中私有镜像，其镜像的下载策略为：NEVER。 Pod的重启策略为： Never."></a>2）创建一个Pod资源对象，使用的是私有仓库中私有镜像，其镜像的下载策略为：NEVER。 Pod的重启策略为： Never.</h2><pre><code>[root@master ~]# vim pod.yaml
kind: Pod
apiVersion: v1
metadata:
  name: test-pod
  namespace: xgp
  labels:
    app: test-web
spec:
  restartPolicy: Never
  containers:
  - name: www
    image: 192.168.1.21:5000/web:v1
    imagePullPolicy: Never
    args:                   
    - /bin/sh
    - -c
    - sleep 90; exit 1
    ports:
    - protocol: TCP
      containerPort: 80</code></pre><h2 id="3）创建出容器之后，执行非正常退出，查看Pod的最终状态。"><a href="#3）创建出容器之后，执行非正常退出，查看Pod的最终状态。" class="headerlink" title="3）创建出容器之后，执行非正常退出，查看Pod的最终状态。"></a>3）创建出容器之后，执行非正常退出，查看Pod的最终状态。</h2><h3 id="（1）执行一下上面pod的yaml文件"><a href="#（1）执行一下上面pod的yaml文件" class="headerlink" title="（1）执行一下上面pod的yaml文件"></a>（1）执行一下上面pod的yaml文件</h3><pre><code>[root@master ~]# kubectl apply -f pod.yaml </code></pre><h3 id="（2）动态查看ns中test-pod的信息"><a href="#（2）动态查看ns中test-pod的信息" class="headerlink" title="（2）动态查看ns中test-pod的信息"></a>（2）动态查看ns中test-pod的信息</h3><pre><code>[root@master ~]# kubectl get pod -n xgp  -w | grep test-pod</code></pre><p>![image-20200109135543482](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109135543482.png)</p>
<blockquote>
<p><strong>删除test-pod</strong></p>
<pre><code>[root@master ~]# kubectl delete pod -n xgp test-pod </code></pre></blockquote>
<h2 id="4-创建一个Service资源对象，与上述Pod对象关联，验证他们的关联性。"><a href="#4-创建一个Service资源对象，与上述Pod对象关联，验证他们的关联性。" class="headerlink" title="4) 创建一个Service资源对象，与上述Pod对象关联，验证他们的关联性。"></a>4) 创建一个Service资源对象，与上述Pod对象关联，验证他们的关联性。</h2><h3 id="（1）修改pod的yaml文件-1"><a href="#（1）修改pod的yaml文件-1" class="headerlink" title="（1）修改pod的yaml文件"></a>（1）修改pod的yaml文件</h3><pre><code>[root@master ~]# vim pod.yaml
kind: Pod
apiVersion: v1
metadata:
  name: test-pod
  namespace: xgp
  labels:
    app: test-web
spec:
  restartPolicy: Never
  containers:
  - name: www
    image: 192.168.1.21:5000/web:v1
    imagePullPolicy: Never
    ports:
    - protocol: TCP
      containerPort: 80</code></pre><h3 id="（1）编写service的yaml文件"><a href="#（1）编写service的yaml文件" class="headerlink" title="（1）编写service的yaml文件"></a>（1）编写service的yaml文件</h3><pre><code>[root@master ~]# vim svc.yaml 
apiVersion: v1
kind: Service
metadata:
  name: test-svc
  namespace: xgp
spec:
  selector:
    app: test-web
  ports:
  - port: 80
    targetPort: 80</code></pre><h3 id="（2）执行一下"><a href="#（2）执行一下" class="headerlink" title="（2）执行一下"></a>（2）执行一下</h3><pre><code>[root@master ~]# kubectl apply -f svc.yaml </code></pre><h3 id="（3）查看一下"><a href="#（3）查看一下" class="headerlink" title="（3）查看一下"></a>（3）查看一下</h3><pre><code>[root@master ~]# kubectl get  pod -o wide -n xgp </code></pre><p>![image-20200109141712910](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109141712910.png)</p>
<h3 id="（4）访问一下"><a href="#（4）访问一下" class="headerlink" title="（4）访问一下"></a>（4）访问一下</h3><pre><code>[root@master ~]# curl 10.244.1.21</code></pre><p>![image-20200109141749352](E:\软件\博客\Blog\blog\source_posts\06 pod资源对象.assets\image-20200109141749352.png)</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script></div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/posts/936d.html">05 Delpoyment、service</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-04-25</time><div class="content"><h1 id="Deployment介绍"><a href="#Deployment介绍" class="headerlink" title="Deployment介绍"></a>Deployment介绍</h1><p><strong>Deployment是kubernetes 1.2引入的概念，用来解决Pod的编排问题。Deployment可以理解为RC的升级版（RC+Reolicat Set）。特点在于可以随时知道Pod的部署进度，即对Pod的创建、调度、绑定节点、启动容器完整过程的进度展示。</strong></p>
<h2 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h2><blockquote>
<p>创建一个Deployment对象来生成对应的Replica Set并完成Pod副本的创建过程。<br>检查Deployment的状态来确认部署动作是否完成（Pod副本的数量是否达到预期值）。<br>更新Deployment以创建新的Pod(例如镜像升级的场景)。<br>如果当前Deployment不稳定，回退到上一个Deployment版本。<br>挂起或恢复一个Deployment。</p>
</blockquote>
<h1 id="Service介绍"><a href="#Service介绍" class="headerlink" title="Service介绍"></a>Service介绍</h1><p><img src="https://img-blog.csdn.net/20170809212910268?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaHV3aF8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p>
<p><strong>Service定义了一个服务的访问入口地址，前端应用通过这个入口地址访问其背后的一组由Pod副本组成的集群实例，Service与其后端的Pod副本集群之间是通过Label Selector来实现“无缝对接”。RC保证Service的Pod副本实例数目保持预期水平。</strong></p>
<h2 id="外部系统访问Service的问题"><a href="#外部系统访问Service的问题" class="headerlink" title="外部系统访问Service的问题"></a>外部系统访问Service的问题</h2><table>
<thead>
<tr>
<th align="left">IP类型</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Node IP</td>
<td align="left">Node节点的IP地址</td>
</tr>
<tr>
<td align="left">Pod IP</td>
<td align="left">Pod的IP地址</td>
</tr>
<tr>
<td align="left">Cluster IP</td>
<td align="left">Service的IP地址</td>
</tr>
</tbody></table>
<h1 id="环境介绍"><a href="#环境介绍" class="headerlink" title="环境介绍"></a>环境介绍</h1><table>
<thead>
<tr>
<th>主机</th>
<th>IP地址</th>
<th>服务</th>
</tr>
</thead>
<tbody><tr>
<td><strong>master</strong></td>
<td><strong>192.168.1.21</strong></td>
<td><strong>k8s</strong></td>
</tr>
<tr>
<td><strong>node01</strong></td>
<td><strong>192.168.1.22</strong></td>
<td><strong>k8s</strong></td>
</tr>
<tr>
<td><strong>node02</strong></td>
<td><strong>192.168.1.23</strong></td>
<td><strong>k8s</strong></td>
</tr>
</tbody></table>
<h1 id="一，Delpoyment和service的简单使用"><a href="#一，Delpoyment和service的简单使用" class="headerlink" title="一，Delpoyment和service的简单使用"></a>一，Delpoyment和service的简单使用</h1><h2 id="1-练习写一个yaml文件，要求使用自己的私有镜像，要求副本数量为三个。"><a href="#1-练习写一个yaml文件，要求使用自己的私有镜像，要求副本数量为三个。" class="headerlink" title="1.练习写一个yaml文件，要求使用自己的私有镜像，要求副本数量为三个。"></a>1.练习写一个yaml文件，要求使用自己的私有镜像，要求副本数量为三个。</h2><pre><code>[root@master ~]# vim xgp.yaml
kind: Deployment
apiVersion: extensions/v1beta1
metadata:
  name: xgp-web
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: xgp-server
    spec:
      containers:
      - name: web
        image: 192.168.1.21:5000/web:v1</code></pre><h3 id="（1）执行一下"><a href="#（1）执行一下" class="headerlink" title="（1）执行一下"></a>（1）执行一下</h3><pre><code>[root@master ~]# kubectl apply -f xgp.yaml  --recore</code></pre><h3 id="（2）查看一下"><a href="#（2）查看一下" class="headerlink" title="（2）查看一下"></a>（2）查看一下</h3><pre><code>[root@master ~]# kubectl get pod</code></pre><p>![image-20200108090638488](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108090638488.png)</p>
<h3 id="（3）访问一下"><a href="#（3）访问一下" class="headerlink" title="（3）访问一下"></a>（3）访问一下</h3><pre><code>[root@master ~]# curl 10.244.2.16</code></pre><p>![image-20200108090817058](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108090817058.png)</p>
<h3 id="（4）更新一下yaml文件，副本加一"><a href="#（4）更新一下yaml文件，副本加一" class="headerlink" title="（4）更新一下yaml文件，副本加一"></a>（4）更新一下yaml文件，副本加一</h3><pre><code>[root@master ~]# vim xgp.yaml
kind: Deployment
apiVersion: extensions/v1beta1
metadata:
  name: xgp-web
spec:
  replicas: 4
  template:
    metadata:
      labels:
        app: xgp-server
    spec:
      containers:
      - name: web
        image: 192.168.1.21:5000/web:v1</code></pre><h4 id="lt-1-gt-执行一下"><a href="#lt-1-gt-执行一下" class="headerlink" title="<1>执行一下"></a>&lt;1&gt;执行一下</h4><pre><code>[root@master ~]# kubectl apply -f xgp.yaml --recore</code></pre><h4 id="lt-2-gt-查看一下"><a href="#lt-2-gt-查看一下" class="headerlink" title="<2>查看一下"></a>&lt;2&gt;查看一下</h4><pre><code>[root@master ~]# kubectl get pod</code></pre><p>![image-20200108091104534](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108091104534.png)</p>
<p><strong><em>副本数量加一，如果yaml文件的副本为0，则副本数量还是之前的状态，并不会更新。</em></strong></p>
<h2 id="2-练习写一个service文件"><a href="#2-练习写一个service文件" class="headerlink" title="2.练习写一个service文件"></a>2.练习写一个service文件</h2><pre><code>[root@master ~]# vim xgp-svc.yaml
kind: Service
apiVersion: v1
metadata:
  name: xgp-svc
spec:
  selector:
    app: xgp-server
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80</code></pre><h3 id="（1）执行一下-1"><a href="#（1）执行一下-1" class="headerlink" title="（1）执行一下"></a>（1）执行一下</h3><pre><code>[root@master ~]# kubectl apply -f xgp-svc.yaml </code></pre><h3 id="（2）查看一下-1"><a href="#（2）查看一下-1" class="headerlink" title="（2）查看一下"></a>（2）查看一下</h3><pre><code>[root@master ~]# kubectl get svc</code></pre><p>![image-20200108091909396](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108091909396.png)</p>
<h3 id="（3）访问一下-1"><a href="#（3）访问一下-1" class="headerlink" title="（3）访问一下"></a>（3）访问一下</h3><pre><code>[root@master ~]# curl 10.107.119.49</code></pre><p>![image-20200108092011164](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108092011164.png)</p>
<h2 id="3-修改yaml文件"><a href="#3-修改yaml文件" class="headerlink" title="3.修改yaml文件"></a>3.修改yaml文件</h2><pre><code>[root@master ~]# vim xgp.yaml 
kind: Deployment
apiVersion: extensions/v1beta1
metadata:
  name: xgp-web
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: xgp-server
    spec:
      containers:
      - name: web
        image: 192.168.1.21:5000/web:v1
        ports:
          - containerPort: 80  #提示端口</code></pre><p><strong><em>注意：在Delpoyment资源对象中，可以添加Port字段，但此字段仅供用户查看，并不实际生效</em></strong></p>
<h3 id="执行一下"><a href="#执行一下" class="headerlink" title="执行一下"></a>执行一下</h3><pre><code>[root@master ~]# kubectl apply -f xgp.yaml --recore</code></pre><h2 id="4-service文件映射端口"><a href="#4-service文件映射端口" class="headerlink" title="4.service文件映射端口"></a>4.service文件映射端口</h2><pre><code>[root@master ~]# vim xgp-svc.yaml 
kind: Service
apiVersion: v1
metadata:
  name: xgp-svc
spec:
  type: NodePort
  selector:
    app: xgp-server
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
      nodePort: 30123</code></pre><h3 id="执行一下-1"><a href="#执行一下-1" class="headerlink" title="执行一下"></a>执行一下</h3><pre><code>[root@master ~]# kubectl apply -f xgp-svc.yaml </code></pre><h3 id="查看一下"><a href="#查看一下" class="headerlink" title="查看一下"></a>查看一下</h3><pre><code>[root@master ~]# kubectl get svc</code></pre><p>![image-20200108094404773](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108094404773.png)</p>
<h3 id="访问一下"><a href="#访问一下" class="headerlink" title="访问一下"></a>访问一下</h3><pre><code>[root@master ~]# curl 127.0.0.1:30123</code></pre><p>![image-20200108094439682](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108094439682.png)</p>
<p>![image-20200108094501253](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108094501253.png)</p>
<h2 id="5-修改三个pod页面内容"><a href="#5-修改三个pod页面内容" class="headerlink" title="5.修改三个pod页面内容"></a>5.修改三个pod页面内容</h2><h3 id="（1）查看一下pod信息"><a href="#（1）查看一下pod信息" class="headerlink" title="（1）查看一下pod信息"></a>（1）查看一下pod信息</h3><pre><code>[root@master ~]# kubectl get pod -o wide</code></pre><p>![image-20200108094953119](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108094953119.png)</p>
<h3 id="（2）修改POD页面内容（三台不一样）"><a href="#（2）修改POD页面内容（三台不一样）" class="headerlink" title="（2）修改POD页面内容（三台不一样）"></a>（2）修改POD页面内容（三台不一样）</h3><pre><code>[root@master ~]# kubectl exec -it xgp-web-8d5f9656f-8z7d9 /bin/bash
//根据pod名称进入pod之中</code></pre><h3 id="进入容器后修改页面内容"><a href="#进入容器后修改页面内容" class="headerlink" title="进入容器后修改页面内容"></a>进入容器后修改页面内容</h3><pre><code>root@xgp-web-8d5f9656f-8z7d9:/usr/local/apache2# echo xgp-v1 &gt; htdocs/index.html 
root@xgp-web-8d5f9656f-8z7d9:/usr/local/apache2# exit</code></pre><h3 id="访问一下-1"><a href="#访问一下-1" class="headerlink" title="访问一下"></a>访问一下</h3><pre><code>[root@master ~]# curl 127.0.0.1:30123</code></pre><p>![image-20200108095626532](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108095626532.png)</p>
<h1 id="二-分析一下k8s负载均衡原理"><a href="#二-分析一下k8s负载均衡原理" class="headerlink" title="二.分析一下k8s负载均衡原理"></a>二.分析一下k8s负载均衡原理</h1><h3 id="（1）查看service的暴露IP"><a href="#（1）查看service的暴露IP" class="headerlink" title="（1）查看service的暴露IP"></a>（1）查看service的暴露IP</h3><pre><code>[root@master ~]# kubectl get svc</code></pre><p>![image-20200108101539835](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108101539835.png)</p>
<h3 id="（2）查看一下iptabes规则"><a href="#（2）查看一下iptabes规则" class="headerlink" title="（2）查看一下iptabes规则"></a>（2）查看一下iptabes规则</h3><pre><code>[root@master ~]# iptables-save 
//查看已配置的规则</code></pre><blockquote>
<p>SNAT：Source NAT（源地址转换）    </p>
<p>DNAT：Destination NAT（目标地址转换）</p>
<p>MASQ：动态的源地址转换</p>
</blockquote>
<h3 id="（3）根据service的暴露IP，查看对应的iptabes规则"><a href="#（3）根据service的暴露IP，查看对应的iptabes规则" class="headerlink" title="（3）根据service的暴露IP，查看对应的iptabes规则"></a>（3）根据service的暴露IP，查看对应的iptabes规则</h3><pre><code>[root@master ~]# iptables-save | grep 10.107.119.49</code></pre><p>![image-20200108101726315](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108101726315.png)</p>
<pre><code>[root@master ~]# iptables-save | grep KUBE-SVC-ESI7C72YHAUGMG5S</code></pre><p>![image-20200108102003596](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108102003596.png)</p>
<h3 id="（4）对应一下IP是否一致"><a href="#（4）对应一下IP是否一致" class="headerlink" title="（4）对应一下IP是否一致"></a>（4）对应一下IP是否一致</h3><pre><code>[root@master ~]# iptables-save | grep KUBE-SEP-ZHDQ73ZKUBMELLJB</code></pre><p>![image-20200108102137062](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108102137062.png)</p>
<pre><code>[root@master ~]# kubectl get pod -o wide</code></pre><p>![image-20200108102203144](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108102203144.png)</p>
<p><strong>Service实现的负载均衡：默认使用的是iptables规则。IPVS</strong></p>
<h1 id="三-回滚到指定版本"><a href="#三-回滚到指定版本" class="headerlink" title="三.回滚到指定版本"></a>三.回滚到指定版本</h1><h3 id="（1）删除之前创建的delpoy和service"><a href="#（1）删除之前创建的delpoy和service" class="headerlink" title="（1）删除之前创建的delpoy和service"></a>（1）删除之前创建的delpoy和service</h3><pre><code>[root@master ~]# kubectl  delete -f xgp.yaml 
[root@master ~]# kubectl  delete -f xgp-svc.yaml </code></pre><h3 id="（2）准备三个版本所使用的私有镜像，来模拟每次升级不同的镜像"><a href="#（2）准备三个版本所使用的私有镜像，来模拟每次升级不同的镜像" class="headerlink" title="（2）准备三个版本所使用的私有镜像，来模拟每次升级不同的镜像"></a>（2）准备三个版本所使用的私有镜像，来模拟每次升级不同的镜像</h3><pre><code>[root@master ~]# vim xgp1.yaml  （三个文件名不相同）
kind: Deployment
apiVersion: extensions/v1beta1
metadata:
  name: xgp-web
spec:
  revisionHistoryLimit: 10
  replicas: 3
  template:
    metadata:
      labels:
        app: xgp-server
    spec:
      containers:
      - name: web
        image: 192.168.1.21:5000/web:v1  （三台版本不同）
        ports:
          - containerPort: 80</code></pre><p>此处3个yaml文件 指定不同版本的镜像</p>
<h3 id="（3）运行三个服务，并记录三个版本信息"><a href="#（3）运行三个服务，并记录三个版本信息" class="headerlink" title="（3）运行三个服务，并记录三个版本信息"></a>（3）运行三个服务，并记录三个版本信息</h3><pre><code>[root@master ~]# kubectl apply -f xgp-1.yaml --record 
[root@master ~]# kubectl apply -f xgp-2.yaml --record 
[root@master ~]# kubectl apply -f xgp-3.yaml --record </code></pre><h3 id="（4）查看有哪些版本信息"><a href="#（4）查看有哪些版本信息" class="headerlink" title="（4）查看有哪些版本信息"></a>（4）查看有哪些版本信息</h3><pre><code>[root@master ~]# kubectl rollout history deployment xgp-web </code></pre><p>![image-20200108105842447](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108105842447.png)</p>
<h3 id="（5）运行之前的service文件"><a href="#（5）运行之前的service文件" class="headerlink" title="（5）运行之前的service文件"></a>（5）运行之前的service文件</h3><pre><code>[root@master ~]# kubectl apply -f xgp-svc.yaml</code></pre><h3 id="（6）查看service暴露端口"><a href="#（6）查看service暴露端口" class="headerlink" title="（6）查看service暴露端口"></a>（6）查看service暴露端口</h3><pre><code>[root@master ~]# kubectl get svc</code></pre><p>![image-20200108110014614](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108110014614.png)</p>
<h3 id="（7）测试访问"><a href="#（7）测试访问" class="headerlink" title="（7）测试访问"></a>（7）测试访问</h3><pre><code>[root@master ~]# curl 127.0.0.1:30123</code></pre><p>![image-20200108110049396](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108110049396.png)</p>
<h3 id="（8）回滚到指定版本"><a href="#（8）回滚到指定版本" class="headerlink" title="（8）回滚到指定版本"></a>（8）回滚到指定版本</h3><pre><code>[root@master ~]# kubectl rollout undo deployment xgp-web --to-revision=1
//这里指定的是版本信息的编号</code></pre><h4 id="lt-1-gt-访问一下"><a href="#lt-1-gt-访问一下" class="headerlink" title="<1>访问一下"></a>&lt;1&gt;访问一下</h4><pre><code>[root@master ~]# curl 127.0.0.1:30123</code></pre><p>![image-20200108110337266](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108110337266.png)</p>
<h4 id="lt-2-gt-查看有哪些版本信息"><a href="#lt-2-gt-查看有哪些版本信息" class="headerlink" title="<2>查看有哪些版本信息"></a>&lt;2&gt;查看有哪些版本信息</h4><pre><code>[root@master ~]# kubectl rollout history deployment xgp-web </code></pre><p>![image-20200108110443558](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108110443558.png)</p>
<p><strong><em>编号1已经被编号2替代，从而生的是一个新的编号4</em></strong></p>
<h1 id="四-用label控制pod的位置"><a href="#四-用label控制pod的位置" class="headerlink" title="四.用label控制pod的位置"></a>四.用label控制pod的位置</h1><blockquote>
<p>默认情况下，scheduler会将pod调度到所有可用的Node，不过有些情况我们希望将 Pod 部署到指定的 Node，比如将有大量磁盘 I/O 的 Pod 部署到配置了 SSD 的 Node；或者 Pod 需要 GPU，需要运行在配置了 GPU 的节点上。</p>
<p>kubernetes通过label来实现这个功能</p>
<p>label 是 key-value 对，各种资源都可以设置 label，灵活添加各种<strong>自定义属性</strong>。比如执行如下命令标注 k8s-node1 是配置了 SSD 的节点</p>
</blockquote>
<h4 id="首先我们给node1节点打上一个ssd的标签"><a href="#首先我们给node1节点打上一个ssd的标签" class="headerlink" title="首先我们给node1节点打上一个ssd的标签"></a>首先我们给node1节点打上一个ssd的标签</h4><pre><code>[root@master ~]# kubectl label nodes node02 disk=ssd</code></pre><h3 id="（1）查看标签"><a href="#（1）查看标签" class="headerlink" title="（1）查看标签"></a>（1）查看标签</h3><pre><code>[root@master ~]# kubectl get nodes --show-labels | grep node02</code></pre><p>![image-20200108111354832](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108111354832.png)</p>
<h3 id="（2）删除副本一"><a href="#（2）删除副本一" class="headerlink" title="（2）删除副本一"></a>（2）删除副本一</h3><pre><code>[root@master ~]# kubectl delete -f xgp-1.yaml 
deployment.extensions "xgp-web" deleted
[root@master ~]# kubectl delete svc xgp-svc </code></pre><h3 id="（3）修改副本一的yaml文件"><a href="#（3）修改副本一的yaml文件" class="headerlink" title="（3）修改副本一的yaml文件"></a>（3）修改副本一的yaml文件</h3><pre><code>[root@master ~]# vim xgp-1.yaml 

kind: Deployment
apiVersion: extensions/v1beta1
metadata:
  name: xgp-web
spec:
  revisionHistoryLimit: 10
  replicas: 3
  template:
    metadata:
      labels:
        app: xgp-server
    spec:
      containers:
      - name: web
        image: 192.168.1.21:5000/web:v1
        ports:
          - containerPort: 80
      nodeSelector:    #添加节点选择器
        disk: ssd      #和标签内容一致</code></pre><h3 id="（4）执行一下"><a href="#（4）执行一下" class="headerlink" title="（4）执行一下"></a>（4）执行一下</h3><pre><code>[root@master ~]# kubectl apply -f xgp-1.yaml </code></pre><h4 id="查看一下-1"><a href="#查看一下-1" class="headerlink" title="查看一下"></a>查看一下</h4><pre><code>[root@master ~]# kubectl get pod -o wide</code></pre><p>![image-20200108112059395](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108112059395.png)</p>
<p><strong><em>现在pod都在node02上运行</em></strong></p>
<h3 id="（5）删除标签"><a href="#（5）删除标签" class="headerlink" title="（5）删除标签"></a>（5）删除标签</h3><pre><code>[root@master ~]# kubectl  label nodes node02 disk-</code></pre><h4 id="查看一下-2"><a href="#查看一下-2" class="headerlink" title="查看一下"></a>查看一下</h4><pre><code>[root@master ~]# kubectl get nodes --show-labels | grep node02</code></pre><p>![image-20200108112245347](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108112245347.png)</p>
<p><strong><em>没有disk标签了</em></strong></p>
<h1 id="五，小实验"><a href="#五，小实验" class="headerlink" title="五，小实验"></a>五，小实验</h1><h3 id="1）使用私有镜像v1版本部署一个Deployment资源对象，要求副本Pod数量为3个，并创建一个Service资源对象相互关联，指定要求3个副本Pod全部运行在node01节点上，记录一个版本。"><a href="#1）使用私有镜像v1版本部署一个Deployment资源对象，要求副本Pod数量为3个，并创建一个Service资源对象相互关联，指定要求3个副本Pod全部运行在node01节点上，记录一个版本。" class="headerlink" title="1）使用私有镜像v1版本部署一个Deployment资源对象，要求副本Pod数量为3个，并创建一个Service资源对象相互关联，指定要求3个副本Pod全部运行在node01节点上，记录一个版本。"></a><strong>1）使用私有镜像v1版本部署一个Deployment资源对象，要求副本Pod数量为3个，并创建一个Service资源对象相互关联，指定要求3个副本Pod全部运行在node01节点上，记录一个版本。</strong></h3><h4 id="（1）用label控制pod的位置"><a href="#（1）用label控制pod的位置" class="headerlink" title="（1）用label控制pod的位置"></a>（1）用label控制pod的位置</h4><pre><code>[root@master ~]# kubectl label nodes node01 disk=ssd</code></pre><h4 id="（2）编写源yaml文件"><a href="#（2）编写源yaml文件" class="headerlink" title="（2）编写源yaml文件"></a>（2）编写源yaml文件</h4><pre><code>[root@master ~]# vim xgp.yaml
kind: Deployment
apiVersion: extensions/v1beta1
metadata:
  name: xgp-web
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: xgp-server
    spec:
      containers:
      - name: web
        image: 192.168.1.21:5000/web:v1
        ports:
          - containerPort: 80
      nodeSelector:    
        disk: ssd  </code></pre><h4 id="（3）编写源service文件"><a href="#（3）编写源service文件" class="headerlink" title="（3）编写源service文件"></a>（3）编写源service文件</h4><pre><code>[root@master ~]# vim xgp-svc.yaml
kind: Service
apiVersion: v1
metadata:
  name: xgp-svc
spec:
  type: NodePort
  selector:
    app: xgp-server
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
      nodePort: 30123</code></pre><h4 id="（4）执行yaml文件，创建控制器。执行service文件创建映射端口"><a href="#（4）执行yaml文件，创建控制器。执行service文件创建映射端口" class="headerlink" title="（4）执行yaml文件，创建控制器。执行service文件创建映射端口"></a>（4）执行yaml文件，创建控制器。执行service文件创建映射端口</h4><pre><code>[root@master ~]# kubectl apply -f  xgp.yaml 
[root@master ~]# kubectl apply -f xgp-svc.yaml </code></pre><h4 id="（5）查看一下pod节点"><a href="#（5）查看一下pod节点" class="headerlink" title="（5）查看一下pod节点"></a>（5）查看一下pod节点</h4><pre><code>[root@master ~]# kubectl get pod -o wide</code></pre><p>![image-20200108122424654](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108122424654.png)</p>
<h4 id="（6）记录一个版本"><a href="#（6）记录一个版本" class="headerlink" title="（6）记录一个版本"></a>（6）记录一个版本</h4><pre><code>[root@master ~]# kubectl rollout history deployment xgp-web &gt; pod.txt</code></pre><p>![image-20200108142016701](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108142016701.png)</p>
<h4 id="（7）访问一下"><a href="#（7）访问一下" class="headerlink" title="（7）访问一下"></a>（7）访问一下</h4><p>![image-20200108122518278](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108122518278.png)</p>
<p>![image-20200108122534683](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108122534683.png)</p>
<h3 id="2）根据上述Deployment，升级为v2版本，记录一个版本。"><a href="#2）根据上述Deployment，升级为v2版本，记录一个版本。" class="headerlink" title="2）根据上述Deployment，升级为v2版本，记录一个版本。"></a><strong>2）根据上述Deployment，升级为v2版本，记录一个版本。</strong></h3><h4 id="（1）修改yaml文件镜像版本"><a href="#（1）修改yaml文件镜像版本" class="headerlink" title="（1）修改yaml文件镜像版本"></a>（1）修改yaml文件镜像版本</h4><pre><code>[root@master ~]# vim xgp.yaml 
kind: Deployment
apiVersion: extensions/v1beta1
metadata:
  name: xgp-web
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: xgp-server
    spec:
      containers:
      - name: web
        image: 192.168.1.21:5000/web:v2    #修改版本为二
        ports:
          - containerPort: 80
      nodeSelector:
        disk: ssd</code></pre><h4 id="（2）刷新一下yaml文件"><a href="#（2）刷新一下yaml文件" class="headerlink" title="（2）刷新一下yaml文件"></a>（2）刷新一下yaml文件</h4><pre><code>[root@master ~]# kubectl apply -f xgp.yaml --recore</code></pre><h4 id="（3）访问一下-2"><a href="#（3）访问一下-2" class="headerlink" title="（3）访问一下"></a>（3）访问一下</h4><p>![image-20200108141825924](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108141825924.png)</p>
<h4 id="（4）记录一个版本"><a href="#（4）记录一个版本" class="headerlink" title="（4）记录一个版本"></a>（4）记录一个版本</h4><pre><code>[root@master ~]# kubectl rollout history deployment xgp-web &gt; pod.txt</code></pre><p>![image-20200108142030157](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108142030157.png)</p>
<h3 id="3）最后升级到v3版本，这时，查看Service关联，并且分析访问流量的负载均衡详细情况。"><a href="#3）最后升级到v3版本，这时，查看Service关联，并且分析访问流量的负载均衡详细情况。" class="headerlink" title="3）最后升级到v3版本，这时，查看Service关联，并且分析访问流量的负载均衡详细情况。"></a><strong>3）最后升级到v3版本，这时，查看Service关联，并且分析访问流量的负载均衡详细情况。</strong></h3><h4 id="1）修改yaml文件镜像版本"><a href="#1）修改yaml文件镜像版本" class="headerlink" title="1）修改yaml文件镜像版本"></a>1）修改yaml文件镜像版本</h4><pre><code>[root@master ~]# vim xgp.yaml 
kind: Deployment
apiVersion: extensions/v1beta1
metadata:
  name: xgp-web
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: xgp-server
    spec:
      containers:
      - name: web
        image: 192.168.1.21:5000/web:v3   #修改版本为二
        ports:
          - containerPort: 80
      nodeSelector:
        disk: ssd</code></pre><h4 id="（2）刷新一下yaml文件-1"><a href="#（2）刷新一下yaml文件-1" class="headerlink" title="（2）刷新一下yaml文件"></a>（2）刷新一下yaml文件</h4><pre><code>[root@master ~]# kubectl apply -f xgp.yaml --recore</code></pre><h4 id="（3）访问一下-3"><a href="#（3）访问一下-3" class="headerlink" title="（3）访问一下"></a>（3）访问一下</h4><p>![image-20200108142329749](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108142329749.png)</p>
<h4 id="（5）分析访问流量的负载均衡详细情况"><a href="#（5）分析访问流量的负载均衡详细情况" class="headerlink" title="（5）分析访问流量的负载均衡详细情况"></a>（5）分析访问流量的负载均衡详细情况</h4><h5 id="lt-1-gt-查看一下service映射端口"><a href="#lt-1-gt-查看一下service映射端口" class="headerlink" title="<1>查看一下service映射端口"></a>&lt;1&gt;查看一下service映射端口</h5><p>![image-20200108142504637](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108142504637.png)</p>
<h5 id="lt-2-gt-以ip为起点，分析访问流量的负载均衡详细情况"><a href="#lt-2-gt-以ip为起点，分析访问流量的负载均衡详细情况" class="headerlink" title="<2>以ip为起点，分析访问流量的负载均衡详细情况"></a>&lt;2&gt;以ip为起点，分析访问流量的负载均衡详细情况</h5><p><strong>Service实现的负载均衡：默认使用的是iptables规则。IPVS</strong></p>
<pre><code>[root@master ~]# iptables-save | grep 10.107.27.229
//根据service的暴露IP，查看对应的iptabes规则</code></pre><p>![image-20200108143052433](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108143052433.png)</p>
<pre><code>[root@master ~]# iptables-save | grep KUBE-SVC-ESI7C72YHAUGMG5S</code></pre><p>![image-20200108143359463](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108143359463.png)</p>
<p><strong><em>这里显示了各节点的负载比例</em></strong></p>
<h5 id="lt-3-gt-对应一下IP是否一致"><a href="#lt-3-gt-对应一下IP是否一致" class="headerlink" title="<3>对应一下IP是否一致"></a>&lt;3&gt;对应一下IP是否一致</h5><pre><code>[root@master ~]# iptables-save | grep KUBE-SEP-VDKW5WQIWOLZMJ6G</code></pre><p>![image-20200108143547946](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108143547946.png)</p>
<pre><code>[root@master ~]# kubectl get pod -o wide</code></pre><p>![image-20200108143608942](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108143608942.png)</p>
<h3 id="4）回滚到指定版本v1，并作验证。"><a href="#4）回滚到指定版本v1，并作验证。" class="headerlink" title="4）回滚到指定版本v1，并作验证。"></a><strong>4）回滚到指定版本v1，并作验证。</strong></h3><h4 id="lt-1-gt-回滚到指定版本"><a href="#lt-1-gt-回滚到指定版本" class="headerlink" title="<1>回滚到指定版本"></a>&lt;1&gt;回滚到指定版本</h4><pre><code>[root@master ~]# kubectl rollout undo deployment xgp-web --to-revision=1
//这里指定的是版本信息的编号</code></pre><h4 id="lt-2-gt-访问一下"><a href="#lt-2-gt-访问一下" class="headerlink" title="<2>访问一下"></a>&lt;2&gt;访问一下</h4><pre><code>[root@master ~]# curl 127.0.0.1:30123</code></pre><p>![image-20200108110337266](E:\软件\博客\Blog\blog\source_posts\05 Delpoyment、service.assets\image-20200108110337266.png)</p>
<blockquote>
<p><strong>排错思路</strong></p>
<pre><code>[root@master ~]# less /var/log/messages  | grep kubelet
[root@master ~]# kubectl  logs -n  kube-system kube-scheduler-master 
[root@master ~]# kubectl describe pod xgp-web-7d478f5bb7-bd4bj </code></pre></blockquote>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script></div><hr></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2020 By Wu Shao Dong</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody" target="_blank" rel="noopener"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.7.0"></script><script src="/js/fancybox.js?version=1.7.0"></script><script src="/js/sidebar.js?version=1.7.0"></script><script src="/js/copy.js?version=1.7.0"></script><script src="/js/fireworks.js?version=1.7.0"></script><script src="/js/transition.js?version=1.7.0"></script><script src="/js/scroll.js?version=1.7.0"></script><script src="/js/head.js?version=1.7.0"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>