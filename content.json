{"meta":{"title":"Xgp & Blog","subtitle":"Today is still beautiful","description":"Small steel gun article","author":"Wu Shao Dong","url":"https://wsdlxgp.top","root":"/"},"pages":[{"title":"网站感想","text":"","path":"about/site.html","date":"05-30","excerpt":""},{"title":"可爱的我","text":"","path":"about/index.html","date":"05-30","excerpt":""},{"title":"contact","text":"","path":"contact/index.html","date":"03-30","excerpt":""},{"title":"categories","text":"","path":"categories/index.html","date":"03-11","excerpt":""},{"title":"tags","text":"","path":"tags/index.html","date":"03-11","excerpt":""},{"title":"","text":"Theme NexT Canvas Nest canvas-nest.js for NexT. Install Step 1 → Go to Hexo dir Change dir to Hexo directory. There must be scaffolds, source, themes and other directories: 123$ cd hexo$ lsscaffolds source themes _config.yml package.json Step 2 → Create footer.swig Create a file named footer.swig in hexo/source/_data directory (create _data directory if it does not exist). Edit this file and add the following content: 1&lt;script color=\"0,0,255\" opacity=\"0.5\" zIndex=\"-1\" count=\"99\" src=\"https://cdn.jsdelivr.net/npm/canvas-nest.js@1/dist/canvas-nest.js\"&gt;&lt;/script&gt; You can customize these options. Step 3 → Set it up In the NexT _config.yml, uncomment footer under the custom_file_path section. 12345678910111213# Define custom file paths.# Create your custom files in site directory `source/_data` and uncomment needed files below.custom_file_path: #head: source/_data/head.swig #header: source/_data/header.swig #sidebar: source/_data/sidebar.swig #postMeta: source/_data/post-meta.swig #postBodyEnd: source/_data/post-body-end.swig footer: source/_data/footer.swig #bodyEnd: source/_data/body-end.swig #variable: source/_data/variables.styl #mixin: source/_data/mixins.styl #style: source/_data/styles.styl","path":"lib/canvas-nest/README.html","date":"03-28","excerpt":""},{"title":"","text":"!function(){ var userAgentInfo = navigator.userAgent; var Agents = [\"iPad\", \"iPhone\", \"Android\", \"SymbianOS\", \"Windows Phone\", \"iPod\", \"webOS\", \"BlackBerry\", \"IEMobile\"]; for (var v = 0; v < Agents.length; v++) { if (userAgentInfo.indexOf(Agents[v]) > 0) { return; } } function o(w,v,i){return w.getAttribute(v)||i}function j(i){return document.getElementsByTagName(i)}function l(){var i=j(\"script\"),w=i.length,v=i[w-1];return{l:w,z:o(v,\"zIndex\",-1),o:o(v,\"opacity\",0.5),c:o(v,\"color\",\"0,0,0\"),n:o(v,\"count\",99)}}function k(){r=u.width=window.innerWidth||document.documentElement.clientWidth||document.body.clientWidth,n=u.height=window.innerHeight||document.documentElement.clientHeight||document.body.clientHeight}function b(){e.clearRect(0,0,r,n);var w=[f].concat(t);var x,v,A,B,z,y;t.forEach(function(i){i.x+=i.xa,i.y+=i.ya,i.xa*=i.x>r||i.xn||i.y","path":"lib/canvas-nest/canvas-nest-nomobile.min.js","date":"03-28","excerpt":""},{"title":"","text":"!function(){function o(w,v,i){return w.getAttribute(v)||i}function j(i){return document.getElementsByTagName(i)}function l(){var i=j(\"script\"),w=i.length,v=i[w-1];return{l:w,z:o(v,\"zIndex\",-1),o:o(v,\"opacity\",0.5),c:o(v,\"color\",\"0,0,0\"),n:o(v,\"count\",99)}}function k(){r=u.width=window.innerWidth||document.documentElement.clientWidth||document.body.clientWidth,n=u.height=window.innerHeight||document.documentElement.clientHeight||document.body.clientHeight}function b(){e.clearRect(0,0,r,n);var w=[f].concat(t);var x,v,A,B,z,y;t.forEach(function(i){i.x+=i.xa,i.y+=i.ya,i.xa*=i.x>r||i.xn||i.y","path":"lib/canvas-nest/canvas-nest.min.js","date":"03-28","excerpt":""}],"posts":[{"title":"企业级私有仓库镜像仓库Harbor","text":"企业级私有仓库镜像仓库Harbor 12curl -L https://github.com/docker/compose/releases/download/1.25.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-composechmod +x /usr/local/bin/docker-compose 在网上下载docker-compose工具。** https://github.com/docker/compose/releases/tag/1.25.1-rc1 123[root@docker02 ~]# tar -zxf docker-compose.tar.gz -C /usr/local/bin///解压到命令目录[root@docker02 ~]# chmod +x /usr/local/bin/docker-compose 12[root@docker02 ~]# yum -y install yum-utils device-mapper-persistent-data lvm2//安装依赖包 123[root@docker02 ~]# docker-compose -v//查看版本信息docker-compose version 1.24.0, build 0aa59064 在网上下载harbor并安装。 https://github.com/goharbor/harbor/releases 12[root@docker02 ~]# tar -zxf harbor-offline-installer-v1.7.4.tgz -C /usr/local///导入harbor离线安装包，解压到/usr/ 修改harbor配置文件，并执行自带的安装脚本 12[root@docker02 ~]# cd /usr/local/harbor/[root@docker02 harbor]#ls 12345[root@docker02 harbor]# vim harbor.cfg hostname = 192.168.1.13 #13 改为本机IP地址harbor_admin_password = Harbor12345 #harbor密码[root@docker02 harbor]# ./install.sh//执行一下自带的安装脚本 在浏览器登陆一下harbor http://192.168.1.13:80 用户名：admin，密码：Harbor12345 创建一个项目 修改docker配置文件，连接私有仓库 12345678[root@docker02 harbor]# vim /usr/lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.13 #13行添加[root@docker02 harbor]# systemctl daemon-reload [root@docker02 harbor]# systemctl restart docker//重启docker 12[root@docker02 harbor]# docker ps//发现运行的容器少了很多 12[root@docker02 harbor]# docker-compose start//启动harker的文件中的容器 登陆harbor 12[root@docker02 harbor]# docker login -u admin -p Harbor12345 192.168.1.13//登陆harbor 上传镜像到仓库 1234[root@docker02 harbor]# docker tag centos:7 192.168.1.13/xgp/centos:7//修改标签[root@docker02 harbor]# docker push 192.168.1.13/xgp/centos:7 //上传镜像 第二台加入仓库，测试下载 12345678[root@docker02 harbor]# vim /usr/lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.13 #13行添加[root@docker02 harbor]# systemctl daemon-reload [root@docker02 harbor]# systemctl restart docker//重启docker 登陆harbor 12[root@docker02 harbor]# docker login -u admin -p Harbor12345 192.168.1.13//登陆harbor 下载刚刚上传的镜像 1[root@docker01 xxx]# docker pull 192.168.1.13/xgp/centos:7 12[root@docker01 xxx]# docker images//查看本地镜像 下载成功","path":"posts/7597.html","date":"06-06","excerpt":"","tags":[]},{"title":"prometheus监控","text":"配置AlertManager AlertManager：用来接收prometheus发送来的报警信息，并且执行设置好的报警方式、报警内容。 下载镜像 12[root@docker01 ~]# docker pull alertmanager//下载alertmanager镜像 基于alertmanager运行一台容器 1[root@docker01 ~]# docker run -d --name alertmanager -p 9093:9093 prom/alertmanager:latest 配置路由转发 12[root@docker01 ~]# echo net.ipv4.ip_forward = 1 &gt;&gt; /etc/sysctl.conf [root@docker01 ~]# sysctl -p 在部署alertmanager之前，我们需要对它的配置文件进行修改,所以我们先运行一个容器，先将其配置文件拷贝出来。 12[root@docker01 ~]# docker cp alertmanager:/etc/alertmanager/alertmanager.yml .///拷贝alertmanager的配置文件到本地 修改alertmanager的配置文件 配置文件简单介绍 AlertManager：用来接收Prometheus发送的报警信息，并且执行设置好的报警方式，报警内容。 AlertManager.yml配置文件： global：全局配置，包括报警解决后的超时时间、SMTP相关配置、各种渠道通知的API地址等消息。 route：用来设置报警的分发策略。 receivers：配置报警信息接收者信息。 inhibit_rules：抑制规则配置，当存在与另一个匹配的报警时，抑制规则将禁用用于有匹配的警报。 修改配置文件 123456789101112131415161718192021222324252627[root@docker01 ~]# vim alertmanager.yml //修改alertmanager配置文件global: resolve_timeout: 5m smtp_from: '2877364346@qq.com' #自己邮箱地址 smtp_smarthost: 'smtp.qq.com:465' #qq的邮箱地址及端口 smtp_auth_username: '2877364346@qq.com' smtp_auth_password: 'osjppnjkbuhcdfff' #需要在qq邮箱获取授权码 smtp_require_tls: false smtp_hello: 'qq.com'route: group_by: ['alertname'] group_wait: 5s group_interval: 5s repeat_interval: 5m receiver: 'email' #接收者改为邮箱receivers:- name: 'email' email_configs: - to: '2877364346@qq.com' send_resolved: true inhibit_rules: - source_match: severity: 'critical' target_match: severity: 'warning' equal: ['alertname', 'dev', 'instance'] 12345678910111213141516171819202122232425global: resolve_timeout: 5m smtp_from: '2877364346@qq.com' smtp_smarthost: 'smtp.qq.com:465' smtp_auth_username: '2877364346@qq.com' smtp_auth_password: 'osjppnjkbuhcdfff' smtp_require_tls: false smtp_hello: 'qq.com'route: group_by: ['alertname'] group_wait: 5s group_interval: 5s repeat_interval: 5m receiver: 'email'receivers:- name: 'email' email_configs: - to: '2877364346@qq.com' send_resolved: trueinhibit_rules: - source_match: severity: 'critical' target_match: severity: 'warning' equal: ['alertname', 'dev', 'instance'] 重新运行 alertmanager 容器 1234[root@docker01 ~]# docker rm -f alertmanager//删除alertmanager容器[root@docker01 ~]# docker run -d --name alertmanager -v /root/alertmanager.yml:/etc/alertmanager/alertmanager.yml -p 9093:9093 prom/alertmanager:latest //运行一台新的alertmanager容器，记得挂载配置文件 Prometheus配置和alertmanager报警规则 创建存放规则的目录 123[root@docker01 ~]# mkdir -p prometheus/rules//创建规则目录[root@docker01 ~]# cd prometheus/rules/ 编写规则 123456789101112[root@docker01 rules]# vim node-up.rules groups:- name: node-up rules: - alert: node-up expr: up&#123;job=\"prometheus\"&#125; == 0 #&#123;job=\"prometheus\"&#125;中的prometheus需要和prometheus配置文件23行的相同 for: 15s labels: severity: 1 team: node annotations: summary: \"&#123;&#123; $labels.instance &#125;&#125; 已停止运行超过 15s！\" 修改 prometheus配置文件 123456789101112[root@docker01 ~]# vim prometheus.yml # Alertmanager configuration #7alerting: alertmanagers: - static_configs: - targets: - 192.168.1.11:9093 #去注释修改# Load rules once and periodically evaluate them according to the global 'evaluation_interval'. #14行rule_files: - \"/usr/local/prometheus/rules/*.rules\" #添加（这个路径是prometheus容器内的路径） 重新运行prometheus 容器 1234[root@docker01 ~]# docker rm -f prometheus //删除prometheus容器[root@docker01 ~]# docker run -d -p 9090:9090 --name prometheus --net=host -v /root/prometheus.yml:/etc/prometheus/prometheus.yml -v /root/prometheus/rules/node-up.rules:/usr/local/prometheus/rules/node-up.rules prom/prometheus//运行一台新的alertmanager容器，记得挂载规则文件 浏览器验证一下http://192.168.1.11:9090/rules 挂起docker02 会收到邮件","path":"posts/fe12.html","date":"06-06","excerpt":"","tags":[]},{"title":"prometheus","text":"Prometheus（普罗米修斯） 实验环境 docker01 docker02 docker03 192.168.1.11 192.168.1.13 192.168.1.20 NodeEXporter NodeEXporter NodeEXporter cAdvisor cAdvisor cAdvisor Prometheus Server 空 空 Grafana 空 空 全部关闭防火墙，禁用selinux 需要部署的组件： Prometheus Server:普罗米修斯的主服务器。 Prometheus是一个开源的服务监控系统，它通过HTTP协议从远程的机器收集数据并存储在本地的时序数据库上。 多维数据模型（时序列数据由metric名和一组key/value组成） 在多维度上灵活的查询语言(PromQl) 不依赖分布式存储，单主节点工作. 通过基于HTTP的pull方式采集时序数据 可以通过push gateway进行时序列数据推送(pushing) 可以通过服务发现或者静态配置去获取要采集的目标服务器 多种可视化图表及仪表盘支持 Prometheus通过安装在远程机器上的exporter来收集监控数据，后面我们将使用到node_exporter收集系统数据。 NodeEXporter:负责收集Host硬件信息和操作系统信息。 cAdvisor:负责收集Host.上运行的容器信息。 Grafana:负责展示普罗米修斯监控界面。 Grafana 是一个开箱即用的可视化工具，具有功能齐全的度量仪表盘和图形编辑器，有灵活丰富的图形化选项，可以混合多种风格，支持多个数据源特点。 这些可以直接docker pull下载镜像（现在是本地导入镜像） 本地上传镜像 docker01 1[09:05:42][docker01$ docker load -i node-exporter.tar &amp;&amp; docker load -i mycadvisor.tar &amp;&amp; docker load -i prometheus.tar &amp;&amp; docker load -i grafana.tar docker02和docker03 1[09:05:22]docker03]$ docker load -i node-exporter.tar &amp;&amp; docker load -i mycadvisor.tar 各主机部署 1) 3个节点，全部部署node-EXporter,和cAdvisor. 部署安装node-EXporter收集节点硬件和操作系统信息。 12[09:21:03[docker01]$ docker run -d -p 9100:9100 -v /proc:/host/proc -v /sys:/host/sys -v /:/rootfs --net=host prom/node-exporter --path.procfs /host/proc --path.sysfs /host/sys --collector.filesystem.ignored-mount-points \"^/(sys|proc|dev|host|etc)($|/)\"//部署node-EXporter,收集硬件和系统信息。 PS: 注意，这里使用了–net=host， 这样Prometheus Server可以直接与Node- EXporter通信。 验证:打开浏览器验证结果。http://192.168.1.11:9100/，http://192.168.1.13:9100/，http://192.168.1.20:9100/ 部署安装cAdvisor,收集节点容器信息。 1[09:39:10[docker01]$ docker run -v /:/rootfs:ro -v /var/run:/var/run/:rw -v /sys:/sys:ro -v /var/lib/docker:/var/lib/docker:ro --detach=true --name=cadvisor --net=host google/cadvisor 验证:打开浏览器验证结果。http://192.168.1.11:8080，http://192.168.1.13:8080，http://192.168.1.20:8080 2)在docker01上部署Prometheus Server服务。 在部署prometheus之前，我们需要对它的配置文件进行修改,所以我们先运行一个容器，先将其配置文件拷贝出来。 123409:51:22][docker01]$ docker run -d -p 9090:9090 --name prometheus --net=host prom/prometheus//打开一台Prometheus[09:51:00[docker01]$ docker cp prometheus:/etc/prometheus/prometheus.yml .///拷贝Prometheus的配置文件到本地 修改Prometheus的配置文件，添加监听端口（29行） 123[09:55:53][docker01][~]$ vim prometheus.yml //修改配置文件这里指定了prometheus的监控项，包括它也会监控自己手机到的数据。- targets: ['localhost:9090','localhost:8080','localhost:9100','192.168.1.13:8080','192.168.1.13:9100','192.168.1.20:8080','192.168.1.20:9100'] 重新运行prometheus容器 1234[10:00:27][docker01][~]$ docker rm -f prometheus //删除 prometheus容器[10:02:45][docker01][~]$ docker run -d -p 9090:9090 --name prometheus --net=host -v /root/prometheus.yml:/etc/prometheus/prometheus.yml prom/prometheus//运行一台新的 prometheus容器 浏览器访问，验证：http://192.168.1.11:9090/graph ps：这里能够查看到我们各个监控项。 如果现在挂起一台虚拟机（测试完之后继续运行） 3)在docker01.上,部署grafana服务,用来展示prometheus收集到的数据。 1234[root@docker01 ~]# mkdir grafana-storage//创建收集信息的目录[root@docker01 ~]# chmod 777 grafana-storage///给予777权限 1[root@docker01 ~]# docker run -d -p 3000:3000 --name grafana -v /root/grafana-storage:/var/lib/grafana -e \"GF_SECURITY_ADMIN_PASSWORD=123.com\" grafana/grafana **浏览器访问验证：**http://192.168.1.11:3000/login （&lt;默认&gt;用户名：admin，密码：123.com） 添加数据源 PS:看到这个提示， 说明prometheus和grafana服务的是 正常连接的。 此时，虽然grafana收集到了数据，但怎么显示它,仍然是个问题，grafana支持自定 义显示信息,不过要自定义起来非常麻烦，不过好在，grafana官方为我们提供了- -些模板，来供我们使用。 **grafana官网:**https://grafana.com/docs/grafana/latest/ 选中一款模板，然后，我们有2种方式可以套用这个模板。 第一种方式：通过JSON文件使用模板。 下载完成之后，来到grafana控制台 第二种导入模板的方式:** 可以直接通过模板的ID号。 //这个id不好用换成8321了 复制模板id之后，来到grafana控制台 排错思路 防火墙是否关闭，selinux是否禁用 主机名称是否更改 镜像是否正常 各服务启动时挂载目录是否正确 grafana服务，是否创建所需目录，目录是否有权限 Prometheus服务是否修改配置文件 总结 恭喜！您已经设置了Prometheus服务器，Node Exporter和Grafana 等所有这些都可以使用的Docker。尽管这些目前都在同一台机器上运行，但这仅用于演示目的。在生产设置中，通常会在每台受监控的计算机上运行节点导出器，多个Prometheus服务器（根据组织的需要），以及单个Grafana服务器来绘制来自这些服务器的数据。","path":"posts/d356.html","date":"06-06","excerpt":"","tags":[]},{"title":"nginx+docker+nfs部署","text":"nginx（两台都是） 安装nginx 12345[root@nginx01 ~]# tar zxf nginx-1.14.0.tar.gz //解压nginx安装包[root@nginx01 ~]# cd nginx-1.14.0/[root@nginx01 nginx-1.14.0]# yum -y install openssl-devel pcre-devel zlib-devel//安装nginx依赖包 12[root@nginx01 nginx-1.14.0]# ./configure --prefix=/usr/local/nginx1.14 --with-http_dav_module --with-http_stub_status_module --with-http_addition_module --with-http_sub_module --with-http_flv_module --with-http_mp4_module --with-pcre --with-http_ssl_module --with-http_gzip_static_module --user=nginx --group=nginx &amp;&amp; make &amp;&amp; make install//编译安装nginx 12345678[root@nginx01 nginx-1.14.0]# useradd nginx -s /sbin/nologin -M//创建所需用户[root@nginx01 nginx-1.14.0]# ln -s /usr/local/nginx1.14/sbin/nginx /usr/local/sbin///链接命令[root@nginx01 nginx-1.14.0]# nginx //开启nginx[root@nginx01 nginx-1.14.0]# netstat -anpt | grep nginx//查看nginx是否开启 部署nginx 12[root@nginx01 ~]# cd /usr/local/nginx1.14/conf/[root@nginx01 conf]# vim nginx.conf ​ http模块加 1234upstream backend &#123;server 192.168.1.11:90 weight=1 max_fails=2 fail_timeout=10s;server 192.168.1.13:90 weight=1 max_fails=2 fail_timeout=10s;&#125; location / { # root html; # index index.html index.htm; proxy_pass http://backend; #添加 } 高可用环境 安装keepalived [root@nginx02 nginx-1.14.0]# yum -y install keepalived 配置keepalived 修改主和备nginx服务器上的keepalived 配置文件 /etc/keepalived/keepalived.conf 文件 主nginx 修改主nginx下/etc/keepalived/keepalived.conf文件 123456789101112131415161718! Configuration File for keepalivedglobal_defs &#123; router_id LVS_DEVEL&#125; vrrp_instance VI_1 &#123; state MASTER interface ens33 virtual_router_id 51 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.1.40 &#125;&#125; 备nginx 修改备nginx下 /etc/keepalived /keepalived.conf文件 配置备nginx时需要注意：需要修改state为BACKUP , priority比MASTER低，virtual_router_id和master的值一致 12345678910111213141516171819! Configuration File for keepalivedglobal_defs &#123; router_id TWO&#125;vrrp_instance VI_1 &#123; state BACKUP interface ens33 virtual_router_id 1 priority 99 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.1.40 &#125;&#125; 测试（在做完docker的时候） 主备nginx都启动keepalived 1systemctl start keepalived 12[root@nginx01 conf]# curl 192.168.1.40wsd666 nfs（两台都是) nfs操作 12345678910[root@localhost ~]# yum -y install nfs-utils//下载nfs服务[root@nfs ~]# mkdir /database//创建共享目录[root@nfs02 ~]# chmod 777 /database///设置权限[root@nfs ~]# vim /etc/exports//设置权限如下/database *(rw,sync,no_root_squash) 开启各项服务 1234[root@nfs ~]# systemctl start rpcbind[root@nfs ~]# systemctl enable rpcbind[root@nfs ~]# systemctl start nfs-server[root@nfs ~]# systemctl enable nfs-server docker01和docker02测试nfs 1234567891011121314[root@nfs01 ~]# vim /etc/rsyncd.conf //建立rsync配置文件uid = nobodygid = nobodyuse chroot = yesaddress = 192.168.1.30port 873log file = /var/log/rsyncd.logpid file = /var/run/rsyncd.pidhosts allow = 192.168.1.0/24[wwwroot]path = /databaseread only = nodont compress = *.gz *.bz2 *.rar *.zip 123456[root@nfs01 ~]# mkdir /database//创建共享目录[root@nfs01 ~]# rsync --daemon//启动rsync[root@nfs01 ~]# netstat -anpt | grep rsync//查看端口 如果需要重启rsync服务，需要： 12345[root@localhost ~]# kill $(cat /var/run/rsyncd.pid)//停止服务[root@localhost ~]# rsync --daemon//启动服务[root@localhost ~]# kill -9 $(cat /var/run/rsyncd.pid) 或者直接使用“netstat -anpt | grep rsync”命令查出进程号，使用“kill 进程号”一样。 使用第一种方法停止rsync服务必须删除存放rsync服务进程的文件： 1[root@localhost ~]# rm -rf /var/run/rsyncd.pid 使用rsync备份工具 配置好rsync同步源服务器之后，客户端就可以使用rsync工具来执行远程同步了。 与rsync主机同步 123456789101112131415rsync命令的选项：-r：递归模式，包含目录及子目录中所有文件-l：对于符号链接文件仍然复制为符号链接文件-p：保留文件的权限标记-t：保留文件的时间标记-g：保留文件的属组标记（仅超级用户使用）-o：保留文件的属主标记（仅超级用户使用）-D：保留设备文件及其他特殊文件-a：归档模式，递归并保留对象属性，等同于 -rlptgoD-v：显示同步过程的详细（verbose）信息-z：在传输文件时进行压缩（compress）-H：保留硬连接文件-A：保留ACL属性信息--delete：删除目标位置有而原始位置没有的文件--checksum：根据对象的校验和来决定是否跳过文件 rsync是一款快速增量备份工具，支持： （1）本地复制； （2）与其他SSH同步； （3）与rsync主机同步。 手动与rsync主机同步 123[root@localhost ~]# rsync -avz 192.168.1.1::wwwroot /root或者[root@localhost ~]# rsync -avz rsync://192.168.1.1/wwwroot /root 123[root@nfs01 database]# vim index.htmlxgp666//创建测试目录 配置inotify+rsync实时同步（两台都是） (1)、软件安装 12rpm -q rsync //查询rsync是否安装，一般为系统自带安装yum install rsync -y //若没有安装，使用yum安装 安装inotify软件包 123[root@nfs02 ~]# tar zxf inotify-tools-3.14.tar.gz [root@nfs02 ~]# cd inotify-tools-3.14/[root@nfs02 inotify-tools-3.14]# ./configure &amp;&amp; make &amp;&amp; make install （2）调整inotify内核参数 1234567[root@nfs02 ~]# vim /etc/sysctl.conffs.inotify.max_queued_events = 16384fs.inotify.max_user_instances = 1024fs.inotify.max_user_watches = 1048576[root@nfs02 ~]# sysctl -p//生效 (3) 编写触发式同步脚本 123456789#!/bin/bashA=\"inotifywait -mrq -e modify,move,create,delete /database/\"B=\"rsync -avz /database/ 192.168.1.40::wwwroot\"$A | while read DIRECTORY EVENT FILEdo if [ $(pgrep rsync | wc -l) -gt 0 ] ; then $B fidone 此处需要注意，在两台服务器需要同步的目录之间，也需要将目录权限放到最大，避免因目录本身权限报错。 1[root@nfs01 inotify-tools-3.14]# chmod +x /opt/ino.sh 设置脚本开机自启 123[root@nfs01 database]# vim /etc/rc.d/rc.local /opt/ino.sh &amp;/usr/bin/rsync --daemon 源服务器端测试 执行脚本后，当前终端会变成实时监控界面，需要重新打开终端操作。 在源服务器端共享模块目录下进行文件操作，然后去备份服务器下，可观察到文件已经被实时同步。 docker(两台都是) 123[root@docker01 ~]# docker pull nginx[root@docker01 ~]# mkdir -p /www //创建挂载目录 nfs创建好之后docker上挂载目录 1[root@docker01 ~]# mount -t nfs 192.168.1.30:/database /www 1[root@docker01 ~]# docker run -itd --name nginx -p 90:80 -v /www/index.html:/usr/share/nginx/html/index.html nginx:latest","path":"posts/24f3.html","date":"06-06","excerpt":"","tags":[]},{"title":"docker数据持久化","text":"数据持久化 Storage Driver 数据存储方式 Centos7版本的docker，Storage Driver（数据存储方式）为：overlay2 ，Backing Filesystem（文件系统类型）: xfs Data Volume 持久化存储：本质上是DockerHost文件系统中的目录或文件，能够直接被Mount到容器的文件系统中。在运行容器时，可通过-v实现。 特点： 1. Data Volume是目录或文件，不能是没有格式化的磁盘（块设备）。 2. 容器可以读写volume中的数据。 3. Volume数据可以永久保存，即使使用它的容器已经被销毁。 4.默认挂载到容器内的文件，容器是有读写权限。可以在运行容器是-v 后边加“:ro”限制容器的写入权限 5并且还可以挂载单独的文件到容器内部，一般他的使用场景是：如果不想对整个目录进行覆盖，而只希望添加某个文件，就可以使用挂载单个文件。 6.删除容器的操作，默认不会对dockerhost上的源文件操作，如果想要在删除容器时把源文件也删除，可以在删除容器时添加-v选项（一般不推荐使用这种方式，因为文件有可能被其他容器使用）&quot;Data Volume不支持&quot; 注意：dockerhost上需要被挂载的源文件或目录，必须是已经存在，否则，会被当作一个目录挂载到容器中。 Docker Manager Volume 123[root@docker01 ~]# docker run -itd --name t1 -P -v /usr/share/nginx/html nginx:latest[root@docker01 ~]# docker ps 1[root@docker01 ~]# docker inspect t1 12345[root@docker01_data]#cd /var/lib/docker/volumes/17c50a065a6b10ccd01ca1ce8091fdf6282dc9dcb77a0f6695906257ecc03a63/_data[root@docker01 _data]# echo \"this is a testfile\" &gt; index.html[root@docker01 _data]# docker ps 1[root@docker01 _data]# curl 127.0.0.1:32777 1[root@docker01 _data]# docker volume ls 容器与容器的数据共享 Volume container：给其他容器提供volume存储卷的容器。并且它可以提供bind mount，也可以提供docker manager volume。 Volume container：给其他容器提供volume存储卷的容器。并且它可以提供bind mount，也可以提供docker manager volume。 创建一个vc_data容器 123[root@docker01 ~]# docker create --name vc_data -v ~/html:/usr/share/nginx/html -v /other/useful/tools busybox[root@docker01 ~]# docker inspect vc_data 123[root@docker01 ~]# docker run -itd --name t3 -P --volumes-from vc_data nginx:latest[root@docker01 ~]# docker ps 1[root@docker01 ~]# curl 127.0.0.1:32779 123456[root@docker01 ~]# mkdir htdocs//创建测试目录[root@docker01 ~]# cd htdocs/[root@docker01 htdocs]# echo The version one &gt; index.html[root@docker01 htdocs]# docker run -itd --name web1 -P -v ~/htdocs:/usr/local/apache2/htdocs httpd:latest [root@docker01 htdocs]# docker ps ![img](file:///C:\\Users\\huawei\\AppData\\Roaming\\Tencent\\Users\\2877364346\\QQ\\WinTemp\\RichOle\\26LA}X1ZTD80SPWLY_TNF.png) 1[root@docker01 htdocs]# curl 127.0.0.1:32768 12[root@docker01 htdocs]# echo The version TWO &gt; index.html [root@docker01 htdocs]# curl 127.0.0.1:32768 12[root@docker01 htdocs]# rm -f index.html [root@docker01 htdocs]# curl 127.0.0.1:32768 1234567891011121314151617181920[root@docker01 ~]# mkdir htdocs///创建一个测试目录[root@docker01 htdocs]# vim index.html//创建测试网页&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;[root@docker01 ~]# docker create --name vc_data -v ~/html:/usr/share/nginx/html -v /other/useful/tools busybox//创建一个共享容器[root@docker01 htdocs]# vim Dockerfile//编写DockerfileFROM busyboxADD index.html /usr/local/apache2/htdocs/index.htmlVOLUME /usr/local/apache/htdocs[root@docker01 htdocs]# docker build -t databack .//通过当前Dockfile创建一个镜像[root@docker01 htdocs]# docker create --name vc-new databack:latest//通过databack创建一个容器 12[root@docker01 htdocs]# docker inspect vc-new//查看vc-new的详细信息 1234 [root@docker01 htdocs]# docker run -itd --name new-web1 -P --volumes-from vc-new httpd:latest //通过volume卷和httpd运行一个容器[root@docker01 htdocs]# docker ps//查看容器信息 12[root@docker01 htdocs]# curl 127.0.0.1:32773//测试访问一下网页 1234[root@docker01 htdocs]# echo 12345 &gt;index.html //更改网页内容[root@docker01 htdocs]# curl 127.0.0.1:32773//测试访问网页还是原来的内容 容器的跨主机数据共享 实验环境 docker01 docker02 docker03 httpd httpd nfs 要求：docker01和docker02的主目录，是一样的。 准备工作 123[root@localhost ~]# hostnamectl set-hostname nfs[root@localhost ~]# hostnamectl set-hostname docker01[root@localhost ~]# hostnamectl set-hostname docker02 nfs操作 123456789[root@localhost ~]# yum -y install nfs-utils//下载nfs服务[root@nfs ~]# mkdir /datashare//创建共享目录[root@nfs ~]# vim /etc/exports//设置权限如下/datashare *(rw,sync,no_root_squash) 开启各项服务 1234[root@nfs ~]# systemctl start rpcbind[root@nfs ~]# systemctl enable rpcbind[root@nfs ~]# systemctl start nfs-server[root@nfs ~]# systemctl enable nfs-server docker01和docker02测试nfs 12[root@docker01 htdocs]# showmount -e 192.168.1.20[root@docker02 htdocs]# showmount -e 192.168.1.20 docker01的操作 12345[root@docker02 ~]# mkdir /xxx[root@docker01 ~]# mount -t nfs 192.168.1.10:/datashare /xxx//挂载nfs上的共享目录[root@docker01 ~]# mount | tail -1//查看是否挂载 nfs创建测试文件 12345678[root@nfs ~]# cd datashare/[root@nfs datashare]# vim index.html&lt;div id=\"datetime\"&gt; &lt;script&gt; setInterval(\"document.getElementById('datetime').innerHTML=new Date().toLocaleString();\", 1000); &lt;/script&gt;&lt;/div&gt;xgp666 docker01查看一下 docker02的操作与docker01上一样 这里先不考虑将代码写入镜像，先以这种方式，分别在docker01和docker02部署httpd服务 12[root@docker01 ~]# docker run -itd --name bdqn-web1 -P -v /xxx/:/usr/local/apache2/htdocs httpd:latest [root@docker02 ~]# docker run -itd --name bdqn-web2 -P -v /xxx/:/usr/local/apache2/htdocs httpd:latest 123456[root@docker01 ~]# docker ps //查看端口0.0.0.0:32775-&gt;80/tcp bdqn-web[root@docker02 ~]# docker ps//查看端口0.0.0.0:32769-&gt;80/tcp bdqn-web2 此时，用浏览器访问,两个WEB服务的主界面是一样。但如果，NFS服务器上的源文件丢失, 则两个web服务都会异常。 想办法将元数据写入镜像内，在基于镜像创建一个vc_data容器，这里因为没有接触到docker-compose和docker-swarm等docker编排工具，所以需手动创建镜像！ nfs操作 12[root@nfs datashare]# echo xgp666 &gt; index.html //更改测试文件 docker02操作 1234567[root@docker02 ~]# cd /xxx/[root@docker02 xxx]# vim Dockerfile//编写Dockerfile[root@docker02 xxx]# cat Dockerfile FROM busyboxADD index.html /usr/local/apache2/htdocs/index.htmlVOLUME /usr/local/apache2/htdocs 创建镜像并运行一个容器 1234[root@docker02 xxx]# docker build -t back_data .//基于Dockerfile创建镜像[root@docker02 xxx]# docker create --name back_container1 back_data:latest //基于刚刚创建的镜像创建容器 运行容器，并导出镜像 1234[root@docker02 xxx]# docker run -itd --name bdqn-web3 -P --volumes-from back_container1 httpd:latest //运行一台容器[root@docker02 xxx]# docker save &gt; back_data.tar back_data:latest//导出镜像，因为是在共享目录所以docker01也可以看到 docker01 123456[root@docker01 xxx]# docker load -i back_data.tar //去共享目录，导入镜像[root@docker01 xxx]# docker create --name back_container2 back_data:latest//基于刚刚创建的镜像运行容器[root@docker01 xxx]# docker run -itd --name bdqn-web4 -P --volumes-from back_container2 httpd:latest//运行一台容器 浏览器访问 123456[root@docker01 ~]# docker ps //查看端口 0.0.0.0:32776-&gt;80/tcp bdqn-web4[root@docker02 ~]# docker ps//查看端口0.0.0.0:32770-&gt;80/tcp bdqn-web3","path":"posts/f8c7.html","date":"06-06","excerpt":"","tags":[]},{"title":"Docker实现服务发现","text":"更改时间 mv /etc/localtime/etc/localtime. bk cp /usr/share/zoneinfo/Asia/Shanghai/etc/localtime 查看端口 [root@docker01 consul]# ss -lnt Consul:分布式、高可用的，服务发现和配置服务的工具。数据中心 Rigistrator:负责收集dockerhost_上,容器服务的信息，并且发送给consul Consul-tpmplate:根据编辑好的模板，生产新的nginx配置文件，并负责重新加载nginx配置文件 Docker + Consul + registrator实现服务发现 1.实验环境 上面示意图的大概流程如下： 1、docker01主机上以二进制包的方式部署consul服务并后台运行，其身份为leader； 2、docker02、docker03以容器的方式运行consul服务，并加入到docker01的consul群集中； 3、在主机docker02、docker03上后台运行registrator容器，使其自动发现docker容器提供的服务； 4、在docker01上部署Nginx，提供反向代理服务，docker02、docker03主机上基于Nginx镜像，各运行两个web容器，提供不同的网页文件，以便测试效果； 5、在docker01上安装consul-template命令，将收集到的信息（registrator收集到容器的信息）写入template模板中，并且最终写入Nginx的配置文件中。 6、至此，实现客户端通过访问Nginx反向代理服务器（docker01），获得docker02、docker03服务器上运行的Nginx容器提供的网页文件。 注：registrator是一个自动发现docker container提供的服务，并且在后端服务注册中心（数据中心）注册服务。主要用来收集容器运行服务的信息，并且发送给consul。数据中心除了consul外，还有etcd、zookeeper等。 主机 iP地址 服务 docker01 192.168.1.11 consul+consul-template+nginx docker02 192.168.1.13 consul+registrator docker03 192.168.1.20 consul+registrator 三台主机关闭防火墙，禁用selinux，更改主机名如上所述。 （1）docker01去官网https://www.consul.io/downloads.html下载consul服务 123456[root@docker01 ~]# unzip consul_1.5.1_linux_amd64.zip //现在是本地导入压缩包，需要解压 [root@docker01 ~]# mv consul /usr/local/bin///移动服务到bin目录[root@docker01 ~]# chmod +x /usr/local/bin/consul//给予一个可执行权限 （2）启动consul 1[root@docker01 ~]# consul agent -server -bootstrap -ui -data-dir=/var/lib/consul-data -bind=192.168.1.11 -client=0.0.0.0 -node=master PS: //-bootstrap: 加入这个选项时，一般都在server单节点的时候用，自选举为leader。 参数解释： -server：添加一个服务 -bootstrap：一般在server单节点的时候使用，自选举为leader。 -data-dir：key/volume指定数据存放的目录 -ui：开启内部的web界面 -bind：指定开启服务的ip -client：指定访问的客户端 -node：在集群内部通信使用的名称，默认是主机名。 现在这个ip是外部使用 PS:开启的端口 8300 集群节点 8301 集群内部的访问 8302 跨数据中心的通信 8500 web ui界面 8600 使用dns协议查看节点信息的端口 这时，这条命令会占用终端，可以使用nohup命令让它保持后台运行。 1[root@docker01 ~]# nohup consul agent -server -bootstrap -ui -data-dir=/var/lib/consule-data -bind=192.168.1.11 -client=0.0.0.0 -node=master &amp; （3）查看consul端口的信息** 1[root@docker01 ~]# consul info （4）查看consul集群成员的信息 1[root@docker01 ~]# consul members 现在这个ip是内部使用 2. docker02，docker03，加入consul集群 这里我们采用容器的方式去运行consul服务。 （1）下载consu所需的l镜像 1[root@docker02 ~]# docker pull consul （2）基于consul镜像开启一台容器 1[root@docker02 ~]# docker run -d --name consul -p 8301:8301 -p 8301:8301/udp -p 8500:8500 -p 8600:8600 -p 8600:8600/udp --restart always progrium/consul -join 192.168.1.11 -advertise 192.168.1.13 -client 0.0.0.0 -node=node01 参数解释： -d：守护进程 –name：容器名称 –restart：容器随着docker服务一直运行 -advertise:声明本机地址 -join:声明服务端地址 -node:consul集群中的名称 （3）docker查看consul集群成员的信息 1[root@docker01 ~]# consul members （4）两台docker开启容器后，docker01查看 （5）浏览器访问http://192.168.1.11:8500 3. docker01下载部署consul-template 在https://github.com/hashicorp/consul-template上，下载consul-template 12345678910111213141516171819[root@docker01 ~]# unzip consul-template_0.19.5_linux_amd64.zip//解压安装好的consul-template包[root@docker01 ~]# mv consul-template /usr/local/bin///移动到命令目录[root@docker01 ~]# chmod +x /usr/local/bin/consul-template //给予一个可执行权限​```shell ## 4. docker02、docker03_ 上部署registrator服务&gt; **registrator是一个能自动发现docker container提供的服务,并在后端服务注册中心注册服务或取消服务的工具，后端注册中心支持conusl、etcd、 skydns2、zookeeper等。**### （1）下载registrator镜像​```shell [root@docker02 ~]# docker pull registrator//下载registrator镜像 （2）基于registrator镜像，开启一台容器 1[root@docker02 ~]# docker run -d --name registrator -v /var/run/docker.sock:/tmp/docker.sock --restart always gliderlabs/registrator consul://192.168.1.13:8500 （3）开启一台nginx容器 1[root@docker02 ~]# docker run -d —P --name nginx nginx:latest （4）浏览器查看一下http://192.168.1.11:8500/ui/dc1/nodes 5.docker01部署一个nginx服务 （1）安装开启nginx服务 安装nginx依赖包 1[root@docker01 ~]# yum -y install pcre pcre-devel openssl openssl-devel zlib zlib-devel 编译安装nginx 12[root@docker01 ~]# cd nginx-1.14.0/[root@docker01 nginx-1.14.0]# ./configure --user=nginx --group=nginx --with-http_stub_status_module --with-http_realip_module --with-pcre --with-http_ssl_module &amp;&amp; make &amp;&amp; make install 创建所需用户和链接命令目录 12[root@docker01 nginx-1.14.0]# useradd -M -s /sbin/nologin nginx[root@docker01 nginx-1.14.0]# ln -s /usr/local/nginx/sbin/* /usr/local/bin/ 检查nginx是否有问题，并开启nginx 1234[root@docker01 nginx-1.14.0]# nginx -tnginx: the configuration file /usr/local/nginx/conf/nginx.conf syntax is oknginx: configuration file /usr/local/nginx/conf/nginx.conf test is successful[root@docker01 nginx-1.14.0]# nginx PS:这里nginx作为反向代理，代理后端docker02、 docker03 上nginx的容器服务,所以我们先去docker02、docker03. 上部署一些服务， 为了方便等会看到负载的效果，所以，我们运行完成容器之后，做一个主界面内容的区分。 （2）安装完成之后，本机测试访问 1[root@docker01 nginx-1.14.0]# curl 127.0.0.1 （3）docker02和docker03部署环境 主机 服务 docker02 nginx web01，web02 docker03 nginx web03，web04 &lt;1&gt;下载nginx镜像（docker02，docker03都要） 12[root@docker02 ~]# docker pull nginx//下载nginx镜像 &lt;2&gt;docker01操作 基于nginx镜像运行上述所说的容器并设置测试页面 1234567891011web01[root@docker02 ~]# docker run -itd --name web01 -P nginx:latest[root@docker02 ~]# docker exec -it web01 /bin/bashroot@44b59d07202f:/# cd /usr/share/nginx/html/root@44b59d07202f:/usr/share/nginx/html# echo web01 &gt; index.htmlweb02[root@docker02 ~]# docker run -itd --name web02 -P nginx:latest[root@docker02 ~]# docker exec -it web02 /bin/bashroot@44b59d07202f:/# cd /usr/share/nginx/html/root@44b59d07202f:/usr/share/nginx/html# echo web02 &gt; index.html &lt;3&gt;docker02操作 基于nginx镜像运行上述所说的容器并设置测试页面 12345678910111213web03[root@docker03 ~]# docker run -itd --name web03 -P nginx:latest[root@docker03 ~]# docker exec -it web03 /bin/bashroot@fd8e8b2df136:/# cd /usr/share/nginx/html/root@fd8e8b2df136:/usr/share/nginx/html# echo web03 &gt; index.htmlroot@fd8e8b2df136:/usr/share/nginx/html# exittrueweb04[root@docker03 ~]# docker run -itd --name web04 -P nginx:latest[root@docker03 ~]# docker exec -it web04 /bin/bashroot@fd8e8b2df136:/# cd /usr/share/nginx/html/root@fd8e8b2df136:/usr/share/nginx/html# echo web04 &gt; index.htmlroot@fd8e8b2df136:/usr/share/nginx/html# exit （4）docker01更改nginx配置文件 123456[root@docker01 ~]# cd /usr/local/nginx///进入nginx配置文件目录[root@docker01 nginx]# mkdir consul//创建consul目录[root@docker01 nginx]# cd consul///进入consul目录 &lt;1&gt;创建nginx.ctmpl模板 1234567891011121314[root@docker01 consul]# vim nginx.ctmplupstream http_backend &#123; &#123;&#123;range service \"nginx\"&#125;&#125; server &#123;&#123; .Address &#125;&#125;:&#123;&#123; .Port &#125;&#125;; &#123;&#123; end &#125;&#125;&#125;server &#123; listen 8000; server_name localhost; location / &#123; proxy_pass http://http_backend; &#125;&#125; &lt;2&gt;修改nginx配置文件，通过 include 参数包含刚刚创建的文件 123[root@docker01 consul]# cd /usr/local/nginx/conf/[root@docker01 conf]# vim nginx.conf include /usr/local/nginx/consul/*.conf; #文件最后添加（要在大括号里面） &lt;3&gt; 生成一个vhost.conf配置文件，并重启nginx（会占用终端) 使用consul-template命令，根据模板生产新的配置文件，并重新加载nginx的配置文件。 1[root@docker01 conf]# consul-template -consul-addr 192.168.1.11:8500 -template \"/usr/local/nginx/consul/nginx.ctmpl:/usr/local/nginx/consul/vhost.conf:/usr/local/bin/nginx -s reload\" 这时，这条命令会占用终端，可以使用nohup命令让它保持后台运行,并重启nginx服务。 1[root@docker01 conf]# nohup consul-template -consul-addr 192.168.1.11:8500 -template \"/usr/local/nginx/consul/nginx.ctmpl:/usr/local/nginx/consul/vhost.conf:/usr/local/sbin/nginx -s reload\" &amp; 查看一下文件是否生成，里面是否有内容 123[root@docker01 ~]# cd /usr/local/nginx/consul/[root@docker01 consul]# lsnginx.ctmpl vhost.conf 1[root@docker01 consul]# cat vhost.conf 此时，应该能够看到，新生产的vhost.conf配置文件已经生效，访问本机8000端口可以得到不同容器提供的服务。 &lt;4&gt;测试访问 12[root@docker01 consul]# curl 127.0.0.1:8000web01 此时可以看到负载均衡的效果！ &lt;5&gt;如果访问不成功 查看端口8000是否开启 1[root@docker01 consul]# ss -lnt 检查nginx配置文件 123[root@docker01 consul]# nginx -tnginx: the configuration file /usr/local/nginx/conf/nginx.conf syntax is oknginx: configuration file /usr/local/nginx/conf/nginx.conf test is successful 检查自己编写的nginx配置文件 123456789101112131415[root@docker01 consul]# cd /usr/local/nginx/consul/[root@docker01 consul]# cat nginx.ctmpl upstream http_backend &#123;true&#123;&#123;range service \"nginx\"&#125;&#125;trueserver &#123;&#123; .Address &#125;&#125;:&#123;&#123; .Port &#125;&#125;;true&#123;&#123; end &#125;&#125;&#125;server &#123;truelisten 8000;trueserver_name localhost;truelocation / &#123;truetrueproxy_pass http://http_backend;true&#125;&#125; 如果nginx配置文件没问题，重启nginx 1[root@docker01 consul]# nginx -s reload &lt;6&gt;测试自动发现 docker02 创建测试容器 12345[root@docker02 ~]# docker run -itd --name web05 -P nginx:latest[root@docker02 ~]# docker exec -it web05 /bin/bashroot@44b59d07202f:/# cd /usr/share/nginx/html/root@44b59d07202f:/usr/share/nginx/html# echo web02 &gt; index.html[root@docker02 ~]# docker ps docker01查看 12[root@docker01 consul]# cd /usr/local/nginx/consul/[root@docker01 consul]# cat vhost.conf docker01测试访问 1[root@docker01 consul]# curl 127.0.0.1:8000 //同上 此时可以看到负载均衡的效果！ 这时不需要考虑后端的web服务器添加还是删除都会自动更新的，这是因为在运行consul-template这条命令后添加的/usr/local/sbin/nginx -s reload的作用！","path":"posts/b5a0.html","date":"06-06","excerpt":"","tags":[]},{"title":"Docker的监控","text":"Docker的监控 Docker自带的监控命令 12[root@docker01 ~]# docker ps//查看容器信息 123[root@docker01 ~]# docker top 容器名称[root@docker01 ~]# docker top wordpress_wordpress_1 //查看容器中运行的进程信息，支持 ps 命令参数。 12[root@docker01 ~]# docker stats wordpress_wordpress_1 //实时查看容器统计信息，查看容器的CPU利用率、内存的使用量以及可用内存总量。 123[root@docker01 ~]# docker logs 容器名称[root@docker01 ~]# docker logs wordpress_wordpress_1 //查看容器的日志 用 Sysdig 监控服务器和 Docker 容器 12[root@docker01 ~]# docker pull sysdig//下载sysdig镜像 通过sysdig运行容器 1[root@docker01 ~]# docker run -it --rm --name sysdig --privileged=true --volume=/var/run/docker.sock:/host/var/run/docker.sock --volume=/dev:/host/dev --volume=/proc:/host/proc:ro --volume=/boot:/host/boot:ro --volume=/lib/modules:/host/lib/modules:ro --volume=/usr:/host/usr:ro sysdig/sysdig 下载插件失败后可以运行下边命令，重新下载 12root@10ccab83a512:/# system-sysdig-loader//下载插件失败后可以运行下边命令，重新下载 下载成功后，可以运行sysdig命令，查看监控项 12root@10ccab83a512:/# sysdig//运行sysdig命令，查看监控项，它会动态查看 使用 csysdig csysdig 就是运 ncurses 库的用户界面的 sysdig 软件包，Ncurses 是一个能提供功能键定义 ( 快捷键 ), 屏幕绘制以及基于文本终端的图形互动功能的动态库。在 sysdig 软件包里还提供了一个工具 csysdig，该工具执行后，运行界面和 top 命令类似。csysdig 工作界面如图 5。 运行csysdig命令，查看监控项 12root@10ccab83a512:/# csysdig//运行csysdig命令，图形化界面查看监控项，它会动态查看 csysdig 使用如下快捷键： P：暂停屏幕输出信息 Enter：进入当前突出显示的条目。 Ctrl+F：列表搜索。 F1- 帮助信息 F2- 显示视图选择器。这将让你切换到另一个视图。 F4- 使用过滤器 F5- 查看 IO 输出信息 F7 显示帮助页面当前显示的视图。 F8 打开视图的操作面板。 F9，打开列排序面板。 Q 放弃退出。 Arrows, PgUP, PgDn, Home, End：图标上下左右的移动控制。 Docker监控方案之Weave Scope 12[root@docker01 ~]# docker pull scope//下载scope镜像 执行如下脚本安装运行Weave Scope 123[root@docker01 ~]# curl -L git.io/scope -o /usr/local/bin/scope[root@docker01 ~]# chmod +x /usr/local/bin/scope[root@docker01 ~]# scope launch 浏览器访问http://192.168.1.11:4040/ 然后就可以更好的监控，管理docker中的容器了 开启第docker02，加入docker01监控项 docker01 删除weavescope容器 1234[root@docker01 ~]# docker stop weavescope weavescope[root@docker01 ~]# docker rm weavescope weavescope docker02 12[root@docker01 ~]# docker pull scope//下载scope镜像 123[root@docker01 ~]# curl -L git.io/scope -o /usr/local/bin/scope[root@docker01 ~]# chmod +x /usr/local/bin/scope[root@docker01 ~]# scope launch docker01 1[root@docker01 ~]# scope launch 192.168.1.11 192.168.1.13 docker02 1[root@docker02 ~]# scope launch 192.168.1.13 192.168.1.11 浏览器访问http://192.168.1.11:4040/ 浏览器访问http://192.168.1.13:4040/也是可以的","path":"posts/bdbf.html","date":"06-06","excerpt":"","tags":[]},{"title":"docker部署LNMP环境","text":"12 ifdown ens33;ifup ens33//重启网卡 首先要有确认环境中有需要的tar包，可以使用docker pull来下载这些镜像 现在我们是使用已经下载好的镜像，所以需要导入一下 12[root@docker01 ~]# docker load -i nginx.tar &amp;&amp; docker load -i wordpress.tar &amp;&amp; docker load -i mysql-5.7.tar &amp;&amp; docker load -i php.7.2-fpm.tar//导入nginx,wordpress,mysql,php镜像 部署LNMP 172.16.10.0/24 Nginx：172.16.10.10 Mysql：172.16.10.20 Php ：172.16.10.30 网站的访问主目录：/wwwroot Nginx的配置文件：/docker /etc/nginx/conf.d #nginx配置文件 12345678[root@docker01 ~]# docker run -itd --name test nginx:latest //先启动一台nginx，用来拷贝配置文件和访问主目录[root@docker01 ~]# mkdir -p /wwwroot /docker//创建挂载目录[root@docker01 ~]# docker cp test:/etc/nginx /docker///拷贝配置文件到挂载目录[root@docker01 ~]# ls /docker/nginx /usr/share/nginx/html #nginx主目录 1234[root@docker01 ~]# docker cp test:/usr/share/nginx/html /wwwroot///拷贝访问目录到挂载目录[root@docker01 ~]# ls /wwwroot/html 1）创建一个自定义网络 1[root@docker01 ~]# docker network create -d bridge --subnet 172.16.10.0/24 --gateway 172.16.10.1 lnmp 2)运行nginx容器 12[root@docker01 ~]# netstat -anpt | grep 80//查看80端口是否被占用 12[root@docker01 ~]# docker run -itd --name nginx -v /docker/nginx:/etc/nginx -v /wwwroot/html:/usr/share/nginx/html -p 80:80 --network lnmp --ip 172.16.10.10 nginx//运行一台nginx服务，并指明ip，映射端口，挂载目录 12[root@docker01 ~]# docker ps//查看容器是否存在 12345678[root@docker01 ~]# cd /wwwroot/html[root@docker01 wwwroot]# vim index.htmlhello lnmp!//创建测试网页[root@docker01 wwwroot]# curl 127.0.0.1hello lnmp!//测试访问 3)运行mysql容器 12[root@docker01 html]# docker run --name mysql -e MYSQL_ROOT_PASSWORD=123.com -d -p 3306:3306 --network lnmp --ip 172.16.10.20 mysql:5.7//运行一台nginx服务，并指明ip，映射端口 -e：设置环境变量 1[root@docker02 ~]# docker ps 安装mysql，并设置密码 123[root@docker01 html]# yum -y install mysql//安装mysql[root@docker01 ~]# mysql -u root -p123.com -h 127.0.0.1 -P 3306 随便新建一个库做验证： 1MySQL [(none)]&gt; create database name; 再查看有没有刚创建的库： 1MySQL [(none)]&gt; show databases; 4)运行php容器，并创建php页面 1[root@docker01 html]# docker run -itd --name phpfpm -p 9000:9000 -v /wwwroot/html:/usr/share/nginx/html --network lnmp --ip 172.16.10.30 php:7.2-fpm 123456[root@docker01 ~]# cd /wwwroot/html[root@docker01 wwwroot]# vim test.php&lt;?phpphpinfo();?&gt;//添加php测试界面 1[root@docker02 ~]# docker ps 5)修改nginx配置文件，nginx和php连接 12[root@docker01 html]# cd /docker/nginx/conf.d/[root@docker01 conf.d]# vim default.conf location / { root /usr/share/nginx/html; index index.html index.htm index.php; #10添加index.php } location ~ \\.php$ { root /usr/share/nginx/html; fastcgi_pass 172.16.10.30:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } 设置完毕后重启nginx 123[root@docker01 conf.d]# docker restart nginx//重启nginx[root@docker01 conf.d]# docker ps 浏览器测试访问nginx和php 说明是nginx和php的连接，没有问题，接下来是php和MySQL的连接。这里我们使用一个phpmyadmin的数据库管理工具 6)修改nginx配置文件，php和mysql连接 1[root@docker01 html]# cd /wwwroot/html 上传phpMyAdmin包如果没有请在https://github.com/phpmyadmin/phpmyadmin/releases下载 123456789[root@docker01 html]# unzip phpMyAdmin-4.9.1-all-languages.zip //解压phpmyadmin包[root@docker01 html]# mv phpMyAdmin-4.9.1-all-languages phpmyadmin//更改刚刚解压文件的名称[root@docker01 html]# cd /docker/nginx/conf.d/[root@docker01 conf.d]# vim default.conf //修改nginx配置文件[root@docker01 conf.d]# docker restart nginx //重启nginx location /phpmyadmin { root /usr/share/nginx/html; index index.html index.htm index.php; } location ~ /phpmyadmin/(?&lt;after_ali&gt;(.*)\\.(php|php5)?$) { root /usr/share/nginx/html; fastcgi_pass 172.16.10.30:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } 12[root@docker01 conf.d]# docker restart nginx [root@docker01 conf.d]# docker ps 浏览器访问 http://192.168.1.11/phpmyadmin/index.php 报红框属于正常现象，不要惊慌，接下来就解决它 需要我们对php镜像做出更改，添加php和MySQL连接模块 编写一个Dockerfile 1234567891011[root@docker01 conf.d]# cd [root@docker01 ~]# vim DockerfileFROM php:7.2-fpmRUN apt-get update &amp;&amp; apt-get install -y \\ libfreetype6-dev \\ libjpeg62-turbo-dev \\ libpng-dev \\ &amp;&amp; docker-php-ext-install -j$(nproc) iconv \\ &amp;&amp; docker-php-ext-configure gd --with-freetype-dir=/usr/include/ --with-jpeg-dir=/usr/include/ \\ &amp;&amp; docker-php-ext-install -j$(nproc) gd \\ &amp;&amp; docker-php-ext-install mysqli pdo pdo_mysql 基于dockerfile创建php镜像 12[root@docker01 ~]# docker build -t phpmysql .//基于Dockerfiler创建一个镜像 删除之前的php容器 123[root@docker01 ~]# docker stop phpfpm[root@docker01 ~]# docker rm phpfpm //关闭并删除php容器 用新的php镜像运行容器 12[root@docker01 ~]# docker run -itd --name phpfpm -p 9000:9000 -v /wwwroot/html:/usr/share/nginx/html --network lnmp --ip 172.16.10.30 phpmysql//用新做的php镜像重新运行 //修改phpmyadmin的配置文件，指定连接的数据库的IP，然后重启php容器 12345678[root@docker01 html]# cd /wwwroot/html/phpmyadmin/[root@docker01 phpmyadmin]# cp config.sample.inc.php config.inc.php[root@docker01 phpmyadmin]# vim config.inc.php$cfg['Servers'][$i]['auth_type'] = 'cookie';/* Server parameters */$cfg['Servers'][$i]['host'] = '172.16.10.20'; #31写mysql数据库的IP地址$cfg['Servers'][$i]['compress'] = false;$cfg['Servers'][$i]['AllowNoPassword'] = false; 12[root@docker01 phpmyadmin]# docker restart phpfpm //重启phpfpm容器 浏览器测试访问http://192.168.1.11/phpmyadmin/index.php 用户名：root 密码：123.com 登陆成功之后可以看到之前mysql创建的数据库","path":"posts/32f5.html","date":"06-06","excerpt":"","tags":[]},{"title":"docker-compose","text":"docker三剑客之docker-compose docker容器的编排工具: 解决相互有依赖关系的多个容器的管理。 12[root@docker01 ~]# docker-compose -v//验证已有docker-compose命令 docker-compose的配置文件实例 通过识别一个docker-compose.yml的配置文件，去管理容器。 设置tab键的空格数量 12345[root@docker01 ~]# vim .vimrcset tabstop=2//设置tab键的空格数量[root@docker01 ~]# source .vimrc //刷新一下 创建一个docker-compose.yml测试文件 123456789101112131415[root@docker01 ~]# mkdir compose_test//创建测试目录[root@docker01 ~]# cd compose_test/[root@docker01 compose_test]# vim docker-compose.yml//创建测试文件docker-compose.ymlversion: \"3\"services: nginx: container_name: web-nginx image: nginx restart: always ports: - 90:80 volumes: - ./webserver:/usr/share/nginx/html docker-compose.yml文件的解释 第一部分: version: 指定语法格式的版本。 第二部分: service: 定义服务,(想要运行什么样的容器) 通过docker-compose.yml文件运行容器 12[root@docker01 compose_test]# docker-compose up -d//后台运行docker-compose规定的容器。（在执行这条命令的当前目录下，也需要一个docker-compose.yml的配置文件，并且通常只有一个。） 12[root@docker01 compose_test]# docker ps//查看容器信息 12[root@docker01 compose_test]# curl 127.0.0.1:90//访问nginx会失败，因为挂载目录没有页面内容 123456[root@docker01 compose_test]# vim webserver/index.html//创建测试网页xgp666[root@docker01 compose_test]# curl 127.0.0.1:90//再次访问，是成功的xgp666 通过docker-compose.yml文件停止运行容器 1[root@docker01 compose_test]# docker-compose stop 通过docker-compose.yml文件重启容器 1[root@docker01 compose_test]# docker-compose restart 不在docker-compose.yml文件所在目录，要使用-f指定目录 1[root@docker01 ~]# docker-compose -f compose_test/docker-compose.yml stop 并且，在运行container（docker-compose.yml）的过程中，还支持Dockerfile 1234[root@docker01 compose_test]# vim Dockerfile//编写dockerfileFROM nginxADD webserver /usr/share/nginx/html 1234567891011[root@docker01 compose_test]# vim docker-compose.yml //修改docker-compose.yml文件version: \"3\"services: nginx: build: . #添加 container_name: web-nginx image: new-nginx:v1.0 #修改镜像名称 restart: always ports: - 90:80 通过docker-compose.yml文件停止并删除容器 123[root@docker01 compose_test]# docker-compose stopStopping web-nginx ... done[root@docker01 compose_test]# docker-compose rm 通过docker-compose.yml文件运行容器 1234[root@docker01 compose_test]# docker-compose up -d//通过docker-compose.yml文件[运行]()容器[root@docker01 compose_test]# docker ps//查看容器信息 测试nginx访问页面 123[root@docker01 compose_test]# curl 127.0.0.1:90//测试访问nginx页面，成功xgp666 搭建wordpress的博客 下载wordpress和mysql:5.7容器 1234[root@docker01 ~]# docker pull wordpress//下载wordpress容器[root@docker01 ~]# docker pull mysql：5.7//下载mysql：5.7容器 编写一个docker-ccompose.yml 1234567891011121314151617181920212223242526[root@docker01 ~]# mkdir wordpress//创建wordpress测试文件[root@docker01 ~]# cd wordpress/[root@docker01 wordpress]# vim docker-compose.yml//编写docker-compose.ymlversion: \"3.1\"services: wordpress: image: wordpress restart: always ports: - 8080:80 environment: WORDPRESS_DB_HOST: db WORDPRESS_DB_USER: wordpress WORDPRESS_DB_PASSWORD: 123.com WORDPRESS_DB_NAME: wordpress db: image: mysql:5.7 restart: always environment: MYSQL_DATABASE: wordpress MYSQL_USER: wordpress MYSQL_PASSWORD: 123.com MYSQL_ROOT_PASSWORD: 123.com 通过docker-compose.yml文件运行容器 1[root@docker01 wordpress]# docker-compose up -d 12[root@docker01 wordpress]# docker ps//查看容器信息 12[root@docker01 wordpress]# docker logs 容器名称//查看容器日志 浏览器访问一下 http://192.168.1.11:8080/ 选择语言 安装wordpress 登陆wordpress 登陆成功后，自己就可以进行设置了","path":"posts/713b.html","date":"06-06","excerpt":"","tags":[]},{"title":"Docker swarm集群","text":"Docker swarm docker swarm集群：三剑客之一 一. 实验环境 主机 IP地址 服务 docker01 192.168.1.11 swarm+overlay+webUI docker02 192.168.1.13 docker docker03 192.168.1.20 docker 三台主机都关闭防火墙，禁用selinux，修改主机名，时间同步，并添加域名解析。 docker版本必须是：v1.12版本开始（可使用docker version查看版本） 1.关闭防火墙，禁用selinux 123[root@localhost ~]# systemctl stop firewalld[root@localhost ~]# hostnamectl set-hostname docker03[root@localhost ~]# su - 2.时间同步 12mv /etc/localtime /etc/localtime.bkcp /usr/share/zoneinfo/Asia/Shanghai/etc/localtime 3.修改主机名（三台都要） 12[root@localhost ~]# hostnamectl set-hostname docker01[root@localhost ~]# su - 4.添加域名解析 1234567[root@docker01 ~]# vim /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.1.11 docker01192.168.1.13 docker02192.168.1.20 docker03 二. swarm原理 **swarm：**作用运行docker engin的多个主机组成的集群 **node：**每一个docker engin都是一个node（节点），分为manager和worker。 **manager node：**负责执行容器的编排和集群的管理工作，保持并维护swarm处于期望的状态。swarm可以有多个manager node，他们会自动协调并选举一个leader执行编排任务。但相反，不能没有manager node。 **worker node：**接受并执行由manager node派发的任务，并且默认manager node也是一个worker node，不过可以将它设置为manager-only node，让他只负责编排和管理工作。 **service：**用来定义worker上执行的命令。 基本命令操作 **docker swarm leave：**申请离开一个集群，之后查看节点状态会变成down，然后可通过manager node 将其删除 **docker node rm xxx：**删除某个节点 docker swarm join-token [manager|worker]：生成令牌，可以是manager或worker身份。 docker node demote（降级）：将swarm节点的为manager降级为worker docker node promote（升级）：将swarm节点的work升级为manager **docker node ls:**查看群集的信息（只可以在manager角色的主机上查看） docker service scale web05=6:容器的动态扩容及缩容 docker service ps web01: 查看创建的容器运行在哪些节点 docker service ls: 查看创建的服务 docker swarm leave: 脱离这个群集 docker node rm docker03: 在manager角色的服务器上移除docker03 docker node update --availability drain docker01: 设置主机docker01以后不运行容器，但已经运行的容器并不会停止 docker node update --label-add mem=max docker03: 更改docker03主机的标签为mem=max docker service update --replicas 8 --image 192.168.20.6:5000/lvjianzhao:v2.0 --container-label-add ‘node.labels.mem==max’ lvjianzhao05: 将服务升级为8个容器，并且指定在mem=max标签的主机上运行 三. docker01 初始化集群 1[root@docker01 ~]# docker swarm init --advertise-addr 192.168.1.11 **–advertise-addr：**指定与其它docker通信的地址。 上边返回的结果告诉我们：初始化成功，并且，如果想要添加work节点运行下面的命令： 注意：token令牌只有24小时的有效期 如果想要添加manager节点：运行下面命令 四.swarm集群的简单操作 1.docker02和docker03以worker加入集群 1[root@docker03 ~]# docker swarm join --token SWMTKN-1-5kxn9wloh7npnytklwbfciesr9di7uvu521gwnqm9h1n0pbokj-1e60wt0yr5583e4mzwbxnn3a8 192.168.1.11:2377 docker01查看集群 1[root@docker01 ~]# docker node ls 注意：这里的”*****“代表的是当前所属的节点 2.删除集群中节点 docker02和docker03申请离开一个集群 1[root@docker02 ~]# docker swarm leave docker删除docker02和docker03节点 12[root@docker01 ~]# docker node rm docker02 [root@docker01 ~]# docker node rm docker03 docker01查看集群 1[root@docker01 ~]# docker node ls 3.docker02和docker03以manager加入集群 docker01生成manager令牌 1[root@docker01 ~]# docker swarm join-token manager docker02和docker03加入集群 1docker swarm join --token SWMTKN-1-5kxn9wloh7npnytklwbfciesr9di7uvu521gwnqm9h1n0pbokj-cz6hbyv9r5htyqwj5tfol65aa 192.168.1.11:2377 docker01查看集群 1[root@docker01 ~]# docker node ls 4.docker02和docker03降级 docker01（manager）把docker02和docker03降级成worker 12[root@docker01 ~]# docker node demote docker02[root@docker01 ~]# docker node demote docker03 查看集群 1[root@docker01 ~]# docker node ls 五.部署docker swarm集群网络 overlay:覆盖型网络 overlay networks 管理Swarm中docker守护进程间的通信。可以将容器附加到一个或多个已存在的overlay网络上，使容器与容器之间能够通信； 12[root@docker01 ~]# docker network create -d overlay --attachable docker//attachable：这个参数必须要加，否则不能用于容器。 在创建网络的时候，我们并没有部署一个存储服务，比如consul，那是因为docker swarm自带存储。 docker01查看网络 但是会发现其他两台并不会发现此网络，需等基于此网络创建service服务就可以看到了 1[root@docker01 ~]# docker network ls 六. docker01部署一个图形化webUI界面 1.docker01 导入镜像 1[root@docker01~]# docker pull dockersamples/visualizer 2.基于镜像启动一台容器 1[root@docker01 ~]# docker run -d -p 8080:8080 -e HOST=192.168.1.100 -e PORT=8080 -v /var/run/docker.sock:/var/run/docker.sock --name visualiaer dockersamples/visualizer 3.通过浏览器访问验证http://192.168.1.11:8080/ 如果访问不到网页，需开启路由转发 12[root@docker01 ~]# echo net.ipv4.ip_forward = 1 &gt;&gt; /etc/sysctl.conf [root@docker01 ~]# sysctl -p 七. 创建service（服务） 1. 基于nginx容器创建一个service服务 1234[root@docker01 ~]#docker pull nginx//下载nginx镜像（三台都要）[root@docker01 ~]# docker service create --replicas 1 --network docker --name web1 -p 80:80 nginx:latest [root@docker01 ~]# docker service create --replicas 1 --network docker --name web2 -p 80 nginx:latest //–replicas：副本数量 大概可以理解为一个副本等于一个容器 2. 查看创建的service服务 1[root@docker01 ~]# docker service ls 单独查看一个servicefuw 1[root@docker01 ~]# docker service ps web1 1[root@docker01 ~]# docker service ps web2 3. web界面查看 4. 基于nginx容器创建五个service服务 1[root@docker01 ~]# docker service create --replicas 5 --network docker --name web -p 80 nginx:latest web界面查看 5. 挂起docker02 web查看（发现服务都分配到其他服务器了） 6. 恢复docker02 web查看（发现服务没有回到docker02） 八、实现docker容器的扩容及缩容 1. 删除web1和web2服务 1[root@docker01 ~]# docker service rm web1 web2 2. 容器的扩容和缩减 （1）扩容 1[root@docker01 ~]# docker service scale web=8 （2）缩减 1[root@docker01 ~]# docker service scale web=3 3.设置manager node不参加工作 1[root@docker01 ~]# docker node update docker01 --availability drain 设置主机docker01以后不运行容器，但已经运行的容器并不会停止 “–availability”选项后面共有三个选项可配置，如下： “active”：工作；“pause”：暂时不工作；“drain”：永久性的不工作 1[root@docker01 ~]# docker node ls web界面查看 九、docker Swarm总结 在我对docker Swarm群集进行一定了解后，得出的结论如下： 参与群集的主机名一定不能冲突，并且可以互相解析对方的主机名； 集群内的所有节点可以都是manager角色，但是不可以都是worker角色； 当指定运行的镜像时，如果群集中的节点本地没有该镜像，那么它将会自动下载对应的镜像； 当群集正常工作时，若一个运行着容器的docker服务器发生宕机，那么，其所运行的所有容器，都将转移到其他正常运行的节点之上，而且，就算发生宕机的服务器恢复正常运行，也不会再接管之前运行的容器；","path":"posts/ca45.html","date":"06-06","excerpt":"","tags":[]},{"title":"docker swarm版本回滚","text":"Docker swarm docker swarm集群：三剑客之一 一. 实验环境 主机 IP地址 服务 docker01 192.168.1.11 swarm+service+webUI+registry docker02 192.168.1.13 docker docker03 192.168.1.20 docker 三台主机都关闭防火墙，禁用selinux，修改主机名，时间同步，并添加域名解析。 docker版本必须是：v1.12版本开始（可使用docker version查看版本） 1.关闭防火墙，禁用selinux 123[root@localhost ~]# systemctl stop firewalld[root@localhost ~]# hostnamectl set-hostname docker03[root@localhost ~]# su - 2.时间同步 12mv /etc/localtime /etc/localtime.bkcp /usr/share/zoneinfo/Asia/Shanghai/etc/localtime 3.修改主机名（三台都要） 12[root@localhost ~]# hostnamectl set-hostname docker01[root@localhost ~]# su - 4.添加域名解析 123[root@docker01 ~]# echo 192.168.1.11 docker01 &gt;&gt; /etc/hosts[root@docker01 ~]# echo 192.168.1.13 docker02 &gt;&gt; /etc/hosts[root@docker01 ~]# echo 192.168.1.20 docker03 &gt;&gt; /etc/hosts 三. docker01 初始化集群 1[root@docker01 ~]# docker swarm init --advertise-addr 192.168.1.11 **–advertise-addr：**指定与其它docker通信的地址。 上边返回的结果告诉我们：初始化成功，并且，如果想要添加work节点运行下面的命令： 注意：token令牌只有24小时的有效期 如果想要添加manager节点：运行下面命令 四，docker02和docker03以worker加入集群 1[root@docker03 ~]# docker swarm join --token SWMTKN-1-5kxn9wloh7npnytklwbfciesr9di7uvu521gwnqm9h1n0pbokj-1e60wt0yr5583e4mzwbxnn3a8 192.168.1.11:2377 docker01查看集群 1[root@docker01 ~]# docker node ls 注意：这里的”*****“代表的是当前所属的节点 五.设置manager node（docker01）不参加工作 1[root@docker01 ~]# docker node update docker01 --availability drain 设置主机docker01以后不运行容器，但已经运行的容器并不会停止 “–availability”选项后面共有三个选项可配置，如下： “active”：工作；“pause”：暂时不工作；“drain”：永久性的不工作 1[root@docker01 ~]# docker node ls 六. docker01部署一个图形化webUI界面 1.docker01 导入镜像 1[root@docker01~]# docker pull dockersamples/visualizer 2.基于镜像启动一台容器 1[root@docker01 ~]# docker run -d -p 8080:8080 -e HOST=192.168.1.100 -e PORT=8080 -v /var/run/docker.sock:/var/run/docker.sock --name visualiaer dockersamples/visualizer 3.通过浏览器访问验证http://192.168.1.11:8080/ 如果访问不到网页，需开启路由转发 12[root@docker01 ~]# echo net.ipv4.ip_forward = 1 &gt;&gt; /etc/sysctl.conf [root@docker01 ~]# sysctl -p 一. Docker01部署一个私有仓库 Docker01部署 123456789101112131415161772 docker pull registry//下载registry镜像73 docker run -itd --name registry -p 5000:5000 --restart=always registry:latest//基于registry镜像，启动一台容器78 vim /usr/lib/systemd/system/docker.service #13行修改ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.11:5000 80 systemctl daemon-reload81 systemctl restart docker.service//重启docker76 docker tag busybox:latest 192.168.1.11:5000/busybox:v1 //把容器重命名一个标签77 docker ps 1234567891078 vim /usr/lib/systemd/system/docker.service #13行修改ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.11:5000 80 systemctl daemon-reload81 systemctl restart docker.service//重启docker100 docker push 192.168.1.11:5000/busybox:v1//上传容器到私有仓库 Docker02和docker03加入私有仓库 12345678978 vim /usr/lib/systemd/system/docker.service #13行修改ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.11:5000 80 systemctl daemon-reload81 systemctl restart docker.service//重启docker99 docker pull 192.168.1.11/busybox:v1//测试下载 2. 自定义镜像 要求：基于httpd镜像，更改访问界面内容。镜像tag版本为v1，v2，v3，对应主机面内容为v1，xgp666、v2，xgp666、v2，xgp666 12[root@docker01 ~]# docker pull httpd//下载httpd镜像 创建三个测试目录 12[root@docker01 ~]# mkdir &#123;v1,v2,v3&#125;//创建测试目录 docker01，v1目录操作 1234567891011121314[root@docker01 ~]# cd v1[root@docker01 v1]# echo v1,xgp666 &gt; index.html//创建测试网页[root@docker01 v1]# vim Dockerfile//编写DockerfileFROM httpdADD index.html /c[root@docker01 v1]# docker build -t 192.168.1.11:5000/httpd:v1 .//基于dockerfile创建镜像[root@docker01 v1]# docker push 192.168.1.11:5000/httpd:v1//上传刚刚创建镜像到私有仓库 docker01，v2目录操作 12345678910111213[root@docker01 v1]# cd ../v2[root@docker01 v2]# echo v2,xgp666 &gt; index.html[root@docker01 v2]# vim Dockerfile //编写DockerfileFROM httpdADD index.html /usr/local/apache2/htdocs/index.html[root@docker01 v2]# docker build -t 192.168.1.11:5000/httpd:v2 .//基于dockerfile创建镜像[root@docker01 v2]# docker push 192.168.1.11:5000/httpd:v2//上传刚刚创建镜像到私有仓库 docker01，v3目录操作 12345678910111213[root@docker01 v1]# cd ../v3[root@docker01 v2]# echo v3,xgp666 &gt; index.html[root@docker01 v2]# vim Dockerfile //编写DockerfileFROM httpdADD index.html /usr/local/apache2/htdocs/index.html[root@docker01 v2]# docker build -t 192.168.1.11:5000/httpd:v3 .//基于dockerfile创建镜像[root@docker01 v2]# docker push 192.168.1.11:5000/httpd:v3//上传刚刚创建镜像到私有仓库 3. 发布一个服务，基于上述镜像 要求:副本数量为3个。服务的名称为: bdqn 1[root@docker01 v3]# docker service create --replicas 3 --name bdqn -p 80:80 192.168.1.11:5000/httpd:v1 查看一下网络 1[root@docker03 ~]# docker network ls 默认的Ingress网络，包括创建的自定义overlay网络, 为后端真正为用户提供服务的container,提供了一个统一的入口。 查看一下创建的副本 1[root@docker01 v3]# docker service ps bdqn 浏览器测试访问http://192.168.1.11:80,http://192.168.1.13:80,http://192.168.1.20:80 修改docker02和docker03测试网页内容 docker02 123[root@docker02 ~]# docker exec -it 388f3bd9dd33 /bin/bashroot@388f3bd9dd33:/usr/local/apache2# cd htdocs/root@388f3bd9dd33:/usr/local/apache2/htdocs# echo 123 &gt; index.html docker03 12[root@docker03 ~]# docker exec -it 281454867fac /bin/bashroot@281454867fac:/usr/local/apache2# echo 321 &gt; htdocs/index.html 测试访问（每一台都会显示，会负载均衡） 要求:副本数量为3个。服务的名称为:test 1[root@docker01 v3]# docker service create --replicas 3 --name test -p 80 192.168.1.11:5000/httpd:v1 查看创建的服务映射端口 1[root@docker01 v3]# docker service ls 默认映射端口30000-32767 4. 服务的扩容与缩容 扩容 1[root@docker01 v3]# docker service scale bdqn=6 缩容 1[root@docker01 v3]# docker service scale bdqn=4 扩容与缩容直接直接通过scale进行设置副本数量。 5.服务的升级与回滚 （1）升级 12[root@docker01 ~]# docker service update --image 192.168.1.11:5000/httpd:v2 bdqn//把bdqn服务升级成v2的版本 测试访问一下 （2）平滑的更新 12[root@docker01 ~]# docker service update --image 192.168.1.11:5000/httpd:v3 --update-parallelism 2 --update-delay 1m bdqn //两个服务一起更新，然后，隔一分钟，继续更新 默认情况下, swarm-次只更新-个副本,并且两个副本之间没有等待时间，我们可以通过 –update-parallelism;设置并行更新的副本数量。 –update-delay：指定滚动更新的时间间隔。 测试访问一下 (3) 回滚操作 1[root@docker01 ~]# docker service rollback bdqn 注意，docker swarm的回滚操作，默认只能回滚到上一-次操作的状态，并不能连续回滚到指定操作。 测试访问一下","path":"posts/4890.html","date":"06-06","excerpt":"","tags":[]},{"title":"AlertManager自定义邮件模板","text":"AlertManager自定义邮件模板 创建模板目录 1234[root@docker01 ~]# cd prometheus//进入之前创建的prometheus目录[root@docker01 prometheus]# mkdir alertmanager-tmpl//创建AlertManager模板目录 编写模板规则 123456789101112131415[root@docker01 prometheus]# vim email.tmpl &#123;&#123; define \"email.from\" &#125;&#125;2877364346@qq.com&#123;&#123; end &#125;&#125;&#123;&#123; define \"email.to\" &#125;&#125;2877364346@qq.com&#123;&#123; end &#125;&#125;&#123;&#123; define \"email.to.html\" &#125;&#125;&#123;&#123; range .Alerts &#125;&#125;=========start==========&lt;br&gt;告警程序: prometheus_alert&lt;br&gt;告警级别: &#123;&#123; .Labels.severity &#125;&#125; 级&lt;br&gt;告警类型: &#123;&#123; .Labels.alertname &#125;&#125;&lt;br&gt;故障主机: &#123;&#123; .Labels.instance &#125;&#125;&lt;br&gt;告警主题: &#123;&#123; .Annotations.summary &#125;&#125;&lt;br&gt;触发时间: &#123;&#123; .StartsAt.Format \"2019-08-04 16:58:15\" &#125;&#125; &lt;br&gt;=========end==========&lt;br&gt;&#123;&#123; end &#125;&#125;&#123;&#123; end &#125;&#125; 修改alertmanager的配置文件 1234567891011121314151617181920212223242526272829[root@docker01 ~]# vim alertmanager.yml global: resolve_timeout: 5m smtp_from: '2877364346@qq.com' smtp_smarthost: 'smtp.qq.com:465' smtp_auth_username: '2877364346@qq.com' smtp_auth_password: 'evjmqipqezlbdfij' smtp_require_tls: false smtp_hello: 'qq.com'templates: #添加模板 - '/etc/alertmanager-tmpl/*.tmpl' #添加路径 route: group_by: ['alertname'] group_wait: 5s group_interval: 5s repeat_interval: 5m receiver: 'email' receivers:- name: 'email' email_configs: - to: '&#123;&#123; template \"email to\" &#125;&#125;' #修改 html: '&#123;&#123; template \"email.to.html\" .&#125;&#125;' #添加 send_resolved: true #删除 inhibit_rules: - source_match: severity: 'critical' target_match: severity: 'warning' equal: ['alertname', 'dev', 'instance'] 重新运行 alertmanager 容器 1234[root@docker01 ~]# docker rm -f alertmanager//删除alertmanager容器[root@docker01 ~]# docker run -itd --name alertmanager -p 9093:9093 -v /root/alertmanager.yml:/etc/alertmanager/alertmanager.yml -v /root/prometheus/alertmanager-tmpl:/etc/alertmanager-tmpl prom/alertmanager:latest//运行一台新的alertmanager容器，记得挂载配置文件 挂起docker02 收到邮件","path":"posts/e914.html","date":"06-06","excerpt":"","tags":[]},{"title":"22 考试","text":"创建镜像的方法 1234567891011121314151617[root@master xgp]# vim DockerfileFROM nginxADD index.htm /usr/share/nginx/html///创建Dockerfile[root@master test]# echo \"&lt;h1&gt;version 01 wsd&lt;/h1&gt;\" &gt; index.html[root@master test]# docker build -t 192.168.1.1:5000/nginx .[root@master test]# echo \"&lt;h1&gt;version 02 wsd&lt;/h1&gt;\" &gt; index.html [root@master test]# docker build -t 192.168.1.1:5000/nginx:v1.14 [root@master test]# echo \"&lt;h1&gt;version 03 wsd&lt;/h1&gt;\" &gt; index.html .[root@master test]# docker build -t 192.168.1.1:5000/nginx:v1.15 .//创建不同index.html文件，生成测试镜像[root@master test]# docker push 192.168.1.1:5000/nginx[root@master test]# docker push 192.168.1.1:5000/nginx:v1.14[root@master test]# docker push 192.168.1.1:5000/nginx:v1.15//上传镜像 2) deployment名字为:nginx,保证运行3个Pod.service名字为：nginx-svc。映射到主机端口：31234.（10分） 12345678910111213141516171819202122232425262728293031[root@master yaml]# docker pull nginx//下载nginx镜像[root@master yaml]# vim deployment.yaml //编写deployment和service的yaml文件apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginxspec: replicas: 3 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx---apiVersion: v1kind: Servicemetadata: name: nginx-svcspec: type: NodePort selector: app: nginx ports: - port: 80 targetPort: 80 nodePort: 31234 执行一下 1[root@master yaml]# kubectl apply -f deployment.yaml 查看一下 1[root@master yaml]# kubectl get pod 1[root@master yaml]# kubectl get svc 访问一下http://192.168.1.21:31234/ 3) 共有3个版本，版本1对应image镜像为：nginx，版本2对应的image为：nginx:1.14.版本3对应的版本为:nginx:1.15.分别运行各版本，每个版本要有在浏览器的访问验证。（10分） 1234[root@master yaml]# docker pull nginx[root@master yaml]# docker pull nginx:1.14[root@master yaml]# docker pull nginx:1.15//下载所需镜像 编写deployment的yaml文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051[root@master yaml]# vim banben1.yaml//编写deployment和service的yaml文件apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginxspec: replicas: 3 template: metadata: labels: app: nginx-svc spec: containers: - name: nginx image: nginx #更改一下镜像（1.14和1.15的）[root@master yaml]# vim banben2.yaml//编写deployment和service的yaml文件apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginx2spec: replicas: 3 template: metadata: labels: app: nginx-svc spec: containers: - name: nginx image: nginx:1.14 #更改一下镜像（1.14和1.15的）[root@master yaml]# vim banben3.yaml//编写deployment和service的yaml文件apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginx3spec: replicas: 3 template: metadata: labels: app: nginx-svc spec: containers: - name: nginx image: nginx:1.15 #更改一下镜像（1.14和1.15的） 编写service的yaml文件 1234567891011121314[root@master yaml]# vim ngnix-svc.yaml apiVersion: v1kind: Servicemetadata: name: nginx-svcspec: type: NodePort selector: app: nginx-svc ports: - port: 80 targetPort: 80 nodePort: 31235 执行一下（记录版本信息） 1234[root@master yaml]# kubectl apply -f banben1.yaml --record [root@master yaml]# kubectl apply -f banben2.yaml --record [root@master yaml]# kubectl apply -f banben3.yaml --record [root@master yaml]# kubectl apply -f ngnix-svc.yaml 查看一下 1[root@master yaml]# kubectl get pod 1[root@master yaml]# kubectl get svc 访问一下 http://192.168.1.21:31235/ 4)运行到版本3之后，进行回滚操作回滚到版本4.（5分） 查看记录的版本信息 1[root@master yaml]# kubectl rollout history deployment nginx 回滚到指定版本 12[root@master ~]# kubectl rollout undo deployment nginx --to-revision=4//这里指定的是版本信息的编号 访问一下 5) 此时更改默认的3个Pod的访问界面,.版本1的访问界面内容为：考生名称+version:No1.版本2的访问界面:考生名称+version:No2,以此类推。（5分） 修改POD页面内容（三台不一样） 12[root@master ~]# kubectl exec -it xgp-web-8d5f9656f-8z7d9 /bin/bash//根据pod名称进入pod之中 进入容器后修改页面内容 12345678910111213141[root@master yaml]# kubectl exec -it nginx-d6c5c85cb-8vcvt /bin/bashroot@nginx-d6c5c85cb-8vcvt:/# echo \"&lt;h1&gt;version 01 wushaodong&lt;/h1&gt;\" &gt; /usr/share/nginx/html/index.html root@nginx-d6c5c85cb-8vcvt:/# exit2[root@master yaml]# kubectl exec -it nginx-d6c5c85cb-bxvvt /bin/bashroot@nginx-d6c5c85cb-bxvvt:/# echo \"&lt;h1&gt;version 02 wushaodong&lt;/h1&gt;\" &gt; /usr/share/nginx/html/index.htmlroot@nginx-d6c5c85cb-bxvvt:/# exit3[root@master yaml]# kubectl exec -it nginx-d6c5c85cb-lhlz9 /bin/bashroot@nginx-d6c5c85cb-lhlz9:/# echo \"&lt;h1&gt;version 03 wushaodong&lt;/h1&gt;\" &gt; /usr/share/nginx/html/index.htmlroot@nginx-d6c5c85cb-lhlz9:/# exit 6) 验证界面是否会会有轮训效果，并加以分析论述。（5分） 不要在浏览器里测试轮询，有缓存 1[root@master ~]# curl 127.0.0.1:31235 答：会有轮询的效果，kubernetes 内部的负载均衡是通过 iptables 的 probability 特性来做到的，kube-proxy通过iptables 将访问 Service 的流量转发到后端 Pod，而且使用类似轮询的负载均衡策略。 7) 创建一个NFS PV，NFS共享目录为：考生名称。PV名称为：new-pv。创建一个PVC，名称为new-pvc。单独创建一个pod，使用new-pv，运行之后，验证nfs是否使用成功。（10分） 12345678910111213[root@master ~]# yum -y install nfs-utils rpcbind[root@master yaml]# mkdir /wushaodong//创建指定名称的共享目录[root@master yaml]# echo \"/wushaodong *(rw,sync,no_root_squash)\" &gt; /etc/exports//编写共享目录的权限[root@master ~]# systemctl start nfs-server[root@master ~]# systemctl start rpcbind//启动服务[root@master yaml]# showmount -e//测试一下 1、创建一个NFS PV的yaml文件 12345678910111213141516171819[root@master yaml]# vim new-pv.yamlapiVersion: v1kind: PersistentVolumemetadata: name: new-xgpspec: capacity: storage: 1Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: nfs nfs: path: /wushaodong/new-pv server: 192.168.1.21[root@master yaml]# mkdir /wushaodong/new-pv//创建指定目录 执行一下 1[root@master yaml]# kubectl apply -f new-pv.yaml 查看一下 1[root@master yaml]# kubectl get pv 2、创建一个PVC的yaml文件 123456789101112[root@master yaml]# vim new-pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: new-pvcspec: accessModes: #要和pv的一直否则关联不成功 - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: nfs #要和pv的一直否则关联不成功 执行一下 1[root@master yaml]# kubectl apply -f new-pvc.yaml 查看一下 1[root@master yaml]# kubectl get pvc 3、单独创建一个pod，使用new-pv 1234567891011121314151617181920[root@master yaml]# vim pod.yamlapiVersion: v1kind: Podmetadata: name: xgp-podspec: containers: - name: xgp-pod image: busybox args: - /bin/sh - -c - sleep 300000 volumeMounts: - mountPath: /wushaodong #容器的被挂载目录 name: volumedata volumes: - name: volumedata persistentVolumeClaim: claimName: new-pvc 执行一下 1[root@master yaml]# kubectl apply -f pod.yaml 查看一下 1[root@master yaml]# kubectl get pod 4、测试一下 12345[root@master yaml]# kubectl exec -it xgp-pod /bin/sh//进入pod# echo \"xgpIwsd\" &gt; /wushaodong/xgp.txt//添加内容到挂载目录# exit 查看一下，挂载目录是否有添加内容 1[root@master yaml]# cat /wushaodong/new-pv/xgp.txt 8）请简述k8s集群中，master节点有哪些组件，node节点有哪些组件，作用分别有什么作用，各组件又是怎么交互的。（5分） master节点 1. API server[资源操作入口]：是k8s集群的前端接口，各种各样客户端工具以及k8s的其他组件可以通过它管理k8s集群的各种资源。它提供了HTTP/HTTPS RESTful API,即K8S API。 提供了资源对象的唯一操作入口，其他所有组件都必须通过它提供的API来操作资源数据，只有API Server与存储通信，其他模块通过API Server访问集群状态。 第一，是为了保证集群状态访问的安全。 第二，是为了隔离集群状态访问的方式和后端存储实现的方式：API Server是状态访问的方式，不会因为后端存储技术etcd的改变而改变。 作为kubernetes系统的入口，封装了核心对象的增删改查操作，以RESTFul接口方式提供给外部客户和内部组件调用。对相关的资源数据“全量查询”+“变化监听”，实时完成相关的业务功能。 2. Scheduler[集群分发调度器]：负责决定将Pod放在哪个Node上运行。在调度时，会充分考虑集群的拓扑结构，当前各个节点的负载情况，以及应对高可用、性能、数据亲和性和需求。 1.Scheduler收集和分析当前Kubernetes集群中所有Minion节点的资源(内存、CPU)负载情况，然后依此分发新建的Pod到Kubernetes集群中可用的节点。 2.实时监测Kubernetes集群中未分发和已分发的所有运行的Pod。 3.Scheduler也监测Minion节点信息，由于会频繁查找Minion节点，Scheduler会缓存一份最新的信息在本地。 4.最后，Scheduler在分发Pod到指定的Minion节点后，会把Pod相关的信息Binding写回API Server。 4. Controller Manager[内部管理控制中心]：负责管理集群的各种资源，保证资源处于预期的状态。它由多种Controller组成，包括Replication Controller、Endpoints Controller、Namespace Controller、Serviceaccounts Controller等。 实现集群故障检测和恢复的自动化工作，负责执行各种控制器，主要有： 1.endpoint-controller：定期关联service和pod(关联信息由endpoint对象维护)，保证service到pod的映射总是最新的。 2.replication-controller：定期关联replicationController和pod，保证replicationController定义的复制数量与实际运行pod的数量总是一致的。 **5. Etcd：**负责保存k8s集群的配置信息和各种资源的状态信息。当数据发生变化时，etcd会快速的通知k8s相关组件。（第三方组件）它有可替换方案。Consul、zookeeper 6. Pod: k8s集群的最小组成单位。一个Pod内，可以运行一个或多个容器。大多数情况下，一个Pod内只有一个Container容器。 **7. Flanner：**是k8s集群网络，可以保证Pod的跨主机通信。也有替换方案。 Node节点 Kubelet[节点上的Pod管家]：它是Node的agent(代理)，当Scheduler确定某 个Node上运行Pod之后，会将Pod的具体配置信息发送给该节点的kubelet,kubelet会根据这些信息创建和运行容器，并向Master报告运行状态。 负责Node节点上pod的创建、修改、监控、删除等全生命周期的管理 定时上报本Node的状态信息给API Server。 kubelet是Master API Server和Minion之间的桥梁，接收Master API Server分配给它的commands和work，与持久性键值存储etcd、file、server和http进行交互，读取配置信息。 具体的工作如下： 设置容器的环境变量、给容器绑定Volume、给容器绑定Port、根据指定的Pod运行一个单一容器、给指定的Pod创建network 容器。 同步Pod的状态、同步Pod的状态、从cAdvisor获取Container info、 pod info、 root info、 machine info。 在容器中运行命令、杀死容器、删除Pod的所有容器。 **kube-proxy[负载均衡、路由转发]:**负责将访问service的TCP/UDP数据流转发到后端的容器。如果有多个 副本，kube-proxy会实现负载均衡。 Proxy是为了解决外部网络能够访问跨机器集群中容器提供的应用服务而设计的，运行在每个Node上。Proxy提供TCP/UDP sockets的proxy，每创建一种Service，Proxy主要从etcd获取Services和Endpoints的配置信息（也可以从file获取），然后根据配置信息在Minion上启动一个Proxy的进程并监听相应的服务端口，当外部请求发生时，Proxy会根据Load Balancer将请求分发到后端正确的容器处理。 Proxy不但解决了同一主宿机相同服务端口冲突的问题，还提供了Service转发服务端口对外提供服务的能力，Proxy后端使用了随机、轮循负载均衡算法。 各个组件的作用以及架构工作流程: 1) kubectl发送部署 请求到API server 2) APIserver通知Controller Manager创建一个Deployment资源。 3) Scheduler执行调度任务,将两个副本Pod分发到node01和node02. 上。 4) node01和node02, 上的kubelet在各自节点上创建并运行Pod。 补充 1.应用的配置和当前的状态信息保存在etcd中，执行kubectl get pod时API server会从etcd中读取这些数据。 2.flannel会为每个Pod分配一个IP。 但此时没有创建Service资源，目前kube-proxy还没有参与进来。 9）部署一个dashboard。（5分） 1、下载所需yaml文件和镜像 12[root@master https]# wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-rc5/aio/deploy/recommended.yaml[root@master https]# docker pull kubernetesui/dashboard:v2.0.0-rc5 2、修改 recommended.yaml 12345678910111213141516[root@master https]#vim recommended.yaml ---kind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboardspec: type: NodePort #添加40 ports: - port: 443 targetPort: 8443 selector: k8s-app: kubernetes-dashboard 执行一下 1[root@master https]# kubectl apply -f recommended.yaml 查看一下 1[root@master https]# kubectl get svc -n kubernetes-dashboard 3、浏览器访问https://192.168.1.21:30949/ PS:如果是使用的旧版本的dashboard, 使用Google浏览器登录，可能是不成功的，需要换成其他的浏览器，比如:火狐。 4、基于token的方法登录dashboard &lt;1&gt;创建一个dashboard的管理用户 1[root@master https]# kubectl create serviceaccount dashboard-admin -n kube-system &lt;2&gt;绑定用户为集群管理用户 1[root@master https]# kubectl create clusterrolebinding dashboard-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin &lt;3&gt;获取Token 12[root@master https]# kubectl get secrets -n kube-system | grep dashboard-admin//先得到Token的名称 12[root@master https]# kubectl describe secrets -n kube-system dashboard-admin-token-j874n//查看上述得到的secret资源的详细信息，会得到token &lt;4&gt;在浏览器上使用token登录。 成功界面 10）使用helm的方式，部署mysql服务，要求使用storageclass作为持久化存储，服务运行之后，进入数据库，创建一个test库，库中一张test表，内容为： 9527. 然后模拟数据库Pod失败，待Pod重启后，查看对应数据是否还存在？（10分） 1、安装部署helm工具 （1）下载helm的包 12[root@master ~]#docker pull gcr.io/kubernetes-helm/tiller:v2.14.3[root@master ~]# wget https://get.helm.sh/helm-v2.14.3-linux-amd64.tar.gz （2）把helm包的命令，复制到本地 123456[root@master helm]# mv linux-amd64/helm /usr/local/bin///移动命令目录到/usr/local/bin/[root@master helm]# chmod +x /usr/local/bin/helm //给予执行权限[root@master helm]# helm help//验证是否安装成功 （3）设置命令自动补全 123[root@master helm]# echo 'source &lt;(helm completion bash)' &gt;&gt; /etc/profile[root@master helm]# . /etc/profile//刷新一下 2、安装Tiller server（服务端，需要创建授权用户） 12345678910111213141516171819[root@master ~]# vim tiller-rbac.yaml #创建授权用户apiVersion: v1kind: ServiceAccountmetadata: name: tiller namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: tillerroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects: - kind: ServiceAccount name: tiller namespace: kube-system 执行一下 1[root@master ~]# kubectl apply -f tiller-rbac.yaml （1）Tiller server的环境初始化 12[root@master helm]# helm init --service-account=tiller//helm的服务端就是Tiller（因为是访问外国的网站，可能需要多次执行） 查看一下 1[root@master helm]# kubectl get deployment. -n kube-system 现在发现没有开启，那是因为默认下载的Google的镜像，下载不下来 （2）设置镜像源改为阿里云的 1[root@master helm]# helm repo add stable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts 查看一下 1[root@master helm]# helm version 3、基于NFS服务，创建共享。 因为上面已经做过了，所以现在只需创建目录和设置权限即可 123456789[root@master heml]# mkdir /xgpwsd//创建目录[root@master heml]# echo '/xgpwsd *(rw,sync,no_root_squash)' &gt;&gt; /etc/exports//设置共享目录权限[root@master heml]# systemctl restart nfs-server[root@master heml]# systemctl restart rpcbind//重启nfs服务[root@master heml]# showmount -e//测试一下 4、创建pv 12345678910111213141516[root@master xgp]# vim nfs-pv1.yml apiVersion: v1kind: PersistentVolumemetadata: name: mysqlpvspec: capacity: storage: 8Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle nfs: path: /xgpwsd/xgp server: 192.168.1.21[root@master xgp]# mkdir /xgpwsd/xgp//创建所需目录 执行一下 1[root@master xgp]# kubectl apply -f nfs-pv1.yml 查看一下 1[root@master xgp]# kubectl get pv 5、创建StorageClass资源对象。 （1）创建rbac权限。 123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@master yaml]# vim rbac.yaml apiVersion: v1kind: ServiceAccountmetadata: name: nfs-provisioner namespace: default---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: nfs-provisioner-runner namespace: defaultrules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"watch\", \"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\", \"endpoints\"] verbs: [\"get\",\"create\",\"list\", \"watch\",\"update\"] - apiGroups: [\"extensions\"] resources: [\"podsecuritypolicies\"] resourceNames: [\"nfs-provisioner\"] verbs: [\"use\"]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: run-nfs-provisionersubjects: - kind: ServiceAccount name: nfs-provisioner namespace: default #必写字段roleRef: kind: ClusterRole name: nfs-provisioner-runner apiGroup: rbac.authorization.k8s.io 执行一下 1[root@master yaml]# kubectl apply -f rbac.yaml （2）创建Deployment资源对象，用Pod代替 真正的NFS服务。 123456789101112131415161718192021222324252627282930313233343536[root@master yaml]# vim nfs-deployment.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nfs-client-provisionerspec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-client-provisioner spec: serviceAccount: nfs-provisioner containers: - name: nfs-client-provisioner image: registry.cn-hangzhou.aliyuncs.com/open-ali/nfs-client-provisioner volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: xgp - name: NFS_SERVER value: 192.168.1.21 - name: NFS_PATH value: /xgpwsd/wsd volumes: - name: nfs-client-root nfs: server: 192.168.1.21 path: /xgpwsd/wsd [root@master heml]# mkdir /xgpwsd/wsd//创建指定目录 执行一下 1[root@master yaml]# kubectl apply -f nfs-deployment.yaml 查看一下 1[root@master yaml]# kubectl get pod （3）创建storageclass的yaml文件 1234567[root@master yaml]# vim xgp-storageclass.yaml apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: xgp-nfsprovisioner: xgp #通过provisioner字段关联到上述DeployreclaimPolicy: Retain 执行一下 1[root@master yaml]# kubectl apply -f xgp-storageclass.yaml 查看一下 1[root@master yaml]# kubectl get sc 6、创建一个mysql服务 123456789[root@master ~]# docker pull mysql:5.7.14//下载所需镜像[root@master yaml]# helm fetch stable/mysql//直接下载stable/mysql的chart包[root@master yaml]# tar -zxf mysql-0.3.5.tgz //解压mysql包[root@master yaml]# cd mysql/[root@master mysql]# vim values.yaml //修改values.yaml文件，添加storageClass存储卷 12[root@master mysql]# helm install stable/mysql -n xgp-mysql --set mysqlRootPassword=123.com -f values.yaml //基于values.yaml和stable/mysql开启一个密码为123.com的mysqlpod 查看一下 1[root@master mysql]# kubectl get svc 1[root@master mysql]# kubectl get pod -o wide 7、进入mysql数据库，创建一个test库，库中一张test表，内容为： 9527。 1[root@master xgp]# kubectl exec -it bdqn-mysql-mysql-7b89c7b99-8ff2r -- mysql -u root -p123.com 创建数据库 1mysql&gt; create database test; 切换数据库 1mysql&gt; use test; 创建表 1mysql&gt; create table test( id int(4))； 在表中插入数据 1mysql&gt; insert test values(9527); 查看表 1mysql&gt; select * from test; 8、模拟数据库Pod失败，待Pod重启后，查看对应数据是否还存在？ 12[root@master mysql]# kubectl delete pod xgp-mysql-mysql-67c6fb5f9-4h4kz//删除这个pod让他重新生成 1[root@master mysql]# kubectl get pod 进入新的pod查看 1[root@master mysql]# kubectl exec -it xgp-mysql-mysql-67c6fb5f9-k4c29 -- mysql -u root -p123.com 123456789101112mysql&gt; use test;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; select * from test;+------+| id |+------+| 9527 |+------+1 row in set (0.00 sec)","path":"posts/gssl.html","date":"05-01","excerpt":"","tags":[]},{"title":"21 k8s的持续集成","text":"实验环境 IP 主机名称 服务 192.168.1.21 master k8s 192.168.1.22 node01 k8s 192.168.1.10 git gitlab 192.168.1.13 jenkins jenkins 总体流程： 在开发机开发代码后提交到gitlab 之后通过webhook插件触发jenkins进行构建，jenkins将代码打成docker镜像，push到docker-registry 之后将在k8s-master上执行rc、service的创建，进而创建Pod，从私服拉取镜像，根据该镜像启动容器 应用构建和发布流程说明。 用户向Gitlab提交代码，代码中必须包含Dockerfile 将代码提交到远程仓库 用户在发布应用时需要填写git仓库地址和分支、服务类型、服务名称、资源数量、实例个数，确定后触发Jenkins自动构建 Jenkins的CI流水线自动编译代码并打包成docker镜像推送到Harbor镜像仓库 Jenkins的CI流水线中包括了自定义脚本，根据我们已准备好的kubernetes的YAML模板，将其中的变量替换成用户输入的选项 生成应用的kubernetes YAML配置文件 更新Ingress的配置，根据新部署的应用的名称，在ingress的配置文件中增加一条路由信息 更新PowerDNS，向其中插入一条DNS记录，IP地址是边缘节点的IP地址。关于边缘节点，请查看边缘节点配置 Jenkins调用kubernetes的API，部署应用 一、前期工作 1、先验证k8s集群（1.21和1.22） 1[root@master ~]# kubectl get nodes 2、master部署私有仓库 Docker01部署 12345678910111213141516171872 docker pull registry//下载registry镜像73 docker run -itd --name registry -p 5000:5000 --restart=always registry:latest//基于registry镜像，启动一台容器78 vim /usr/lib/systemd/system/docker.service #13行修改ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.21:5000 80 systemctl daemon-reload81 systemctl restart docker.service//重启docker76 docker tag httpd:latest 192.168.1.11:5000/web:v1 76 docker tag httpd:latest 192.168.1.11:5000/web:v2//把容器重命名一个标签77 docker ps 123456789101178 vim /usr/lib/systemd/system/docker.service #13行修改ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.11:5000 80 systemctl daemon-reload81 systemctl restart docker.service//重启docker100 docker push 192.168.1.11:5000/web:v1100 docker push 192.168.1.11:5000/web:v2//上传容器到私有仓库 Docker02和docker03加入私有仓库 12345678978 vim /usr/lib/systemd/system/docker.service #13行修改ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.11:5000 80 systemctl daemon-reload81 systemctl restart docker.service//重启docker99 docker pull 192.168.1.21:5000/web:v1//测试下载 3、然后重要的地方到了，建立 yaml配置文件让kubernetes自己控制容器集群。 用来模拟我们部署的服务 12345678910111213141516171819[root@master app]# vim deploy.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: webspec: replicas: 2 template: metadata: labels: name: web spec: containers: - name: web image: 192.168.1.21:5000/web:v1 imagePullPolicy: Always #改为本地仓库下载 ports: - containerPort: 80 执行一下 1[root@master app]# kubectl apply -f deploy.yaml 查看一下 1[root@master app]# kubectl get pod 可是容器的ip只能在容器本机上访问，集群内的其他主机和集群外的主机都没办法访问，这个时候就需要将容器的端口映射到服务器上的端口了，所以需要做一个service的模板。service 模板可以将容器的端口映射到服务器的端口上，并且可以固定映射在服务器上的端口。 12345678910111213141516[root@master app]# vim deploy-svc.yamlapiVersion: v1kind: Servicemetadata: labels: name: web name: webspec: type: NodePort ports: - port: 80 targetPort: 80 nodePort: 31234 selector: name: web 执行一下 1[root@master app]# kubectl apply -f deploy-svc.yaml 查看一下 1[root@master app]# kubectl get svc 访问一下http://192.168.1.21:31234/ 《ok kubernetes 完毕， 开始配置 jenkins+gitlab联动》 4、git和jenkins加入私有仓库 12345678978 vim /usr/lib/systemd/system/docker.service #13行修改ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.11:5000 80 systemctl daemon-reload81 systemctl restart docker.service//重启docker99 docker pull 192.168.1.11/busybox:v1//测试下载 5、jenkins服务器向k8smaster做免密登录 1100 ssh-copy-id 192.168.1.21 二、安装jenkins（1.13） 安装java环境 123456789101112131415[root@jenkins ~]# tar -zxf jdk-8u231-linux-x64.tar.gz[root@jenkins ~]# mv jdk1.8.0_131 /usr/java#注意 这里有位置敏感，不要多一个“/”[root@jenkins ~]# vim /etc/profile #在最下面写export JAVA_HOME=/usr/javaexport JRE_HOME=/usr/java/jreexport PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$PATHexport CLASSPATH=$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar[root@jenkins ~]# source /etc/profile//环境变量生效[root@jenkins ~]# java -version//验证环境变量 安装tomcat 1234567[root@jenkins ~]# tar -zxf apache-tomcat-7.0.54.tar.gz [root@jenkins ~]# mv apache-tomcat-7.0.54 /usr/tomcat7[root@jenkins ~]# cd /usr/tomcat7/webapps/[root@jenkins webapps]# rm -rf *[root@jenkins webapps]# cp /root/jenkins.war . #这几步是jenkins的包放进了tomcat里[root@jenkins webapps]# vim /usr/tomcat7/conf/server.xml //修改tomcat的字符集 12345678910[root@jenkins webapps]# cd /usr/tomcat7/bin/[root@jenkins bin]# vim catalina.sh #!/bin/shexport CATALINA_OPTS=\"-DJENKINS_HOME=/data/jenkins\"export JENKINS_JAVA_OPTIONS=\"-Djava.awt.headless=true -Dhudson.ClassicPluginStrategy.noBytecodeTransformer=true\"//这两行添加的是jenkins的家目录位置，这个很重要[root@jenkins bin]# ./catalina.sh start //启动tomcat 1[root@jenkins bin]# netstat -anput | grep 8080 浏览器安装jenkins http://192.168.1.11:8080/jenkins 12[root@jenkins bin]# cat /data/jenkins/secrets/initialAdminPasswordc577cbf75d934878a94b0f9e00ada328 //复制密码 （1）推荐安装 #左边是自动安装， 右边是自定义安装，我们选左边的，如果不是这个画面则说明网络很卡或者没有网(推荐使用右边的，然后选择不安装插件，之后可以自定义安装） （2）这个是自定义安装（自己上传的包） 12345678[root@autoweb bin]# ./catalina.sh stop[root@autoweb ~]# cd /data/jenkins/plugins/[root@autoweb jenkins]# mv plugins plugins/.bk然后上传plugins.tar.gz包：[root@autoweb jenkins]# tar -zxf plugins.tar.gz [root@autoweb ~]# cd /usr/tomcat7/bin/[root@autoweb bin]# ./catalina.sh stop[root@autoweb bin]# ./catalina.sh start 输入密码后断网 （3）两个剩下的方法一样 下载中文插件 系统管理-----&gt;插件管理-----&gt;avalilable(可选)然后搜索localization-zh-cn 然后还需要3个插件 三、安装gitlab（1.10） GitLab CI 是 GitLab 默认集成的 CI 功能，GitLab CI 通过在项目内 .gitlab-ci.yaml 配置文件读取 CI 任务并进行相应处理；GitLab CI 通过其称为 GitLab Runner 的 Agent 端进行 build 操作；Runner 本身可以使用多种方式安装，比如使用 Docker 镜像启动等；Runner 在进行 build 操作时也可以选择多种 build 环境提供者；比如直接在 Runner 所在宿主机 build、通过新创建虚拟机(vmware、virtualbox)进行 build等；同时 Runner 支持 Docker 作为 build 提供者，即每次 build 新启动容器进行 build；GitLab CI 其大致架构如下 12345# yum -y install curl policycoreutils openssh-server openssh-clients postfix git# systemctl enable sshd# systemctl start sshd# systemctl enable postfix# systemctl start postfix 安装gitlab-ce 1[root@git ~]# curl -sS https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.rpm.sh | sudo bash 注：由于网络问题，国内用户，使用清华大学的镜像源进行安装： 1234567891011121314151617181920212223242526272829[root@git ~]# vim /etc/yum.repos.d/gitlab-ce.repo[gitlab-ce]name=gitlab-cebaseurl=http://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7repo_gpgcheck=0gpgcheck=0enabled=1gpgkey=https://packages.gitlab.com/gpg.key[root@git ~]# yum makecache//保存到本地[root@git ~]# yum -y install gitlab-ce #这两条命令是把gitlab源先加入了yum，然后yum下载gitlab[root@git ~]# vim /etc/gitlab/gitlab.rb //修改端口是为了防止端口冲突，因为80默认是http服务的 external_url 'http://192.168.1.21:90' #端口， unicorn默认是8080 也是tomcat的端口 unicorn['listen'] = '127.0.0.1'unicorn['port'] = 3000 [root@git ~]# gitlab-ctl reconfigure //启动gitlab，这个过程可能会有点慢[root@git ~]# ls /etc/yum.repos.d///查看一下 访问192.168.1.10:90 在网页配置用户密码后则安装完毕。用户默认root，这里让设置一个密码再登录，这里设置12345.com（相对较短的密码不让设置） 四、jenkins和gitlab相互关联 jenkins：工具集成平台 gitlab: 软件托管平台 部署这两个服务的联动，需要经过ssh验证。 1、首先我们需要在gitlab上绑定jenkins服务器的ssh公钥，这里我们使用的是root用户的公私钥，切记生产环境是不允许随便用root的 （1）jenkins 12[root@jenkins ~]# ssh-keygen -t rsa //然后不输入只回车会生成一对公私钥 默认在/root/.ssh/目录里 123[root@jenkins ~]# cat /root/.ssh/id_rsa.pub //查看公钥并复制ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDMA4+je3NsxZrF2v8TPLXJp1ejwy1YokXipEFyGVNo5IbtkiBDwBLOAl5i7yromY8YGgoNNriE2g89IM/44BGC5UDCokQ69Ze9Ta9Kynv3/1PDFXIABJJG0f6LsUqt0nKFaFoGz3ZuYAnl6AzLpXEic8DBDrsFk+UGrxvMfSEqHlYO2b7jRXE1HGRnqI/IcVB190cLT1kmBKi7hSqUNBc1cY6t3a6gGiBpp9tc8PW4r/RcLblhAL1LKx8x37NOZkqox8IMh3eM/wtWwAVFlI8XU+sz9akzJOVmd1ArT5Q4w8WA/uVHCDUGVI/fli/ZRv+mNZyF3EH26runctb5LkCT root@jenkins （2）gitlab 在这里放刚才拷贝的公钥保存就行了。 我们先在gitlab上创建一个代码仓库 点击 new project 输入一个仓库的名字，权限选择公共的（public）然后直接点击创建 点击新建一个new.file 写入代码，起一个名字然后保存 创建好了，然后在本地测试一下是否可用 12345678910[root@git ~]# mkdir xgp[root@git ~]# cd xgp/[root@git xgp]# git clone git@192.168.1.10:root/xgp-demo.git//克隆xgp-demo仓库到本地[root@git xgp]# ls xgp-demo/index.html[root@git xgp]# cat xgp-demo/index.html print: \"hello word!!!\"//查看一下 （3）自动构建 安装插件 先进入到之前查看插件的地方 系统设置----插件管理----高级_—上传插件gitlab-oauth、gitlab-plugin、 windows-slaves、ruby-runt ime、gitlab-hook （4）如果可以用，则打开jenkins 点击新建 地址粘贴进去以后没有报错则没错 但是很伤心它报错了，那是因为jenkins和git没有关联上 解决 git主机生成ssh密钥 1234[root@jenkins ~]# ssh-keygen -t rsa //然后不输入只回车会生成一对公私钥[root@jenkins ~]# cat /root/.ssh/id_rsa //查看密钥并复制 下面的这个插件很重要，就是他实现自动化更新的webhook插件，安装过了就会有这条，然后点击这条下面出来的这些东西保持默认就行。同时注意复制 这个里面写的是jenkins构建时候会执行的shell脚本，这个是最重要的，就是他实现了下端kubernetes自动更新容器的操作。 12345678910111213#!/bin/bashbackupcode=\"/data/backcode/$JOB_NAME/$BUILD_NUMBER\" mkdir -p $backupcode #jenkins创建上述目录chmod 644 \"$JENKINS_HOME\"/workspace/\"$JOB_NAME\"/*rsync -acP \"$JENKINS_HOME\"/workspace/\"$JOB_NAME\"/* $backupcode #$JENKINS_HOME和$JOB_NAME同步最新消息#ssh root@192.168.1.21 sed -i 's/v1/v2/g' /root/app/deploy.yaml #更改镜像版本echo From 192.168.1.21:5000/web:v1 &gt; \"$JENKINS_HOME\"/workspace/Dockerfileecho COPY ./\"$JOB_NAME\"/* /usr/local/apache2/htdocs/ &gt;&gt; \"$JENKINS_HOME\"/workspace/Dockerfiledocker rmi 192.168.1.21:5000/web:v1docker build -t 192.168.1.21:5000/web:v1 /\"$JENKINS_HOME\"/workspace/.docker push 192.168.1.21:5000/web:v1ssh root@192.168.1.21 kubectl delete deployment webssh root@192.168.1.21 kubectl apply -f /root/app/deploy.yaml $JOB_NAME：项目名称 $BUILD_NUMBER：第几次构建 $JENKINS_HOME：jenkins的家目录 完事以后先别保存，首先复制一下上面的jenkins地址，然后去gitlab上绑定webhook 保存，登陆gitlab，点击下图这个设置 测试显示下图 的蓝条说明jenkins 已经连通了gitlab 回到Jenkins开启匿名访问权限 测试显示下图 的蓝条说明jenkins 已经连通了gitlab 好了，jenkins和gitlab 都已经互相的ssh通过了，然后我们最后需要做的一个ssh是关于jenkins ///注意，这里是从git和jenkins向master节点做免密登录。 12[root@git ~]# ssh-copy-id root@192.168.1.21[root@jenkins ~]# ssh-copy-id root@192.168.1.21 好了，环境全部部署完毕！！！。开始测试 五、测试 测试的方法很简单，就是在gitlab上新建代码，删除代码，修改代码，都会触发webhook进行自动部署。最终会作用在所有的nginx容器中，也就是我们的web服务器。 这里我修改了之前建立的 index.html文件 保存以后，就打开浏览器 一直访问kubernetes-node 里面的容器了 访问一下http://192.168.1.21:31234/ 如果没有变，应该注意查看是否在jenkins上构建完成，等以小会就可以了。 构建成功 六、GitLab CI 总结 CS 架构 GitLab 作为 Server 端，控制 Runner 端执行一系列的 CI 任务；代码 clone 等无需关心，GitLab 会自动处理好一切；Runner 每次都会启动新的容器执行 CI 任务 容器即环境 在 Runner 使用 Docker build 的前提下；所有依赖切换、环境切换应当由切换不同镜像实现，即 build 那就使用 build 的镜像，deploy 就用带有 deploy 功能的镜像；通过不同镜像容器实现完整的环境隔离 CI即脚本 不同的 CI 任务实际上就是在使用不同镜像的容器中执行 SHELL 命令，自动化 CI 就是执行预先写好的一些小脚本 敏感信息走环境变量 一切重要的敏感信息，如账户密码等，不要写到 CI 配置中，直接放到 GitLab 的环境变量中；GitLab 会保证将其推送到远端 Runner 的 SHELL 变量中","path":"posts/c0fd.html","date":"05-01","excerpt":"","tags":[]},{"title":"20 k8s的helm模板","text":"自定义helm模板 https://hub.helm.sh/ 1、开发自己的chare包 12345678910111213141516[root@master ~]# helm create mychare//创建一个名为mychare的chare包[root@master ~]# tree -C mychare///以树状图查看一下chare包mychare/├── charts├── Chart.yaml├── templates│ ├── deployment.yaml│ ├── _helpers.tpl│ ├── ingress.yaml│ ├── NOTES.txt│ ├── service.yaml│ └── tests│ └── test-connection.yaml└── values.yaml 2、调试chart 123[root@master mychare]# cd[root@master ~]# helm install --dry-run --debug mychare//检查这个mychare是否有问题 3、安装chart 1[root@node02 ~]# docker pull nginx:stable （1）通过仓库安装 12[root@master mychare]# helm search redis//搜索chare包 12[root@master mychare]# helm repo list//查看是否有能访问仓库 12[root@master mychare]# helm install stable/redis//安装 （2）通过tar包安装 1234567891011121314151617[root@master ~]# helm fetch stable/redis//直接下载chare包[root@master ~]# tar -zxf redis-1.1.15.tgz//解压下载的chare包[root@master ~]# tree -C redisredis├── Chart.yaml├── README.md├── templates│ ├── deployment.yaml│ ├── _helpers.tpl│ ├── networkpolicy.yaml│ ├── NOTES.txt│ ├── pvc.yaml│ ├── secrets.yaml│ └── svc.yaml└── values.yaml （3）通过chare本地目录安装 12345[root@master ~]# helm fetch stable/redis//直接下载chare包[root@master ~]# tar -zxf redis-1.1.15.tgz//解压下载的chare包[root@master ~]# helm install redis （4）通过URL安装 1[root@master ~]# helm install https://example.com/charts/foo-1.2.3.tgz 使用本地目录安装： 12[root@master ~]# cd mychare/[root@master mychare]# vim values.yaml 12[root@master mychare]# cd templates/[root@master templates]# vim service.yaml 123[root@master templates]# cd ..[root@master mychare]# helm install -n test ../mychare/[root@master ~]# helm upgrade test mychare/ -f mychare/values.yaml 4、例子 使用mychart部署一个实例: xgp。使用镜像为私有镜像v1 版本。 完成之后，镜像版本。 全部成功之后，将实例做一个升级，将镜像改为v2版本。 更改镜像为私有镜像 1[root@master ~]# vim mychare/values.yaml 12[root@master ~]# helm install -n xgp mychare/ -f mychare/values.yaml[root@master ~]# kubectl get deployments. -o wide 1[root@master ~]# vim mychare/values.yaml 12[root@master ~]# helm upgrade xgp mychare/ -f mychare/values.yaml [root@master ~]# kubectl get deployments. -o wide 1[root@master ~]# kubectl edit deployments. xgp-mychare 1[root@master ~]# kubectl get deployments. -o wide 创建自己的Repo仓库 1、node01启动一个httpd的容器 123456[root@node01 ~]# mkdir /var/xgp//创建一个目录[root@node01 ~]# docker pull httpd//下载httpd镜像[root@node02 ~]# docker run -d -p 8080:80 -v /var/xgp:/usr/local/apache2/htdocs httpd//启动一个httpd的容器 2、master节点上，将mychart目录打包。 12[root@master ~]# helm package mychare/Successfully packaged chart and saved it to: /root/mychare-0.1.0.tgz 3、生成仓库的index文件。 12345678[root@master ~]# mkdir myrepo//创建一个目录存放打包的chare[root@master ~]# mv mychare-0.1.0.tgz myrepo///移动打包好的文件[root@master ~]# helm repo index myrepo/ --url http://192.168.1.22:8080/charts//生成仓库的index文件[root@master ~]# ls myrepo/index.yaml mychare-0.1.0.tgz 4、将生成的tar包和index.yaml上传到node01的/var/www/charts目录下. node01创建目录 1[root@node01 ~]# mkdir /var/xgp/charts master移动动到 1[root@master ~]# scp myrepo/* node01:/var/xgp/charts/ node01查看一下 12[root@node01 ~]# ls /var/xgp/charts/index.yaml mychare-0.1.0.tgz 5、添加新的repo仓库。 1[root@master ~]# helm repo add newrepo http://192.168.1.22:8080/charts 1[root@master ~]# helm repo list 1[root@master ~]# helm search mychare 6、我们就可以直接使用新的repo仓库部署实例了。 1[root@master ~]# helm install newrepo/mychare -n wsd 1[root@master ~]# helm list 7.如果以后仓库中新添加了chart包,需要用helm repo update命玲更新本地的index文件。 练习： 新创建一个bdqn.的chart包。然后将chart包上传到上述repo源中。 12345678[root@master ~]# helm create bdqn[root@master ~]# helm package bdqn/[root@master ~]# mv bdqn-0.1.0.tgz myrepo/[root@master ~]# helm repo index myrepo/ --url http://192.168.1.22:8080/charts[root@master myrepo]# scp bdqn-0.1.0.tgz index.yaml node01:/var/xgp/charts[root@master myrepo]# helm repo update[root@master myrepo]# helm search bdqn[root@master myrepo]# helm install http://192.168.1.22:8080/charts/bdqn-0.1.0.tgz 1）创建helm的私有仓库，以自己的名字命名。 1、node01启动一个httpd的容器 123456[root@node01 ~]# mkdir /var/xgp//创建一个目录[root@node01 ~]# docker pull httpd//下载httpd镜像[root@node02 ~]# docker run -d -p 8080:80 -v /var/xgp:/usr/local/apache2/htdocs httpd//启动一个httpd的容器 3、生成仓库的index文件。 1234[root@master ~]# mkdir xgprepo//创建一个目录存放打包的chare[root@master ~]# helm repo index xgprepo/ --url http://192.168.1.22:8080/charts//生成仓库的index文件 4、将生成的index.yaml上传到node01的/var/www/charts目录下. node01创建目录 1[root@node01 ~]# mkdir /var/xgp/charts master移动动到 1[root@master ~]# scp xgprepo/* node01:/var/xgp/charts/ node01查看一下 12[root@node01 ~]# ls /var/xgp/charts/index.yaml 5、添加新的repo仓库 12[root@master ~]# helm repo add xgp http://192.168.1.22:8080/charts[root@master ~]# helm repo list 2） 自定义一个chart包，要求这个包运行一个httpd的服务，使用私有镜像v1版本。3个副本Pod，service类型更改为NodePort，端口指定为:30000 自定义一个chart包 12[root@master ~]# helm create wsd//创建一个名为wsd的chares包 按照要求修改配置文件 123456789101112131415161718192021222324252627282930313233343536373839[root@master ~]# cd wsd///进入这个chart包[root@master wsd]# vim values.yaml//修改wsd的配置文件replicaCount: 3 #三个副本image: repository: 192.168.1.21:5000/web #更改镜像为私有镜像 tag: v1 #镜像标签v1 pullPolicy: IfNotPresent imagePullSecrets: []nameOverride: \"\"fullnameOverride: \"\"service: type: NodePort #修改模式为映射端口 port: 80 nodePort: 30000 #添加端口[root@master wsd]# vim templates/service.yaml apiVersion: v1kind: Servicemetadata: name: &#123;&#123; include \"wsd.fullname\" . &#125;&#125; labels:&#123;&#123; include \"wsd.labels\" . | indent 4 &#125;&#125;spec: type: &#123;&#123; .Values.service.type &#125;&#125; ports: - port: &#123;&#123; .Values.service.port &#125;&#125; targetPort: http protocol: TCP name: http nodePort: &#123;&#123; .Values.service.nodePort &#125;&#125; #“添加”能让服务识别到nodePort的端口 selector: app.kubernetes.io/name: &#123;&#123; include \"wsd.name\" . &#125;&#125; app.kubernetes.io/instance: &#123;&#123; .Release.Name &#125;&#125; 测试一下 1[root@master ~]# helm install -n wsd wsd/ -f wsd/values.yaml 查看一下镜像版本 1[root@master ~]# kubectl get deployments. -o wide 访问一下 1[root@master ~]# curl 127.0.0.1:30000 3) 将实例进行更新，要求镜像生产v2版本。 私有镜像和官方镜像升级有所不同，官方的只需通过 （helm upgrade --set imageTag=“标签” 服务名称 charts包名 ）进行更改标签即可，而私有镜像需通过更改values.yaml中的标签才行比较麻烦一点。 1、修改values.yaml 1234567891011121314[root@master ~]# vim wsd/values.yaml # Default values for wsd.# This is a YAML-formatted file.# Declare variables to be passed into your templates.replicaCount: 3image: repository: 192.168.1.21:5000/web tag: v2 #修改标签为v2 pullPolicy: IfNotPresent[root@master ~]# helm upgrade wsd wsd/ -f wsd/values.yaml//基于配置文件刷新一下wsd服务 查看一下 1[root@master ~]# kubectl get deployments. -o wide 访问一下 1[root@master ~]# curl 127.0.0.1:30000 2、使用edit进行版本更新 确定wsd这个服务开启 1[root@master ~]# kubectl edit deployments. wsd 查看一下 1[root@master ~]# kubectl get deployments. -o wide 访问一下 1[root@master ~]# curl 127.0.0.1:30000 4）重新定义一个chart包，名称为: new-test,将这个包上传到上述私有仓库中。 1[root@master ~]# helm repo list 1234567891011121314151617[root@master ~]# helm create xgp-wsd//创建一个名为xgp-wsd的charts包[root@master ~]# helm package xgp-wsd///将xgp-wsd打包在当前目录[root@master ~]# mv xgp-wsd-0.1.0.tgz xgprepo///把打包文件放到仓库目录[root@master ~]# helm repo index xgprepo/ --url http://192.168.1.22:8080/charts//把仓库目录新加入的charts包信息记录在index.yaml中，使得其他加入的主机可以识别到，仓库的charts包[root@master ~]# scp xgprepo/* node01:/var/xgp/charts//将仓库目录的文件移动到httpd服务上，使各个主机可以访问，下载仓库的charts包[root@master ~]# helm repo update //更新一下chart存储库 查看一下 1[root@master ~]# helm search xgp-wsd","path":"posts/c224.html","date":"05-01","excerpt":"","tags":[]},{"title":"19 k8s的helm入门","text":"一、Helm介绍 helm是基于kubernetes 的包管理器。它之于 kubernetes 就如 yum 之于 centos，pip 之于 python，npm 之于 javascript 那 helm 的引入对于管理集群有哪些帮助呢？ 更方便地部署基础设施，如 gitlab，postgres，prometheus，grafana 等 更方便地部署自己的应用，为公司内部的项目配置 Chart，使用 helm 结合 CI，在 k8s 中部署应用一行命令般简单 1、Helm用途 Helm把Kubernetes资源(比如deployments、services或 ingress等) 打包到一个chart中，而chart被保存到chart仓库。通过chart仓库可用来存储和分享chart。Helm使发布可配置，支持发布应用配置的版本管理，简化了Kubernetes部署应用的版本控制、打包、发布、删除、更新等操作。 做为Kubernetes的一个包管理工具，用来管理charts——预先配置好的安装包资源，有点类似于Ubuntu的APT和CentOS中的yum。 Helm具有如下功能： 创建新的chart chart打包成tgz格式 上传chart到chart仓库或从仓库中下载chart 在Kubernetes集群中安装或卸载chart 管理用Helm安装的chart的发布周期 使用Helm可以完成以下事情： 管理Kubernetes manifest files 管理Helm安装包charts 基于chart的Kubernetes应用分发 2、Helm组件及相关术语 开始接触Helm时遇到的一个常见问题就是Helm中的一些概念和术语非常让人迷惑，我开始学习Helm就遇到这个问题。 因此我们先了解一下Helm的这些相关概念和术语。 包管理工具: Helm: Kubernetes的应用打包工具，也是命令行工具的名称。 Helm CLI：是 Helm 客户端，可以在本地执行 Tiller: Helm的服务端，部署在Kubernetes集群中，用于处理Helm的相关命令。 helm的作用：像centos7中的yum命令一样，管理软件包，只不过helm这儿管理的是在k8s上安装的各种容器。 tiller的作用：像centos7的软件仓库一样，简单说类似于/etc/yum.repos.d目录下的xxx.repo。 Repoistory: Helm的软件仓库，repository本质上是一个web服务器，该服务器保存了chart软件包以供下载，并有提供一个该repository的chart包的清单文件以供查询。在使用时，Helm可以对接多个不同的Repository。 Charts：是一个Helm的程序包，它包含了运行一个kubernetes应用程序所需要的镜像、依赖关系和资源定义等。 Release：应用程序运行Charts之后，得到的一个实例。 需要特别注意的是， Helm中提到的Release和我们通常概念中的版本有所不同，这里的Release可以理解为Helm使用Chart包部署的一个应用实例。 其实Helm中的Release叫做Deployment更合适。估计因为Deployment这个概念已经被Kubernetes使用了，因此Helm才采用了Release这个术语。 命令介绍 123456789101112131415161718[root@master ~]# helm search//查看可用的Charts包[root@master ~]# helm inspect stable/redis//查看stable/redis包的详细信息[root@master mysql]# helm fetch stable/mysql//直接下载stable/mysql的chart包[root@master ~]# helm install stable/redis -n redis --dry-run //基于stable/redis包运行一个名为redis的服务（把--dry-run去掉之后相当于安装了一个服务）[root@master ~]# helm list//查看安装的服务[root@master ~]# helm delete redis//删除这个服务[root@master mysql]# helm upgrade --set imageTag=5.7.15 xgp-mysql stable/mysql -f values.yaml //mysql服务的升级[root@master mysql]# helm history xgp-mysql//查看历史版本[root@master mysql]# helm rollback xgp-mysql 1 //回滚到版本一 3、组件架构 ![](E:\\软件\\博客\\Blog\\blog\\source_posts\\19 k8s的helm入门.assets\\image-20200302214010170.png) Helm Client 是用户命令行工具，其主要负责如下： 本地 chart 开发 仓库管理 与 Tiller sever 交互 发送预安装的 chart 查询 release 信息 要求升级或卸载已存在的 release Tiller Server是一个部署在Kubernetes集群内部的 server，其与 Helm client、Kubernetes API server 进行交互。Tiller server 主要负责如下： 监听来自 Helm client 的请求 通过 chart 及其配置构建一次发布 安装 chart 到Kubernetes集群，并跟踪随后的发布 通过与Kubernetes交互升级或卸载 chart 简单的说，client 管理 charts，而 server 管理发布 release helm客户端 helm客户端是一个命令行工具，负责管理charts、reprepository和release。它通过gPRC API（使用kubectl port-forward将tiller的端口映射到本地，然后再通过映射后的端口跟tiller通信）向tiller发送请求，并由tiller来管理对应的Kubernetes资源。 tiller服务端 tiller接收来自helm客户端的请求，并把相关资源的操作发送到Kubernetes，负责管理（安装、查询、升级或删除等）和跟踪Kubernetes资源。为了方便管理，tiller把release的相关信息保存在kubernetes的ConfigMap中。 tiller对外暴露gRPC API，供helm客户端调用。 4、工作原理 Chart Install 过程： Helm从指定的目录或者tgz文件中解析出Chart结构信息 Helm将指定的Chart结构和Values信息通过gRPC传递给Tiller Tiller根据Chart和Values生成一个Release Tiller将Release发送给Kubernetes运行。 Chart Update过程： Helm从指定的目录或者tgz文件中解析出Chart结构信息 Helm将要更新的Release的名称和Chart结构，Values信息传递给Tiller Tiller生成Release并更新指定名称的Release的History Tiller将Release发送给Kubernetes运行 Chart Rollback helm将会滚的release名称传递给tiller tiller根据release名称查找history tiller从history中获取到上一个release tiller将上一个release发送给kubernetes用于替换当前release Chart处理依赖 Tiller 在处理 Chart 时，直接将 Chart 以及其依赖的所有 Charts 合并为一个 Release，同时传递给 Kubernetes。因此 Tiller 并不负责管理依赖之间的启动顺序。Chart 中的应用需要能够自行处理依赖关系。 二、安装部署helm工具（客户端） 前提要求 Kubernetes1.5以上版本 集群可访问到的镜像仓库 执行helm命令的主机可以访问到kubernetes集群 （1）下载helm的包 12[root@master ~]#docker pull gcr.io/kubernetes-helm/tiller:v2.14.3[root@master ~]# wget https://get.helm.sh/helm-v2.14.3-linux-amd64.tar.gz （2）把helm包的命令，复制到本地 123456[root@master helm]# mv linux-amd64/helm /usr/local/bin///移动命令目录到/usr/local/bin/[root@master helm]# chmod +x /usr/local/bin/helm //给予执行权限[root@master helm]# helm help//验证是否安装成功 （3）设置命令自动补全 123[root@master helm]# echo 'source &lt;(helm completion bash)' &gt;&gt; /etc/profile[root@master helm]# . /etc/profile//刷新一下 2、安装Tiller server（服务端，需要创建授权用户） 12345678910111213141516171819[root@master ~]# vim tiller-rbac.yaml #创建授权用户apiVersion: v1kind: ServiceAccountmetadata: name: tiller namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: tillerroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects: - kind: ServiceAccount name: tiller namespace: kube-system 执行一下 1[root@master ~]# kubectl apply -f tiller-rbac.yaml （1）Tiller server的环境初始化 12[root@master helm]# helm init --service-account=tiller//helm的服务端就是Tiller（因为是访问外国的网站，可能需要多次执行） 查看一下 1[root@master helm]# kubectl get deployment. -n kube-system 现在发现没有开启，那是因为默认下载的Google的镜像，下载不下来 （2）设置镜像源改为阿里云的 1[root@master helm]# helm repo add stable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts 查看一下 1[root@master helm]# helm version 3、部署一个实例helm install + charts -n Release名称。 1、关于这个Release的描述。 2、关于这个Release资源的描述。 3、怎么使用这个Release。 （1）Helm部署安装一个Mysql服务。 12[root@master ~]# helm search mysql//查看关于mysqk的Charts包 12[root@master ~]# helm install stable/mysql -n mysql //基于stable/mysql包安装一个名为MySQL的服务 查看一下 1[root@master ~]# helm list （2）Charts包解压过后的目录: 123[root@master ~]# cd .helm/cache/archive//查看helm缓存[root@master archive]# ls 123456[root@master mysql]# helm fetch stable/mysql//直接下载stable/mysql的chart包[root@master archive]# tar -zxf mysql-0.3.5.tgz //解压一下MySQL包[root@master archive]# tree -C mysql //树状图查看解压出来的mysql目录，-C:显示颜色 Chart.yaml：这个chart包的概要信息。（name和version 这两是必填项，其他可选。） README md：是这个chart包的一个使用帮助文档。 templates：chart包内各种资源对象的模板。 deployment.yaml：deployment 控制器的 Go 模板文件 _helpers.tpl：以 _ 开头的文件不会部署到 k8s 上，可用于定制通用信息 NOTES.txt：Chart 部署到集群后的一些信息 service.yaml：service 的 Go 模板文件 values.yaml：是这个chart包的默认的值，可以被templet内的yaml文件使用。 （3）Helm部署安装-个Mysql服务。 123456[root@master ~]# docker pull mysql:5.7.14[root@master ~]# docker pull mysql:5.7.15[root@master ~]# docker pull busybox:1.25.0下载所需的mysql镜像[root@master ~]# helm delete mysql --purge //删除之前的MySQL服务并清除缓存 （4）设置共享目录 12345678910111213[root@master ~]# yum -y install rpcbind nfs-utils//安装nfs[root@master ~]# mkdir /data//创建共享目录[root@master ~]# vim /etc/exports/data *(rw,sync,no_root_squash)//设置共享目录权限[root@master ~]# systemctl restart rpcbind[root@master ~]# systemctl restart nfs-server//重启nfs服务测试一下[root@master ~]# showmount -e （5）创建pv 12345678910111213141516[root@master xgp]# vim nfs-pv1.yml apiVersion: v1kind: PersistentVolumemetadata: name: mysqlpvspec: capacity: storage: 8Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle nfs: path: /data/mysqlpv server: 192.168.1.21[root@master xgp]# mkdir /data/mysqlpv//创建所需目录 执行一下 1[root@master xgp]# kubectl apply -f nfs-pv1.yml 查看一下 1[root@master xgp]# kubectl get pv （6）创建一个mysql服务 1[root@master xgp]# helm install stable/mysql -n bdqn-mysql --set mysqlRootPassword=123.com 查看一下 1[root@master xgp]# kubectl get pod （7）进入pod并查看一下 1234567891011[root@master xgp]# kubectl exec -it bdqn-mysql-mysql-7b89c7b99-8ff2r -- mysql -u root -p123.commysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || sys |+--------------------+4 rows in set (0.01 sec) 4、mysql服务的升级与回滚 （1）mysql服务的升级 1[root@master mysql]# helm upgrade --set imageTag=5.7.15 bdqn-mysql stable/mysql -f values.yaml 查看一下 1[root@master mysql]# kubectl get deployments. -o wide （2）mysql服务的回滚 12[root@master mysql]# helm history bdqn-mysql//查看历史版本 回滚到版本一 1[root@master mysql]# helm rollback bdqn-mysql 1 查看一下 1[root@master mysql]# kubectl get deployments. -o wide 三、小实验 在部署mysql的时候，如何开启storageclass，以及如何将service资源对象的类型更改为NodePort, 如何使用? 将上述部署的实例进行升级回滚操作。升级的时候镜像改为： mysql:5.7.15版本。回滚到最初的版本。 1、基于NFS服务，创建NFS服务。 下载nfs所需安装包 1[root@node02 ~]# yum -y install nfs-utils rpcbind 创建共享目录 1[root@master ~]# mkdir -p /xgp/wsd 创建共享目录的权限 12[root@master ~]# vim /etc/exports/xgp *(rw,sync,no_root_squash) 开启nfs和rpcbind（三台都要） 12[root@master ~]# systemctl start nfs-server.service [root@master ~]# systemctl start rpcbind 测试一下 1[root@master ~]# showmount -e 2、创建StorageClass资源对象。 （1）创建rbac权限。 123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@master yaml]# vim rbac.yaml apiVersion: v1kind: ServiceAccountmetadata: name: nfs-provisioner namespace: default---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: nfs-provisioner-runner namespace: defaultrules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"watch\", \"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\", \"endpoints\"] verbs: [\"get\",\"create\",\"list\", \"watch\",\"update\"] - apiGroups: [\"extensions\"] resources: [\"podsecuritypolicies\"] resourceNames: [\"nfs-provisioner\"] verbs: [\"use\"]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: run-nfs-provisionersubjects: - kind: ServiceAccount name: nfs-provisioner namespace: default #必写字段roleRef: kind: ClusterRole name: nfs-provisioner-runner apiGroup: rbac.authorization.k8s.io 执行一下 1[root@master yaml]# kubectl apply -f rbac.yaml （2）创建Deployment资源对象，用Pod代替 真正的NFS服务。 123456789101112131415161718192021222324252627282930313233[root@master yaml]# vim nfs-deployment.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nfs-client-provisionerspec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-client-provisioner spec: serviceAccount: nfs-provisioner containers: - name: nfs-client-provisioner image: registry.cn-hangzhou.aliyuncs.com/open-ali/nfs-client-provisioner volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: xgp - name: NFS_SERVER value: 192.168.1.21 - name: NFS_PATH value: /xgp/wsd volumes: - name: nfs-client-root nfs: server: 192.168.1.21 path: /xgp/wsd 执行一下 1[root@master yaml]# kubectl apply -f nfs-deployment.yaml 查看一下 1[root@master yaml]# kubectl get pod （3）创建storageclass的yaml文件 1234567[root@master yaml]# vim xgp-storageclass.yaml apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: xgp-nfsprovisioner: xgp #通过provisioner字段关联到上述DeployreclaimPolicy: Retain 执行一下 1[root@master yaml]# kubectl apply -f test-storageclass.yaml 查看一下 1[root@master yaml]# kubectl get sc 3、创建一个mysql服务 1234567891011[root@master ~]# docker pull mysql:5.7.14[root@master ~]# docker pull mysql:5.7.15[root@master ~]# docker pull busybox:1.25.0//下载所需镜像[root@master yaml]# helm fetch stable/mysql//直接下载stable/mysql的chart包[root@master yaml]# tar -zxf mysql-0.3.5.tgz //解压mysql包[root@master yaml]# cd mysql/[root@master mysql]# vim values.yaml //修改values.yaml文件，添加storageClass存储卷和更改svc的模式为NodePort 12[root@master mysql]# helm install stable/mysql -n xgp-mysql --set mysqlRootPassword=123.com -f values.yaml //基于values.yaml和stable/mysql开启一个密码为123.com的mysqlpod 查看一下 1[root@master mysql]# kubectl get svc 1[root@master mysql]# kubectl get pod -o wide 4、进入pod并查看一下 1234567891011[root@master mysql]# kubectl exec -it xgp-mysql-mysql-67c6fb5f9-dn7s2 -- mysql -u root -p123.commysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || sys |+--------------------+4 rows in set (0.01 sec) 5、mysql服务的升级与回滚 （1）mysql服务的升级 1[root@master mysql]# helm upgrade --set imageTag=5.7.15 xgp-mysql stable/mysql -f values.yaml 查看一下 1[root@master mysql]# kubectl get deployments. -o wide （2）服务的回滚 12[root@master mysql]# helm history xgp-mysql//查看历史版本 回滚到版本一 1[root@master mysql]# helm rollback xgp-mysql 1 查看一下 1[root@master mysql]# kubectl get deployments. -o wide 6、进入pod并查看一下 1234567891011[root@master mysql]# kubectl exec -it xgp-mysql-mysql-67c6fb5f9-dn7s2 -- mysql -u root -p123.commysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || sys |+--------------------+4 rows in set (0.01 sec) 四、总结 Helm作为kubernetes应用的包管理以及部署工具，提供了应用打包，发布，版本管理以及部署，升级，回退等功能。Helm以Chart软件包的形式简化Kubernetes的应用管理，提高了对用户的友好性。 使用心得 helm 客户端的功能非常简单，直接参考官网文档即可。 列一下相关使用心得： Helm 的所有功能都是围绕着 chart、release 和 repository 的； 仅初始化客户端相关配置且仅建立本地仓库，可执行 helm init --client-only --skip-refresh； 查找 chart 的方式是通过 HELM_HOME（默认是 ~/.helm 目录）下的 repositories 目录进行的，几个重要文件或目录为 cache、repositories/cache； 修改 chart index.yaml 的 url，可执行 helm serve --url http://demo.com 来重新 reindex； 依赖关系管理，requirements定义，子 chart 值定义； install 、 update 的方式管理不方便，这样需要维护 chart 的版本关系，集成 install 和 update ，组成类似 k8s 中的 apply 命令； package 命令 -u 可以更新依赖，建议推到 repositiories 前先 package ，否则后期可能出现依赖检测不全的错误； release 相关的信息存储在 k8s 的 configmap 中，命名形式为 release_name.v1 的格式。 rollback 相关功能就是通过存储在 configmap 中的信息进行回滚的； Helm 客户端与 k8s 中的 TillerServer 是通过 k8s 提供的 port-forward 来实现的，而 port-forward 需要在指定节点上部署 socat； TillerServer 可以不部署在 k8s 中， 此时 Helm 客户端需要通过 HELM_HOST 环境变量来指定 TillerServer 的地址和端口； 建议 TillerServer 部署在 k8s 中，既然 Helm 为 CNCF 的一员，那么就尽量把云原生做到极致吧； 写 chart 时多参考官方最佳实践，The Chart Best Practices Guide； 不足 Helm 虽然提供了 install、update 命令来安装或更新对应的 release，但这给使用者带来了需要维护 release 状态的压力。举个例子，在还没安装 release 之前，release 是不存在的，update 操作是会失败的。反之已经存在的 release，install 操作也会失败。其实大部分情况下我是不需要知道 release 的状态的，不管它存在还是不存在，我执行的命令就是我希望的意图，我希望 release 能成为我执行命令后的状态。这一点上 k8s 的 apply 命令就非常好，不需要用户来维护资源的状态。","path":"posts/wqrw.html","date":"05-01","excerpt":"","tags":[]},{"title":"18 k8s的HPA自动扩容与缩容","text":"HPA 可以根据当前Pod资源的使用率，比如说CPU、磁盘、内存等进行副本Pod的动态的扩容与缩容。 前提条件:系统应该能否获取到当前Pod的资源使用情况 (意思是可以执行kubectl top pod命令,并且能够得到反馈信息)。 heapster：这个组件之前是集成在k8s集群的,不过在1.12版本之后被移除了。如果还想使用此功能，应该部署metricServer, 这个k8s集群资源使用情况的聚合器。 这里，我们使用一个测试镜像， 这个镜像基于php-apache制作的docker镜像，包含了一些可以运行cpu密集计算任务的代码。 1、创建一个deployment控制器 12345[root@master ~]#docker pull mirrorgooglecontainers/hpa-example:latest//下载hpa-example镜像[root@master ~]# kubectl run php-apache --image=mirrorgooglecontainers/hpa-example --requests=cpu=200m --expose --port=80//基于hpa-example镜像，运行一个deployment控制器，请求CPU的资源为200m，暴露一个80端口 查看一下 1[root@master ~]# kubectl get deployments. 2、创建HPA控制器 12[root@master ~]# kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10//当deployment资源对象的CPU使用率达到50%时，就进行扩容，最多可以扩容到10个 查看一下 1[root@master ~]# kubectl get hpa 3、测试（master开启三个端口） 新开启多个终端，对pod进行死循环请求php-apache的pod 端口一 （1）创建一个应用，用来不停的访问我们刚刚创建的php-apache的svc资源。 1[root@master ~]# kubectl run -i --tty load-generator --image=busybox /bin/sh （2）进入Pod内，执行以下这条命令.用来模拟访问php-apache的svc资源。 12[root@master ~]# while true; do wget -q -O- http://php-apache.default.svc.cluster.local ; done//不停地向php-apache的svc资源，发送ok 端口二 12[root@master ~]# kubectl get hpa -w//实时查看pod的cpu状态 ![image-20200228133816724](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\18 k8s的HPA自动容与缩容.assets\\image-20200228133816724.png) 可以看到php-apache的cpu使用情况已经超过了50% 端口三 12[root@master images]# kubectl get pod -w//实时查看pod的状态 ![image-20200228134105507](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\18 k8s的HPA自动容与缩容.assets\\image-20200228134105507.png) 可以看到当php-apache的cpu使用情况超过50%后，就会不断生成新的php-apache来进行负载均衡（目前设置的上线时10个），当然，如果cpu使用情况下降到50%，master就会陆续地删除php-apache，这样的使用可以减少不必要的资源浪费、资源分配不均等情况。 二、资源限制 1、基于Pod Kubernetes对资源的限制实际上是通过cgroup来控制的，cgroup 是容器的一组用来控制内核如何运行进程的相关属性集合。针对内存、CPU 和各种设备都有对应的cgroup 默认情况下，Pod运行没有CPU和内存的限额。这意味着系统中的任何 Pod将能够像执行该Pod所在的节点一样，消耗足够多的CPU和内存。一般会针对某些应用的pod资源进行资源限制，这个资源限制是通过 resources的requests和limits来实现 1[root@master ~]# vim cgroup-pod.yaml ![image-20200228153809932](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\18 k8s的HPA自动容与缩容.assets\\image-20200228153809932.png) requests: 要分配的资源，limits为最高请求的资源值。可以简单的理解为初始值和最大值。 2、基于名称空间 1） 计算资源配额 1[root@master ~]# vim compute-resources.yaml ![image-20200228153818288](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\18 k8s的HPA自动容与缩容.assets\\image-20200228153818288.png) 2）配置对象数量配额限制 1[root@master ~]# vim object-counts.yaml ![image-20200228153828002](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\18 k8s的HPA自动容与缩容.assets\\image-20200228153828002.png) 3） 配置CPU和内存的LimitRange 1[root@master ~]# vim limitRange.yaml ![image-20200228153834705](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\18 k8s的HPA自动容与缩容.assets\\image-20200228153834705.png) default 即 limit的值。 defaultRequest 即 request的值。","path":"posts/2643.html","date":"05-01","excerpt":"","tags":[]},{"title":"17 k8s的监控","text":"一、k8s的UI访问界面-dashboard 在dashboard中，虽然可以做到创建、删除、修改资源等操作，但通常情况下，我们会把它当做健康k8s集群的软件。 作为Kubernetes的Web用户界面，用户可以通过Dashboard在Kubernetes集群中部署容器化的应用，对应用进行问题处理和管理，并对集群本身进行管理。通过Dashboard，用户可以查看集群中应用的运行情况，同时也能够基于Dashboard创建或修改部署、任务、服务等Kubernetes的资源。通过部署向导，用户能够对部署进行扩缩容，进行滚动更新、重启Pod和部署新应用。当然，通过Dashboard也能够查看Kubernetes资源的状态。 1、Dashboard提供的功能 在默认情况下，Dashboard显示默认(default)命名空间下的对象，也可以通过命名空间选择器选择其他的命名空间。在Dashboard用户界面中能够显示集群大部分的对象类型。 1）集群管理 集群管理视图用于对节点、命名空间、持久化存储卷、角色和存储类进行管理。 节点视图显示CPU和内存的使用情况，以及此节点的创建时间和运行状态。 命名空间视图会显示集群中存在哪些命名空间，以及这些命名空间的运行状态。角色视图以列表形式展示集群中存在哪些角色，这些角色的类型和所在的命名空间。 持久化存储卷以列表的方式进行展示，可以看到每一个持久化存储卷的存储总量、访问模式、使用状态等信息；管理员也能够删除和编辑持久化存储卷的YAML文件。 2） 工作负载 工作负载视图显示部署、副本集、有状态副本集等所有的工作负载类型。在此视图中，各种工作负载会按照各自的类型进行组织。 工作负载的详细信息视图能够显示应用的详细信息和状态信息，以及对象之间的关系。 3） 服务发现和负载均衡 服务发现视图能够将集群内容的服务暴露给集群外的应用，集群内外的应用可以通过暴露的服务调用应用，外部的应用使用外部的端点，内部的应用使用内部端点。 4） 存储 存储视图显示被应用用来存储数据的持久化存储卷申明资源。 5） 配置 配置视图显示集群中应用运行时所使用配置信息，Kubernetes提供了配置字典（ConfigMaps）和秘密字典（Secrets），通过配置视图，能够编辑和管理配置对象，以及查看隐藏的敏感信息。 6） 日志视图 Pod列表和详细信息页面提供了查看日志视图的链接，通过日志视图不但能够查看Pod的日志信息，也能够查看Pod容器的日志信息。通过Dashboard能够根据向导创建和部署一个容器化的应用，当然也可以通过手工的方式输入指定应用信息，或者通过上传YAML和JSON文件来创建和不受应用。 2、下载所需yaml文件和镜像 12[root@master https]# wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-rc5/aio/deploy/recommended.yaml[root@master https]# docker pull kubernetesui/dashboard:v2.0.0-rc5 3、修改 recommended.yaml 12345678910111213141516[root@master https]#vim recommended.yaml ---kind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboardspec: type: NodePort #添加40 ports: - port: 443 targetPort: 8443 selector: k8s-app: kubernetes-dashboard 执行一下 1[root@master https]# kubectl apply -f recommended.yaml 查看一下 1[root@master https]# kubectl get svc -n kubernetes-dashboard 3、浏览器访问https://192.168.1.21:32306 PS:如果是使用的旧版本的dashboard, 使用谷歌浏览器登录，可能是不成功的，需要换成其他的浏览器，比如:火狐。 4、基于token的方法登录dashboard &lt;1&gt;创建一个dashboard的管理用户 1[root@master https]# kubectl create serviceaccount dashboard-admin -n kube-system &lt;2&gt;绑定用户为集群管理用户 1[root@master https]# kubectl create clusterrolebinding dashboard-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin &lt;3&gt;获取Token 12[root@master https]# kubectl get secrets -n kube-system | grep dashboard-admin//先得到Token的名称 12[root@master https]# kubectl describe secrets -n kube-system dashboard-admin-token-62bh9//查看上述得到的secret资源的详细信息，会得到token &lt;4&gt;在浏览器上使用token登录。 创建一个资源 查看是否创建成功 5、基于kubeconfig配置文件的方法登录dashboard &lt;1&gt;获取Token 12[root@master https]# kubectl get secrets -n kube-system | grep dashboard-admin//先得到Token的名称 12[root@master https]# kubectl describe secrets -n kube-system dashboard-admin-token-62bh9//查看上述得到的secret资源的详细信息，会得到token &lt;2&gt;生成kubeconfig配置文件。 设置一个环境变量代表获取的token 1[root@master https]# DASH_TOKEN=$(kubectl get secrets -n kube-system dashboard-admin-token-62bh9 -o jsonpath=&#123;.data.token&#125; | base64 -d) 将k8s集群的配置信息写入kubeconfig配置文件中。 1[root@master https]# kubectl config set-cluster kubernetes --server=192.168.1.21:6443 --kubeconfig=/root/.dashboard-admin.conf 1[root@master https]# kubectl config set-credentials dashboard-admin --token=$DASH_TOKEN --kubeconfig=/root/.dashboard-admin.conf 1[root@master https]# kubectl config set-context dashboard-admin@kubernetes --cluster=kubernetes --user=dashboard-admin --kubeconfig=/root/.dashboard-admin.conf 1[root@master https]# kubectl config use-context dashboard-admin@kubernetes --kubeconfig=/root/.dashboard-admin.conf &lt;3&gt;将生成的/root/.dashboard-admin.conf的配置文件，导出并做保存。 12[root@master https]# sz /root/.dashboard-admin.conf //导出到自己习惯的位置即可 &lt;4&gt;从浏览器选择kubeconfig的登录方式，然后导入配置文件即可。 二、部署weave-scope监控k8s集群 1、在github上查找scope的yaml文件 （1）github上搜索scope （2）进入k8s的部署scope的说明 （3）选择k8s的部署 （4）复制上面的链接，并下载yaml文件 1[root@master https]# wget https://cloud.weave.works/k8s/scope.yaml 2、修改下载的yaml文件并运行 123456789[root@master ~]# vim scope.yaml #编辑yaml文件#跳转至213行，修改其service的端口类型 spec: type: NodePort #修改类型为NodePort ports: - name: app port: 80 protocol: TCP targetPort: 4040 （1）执行一下 1[root@master https]# kubectl apply -f scope.yaml （2）查看容器的运行情况，确定处于正常运行 1[root@master https]# kubectl get pod -o wide -n weave 1[root@master https]# kubectl get svc -n weave #DaemonSet资源对象：weave-scope-agent（代理）：负责收集节点的信息； #deployment资源对象:weave-scope-app(应用)：从agent获取数据，通过web UI展示并与用户交互； #DaemonSet资源对象的特性和deployment相比，就是DaemonSet资源对象会在每个节点上都运行且只能运行一个pod。 #由于每个节点都需要监控，所以用到了DaemonSet这种资源对象 3、浏览器访问一下http://192.168.1.21:31841/ 在scope的web界面中，可以查看很多的东西，pod、node节点等详细信息，包括打开容器的终端，查看其日志信息等等 三、部署Prometheus服务 PS:在这里部署的prometheus,并不是Prometheus官网提供的，而是使用的coreos提供的prometheus项目。 在部署之前，先来了解一下Prometheus各个组件的作用吧！ MetricsServer: 是k8s集群资源使用情况的聚合器，收集数据给k8s集群内使用，如kubectl,hpa,scheduler等。 Prometheus Operator : 是一个系统检测和警报工具箱，用来存储监控数据。 Prometheus node-exporter ：收集k8s集群资源的数据，指定告警规则。 Prometheus ：收集apiserver，scheduler，controller-manager，kubelet组件的数据，通过http协议传输。 Grafana: 可视化数据统计和监控平台。 1、在github上搜索coreos/prometheus 复制链接 2、克隆github上的promethes项目 1234[root@master promethes]# yum -y install git//下载git命令[root@master promethes]# git clone https://github.com/coreos/kube-prometheus.git//克隆github上的项目 3、修改grafapa-service.yaml文件, 更改为nodePort的暴露方式，暴露端口为31001.。 1234567891011121314151617181920[root@master promethes]# cd kube-prometheus/manifests///进入kube-prometheus的manifests目录[root@master manifests]# vim grafana-service.yaml #修改grafana的yaml文件apiVersion: v1kind: Servicemetadata: labels: app: grafana name: grafana namespace: monitoringspec: type: NodePort #改为NodePort类型 ports: - name: http port: 3000 targetPort: http nodePort: 31001 #映射到宿主机31001端口 selector: app: grafana 3.修改prometheus-service.yaml文件， 更改为nodePort的暴露方式，暴露端口为31002. 1234567891011121314151617181920[root@master manifests]# vim prometheus-service.yaml #修改prometheus的yaml文件apiVersion: v1kind: Servicemetadata: labels: prometheus: k8s name: prometheus-k8s namespace: monitoringspec: type: NodePort #改为NodePort类型 ports: - name: web port: 9090 targetPort: web nodePort: 31002 #映射到宿主机31002端口 selector: app: prometheus prometheus: k8s sessionAffinity: ClientIP 4、修改alertmanager-service.yaml文件， 更改为nodePort的暴露方式，暴露端口为31003 1234567891011121314151617181920[root@master manifests]# vim alertmanager-service.yaml #修改alertmanager的yaml文件apiVersion: v1kind: Servicemetadata: labels: alertmanager: main name: alertmanager-main namespace: monitoringspec: type: NodePort #改为NodePort类型 ports: - name: web port: 9093 targetPort: web nodePort: 31003 #映射到宿主机31003端口 selector: alertmanager: main app: alertmanager sessionAffinity: ClientIP 5、将setup目录中所有的yaml文件,全部运行。是运行以上yaml文件的基础环境配置。 1234[root@master manifests]# cd setup///进入setup/目录[root@master manifests]# kubectl apply -f setup///运行setup目录中所有的yaml文件 6、将主目录(kube-prometheus)中所有的yaml文件,全部运行。 1234[root@master manifests]# cd ..//返回上一级目录（kube-prometheus）[root@master kube-prometheus]# kubectl apply -f manifests///运行kube-prometheus目录中所有的yaml文件 查看一下 1[root@master ~]# kubectl get pod -n monitoring 部署成功之后，可以运行一条命令， 查看资源使用情况(MetricsServer必须部署成功) 1[root@master images]# kubectl top node 7、浏览器访问一下http://192.168.1.21:31001 客户端访问群集中任意节点的IP+30100端口，即可看到以下界面（默认用户名和密码都是admin） 根据提示更改密码： （1）添加模板 依次点击“import”进行导入下面三个模板： （2）进行以下点击，即可查看群集内的监控状态 以下可看到监控状态 8、导入监控模板 从grafana的官网搜索https://grafana.com/ 复制以下这个模板的id 现在可以看到监控画面了","path":"posts/ewrt.html","date":"05-01","excerpt":"","tags":[]},{"title":"16 ingress资源的应用 ","text":"Ingress实现虚拟主机的方案 1、首先确定要运行ingress-nginx-controller服务。 在gitbub上找到所需的ingress的yaml文件 4. master下载 1[root@master ingress]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.29.0/deploy/static/mandatory.yaml 5. 修改 mandatory.yaml 文件 12[root@master ingress]# vim mandatory.yaml hostNetwork: true #213 （1）执行一下 1[root@master ingress]# kubectl apply -f mandatory.yaml （2）查看一下 1[root@master ingress]# kubectl get pod -n ingress-nginx 2、将ingress-nginx-controller暴露为一个Service资源对象。 1234567891011121314151617181920212223242526[root@master yaml]# vim service-nodeport.yaml apiVersion: v1kind: Servicemetadata: name: ingress-nginx namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxspec: type: NodePort ports: - name: http port: 80 targetPort: 80 protocol: TCP - name: https port: 443 targetPort: 443 protocol: TCP selector: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx--- （1）执行一下 1[root@master ingress]# kubectl apply -f service-nodeport.yaml （2）查看一下 1[root@master ingress]# kubectl get svc -n ingress-nginx 3、创建一个deployment资源，和一个service资源， 并相互关联。 123456789101112131415161718192021222324252627[root@master yaml]# vim deploy1.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: deploy1spec: replicas: 2 template: metadata: labels: app: nginx1 spec: containers: - name: nginx1 image: nginx---apiVersion: v1kind: Servicemetadata: name: svc-1spec: selector: app: nginx1 ports: - port: 80 targetPort: 80 执行一下 1[root@master yaml]# kubectl apply -f deploy1.yaml 查看一下 1[root@master yaml]# kubectl get pod 1[root@master yaml]# kubectl get svc 然后复制deploy1.yaml资源工创建另外”一对“服务。 123456789101112131415161718192021222324252627[root@master yaml]# vim deploy2.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: deploy2spec: replicas: 2 template: metadata: labels: app: nginx2 spec: containers: - name: nginx2 image: nginx---apiVersion: v1kind: Servicemetadata: name: svc-2spec: selector: app: nginx2 ports: - port: 80 targetPort: 80 执行一下 1[root@master yaml]# kubectl apply -f deploy2.yaml 查看一下 1[root@master yaml]# kubectl get deployments. 4. 创建ingress的yaml文件，关联是svc1和svc2 12345678910111213141516171819202122232425262728[root@master yaml]# vim ingress.yamlapiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-1spec: rules: - host: www1.bdqn.com http: paths: - path: / backend: serviceName: svc-1 servicePort: 80---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-2spec: rules: - host: www2.bdqn.com http: paths: - path: / backend: serviceName: svc-2 servicePort: 80 执行一下 1[root@master yaml]# kubectl apply -f ingress.yaml 查看一下 1[root@master yaml]# kubectl get ingresses. 1[root@master yaml]# kubectl describe ingresses. ingress-1 1[root@master yaml]# kubectl describe ingresses. ingress-2 5、由于实验环境限制，所以自己用来模拟-一个域名。 进入本机的 C:\\Windows\\System32\\drivers\\etc ， 修改hosts文件，添加Pod（ingress-controller）运行所在的节点IP。 访问一下 12[root@master yaml]# kubectl get svc -n ingress-nginx //查看映射的端口 http://www1.bdqn.com:30817/ http://www2.bdqn.com:30817/ 总结上述示例的pod是如何一步一步可以使client访问到的，总结如下： 后端pod===》service====》ingress规则====》写入Ingress-nginx-controller配置文件并自动重载使更改生效===》对本机进行域名解析====》实现client通过域名的IP+端口都可以访问到后端pod Ingress资源实现https代理安全访问。 在上面的操作中，实现了使用ingress-nginx为后端所有pod提供一个统一的入口，那么，有一个非常严肃的问题需要考虑，就是如何为我们的pod配置CA证书来实现HTTPS访问？在pod中直接配置CA么？那需要进行多少重复性的操作？而且，pod是随时可能被kubelet杀死再创建的。当然这些问题有很多解决方法，比如直接将CA配置到镜像中，但是这样又需要很多个CA证书。 这里有更简便的一种方法，就拿上面的情况来说，后端有多个pod，pod与service进行关联，service又被ingress规则发现并动态写入到ingress-nginx-controller容器中，然后又为ingress-nginx-controller创建了一个Service映射到群集节点上的端口，来供client来访问。 在上面的一系列流程中，关键的点就在于ingress规则，我们只需要在ingress的yaml文件中，为域名配置CA证书即可，只要可以通过HTTPS访问到域名，至于这个域名是怎么关联到后端提供服务的pod，这就是属于k8s群集内部的通信了，即便是使用http来通信，也无伤大雅。 1. 生成证书 12345[root@master yaml]# mkdir https//创建一个放置证书的目录[root@master yaml]# cd https/[root@master https]# openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \"/CN=testsvc /O=testsvc\"//生成证书 2. 创建secret资源， 保存证书。 1[root@master https]# kubectl create secret tls tls-secret --key=tls.key --cert tls.crt 3、创建一个deploy3.yaml文件，模拟一个web服务。 123456789101112131415161718192021222324252627[root@master yaml]# vim deploy3.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: deploy3spec: replicas: 2 template: metadata: labels: app: nginx3 spec: containers: - name: nginx3 image: nginx---apiVersion: v1kind: Servicemetadata: name: svc-3spec: selector: app: nginx3 ports: - port: 80 targetPort: 80 执行一下 1[root@master https]# kubectl apply -f deploy3.yaml 查看一下 1[root@master https]# kubectl get pod 1[root@master https]# kubectl get svc 4、创建对应的ingress规则。 12345678910111213141516171819[root@master https]# vim ingress.yamlapiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-3spec: tls: - hosts: - www3.bdqn.com #域名 secretName: tls-secret #保存的证书 rules: - host: www3.bdqn.com http: paths: - path: / backend: serviceName: svc-3 servicePort: 80 执行一下 1[root@master https]# kubectl apply -f ingress.yaml 查看一下 1[root@master https]# kubectl get ingresses. 5.查找对应service nodePort的443端口映射的端口，直接用浏览器访问即可。 进入本机的 C:\\Windows\\System32\\drivers\\etc ， 修改hosts文件，添加Pod（ingress-controller）运行所在的节点IP。 查看映射端口 1[root@master https]# kubectl get svc -n ingress-nginx https://www3.bdqn.com:31372/ k8s集群利用了“一切皆为资源”的原理，把生成的ca证书当成一个公共的资源来使用，使用时只需绑定保存的ca证书即可，不像之前一样，需要一个一个的创建ca证书，然后在关联起来，方便好用又快捷。","path":"posts/qwen.html","date":"05-01","excerpt":"","tags":[]},{"title":"15 Ingress-nginx","text":"Ingress-nginx 简单的理解: 原先暴露的service,现在给定个统一的访问入口。 1) 创建一个web服务，用deployment资源， 用httpd镜像，然后创建一个service资源与之关联。 1234567891011121314151617181920212223242526272829303132333435363738[root@master ingress]# vim deploy_1.yamlapiVersion: v1kind: Namespacemetadata: name: bdqn-ns labels: name: bdqn-ns---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: httpd-deploy namespace: bdqn-nsspec: replicas: 2 template: metadata: labels: app: bdqn-ns spec: containers: - name: httpd image: httpd---apiVersion: v1kind: Servicemetadata: name: httpd-svc namespace: bdqn-nsspec: type: NodePort selector: app: bdqn-ns ports: - name: http-port port: 80 targetPort: 80 nodePort: 31033 执行一下 1[root@master ingress]# kubectl apply -f deploy_1.yaml 查看一下 1[root@master ingress]# kubectl get svc -n bdqn-ns 1[root@master ingress]# kubectl get pod -n bdqn-ns 访问一下 2) 创建一个web服务，用deployment 资源，用tomcat:8.5.45镜像。 1234567891011121314151617181920212223242526272829303132[root@master ingress]# vim deploy_2.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: tomcat-deploy namespace: bdqn-nsspec: replicas: 2 template: metadata: labels: app: bdqn-tomcat spec: containers: - name: tomcat image: tomcat:8.5.45---apiVersion: v1kind: Servicemetadata: name: tomcat-svc namespace: bdqn-nsspec: type: NodePort selector: app: bdqn-tomcat ports: - name: tomcat-port port: 8080 targetPort: 8080 nodePort: 32033 执行一下 1[root@master ingress]# kubectl apply -f deploy_2.yaml 查看一下 1[root@master ingress]# kubectl get pod -n bdqn-ns 1[root@master ingress]# kubectl get svc -n bdqn-ns 访问一下 3) 在k8s集群前边部署一个反向代理服务器，这个服务器代理这k8s集群内部的service资源。 1. Ingress: （1）Ingress controller: 将新加入的Ingress转化为反向代理服务器的配置文件，并使之生效。(动态的感知k8s集群内Ingress资源的变化。） （2）Ingress : Ingress:将反向代理服务器的配置抽象成一个Ingress对象，每添加一个新的服务，只需要写一个新的Ingress的yaml文件即可。 2. Nginx :反向代理服务器。 ​ 需要解决了两个问题: ​ 1、动态的配置服务。 ​ 2、减少不必要的端口暴露。 ​ 基于nginx的ingress controller根据不同的开发公司，又分为两种: ​ 1、k8s社区版的: Ingerss - nginx. ​ 2、nginx公司自己开发的: nginx- ingress . 3. 在gitbub上找到所需的ingress的yaml文件 4. master下载 1[root@master ingress]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.29.0/deploy/static/mandatory.yaml 5. 修改 mandatory.yaml 文件 12[root@master ingress]# vim mandatory.yaml hostNetwork: true #213 ---------如果ingress-controller镜像下载不成功，可以直接使用下边的镜像。 docker pull registry.cn-hangzhou.aliyuncs.com/ilanni/nginx-ingress-controller:0.22.0 需要注意的是，如果使用上述镜像，需要将deployment资源指定的镜像名称进行修改。 修改的是madatory.yaml文件里的deployment资源。 在deployment资源中，如果添加了此字段，意味着Pod中运行的应用可以直接使用node节点的端口，这样node节 点主机所在网络的其他主机，就可以通过访问该端口访问此应用。(类似于docker映射到宿主机 上的端口。) （1）执行一下 1[root@master ingress]# kubectl apply -f mandatory.yaml （2）查看一下 1[root@master ingress]# kubectl get pod -n ingress-nginx 6. 创建一个service的yaml文件 （1）执行一下 1[root@master ingress]# kubectl apply -f mandatory.yaml （2）查看一下 1234567891011121314151617[root@master ingress]# vim mandatory-svc.yaml apiVersion: v1kind: Servicemetadata: name: ingress-nginx namespace: ingress-nginxspec: type: NodePort ports: - name: httpd port: 80 targetPort: 80 - name: https port: 443 selector: app: ingress-nginx （1）执行一下 1[root@master ingress]# kubectl apply -f mandatory-svc.yaml （2）查看一下 1[root@master ingress]# kubectl get svc -n ingress-nginx 4）创建Ingress资源。 ingress ： ingress-nginx-controller: 动态感知ingress 资源的变化 ingress: 创建svc与ingress-nginx-controller 关联的规则 （1）编写ingress的yaml文件 123456789101112131415161718192021[root@master yaml]# vim ingress.yaml apiVersion: extensions/v1beta1kind: Ingressmetadata: name: bdqn-ingress namespace: bdqn-ns annotations: nginx.ingress.kubernetes.io/rewrite-target: /spec: rules: #规则 - host: ingress.bdqn.com #域名 http: paths: - path: / backend: serviceName: httpd-svc #关联service servicePort: 80 #关联service的映射端口 - path: /tomcat backend: serviceName: tomcat-svc #关联service servicePort: 8080 #关联service的映射端口 执行一下 1[root@master yaml]# kubectl apply -f ingress.yaml 查看一下 1[root@master yaml]# kubectl get pod -n ingress-nginx -o wide ![image-20200221094602218](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\15 Ingress-nginx.assets\\image-20200221094602218.png) 1[root@master yaml]# kubectl get ingresses. -n bdqn-ns ![image-20200221092912191](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\15 Ingress-nginx.assets\\image-20200221092912191.png) 1[root@master yaml]# kubectl describe ingresses. -n bdqn-ns ![image-20200221093013134](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\15 Ingress-nginx.assets\\image-20200221093013134.png) 进入pod查看一下 12[root@master yaml]# kubectl exec -it -n ingress-nginx nginx-ingress-controller-5954d475b6-24k92 /bin/sh/etc/nginx $ cat nginx.conf ![image-20200221094404491](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\15 Ingress-nginx.assets\\image-20200221094404491.png) ![image-20200221094408211](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\15 Ingress-nginx.assets\\image-20200221094408211.png) （2）访问一下 进入本机的 C:\\Windows\\System32\\drivers\\etc ， 修改hosts文件，添加Pod（ingress-controller）运行所在的节点IP。 ![image-20200221103807318](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\15 Ingress-nginx.assets\\image-20200221103807318.png) 访问http://ingress.bdqn.com/ ![image-20200221095323635](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\15 Ingress-nginx.assets\\image-20200221095323635.png) 访问http://ingress.bdqn.com/tomcat ![image-20200221102354657](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\15 Ingress-nginx.assets\\image-20200221102354657.png) 5）为ingress-nginx创建一个service（使用官网的service文件就可以） ![image-20200221103351973](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\15 Ingress-nginx.assets\\image-20200221103351973.png) 复制上面的网址 12[root@master yaml]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.29.0/deploy/static/provider/baremetal/service-nodeport.yaml//下载文件到master节点 执行一下，下载的service文件 1[root@master yaml]# kubectl apply -f service-nodeport.yaml 查看一下 1[root@master yaml]# kubectl get service -n ingress-nginx ![image-20200221103644779](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\15 Ingress-nginx.assets\\image-20200221103644779.png) 访问一下 进入本机的 C:\\Windows\\System32\\drivers\\etc ， 修改hosts文件，添加Pod（ingress-controller）运行所在的节点IP。 ![image-20200221103835847](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\15 Ingress-nginx.assets\\image-20200221103835847.png) 访问http://ingress.bdqn.com:30817/ ![image-20200221103927247](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\15 Ingress-nginx.assets\\image-20200221103927247.png) 访问http://ingress.bdqn.com:30817/tomcat ![image-20200221103950477](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\15 Ingress-nginx.assets\\image-20200221103950477.png) Service -Nodeport:因为ingress - nginx - controller运行在了集群内的其中一个节点，为了保证即使这个节点宕机，我们对应的域名仍然能够正常访问服务，所以我们将ingress -nginx- controller也暴露为一个service资源。 练习: ​ 创建一个deploymen资源，基于nginx镜像，repolicas：2个.然后创建一个service资源关联这个deployment资源。最后创建一个ingress资源，将上述svc关联到ingress.bdqn.com/nginx 目录下。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455[root@master yaml]# vim lianxi.yamlapiVersion: v1kind: Namespacemetadata: name: xgp-666 labels: name: xgp-666---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: xgp namespace: xgp-666spec: replicas: 2 template: metadata: labels: app: xgp-nginx spec: containers: - name: xgp-nginx image: nginx---apiVersion: v1kind: Servicemetadata: name: xgp-svc namespace: xgp-666spec: type: NodePort selector: app: xgp-nginx ports: - name: xgp-port port: 80 targetPort: 80 nodePort: 30000---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: xgp-ingress namespace: xgp-666 annotations: nginx.ingress.kubernetes.io/rewrite-target: /spec: rules: - host: ingress.xgp.com http: paths: - path: / backend: serviceName: xgp-svc servicePort: 80 执行一下 1[root@master yaml]# kubectl apply -f lianxi.yaml 查看一下 1[root@master yaml]# kubectl describe ingresses. -n xgp-666 ![image-20200221112302483](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\15 Ingress-nginx.assets\\image-20200221112302483.png) 进入本机的 C:\\Windows\\System32\\drivers\\etc ， 修改hosts文件，添加Pod（ingress-controller）运行所在的节点IP。 添加完之后访问一下http://ingress.xgp.com/ ![image-20200221112416946](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\15 Ingress-nginx.assets\\image-20200221112416946.png)","path":"posts/dsfj.html","date":"05-01","excerpt":"","tags":[]},{"title":"14 k8s的Secret（密文）和configmap（明文）的使用教程","text":"一、Secret Secret :用来保存一些敏感信息，比如数据库的用户名密码或者秘钥。 举例:保存数据库的用户名和密码 用户名： root 密码： 123.com 1、通过–from-literal（文字的） 1[root@master secret]# kubectl create secret generic mysecret1 --from-literal=username=root --from-literal=password=123.com generic：通用的，一般的加密方式 查看一下 1[root@master secret]# kubectl get secrets 类型是Opaque（不透明的） 2、通过from-file（文件） 新建两个文件并分别写入用户名和密码 12[root@master secret]# echo root &gt; username[root@master secret]# echo 123.com &gt; password 创建一个secret 1[root@master secret]# kubectl create secret generic mysecret2 --from-file=username --from-file=password 查看一下 1[root@master secret]# kubectl get secrets 3、通过-- from- env-file: 创建一个文件写入用户名和密码 123[root@master secret]#vim env.txt username=rootpassword=123.com 创建一个secret 1[root@master secret]# kubectl create secret generic mysecret3 --from-env-file=env.txt 查看一下 1[root@master secret]# kubectl get secrets 4、通过yaml配置文件 （1）把需要保存的数据加密（”base64“的方式） 1234[root@master secret]# echo root | base64cm9vdAo=[root@master secret]# echo 123.com | base64MTIzLmNvbQo= 解码： 1234[root@master secret]# echo -n cm9vdAo | base64 --decode root[root@master secret]# echo -n MTIzLmNvbQo | base64 --decode 123.com （2）编写secre4的yaml文件 12345678[root@master secret]# vim secret4.yamlapiVersion: v1kind: Secretmetadata: name: mysecret4data: username: cm9vdAo= password: MTIzLmNvbQo= 执行一下 1[root@master secret]# kubectl apply -f secret4.yaml （3）查看一下 1[root@master secret]# kubectl get secrets 如果来使用Secret资源 1. 以Volume挂载的方式 编写pod的yaml文件 12345678910111213141516171819202122[root@master secret]# vim pod.yaml apiVersion: v1kind: Podmetadata: name: mypodspec: containers: - name: mypod image: busybox args: - /bin/sh - -c - sleep 300000 volumeMounts: - name: secret-test mountPath: \"/etc/secret-test\" #pod中的路径 readOnly: true #是否只读 volumes: - name: secret-test secret: secretName: mysecret1 还可以自定义存放数据的文件名 执行一下 1[root@master secret]# kubectl apply -f pod.yaml 进入容器查看保存的数据 1234[root@master secret]# kubectl exec -it mypod /bin/sh/ # cd /etc/secret-test//etc/secret-test # lspasword username 1234/etc/secret-test # cat username root/etc/secret-test # cat pasword 123.com 测试是否有只读权限 12123.com/etc/secret-test # echo admin &gt; username/bin/sh: can't create username: Read-only file system 1.1 自定义存放数据的文件名的yaml文件 1234567891011121314151617181920212223242526[root@master yaml]# vim pod.yaml apiVersion: v1kind: Podmetadata: name: mypodspec: containers: - name: mypod image: busybox args: - /bin/sh - -c - sleep 300000 volumeMounts: - name: secret-test mountPath: \"/etc/secret-test\" #pod中的路径 readOnly: true #是否只读 volumes: - name: secret-test secret: secretName: mysecret1 items: - key: username path: my-group/my-username #自定义的容器中的目录 - key: password path: my-group/my-password #自定义的容器中的目录 执行一下 1[root@master yaml]# kubectl apply -f pod.yaml 查看一下 123456[root@master secret]# kubectl exec -it mypod /bin/sh//进入容器查看 # cat /etc/secret-test/my-group/my-password 123.com # cat /etc/secret-test/my-group/my-username root 1.2 如果，现在将secret资源内保存的数据进行更新，请问，使用此数据的应用内，数据是是否也会更新? 会实时更新(这里引用数据，是以volumes挂 载使用数据的方式)。 更新mysecret1的数据: password —&gt; admin YWRtaW4K (base64) 可以通过edit 命令，直接修改。 1[root@master secret]# kubectl edit secrets mysecret1 查看一下 123456[root@master secret]# kubectl exec -it mypod /bin/sh//进入容器查看 # cat /etc/secret-test/my-group/my-password admin # cat /etc/secret-test/my-group/my-username root 数据已经成功更新了 2、以环境变量的方式 编写pod的yaml文件 123456789101112131415161718192021222324[root@master secret]# vim pod-env.yaml apiVersion: v1kind: Podmetadata: name: mypod2spec: containers: - name: mypod image: busybox args: - /bin/sh - -c - sleep 300000 env: - name: SECRET_USERNAME valueFrom: secretKeyRef: name: mysecret2 key: username - name: SECRET_PASSWORD valueFrom: secretKeyRef: name: mysecret2 key: password 执行一下 1[root@master secret]# kubectl apply -f pod-env.yaml 查看一下 1[root@master secret]# kubectl get pod 进入容器查看保存的数据 12345[root@master secret]# kubectl exec -it mypod2 /bin/sh/ # echo $SECRET_USERNAMEroot/ # echo $SECRET_PASSWORD123.com 2.1 更新sevret文件的内容 12[root@master yaml]# kubectl edit secrets mysecret2//修改保存文件的内容 查看一下 12345[root@master secret]# kubectl exec -it mypod2 /bin/sh/ # echo $SECRET_USERNAMEroot/ # echo $SECRET_PASSWORD123.com 等待了一定时间后，可以看到这个数据并没有没有改变 总结 如果引用secret数据的应用， 要求会随着secret资源对象内保存的数据的更新，而实时更新，那么应该使用volumes挂载的方式引用资源因为用环境变量的方式引用不会实时更新数据。 二、ConfigMap 和Secret资源类似，不同之处在于，secret 资源保存的是敏感信息，而Configmap保存的是以明文方式存放的数据。 username：adam age：18 创建的四种方式 1、通过-- from- literal(文字的): 1[root@master yaml]# kubectl create configmap myconfigmap1 --from-literal=username=adam --from-literal=age=18 查看一下 1[root@master yaml]# kubectl get cm 1[root@master yaml]# kubectl describe cm 2、通过–from-file (文件) : 12[root@master yaml]# echo adam &gt; username[root@master yaml]# echo 18 &gt; age 创建 1[root@master yaml]# kubectl create configmap myconfigmap2 --from-file=username --from-file=age 查看一下 1[root@master yaml]# kubectl describe cm 3、通过–from- env-file: 123[root@master yaml]# vim env.txt username=adamage=18 创建 1[root@master yaml]# kubectl create configmap myconfigmap3 --from-env-file=env.txt 查看一下 1[root@master configmap]# kubectl describe cm 4、通过yaml配置文件: 12345678[root@master yaml]# vim configmap.yamlapiVersion: v1kind: ConfigMapmetadata: name: myconfigmap4data: username: 'adam' age: '18' 创建 1[root@master yaml]# kubectl apply -f configmap.yaml 查看一下 1[root@master yaml]# kubectl describe cm 如何来使用configmap资源 1. 以Volume挂载的方式 123456789101112131415161718192021[root@master yaml]# vim v-pod.yaml apiVersion: v1kind: Podmetadata: name: pod1spec: containers: - name: mypod image: busybox args: - /bin/sh - -c - sleep 300000 volumeMounts: - name: cmp-test mountPath: \"/etc/cmp-test\" readOnly: true volumes: - name: cmp-test configMap: name: myconfigmap1 执行一下 1[root@master configmap]# kubectl apply -f v-pod.yaml 查看一下 123456[root@master configmap]# kubectl exec -it pod1 /bin/sh//进入容器查看一下 # cat /etc/cmp-test/age 18/ # cat /etc/cmp-test/username adam/ 1.1 自定义存放数据的文件名的yaml文件 1234567891011121314151617181920212223242526[root@master configmap]# vim v-pod2.yaml apiVersion: v1kind: Podmetadata: name: pod3spec: containers: - name: mypod image: busybox args: - /bin/sh - -c - sleep 300000 volumeMounts: - name: cmp-test mountPath: \"/etc/cmp-test\" readOnly: true volumes: - name: cmp-test configMap: name: myconfigmap1 items: - key: username path: my-group/my-username #自定义的容器中的目录 - key: age path: my-group/my-age #自定义的容器中的目录 执行一下 1[root@master configmap]# kubectl apply -f v-pod2.yaml 查看一下 123456[root@master configmap]# kubectl exec -it pod3 /bin/sh//进入容器查看# cat /etc/cmp-test/my-group/my-username adam/ # cat /etc/cmp-test/my-group/my-age 18/ 1.2 如果，现在将secret资源内保存的数据进行更新，请问，使用此数据的应用内，数据是是否也会更新? 1[root@master configmap]# kubectl edit cm myconfigmap1 查看一下 123456[root@master configmap]# kubectl exec -it pod3 /bin/sh//进入容器查看# cat /etc/cmp-test/my-group/my-username adam/ # cat /etc/cmp-test/my-group/my-age 10 可以看到更新成功 2.以环境变量的方式 123456789101112131415161718192021222324[root@master configmap]# vim e-pod.yaml apiVersion: v1kind: Podmetadata: name: pod2spec: containers: - name: mypod image: busybox args: - /bin/sh - -c - sleep 300000 env: - name: CONFIGMAP_NAME valueFrom: configMapKeyRef: name: myconfigmap2 key: username - name: CONFIGMAP_AGE valueFrom: configMapKeyRef: name: myconfigmap2 key: age 执行一下 1[root@master configmap]# kubectl apply -f e-pod.yaml 查看一下 123456[root@master configmap]# kubectl exec -it pod2 /bin/sh//进入容器查看一下 # echo $CONFIGMAP_NAMEadam # echo $CONFIGMAP_AGE18 2.1 更新sevret文件的内容 12[root@master configmap]# kubectl edit cm myconfigmap2 //修改保存文件的内容 查看一下 123456[root@master configmap]# kubectl exec -it pod2 /bin/sh//进入容器查看一下 # echo $CONFIGMAP_NAMEadam # echo $CONFIGMAP_AGE18 等待了一定时间后，可以看到这个数据并没有没有改变 可以看出这个configmap和secret的更新效果基本没有区别。 总结configmap、与secret资源有什么相同和不同之处。 Secret 与 ConfigMap 对比 相同点： key/value的形式 属于某个特定的namespace 可以导出到环境变量 可以通过目录/文件形式挂载 通过 volume 挂载的配置信息均可热更新 不同点： Secret 可以被 ServerAccount 关联 Secret 可以存储 docker register 的鉴权信息，用在 ImagePullSecret 参数中，用于拉取私有仓库的镜像 Secret 支持 Base64 加密 Secret 分为 kubernetes.io/service-account-token、kubernetes.io/dockerconfigjson、Opaque 三种类型，而 Configmap 不区分类型 总结以volumes挂载、和环境变量方式引用资源的相同和不同之处。 volumes挂载(可根据更改数据更新)：引用自己创建的secret（密文）或configmap（明文），挂载到容器中指定的目录下。查看保存的文件时，根据自己所填路径和secret或configmap创建的文件，进行查看。 环境变量(不因更改数据更新)：引用自己创建的secret（密文）或configmap（明文），挂载到容器中指定的目录下。查看保存的文件时，根据自己环境变量，进行查看。","path":"posts/a50d.html","date":"05-01","excerpt":"","tags":[]},{"title":"13 k8s的StateFulSet","text":"StatefulSet介绍 遇到的问题： 使用Deployment创建的Pod是无状态的，当挂在Volume之后，如果该Pod挂了，Replication Controller会再run一个来保证可用性，但是由于是无状态的，Pod挂了的时候与之前的Volume的关系就已经断开了，新起来的Pod无法找到之前的Pod。但是对于用户而言，他们对底层的Pod挂了没有感知，但是当Pod挂了之后就无法再使用之前挂载的磁盘了。 StatefulSet: 是一种给Pod提供唯一标志的控制器，它可以保证部署和扩展的顺序。 Pod一致性：包含次序（启动、停止次序）、网络一致性。此一致性与Pod相关，与被调度到哪个node节点无关。 稳定的次序：对于N个副本的StatefulSet，每个Pod都在[0，N)的范围内分配一个数字序号，且是唯一的。 稳定的网络：Pod的hostname模式为(statefulset名称)- (序号)。 稳定的存储：通过VolumeClaimTemplate为每个Pod创建一个PV。删除、减少副本，不会删除相关的卷。 (1) RC、 RS、Deployment、DS。-----&gt; 无状态服务 template(模板):根据模板 创建出来的Pod,它们J的状态都是一模一样的(除了名称，IP, 域名之外) 可以理解为:任何一个Pod, 都可以被删除，然后用新生成的Pod进行替换。 (2) 有状态的服务: 需要记录前一 次或者多次通信中的相关事件，以作为一下通信的分类标准。比如: mysql等数据库服务。(Pod的名称，不能随意变化。数据持久化的目录也是不一样，每一个Pod都有自己独有的数据持久化存储目录。) mysql:主从关系。 如果把之前无状态的服务比喻为牛、羊等牲畜，因为，这些到一定时候就可以出售。那么，有状态就比喻为:宠物，而宠物不像牲畜一样到达一定时候出售，人们往往会照顾宠物的一生。 (3) 每一个Pod----&gt;对应一个PVC----&gt;每一个PVC对应一个PV。 storageclass:自动创建PV 需要解决:自动创建PVC。 1，例子 （1）创建一个statefulset的yaml文件 12345678910111213141516171819202122232425262728293031323334[root@master yaml]# vim statefulset.yamlapiVersion: v1kind: Servicemetadata: name: headless-svc labels: app: headless-svcspec: ports: - port: 80 selector: app: headless-pod clusterIP: None #没有同一的ip---apiVersion: apps/v1kind: StatefulSetmetadata: name: statefulset-testspec: serviceName: headless-svc replicas: 3 selector: matchLabels: app: headless-pod template: metadata: labels: app: headless-pod spec: containers: - name: myhttpd image: httpd ports: - containerPort: 80 Deployment : Deploy+RS+随机字符串(Pod的名称。)没有顺序的，可 以没随意替代的。 1、headless-svc :无头服务。因为没有IP地址，所以它不具备负载均衡的功能了。因为statefulset要求Pod的名称是有顺序的，每一个Pod都不能被随意取代，也就是即使Pod重建之后，名称依然不变。为后端的每一个Pod去命名。 2、statefulSet:定义具体的应用 3、volumeClaimT emplates:自动创建PVC，为后端的Pod提供专有的存储。 执行一下 1[root@master yaml]# kubectl apply -f statefulset.yaml 查看一下 1[root@master yaml]# kubectl get svc 12[root@master yaml]# kubectl get pod//可看到这些pod是有顺序的 一、创建StorageClass资源对象。 1、基于NFS服务，创建NFS服务。 下载nfs所需安装包 1[root@node02 ~]# yum -y install nfs-utils rpcbind 创建共享目录 1[root@master ~]# mkdir /nfsdata 创建共享目录的权限 12[root@master ~]# vim /etc/exports/nfsdata *(rw,sync,no_root_squash) 开启nfs和rpcbind 12[root@master ~]# systemctl start nfs-server.service [root@master ~]# systemctl start rpcbind 测试一下 1[root@master ~]# showmount -e 2、创建rbac权限。 123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@master yaml]# vim rbac-rolebind.yaml apiVersion: v1kind: ServiceAccountmetadata: name: nfs-provisioner namespace: default---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: nfs-provisioner-runner namespace: defaultrules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"watch\", \"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\", \"endpoints\"] verbs: [\"get\",\"create\",\"list\", \"watch\",\"update\"] - apiGroups: [\"extensions\"] resources: [\"podsecuritypolicies\"] resourceNames: [\"nfs-provisioner\"] verbs: [\"use\"]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: run-nfs-provisionersubjects: - kind: ServiceAccount name: nfs-provisioner namespace: default #必写字段roleRef: kind: ClusterRole name: nfs-provisioner-runner apiGroup: rbac.authorization.k8s.io 执行一下 1[root@master yaml]# kubectl apply -f rbac-rolebind.yaml 3、创建Deployment资源对象，用Pod代替 真正的NFS服务。 123456789101112131415161718192021222324252627282930313233[root@master yaml]# vim nfs-deployment.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nfs-client-provisionerspec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-client-provisioner spec: serviceAccount: nfs-provisioner containers: - name: nfs-client-provisioner image: registry.cn-hangzhou.aliyuncs.com/open-ali/nfs-client-provisioner volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: bdqn - name: NFS_SERVER value: 192.168.1.21 - name: NFS_PATH value: /nfsdata volumes: - name: nfs-client-root nfs: server: 192.168.1.21 path: /nfsdata 执行一下 1[root@master yaml]# kubectl apply -f nfs-deployment.yaml 查看一下 1[root@master yaml]# kubectl get pod 4、创建storageclass的yaml文件 1234567[root@master yaml]# vim test-storageclass.yaml apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: stateful-nfsprovisioner: bdqn #通过provisioner字段关联到上述DeployreclaimPolicy: Retain 执行一下 1[root@master yaml]# kubectl apply -f test-storageclass.yaml 查看一下 1[root@master yaml]# kubectl get sc 二，解决自动创建pvc 1、创建statefulset的yaml文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[root@master yaml]# vim statefulset.yaml apiVersion: v1kind: Servicemetadata: name: headless-svc labels: app: headless-svcspec: ports: - port: 80 name: myweb selector: app: headless-pod clusterIP: None---apiVersion: apps/v1kind: StatefulSetmetadata: name: statefulset-testspec: serviceName: headless-svc replicas: 3 selector: matchLabels: app: headless-pod template: metadata: labels: app: headless-pod spec: containers: - image: httpd name: myhttpd ports: - containerPort: 80 name: httpd volumeMounts: - mountPath: /mnt name: test volumeClaimTemplates: #&gt; 自动创建PVC，为后端的Pod提供专有的存储。** - metadata: name: test annotations: #这是指定storageclass volume.beta.kubernetes.io/storage-class: stateful-nfs spec: accessModes: - ReadWriteOnce resources: requests: storage: 100Mi 在此示例中： 创建了一个名为 headless-svc 的 Service 对象，由 metadata: name 字段指示。该 Service 会定位一个名为 headless-svc 的应用，由 labels: app: headless-svc 和 selector: app: headless-pod 指示。该 Service 会公开端口 80 并将其命名为 web。而且该 Service 会控制网域并将互联网流量路由到 StatefulSet 部署的容器化应用。 使用三个副本 Pod (replicas: 3) 创建了一个名为 web 的 StatefulSet。 Pod 模板 (spec: template) 指示其 Pod 标记为 app: headless-pod。 Pod 规范 (template: spec) 指示 StatefulSet 的 Pod 运行一个容器 myhttpd，该容器运行版本为 httpd 映像。容器映像由 Container Registry 托管。 Pod 规范使用由 Service 打开的 web 端口。 template: spec: volumeMounts 指定一个名为 test 的 mountPath。mountPath 是容器中应装载存储卷的路径。 StatefulSet 预配了一个具有 100mb 预配存储空间的 PersistentVolumeClaim：test。 执行一下 1[root@master yaml]# kubectl apply -f statefulset.yaml 查看一下 1[root@master yaml]# kubectl get pod 如果第一个pod出现了问题，后面的pod就不会生成。 1[root@master yaml]# kubectl get statefulsets 2、 验证一下数据存储 容器中创建文件 1234[root@master yaml]# kubectl exec -it statefulset-test-0 /bin/sh# cd /mnt# touch testfile# exit 宿主机查看一下 12[root@master yaml]# ls /nfsdata/default-test-statefulset-test-0-pvc-bf1ae1d0-f496-4d69-b33b-39e8aa0a6e8d/testfile 三、小实验 以自己的名称创建一个名称空间，以下所有资源都运行在此空间中。用statefuset资源运行一个httpd web服务，要求3个Pod，但是每个Pod的主界面内容不一样，并且都要做专有的数据持久化，尝试删除其中一个Pod，查看新生成的Pod，总结对比与之前Deployment资源控制器控制的Pod有什么不同之处？ （一）创建StorageClass资源对象。 注意：nfs服务要开启 1、创建namespace的yaml文件 12345[root@master yaml]# vim namespace.yaml kind: NamespaceapiVersion: v1metadata: name: xgp-lll #namespave的名称 执行一下 1[root@master yaml]# kubectl apply -f namespace.yaml 查看一下 1[root@master yaml]# kubectl get namespaces 2. 创建rbac权限。 123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@master yaml]# vim rbac-rolebind.yamlapiVersion: v1kind: ServiceAccountmetadata: name: nfs-provisioner namespace: xgp-lll---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: nfs-provisioner-runner namespace: xgp-lllrules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"watch\", \"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\", \"endpoints\"] verbs: [\"get\",\"create\",\"list\", \"watch\",\"update\"] - apiGroups: [\"extensions\"] resources: [\"podsecuritypolicies\"] resourceNames: [\"nfs-provisioner\"] verbs: [\"use\"]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: run-nfs-provisionersubjects: - kind: ServiceAccount name: nfs-provisioner namespace: xgp-lllroleRef: kind: ClusterRole name: nfs-provisioner-runner apiGroup: rbac.authorization.k8s.io 执行一下 1[root@master yaml]# kubectl apply -f rbac-rolebind.yaml 3、创建Deployment资源对象，用Pod代替 真正的NFS服务。 1234567891011121314151617181920212223242526272829303132333435[root@master yaml]# vim nfs-deployment.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nfs-client-provisioner namespace: xgp-lllspec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-client-provisioner spec: serviceAccount: nfs-provisioner containers: - name: nfs-client-provisioner image: registry.cn-hangzhou.aliyuncs.com/open-ali/nfs-client-provisioner volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: xgp - name: NFS_SERVER value: 192.168.1.21 - name: NFS_PATH value: /nfsdata volumes: - name: nfs-client-root nfs: server: 192.168.1.21 path: /nfsdata 执行一下 1[root@master yaml]# kubectl apply -f nfs-deployment.yaml 查看一下 1[root@master yaml]# kubectl get pod -n xgp-lll 4、创建storageclass的yaml文件 12345678[root@master yaml]# vim test-storageclass.yaml apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: stateful-nfs namespace: xgp-lllprovisioner: xgp #通过provisioner字段关联到上述DeployreclaimPolicy: Retain 执行一下 1[root@master yaml]# kubectl apply -f test-storageclass.yaml 查看一下 1[root@master yaml]# kubectl get sc -n xgp-lll （二）解决自动创建pvc 1、创建statefulset的yaml文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051apiVersion: v1kind: Servicemetadata: name: headless-svc namespace: xgp-lll labels: app: headless-svcspec: ports: - port: 80 name: myweb selector: app: headless-pod clusterIP: None---apiVersion: apps/v1kind: StatefulSetmetadata: name: statefulset-test namespace: xgp-lllspec: serviceName: headless-svc replicas: 3 selector: matchLabels: app: headless-pod template: metadata: labels: app: headless-pod spec: containers: - image: httpd name: myhttpd ports: - containerPort: 80 name: httpd volumeMounts: - mountPath: /usr/local/apache2/htdocs name: test volumeClaimTemplates: #&gt; 自动创建PVC，为后端的Pod提供专有的存储。** - metadata: name: test annotations: #这是指定storageclass volume.beta.kubernetes.io/storage-class: stateful-nfs spec: accessModes: - ReadWriteOnce resources: requests: storage: 100Mi 执行一下 1[root@master yaml]# kubectl apply -f statefulset.yaml 查看一下 1[root@master yaml]# kubectl get pod -n xgp-lll 2、 验证一下数据存储 容器中创建文件 1234567891011第一个[root@master yaml]# kubectl exec -it -n xgp-lll statefulset-test-0 /bin/bash root@statefulset-test-0:/usr/local/apache2# echo 123 &gt; /usr/local/apache2/htdocs/index.html第二个[root@master yaml]# kubectl exec -it -n xgp-lll statefulset-test-1 /bin/bash root@statefulset-test-2:/usr/local/apache2# echo 456 &gt; /usr/local/apache2/htdocs/index.html第三个[root@master yaml]# kubectl exec -it -n xgp-lll statefulset-test-2 /bin/bash root@statefulset-test-1:/usr/local/apache2# echo 789 &gt; /usr/local/apache2/htdocs/index.html 宿主机查看一下 123456789101112第一个[root@master yaml]# cat /nfsdata/xgp-lll-test-statefulset-test-0-pvc-ccaa02df-4721-4453-a6ec-4f2c928221d7/index.html 123第二个[root@master yaml]# cat /nfsdata/xgp-lll-test-statefulset-test-1-pvc-88e60a58-97ea-4986-91d5-a3a6e907deac/index.html 456第三个[root@master yaml]# cat /nfsdata/xgp-lll-test-statefulset-test-2-pvc-4eb2bbe2-63d2-431a-ba3e-b7b8d7e068d3/index.html 789 访问一下 扩容、缩容:在此过程中，Pod的生成或删除操作也是有顺序性的。 升级操作 1kubectl explain sts.spec.updateStrategy.rollingUpdate.partition partition：如果partition后面的值等于N, N+的都会更新。默认值为0（所有都会更新）。","path":"posts/asdf.html","date":"05-01","excerpt":"","tags":[]},{"title":"12 k8s的存储类","text":"k8s有很多的服务，很多的资源对象。 如果要去创建服务，做数据持久化，需要预先知道可用PV有哪些? 如果为了这个服务去提前创建PV，那么我们还需要知道，这个服务，大概需要多大的空间? 一，Storage Class（存储类） 作用：它可以动态的自动的创建所需要的PV Provisioner（供给方，提供者）：及提供了存储资源的存储系统。k8s内建有多重供给方，这些供给方的名字都以“kubernetes.io”为前缀。并且还可以自定义。 Parameters（参数）：存储类使用参数描述要关联到的存储卷，注意不同的供给方参数也不同。 ReclaimPlicy: PV的回收策略，可用值有Delete(默认)和Retain （1）确定基于NFS服务来做的SC。NFS开启 1[root@master yaml]# showmount -e （2）需要RBAC权限。 RBAC：rbac是k8s的API的安全策略，是基于用户的访问权限的控制。规定了谁，可以有什么样的权限。 为了给SC资源操作k8s集群的权限。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[root@master yaml]# vim rbac-rolebind.yamlkind: NamespaceapiVersion: v1metadata: name: bdqn-test---apiVersion: v1kind: ServiceAccountmetadata: name: nfs-provisioner namespace: bdqn-test---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: nfs-provisioner-runner namespace: bdqn-testrules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"watch\", \"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\", \"endpoints\"] verbs: [\"get\",\"create\",\"list\", \"watch\",\"update\"] - apiGroups: [\"extensions\"] resources: [\"podsecuritypolicies\"] resourceNames: [\"nfs-provisioner\"] verbs: [\"use\"]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: run-nfs-provisionersubjects: - kind: ServiceAccount name: nfs-provisioner namespace: bdqn-testroleRef: kind: ClusterRole name: nfs-provisioner-runner apiGroup: rbac.authorization.k8s.io 运行一下 1[root@master yaml]# kubectl apply -f rbac-rolebind.yaml （3）nfs-deployment 作用：其实它是一个NFS客户端。但它通过K8S的内置的NFS驱动挂载远端的NFS服务器到本地目录；然后将自身作为storage provider，关联storage class。 1234567891011121314151617181920212223242526272829303132333435[root@master yaml]# vim nfs-deployment.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nfs-client-provisioner namespace: bdqn-testspec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-client-provisioner spec: serviceAccount: nfs-provisioner #指定账户 containers: - name: nfs-client-provisioner image: registry.cn-hangzhou.aliyuncs.com/open-ali/nfs-client-provisioner volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes #指定容器内的挂载目录 env: - name: PROVISIONER_NAME #这是这个容器内置的变量 value: bdqn-test #这是上面变量的值（名字） - name: NFS_SERVER #内置变量，用于指定nfs服务的IP value: 192.168.1.21 - name: NFS_PATH #内置变量，指定的是nfs共享的目录 value: /nfsdata volumes: #这下面是指定上面挂载到容器内的nfs的路径及IP - name: nfs-client-root nfs: server: 192.168.1.21 path: /nfsdata 执行一下 1[root@master yaml]# kubectl apply -f nfs-deployment.yaml （4）创建storageclass 123456789[root@master yaml]# vim test-storageclass.yamlapiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: stateful-nfs namespace: bdqn-testprovisioner: bdqn-test #这里要和第三个nfs-client-provisioner的env环境变量中的value值对应。reclaimPolicy: Retain #回收策略为：retain，还有一个默认的值为“default” 执行一下 1[root@master yaml]# kubectl apply -f test-storageclass.yaml （5）创建PVC 1234567891011121314[root@master yaml]# vim test-pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: test-claim namespace: bdqn-testspec: storageClassName: stateful-nfs #定义存储类的名字，要和SC的名字对应 accessModes: - ReadWriteMany #访问模式为RWM resources: requests: storage: 500Mi 执行一下 1[root@master yaml]# kubectl apply -f test-pvc.yaml 查看一下 1[root@master yaml]# kubectl get pvc （6）创建一个Pod 12345678910111213141516171819202122[root@master yaml]# vim test-pod.yamlkind: PodapiVersion: v1metadata: name: test-pod namespace: bdqn-testspec: containers: - name: test-pod image: busybox args: - /bin/sh - -c - sleep 30000 volumeMounts: - name: nfs-pvc mountPath: /test restartPolicy: OnFailure volumes: - name: nfs-pvc persistentVolumeClaim: claimName: test-claim #这的名字要和PVC的名字一致 执行一下 1[root@master yaml]# kubectl apply -f test-pod.yaml 查看一下 1[root@master yaml]# kubectl get pod -n bdqn-test （7）容器中添加内容，并查看挂载目录 进入容器修改页面内容 123456[root@master yaml]# kubectl exec -it test-pod -n bdqn-test /bin/sh/ # cd test//test # touch test-file/test # echo 123456 &gt; test-file /test # cat test-file 123456 查看挂载目录 123456[root@master yaml]# ls /nfsdata/bdqn-test-test-claim-pvc-79ddfcf1-65ae-455f-9e03-5bcfe6c6ce15web1web2[root@master yaml]# cat /nfsdata/bdqn-test-test-claim-pvc-79ddfcf1-65ae-455f-9e03-5bcfe6c6ce15/test-file 123456 二，如果，K8S集群中， 有很多类似的PV, PVC在去向PV申请空间的时候，不仅会考虑名称以及访问控制模式，还会考虑你申请空间的大小，会分配给你最合适大小的PV。 运行一个web服务，采用Deployment资源，基于nginx镜像，replicas为3个。数据持久化目录为nginx服务的主访问目录：/usr/share/nginx/html 创建一个PVC,与上述资源进行关联。 1. 基于nfs服务来做的PV和pvc 下载nfs所需安装包 1[root@node02 ~]# yum -y install nfs-utils rpcbind 创建共享目录 1[root@master ~]# mkdir /nfsdata 创建共享目录的权限 12[root@master ~]# vim /etc/exports/nfsdata *(rw,sync,no_root_squash) 开启nfs和rpcbind 12[root@master ~]# systemctl start nfs-server.service [root@master ~]# systemctl start rpcbind 测试一下 1[root@master ~]# showmount -e 2.先创建两个PV, web- pV1(1G) ,web-pv2 (2G) web1 12345678910111213141516[root@master yaml]# vim web.yaml apiVersion: v1kind: PersistentVolumemetadata: name: web-pvspec : capacity: storage: 1Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: nfs nfs: path: /nfsdata/web1 server: 192.168.1.21 web2 12345678910111213141516[root@master yaml]# vim web2.yaml apiVersion: v1kind: PersistentVolumemetadata: name: web-pv2spec : capacity : storage: 2Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: nfs nfs: path: /nfsdata/web2 server: 192.168.1.21 3.创建所需文件夹 12[root@master yaml]# mkdir /nfsdata/web1[root@master yaml]# mkdir /nfsdata/web2 4.执行一下web和web2 12[root@master yaml]# kubectl apply -f web.yaml [root@master yaml]# kubectl apply -f web2.yaml 5.查看一下 1[root@master yaml]# kubectl get pv 6.创建web的pvc的yaml文件 12345678910111213[root@master yaml]# vim web-pvc.yaml apiVersion: v1kind: PersistentVolumeClaimmetadata: name: web-pvcspec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: nfs ​ 执行一下 1[root@master yaml]# kubectl apply -f web-pvc.yaml ​ 查看一下 1[root@master yaml]# kubectl get pvc 系统会自动给pvc一个相近内存的pv，所以选择了1G的那个 7.创建pod的yaml文件 123456789101112131415161718192021222324[root@master yaml]# vim web-pod.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: web-podspec: selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - image: nginx name: nginx volumeMounts: - name: web-test mountPath: /usr/share/nginx/html volumes: - name: web-test persistentVolumeClaim: claimName: web-pvc 执行一下 1[root@master yaml]# kubectl apply -f web-pod.yaml 查看一下 1[root@master yaml]# kubectl get pod 8. 访问一下nginx的网页 查看一下nginx的ip 1[root@master yaml]# kubectl get pod -o wide 进入容器设置网页内容 12345root@master yaml]# kubectl exec -it web-pod-8686d9c594-qxhr9 /bin/bashroot@web-pod-8686d9c594-qxhr9:/# cd /usr/share/nginx/html/root@web-pod-8686d9c594-qxhr9:/usr/share/nginx/html# lsroot@web-pod-8686d9c594-qxhr9:/usr/share/nginx/html# echo 123456 &gt; index.htmlroot@web-pod-8686d9c594-qxhr9:/usr/share/nginx/html# exit 访问一下 1[root@master yaml]# curl 10.244.2.17 三，如果两个PV，大小一样，名称一样，访问控制模式不一样，PVC会关联哪一个? (验证PV和PVC 关联的时候，访问模式必须一样) 两个PV，大小一样，名称一样，访问控制模式不一样 &lt;1&gt;创建两个pv web1 123456789101112131415[root@master yaml]# vim web1.yaml apiVersion: v1kind: PersistentVolumemetadata: name: web-pvspec : capacity: storage: 1Gi accessModes: - ReadWriteOnce #能以读-写mount到单个的节点 persistentVolumeReclaimPolicy: Recycle storageClassName: nfs nfs: path: /nfsdata/web1 server: 192.168.1.21 web2 123456789101112131415[root@master yaml]# vim web2.yaml apiVersion: v1kind: PersistentVolumemetadata: name: web-pvspec : capacity: storage: 1Gi accessModes: - ReadWriteMany #能以读-写mount到多个的节点 persistentVolumeReclaimPolicy: Recycle storageClassName: nfs nfs: path: /nfsdata/web1 server: 192.168.1.21 创建所需文件 1[root@master yaml]# mkdir /nfsdata/web1 执行一下 12[root@master yaml]# kubectl apply -f web1.yaml [root@master yaml]# kubectl apply -f web2.yaml &lt;2&gt;创建pvc 123456789101112[root@master yaml]# vim web-pvc.yaml apiVersion: v1kind: PersistentVolumeClaimmetadata: name: web-pvcspec: accessModes: - ReadWriteMany #能以读-写mount到多个的节点 resources: requests: storage: 1Gi storageClassName: nfs 执行一下 1[root@master yaml]# kubectl apply -f web-pvc.yaml &lt;3&gt;查看一下 1[root@master yaml]# kubectl get pv 1[root@master yaml]# kubectl get pvc 现在可以看到pv和pvc关联成功，但是为什么只有一个pv呢？（pv挂载的目录要相同） 那是因为当创建了两个相同名字的pv时它并不会认为这是两个不同的pv，而会把他们当成是同一个pv，后创建的pv会刷新前面创建的pv。然后，当创建了pvc，并且pvc的访问模式和后面创建pv的访问模式一样，他们就会关联成功，反之不成功。（当然这些条件下还需要考虑，pv的内存） 三，小实验 （1）以自己的名称创建一个名称空间。以下所有资源都在此名称空间之下。 &lt;1&gt;编写namespace的yam文件 12345[root@master yaml]# vim namespace.yaml kind: NamespaceapiVersion: v1metadata: name: xgp-znb &lt;2&gt;执行一下 1[root@master yaml]# kubectl apply -f namespace.yaml &lt;3&gt;查看一下 1[root@master yaml]# kubectl get ns （2）设置rbac权限。 下载所需镜像 1docker pull registry.cn-hangzhou.aliyuncs.com/open-ali/nfs-client-provisioner &lt;1&gt;编写rbac的yam文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[root@master yaml]# vim rbac-rolebind.yamlkind: NamespaceapiVersion: v1metadata: name: xgp-znb---apiVersion: v1kind: ServiceAccountmetadata: name: nfs-provisioner namespace: xgp-znb---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: nfs-provisioner-runner namespace: xgp-znbrules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"watch\", \"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\", \"endpoints\"] verbs: [\"get\",\"create\",\"list\", \"watch\",\"update\"] - apiGroups: [\"extensions\"] resources: [\"podsecuritypolicies\"] resourceNames: [\"nfs-provisioner\"] verbs: [\"use\"]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: run-nfs-provisionersubjects: - kind: ServiceAccount name: nfs-provisioner namespace: xgp-znbroleRef: kind: ClusterRole name: nfs-provisioner-runner apiGroup: rbac.authorization.k8s.io &lt;2&gt;执行一下 1[root@master yaml]# kubectl apply -f rbac-rolebind.yaml （3）创建nfs-deployment.yaml &lt;1&gt;编写deployment的yam文件 1234567891011121314151617181920212223242526272829303132333435[root@master yaml]# vim nfs-deployment.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nfs-client-provisioner namespace: xgp-znbspec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-client-provisioner spec: serviceAccount: nfs-provisioner containers: - name: nfs-client-provisioner image: registry.cn-hangzhou.aliyuncs.com/open-ali/nfs-client-provisioner volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: xgp-znb - name: NFS_SERVER value: 192.168.1.21 - name: NFS_PATH value: /nfsdata volumes: - name: nfs-client-root nfs: server: 192.168.1.21 path: /nfsdata &lt;2&gt;执行一下 1[root@master yaml]# kubectl apply -f nfs-deployment.yaml （4）创建storageclass自动创建PV。 &lt;1&gt;编写storageclass的yam文件 1234567[root@master yaml]# vim storageclass.yamlapiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: test-scprovisioner: xgp-znb #通过provisioner字段关联到上述DeployreclaimPolicy: Retain &lt;2&gt;执行一下 1[root@master yaml]# kubectl apply -f storageclass.yaml （5）创建PVC &lt;1&gt;编写PVC的yaml文件 12345678910111213[root@master yaml]# vim pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: test-claim namespace: xgp-znbspec: storageClassName: test-sc accessModes: - ReadWriteMany resources: requests: storage: 500Mi &lt;2&gt;执行一下 1[root@master yaml]# kubectl apply -f pvc.yaml &lt;3&gt;查看一下 1[root@master yaml]# kubectl get pvc -n xgp-znb （6）创建一个Pod, 基于nginx运行一个web服务，使用Deployment资源对象，replicas=3.持久化存储目录为默认主目录 &lt;1&gt;编写deployment的yam文件 1234567891011121314151617181920212223242526[root@master yaml]# vim pod.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: web-pod namespace: xgp-znbspec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - image: nginx name: nginx volumeMounts: - name: web-test mountPath: /usr/share/nginx/html volumes: - name: web-test persistentVolumeClaim: claimName: test-claim &lt;2&gt;执行一下 1[root@master yaml]# kubectl apply -f pvc.yaml &lt;3&gt;查看一下 1[root@master yaml]# kubectl get pod -n xgp-znb （7）访问nginx页面 修改nginx主页 1234[root@master yaml]# kubectl exec -it web-pod-8cd956cc7-6szjb -n xgp-znb /bin/bash//进入容器之中root@web-pod-8cd956cc7-6szjb:/# echo xgp-znb &gt; /usr/share/nginx/html/index.html//添加自定义内容主机 访问一下 1[root@master yaml]# curl 10.244.2.18 四，五个可移植性建议 把你的 pvc，和 其它一系列配置放一起， 比如说deployment，configmap 不要把你的pv放在其它配置里， 因为用户可能没有权限创建pv 初始化pvc 模版的时候， 提供一个storageclass 在你的工具软件中，watch那些没有bound的pvc，并呈现给用户 集群启动的时候启用DefaultStorageClass， 但是不要指定某一类特定的class， 因为不同provisioner的class，参数很难一致 五，四个阶段(volumn phase) 1. 在PVC中绑定一个PV，可以根据下面几种条件组合选择 Access Modes， 按照访问模式选择pv Resources， 按照资源属性选择， 比如说请求存储大小为8个G的pv Selector， 按照pv的label选择 Class， 根据StorageClass的class名称选择, 通过annotation指定了Storage Class的名字, 来绑定特定类型的后端存储 2. 关于根据class过滤出pv的说明： 所有的 PVC 都可以在不使用 StorageClass 注解的情况下，直接使用某个动态存储。把一个StorageClass 对象标记为 “default” 就可以了。StorageClass 用注解http://storageclass.beta.kubernetes.io/is-default-class 就可以成为缺省存储。有了缺省的 StorageClass，用户创建 PVC 就不用 storage-class 的注解了，1.4 中新加入的DefaultStorageClass 准入控制器会自动把这个标注指向缺省存储类。PVC 指定特定storageClassName，如fast时， 绑定名称为fast的storageClassPVC中指定storageClassName为“”时， 绑定no class的pv（pv中无class annotation， 或者其值为“”）PVC不指定storageClassName时， DefaultStorageClass admission plugin 开启与否（在apiserver启动时可以指定）， 对default class的解析行为是不同的。当DefaultStorageClass admission plugin启用时， 针对没有storageClass annotation的pvc，DefaultStorageClass会分配一个默认的class， 这个默认的class需要用户指定，比如在创建storageclass对象时加入annotation,如 http://storageclass.beta.kubernetes.io/is-default-class: “true” 。如果有多个默认的class， 则pvc会被拒绝创建， 如果用户没有指定默认的class， 则这个DefaultStorageClass admission plugin不会起任何作用。 pvc会找那些no class的pv做绑定。当DefaultStorageClass admission plugin没有启用时， 针对没有storageClass annotation的pvc， 会绑定no class的pv（pv中无class annotation， 或者其值为“”）","path":"posts/sadd.html","date":"05-01","excerpt":"","tags":[]},{"title":"11 k8s持久化存储应用","text":"k8s存储: (持久化) docker容器是有生命周期的。 volume **1.emptyDir（空目录）：**类似docker 数据持久化的:docer manager volume 使用场景:在同一 个Pod里，不同的容器，共享数据卷。 如果容器被删除，数据仍然存在，如果Pod被 删除，数据也会被删除。 测试编写一个yaml文件 12345678910111213141516171819202122232425262728[root@master yaml]# vim emptyDir.yamlapiVersion: v1kind: Podmetadata: name: producer-consumerspec: containers: - image: busybox name: producer volumeMounts: - mountPath: /producer_dir name: shared-volume args: - /bin/sh - -c - echo \"hello k8s\" &gt; /producer_dir/hello; sleep 30000 - image: busybox name: consumer volumeMounts: - mountPath: /consumer_dir name: shared-volume args: - /bin/sh - -c - cat /consumer_dir/hello; sleep 30000 volumes: - name: shared-volume emptyDir: &#123;&#125; 执行一下 1[root@master yaml]# kubectl apply -f emptyDir.yaml 查看一下 1[root@master yaml]# kubectl get pod 查看日志 12[root@master yaml]# kubectl logs producer-consumer producer[root@master yaml]# kubectl logs producer-consumer consumer 查看挂载的目录 node节点查看容器名，并通过容器名查看挂载的目录 1[root@node01 shared-volume]# docker ps 1[root@node01 shared-volume]# docker inspect k8s_consumer_producer-consumer_default_9ec83f9e-e58b-4bf8-8e16-85b0f83febf9_0 进入挂载目录查看一下 2.hostPath Volume：类似docker 数据持久化的:bind mount 如果Pod被删除，数据会保留，相比较emptyDir要好一点。不过一旦host崩溃，hostPath也无法访问 了。 docker或者k8s集群本身的存储会采用hostPath这种方式。 3.Persistent Volume| PV(持久卷) 提前做好的，数据持久化的数据存放目录。 Psesistent Volume Claim| PVC( 持久卷使用声明|申请) （1）基于nfs服务来做的PV和pvc 下载nfs所需安装包 1[root@node02 ~]# yum -y install nfs-utils rpcbind 创建共享目录 1[root@master ~]# mkdir /nfsdata 创建共享目录的权限 12[root@master ~]# vim /etc/exports/nfsdata *(rw,sync,no_root_squash) 开启nfs和rpcbind 12[root@master ~]# systemctl start nfs-server.service [root@master ~]# systemctl start rpcbind 测试一下 1[root@master ~]# showmount -e &lt;1&gt;创建nfs-pv的yaml文件 12345678910111213141516[root@master yaml]# cd yaml/[root@master yaml]# vim nfs-pv.yamlapiVersion: v1kind: PersistentVolumemetadata: name: test-pvspec: capacity: #pv容量的大小 storage: 1Gi accessModes: #访问pv的模式 - ReadWriteOnce #能以读-写mount到单个的节点 persistentVolumeReclaimPolicy: Recycle storageClassName: nfs nfs: path: /nfsdata/pv1 server: 192.168.1.21 1234 accessModes:(PV支持的访问模式) - ReadWriteOnce: 能以读-写mount到单个的节点 - ReadWriteMany: 能以读-写mount到多个的节点。- ReadOnlyMnce: 能以只读的方式mount到多个节点。 1234persistentVolumeReclaimPolicy : (PV存储空间的回收策略是什么)trueRecycle: 自动清除数据。trueRetain: 需要管理员手动回收。trueDelete： 云存储专用。 &lt;2&gt;执行一下 1[root@master yaml]# kubectl apply -f nfs-pv.yaml &lt;3&gt;查看一下 1[root@master yaml]# kubectl get pv &lt;1&gt;创建nfs-pvc的yaml文件 12345678910111213[root@master yaml]# vim nfs-pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: test-pvcspec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: nfs &lt;2&gt;执行一下 1[root@master yaml]# kubectl apply -f nfs-pvc.yaml &lt;3&gt;查看一下 1[root@master yaml]# kubectl get pvc 1[root@master yaml]# kubectl get pv （2）创建一个pod资源 1234567891011121314151617181920[root@master yaml]# vim pod.yamlkind: PodapiVersion: v1metadata: name: test-podspec: containers: - name: pod1 image: busybox args: - /bin/sh - -c - sleep 30000 volumeMounts: - mountPath: \"/mydata\" name: mydata volumes: - name: mydata persistentVolumeClaim: claimName: test-pvc &lt;1&gt; 执行一下 1[root@master yaml]# kubectl apply -f pod.yaml &lt;2&gt;查看一下 1[root@master yaml]# kubectl get pod -o wide 可以看到现在没有开启成功 查看一下test-pod的信息看看是哪里的问题 1[root@master yaml]# kubectl describe pod test-pod 那是因为pv的本地挂载目录没有创建好 12[root@master yaml]# mkdir /nfsdata/pv1///要和nfs-pv.yaml的名字一样 重新创建一下pod 123[root@master yaml]# kubectl delete -f pod.yaml [root@master yaml]# kubectl apply -f pod.yaml [root@master yaml]# kubectl get pod -o wide （3）test-pod创建hello创建文件并添加内容 1[root@master yaml]# kubectl exec test-pod touch /mydata/hello 进入容器 123[root@master yaml]# kubectl exec -it test-pod /bin/sh/ # echo 123 &gt; /mydata/hello/ # exit 挂载目录查看一下 1[root@master yaml]# cat /nfsdata/pv1/hello 和刚刚的一样 （4）测试回收策略 删除pod和pvc，pv 123[root@master yaml]# kubectl delete pod test-pod [root@master yaml]# kubectl delete pvc test-pvc [root@master yaml]# kubectl delete pv test-pv 查看一下 1[root@master yaml]# kubectl get pv 1[root@master yaml]# cat /nfsdata/pv1/hello 文件已被回收 （5）修改pv的回收策略为手动 修改 123456789101112131415[root@master yaml]# vim nfs-pv.yaml apiVersion: v1kind: PersistentVolumemetadata: name: test-pvspec : capacity : storage: 1Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain #修改 storageClassName: nfs nfs: path: /nfsdata/pv1 server: 192.168.1.21 执行一下 1[root@master yaml]# kubectl apply -f nfs-pv.yaml 创建pod 1[root@master yaml]# kubectl apply -f pod.yaml 查看一下 1[root@master yaml]# kubectl describe pod test-pod 创建pvc 1[root@master yaml]# kubectl apply -f nfs-pvc.yaml 查看一下pod 1[root@master yaml]# kubectl get pod （6）test-pod创建hello创建文件并添加内容 1[root@master yaml]# kubectl exec test-pod touch /mydata/k8s 查看一下挂载目录 1[root@master yaml]# ls /nfsdata/pv1/ 删除pod和pvc，pv，再次查看挂载目录 123[root@master yaml]# kubectl delete pod test-pod [root@master yaml]# kubectl delete pvc test-pvc[root@master yaml]# kubectl delete pv test-pv 查看挂载目录 1[root@master yaml]# ls /nfsdata/pv1/ 内容还在 4.mysql对数据持久化的应用 最小化安装系统需要 1yum -y install mariadb （1）通过之前的yaml文件，创建pv和pvc 12[root@master yaml]# kubectl apply -f nfs-pv.yaml [root@master yaml]# kubectl apply -f nfs-pvc.yaml 查看一下 1[root@master yaml]# kubectl get pv 1[root@master yaml]# kubectl get pvc （2）编写一个mysql的yaml文件 123456789101112131415161718192021222324252627282930313233343536[root@master yaml]# vim mysql.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: test-mysqlspec: selector: matchLabels: #支持等值的标签 app: mysqlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: test-mysqlspec: selector: matchLabels: app: mysql template: metadata: labels: app: mysql spec: containers: - image: mysql:5.6 name: mysql env: - name: MYSQL_ROOT_PASSWORD value: 123.com volumeMounts: - name: mysql-storage mountPath: /var/lib/mysql volumes: - name: mysql-storage persistentVolumeClaim: claimName: test-pvc 执行一下 1[root@master yaml]# kubectl apply -f mysql.yaml 查看一下 1[root@master yaml]# kubectl get pod （3）进入mysql容器 1[root@master yaml]# kubectl exec -it test-mysql-569f8df4db-rkpwm -- mysql -u root -p123.com 创建数据库 1mysql&gt; create database yun33; 切换数据库 1mysql&gt; use yun33; 创建表 1mysql&gt; create table my_id( id int(4))； 在表中插入数据 1mysql&gt; insert my_id values(9527); 查看表 1mysql&gt; select * from my_id; （4）查看本地的挂载目录 1[root@master yaml]# ls /nfsdata/pv1/ 查看一下pod 1[root@master yaml]# kubectl get pod -o wide -w 挂起node01 （5）查看node02上面数据是否和刚才一样（验证数据的一致性） 进入数据库 1[root@master yaml]# kubectl exec -it test-mysql-569f8df4db-nsdnz -- mysql -u root -p123.com 查看数据库 1mysql&gt; show databases; 查看表 1mysql&gt; show tables; 1mysql&gt; select * from my_id; 可以看到数据还在 5. 总结 PV的访问控制类型 accessModes:(PV支持的访问模式) ReadWriteOnce: 能以读-写mount到单个的节点 ReadWriteMany: 能以读-写mount到多个的节点。 ReadOnlyOnce: 能以只读的方式mount到单个节点。 PV的空间回收策略 persistentVolumeReclaimPolicy : (PV存储空间的回收策略是什么) ​ Recycle: 自动清除数据。 ​ Retain: 需要管理员手动回收。 ​ Delete： 云存储专用。 PV和PVC相互关联 是通过accessModes和storageClassName模块关联的 Pod不断的重启: 1、swap,没有关闭，导致集群运行不正常。 2、内存不足，运行服务也会重后。 kubectl describe kubectl logs /var/ log/messages 查看该节点的kubelet的日志。","path":"posts/5849.html","date":"05-01","excerpt":"","tags":[]},{"title":"10 复习 ","text":"虚拟化 云计算的分类： 基础及服务：laas 平台及服务：paas 软件及服务：saas docker虚拟化的底层原理: Namespace + Cgroup Namespace六项隔离: IPC: 共享内存,消息列队 MNT: 挂载点 文件系统 NET: 网络栈 PID: 进程编号 USER: 用户 组 UTS: 主机名 域名 namespace 六项隔离 实现了容器与宿主机 容器与容器之间的隔离 Cgroup 四项作用： **1） 资源的限制：**cgroup可以对进程组使用的资源总额进行限制 **2） 优先级分配：**通过分配的cpu时间片数量以及硬盘IO带宽的大小，实际上相当于控制了进程运行的优先级别 3） 资源统计： group可以统计系统资源使用量，比如gpu使用时间，内存使用量等，用于按量计费。同时还支持挂起动能，也就是说通过cgroup把所有 资源限制起来,对资源都不能使用，注意着并不是说我们的程序不能使用了,知识不能使用资源，处于等待状态。 **4） 进程控制：**可以对进程组执行挂起、恢复等操作。 镜像是容器运行的核心，容器是镜像运行的后的实例。 DockerHub| registry ----&gt; pull image : save &gt; | load &lt; run ----&gt; Container ----&gt; commit* Dockerfile Docker 三剑客。 docker machine :自动化部署多台dockerHost 。 ​ Docker-compose: 它可以同时控制多个容器。 ​ yaml。 Docker Swarm： ​ 从单个的服务向集群的形势发展。 ​ 高可用、高性能、高并发 ：为了防止单点故障。 ​ Service：服务 ----&gt; 包括运行什么服务，需要多个 rep1icas（副本）, 外网如何访问。 k8s 关闭防火墙、禁用selinux、修改主机名并加入域名解析、关闭swap 、时间同步、免密登录、打开iptables桥接 对硬件的基本要求： CPU：2核 MEM：2G 主机名：master node01 node02 时间必须同步 kubctl：k8s客户端 kubeadm：工具 kubelet：客户端代理 组件： ​ 三层网络： DockerHost &gt; Pod &gt; Service ​ Deployment: Service: **master组件: ** kube- api( application interface) k8s的前端接口 **Scheduler[集群分发调度器]**负责决定将Pod放在哪个Node上运行。在调度时，会充分考虑集群的拓扑结构，当前各个节点的负载情况，以及应对高可用、性能、数据亲和性和需求。 Controller Manager[内部管理控制中心]：负责管理集群的各种资源，保证资源处于预期的状态。它由多种Controller组成，包括Replication Controller、Endpoints Controller、Namespace Controller、Serviceaccounts Controller等。 **Etcd：**负责保存k8s集群的配置信息和各种资源的状态信息。当数据发生变化时，etcd会快速的通知k8s相关组件。（第三方组件）它有可替换方案。Consul、zookeeper **Flanner：**是k8s集群网络，可以保证Pod的跨主机通信。也有替换方案。 Node组件： Kubelet[节点上的Pod管家]：它是Node的agent(代理)，当Scheduler确定某 个Node上运行Pod之后，会将Pod的具体配置信息发送给该节点的kubelet,kubelet会根据这些信息创建和运行容器，并向Master报告运行状态。 **kube-proxy[负载均衡、路由转发]:**负责将访问service的TCP/UDP数据流转发到后端的容器。如果有多个副本，kube-proxy会实现负载均衡。 yaml文件的一级字段: ​ VERSION: ​ KIND: ​ METADATA: ​ SPEC : 12345678910111213141516[root@master ~]# vim web.yamlkind: Deployment #资源对象是控制器apiVersion: extensions/v1beta1 #api的版本metadata: #描述kind（资源类型） name: web #定义控制器名称 namespace： #名称空间spec: replicas: 2 #副本数量 template: #模板 metadata: labels: #标签 app: web_server spec: containers: #指定容器 - name: nginx #容器名称 image: nginx #使用的镜像 ​ Deployment（控制器)： ​ **ReplicationController：**用来确保由其管控的Pod对象副本数量，能够满足用户期望，多则删除，少则通过模本创建 ​ **RS（RpelicaSet）:**RS也是用于保证与label selector匹配的pod数量维持在期望状态 ​ Service： ​ type：默认Cluster IP ​ NodePort： 30000-32767 ​ Deployment和Service关联：标签和标签选择器 ​ Namespace： ​ Pod：最小单位 ​ 镜像的下载策略： ​ **Always：**镜像标签为“laster”或镜像不存在时，总是从指定的仓库中获取镜像。 ​ **IfNotPresent：**仅当本地镜像不存在时才从目标仓库下载。 ​ **Never：**禁止从仓库中下载镜像，即只使用本地镜像。 ​ 默认的标签 为latest：always ​ Pod的重启策略： ​ Always：（默认情况下使用）但凡Pod对象终止就将其重启； ​ **OnFailure：**仅在Pod对象出现错误时才将其重启； ​ **Never：**从不重启； ​ Pod的健康检查: ​ Liveness: 探测失败重启pod ​ Readiness: 探测失败将pod设置为不可用 kubelet：控制pod DaemonSet :会在每一个节点都会运行，并且只运行一个Pod","path":"posts/1b18.html","date":"05-01","excerpt":"","tags":[]},{"title":"09 Job资源对象","text":"Job资源对象 **服务类的Pod容器：**RC、RS、DS、Deployment **工作类的Pod容器：**Job—&gt;执行一次，或者批量执行处理程序，完成之后退出容器。 注意： 如果容器内执行任务有误，会根据容器的重启策略操作容器，不过这里 的容器重启策略只能是: Never和 OnFailure。 概念 在有些场景下，是想要运行一些容器执行某种特定的任务，任务一旦执行完成，容器也就没有存在的必要了。在这种场景下，创建pod就显得不那么合适。于是就是了Job，Job指的就是那些一次性任务。通过Job运行一个容器，当其任务执行完以后，就自动退出，集群也不再重新将其唤醒。 从程序的运行形态上来区分，可以将Pod分为两类：长时运行服务（jboss、mysql等）和一次性任务（数据计算、测试）。RC创建的Pod都是长时运行的服务，Job多用于执行一次性任务、批处理工作等，执行完成后便会停止（status.phase变为Succeeded）。 一、kubernetes支持以下几种job 非并行job：通常创建一个pod直至其成功结束。 固定结束次数的job：设置spec.completions,创建多个pod，直到.spec.completions个pod成功结束。 带有工作队列的并行job：设置.spec.Parallelism但不设置.spec.completions,当所有pod结束并且至少一个成功时，job就认为是成功。 Job Controller Job Controller负责根据Job Spec创建pod，并持续监控pod的状态，直至其成功结束，如果失败，则根据restartPolicy（只支持OnFailure和Never，不支持Always）决定是否创建新的pod再次重试任务。 例子 （1）编写一个job的yaml文件 123456789101112131415[root@master yaml]# vim jop.yamlkind: JobapiVersion: batch/v1metadata: name: test-jobspec: template: metadata: name: test-job spec: containers: - name: hello image: busybox command: [\"echo\",\"hello k8s job!\"] restartPolicy: Never （2）执行一下 1[root@master yaml]# kubectl apply -f jop.yaml （3）查看一下 1[root@master yaml]# kubectl get pod 查看日志 1[root@master yaml]# kubectl logs test-job-gs45w 我们可以看到job与其他资源对象不同，仅执行一次性任务，默认pod借宿运行后job即结束，状态为Completed。 （4）修改一下jop的yaml文件，把echo命令换成乱码 123456789101112131415[root@master yaml]# vim jop.yamlkind: JobapiVersion: batch/v1metadata: name: test-jobspec: template: metadata: name: test-job spec: containers: - name: hello image: busybox command: [\"asdasxsddwefew\",\"hello k8s job!\"] #修改 restartPolicy: Never （5）先删除之前的pod 1[root@master yaml]# kubectl delete jobs.batch test-job （6）执行一下 1[root@master yaml]# kubectl apply -f jop.yaml （7）查看一下 1[root@master yaml]# kubectl get pod -w 它会一直创建pod直到完成命令。 （8）修改一下jop的yaml文件，修改重启策略 123456789101112131415[root@master yaml]# vim jop.yaml kind: JobapiVersion: batch/v1metadata: name: test-jobspec: template: metadata: name: test-job spec: containers: - name: hello image: busybox command: [\"asdasxsddwefew\",\"hello k8s job!\"] restartPolicy: OnFailure （9）先删除之前的pod 1[root@master yaml]# kubectl delete jobs.batch test-job （10）执行一下 1[root@master yaml]# kubectl apply -f jop.yaml （11）查看一下 1[root@master yaml]# kubectl get pod -w ![image-20200115092801882](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\09 Job资源对象.assets\\image-20200115092801882.png) 它会一直重启pod完成命令，直到重启到一定次数就会删除job。 二、提高Job的执行效率 1. 我们可以在Job.spec字段下加上parallelism选项。表示同时运行多少个Pod执行任务。 （1）编写一个job的yaml文件 12345678910111213141516[root@master yaml]# vim jop.yamlkind: JobapiVersion: batch/v1metadata: name: test-jobspec: parallelism: 2 #同时启用几个pod template: metadata: name: test-job spec: containers: - name: hello image: busybox command: [\"echo\",\"hello k8s job!\"] restartPolicy: OnFailure （3）执行一下 1[root@master yaml]# kubectl apply -f jop.yaml （4）查看一下 1[root@master yaml]# kubectl get pod ![image-20200115093854913](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\09 Job资源对象.assets\\image-20200115093854913.png) 查看日志 ![image-20200115094002236](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\09 Job资源对象.assets\\image-20200115094002236.png) 2. 我们可以在Job.spec字段下加上complations选项。表示总共需要完成Pod的数量 （1）编写一个job的yaml文件 1234567891011121314151617[root@master yaml]# vim jop.yamlkind: JobapiVersion: batch/v1metadata: name: test-jobspec: complations: 8 #运行pod的总数量8个 parallelism: 2 #同时运行2个pod template: metadata: name: test-job spec: containers: - name: hello image: busybox command: [\"echo\",\"hello k8s job!\"] restartPolicy: OnFailure job 字段解释： 标志Job结束需要成功运行的Pod个数，默认为1 parallelism：标志并行运行的Pod的个数，默认为1 activeDeadlineSeconds：标志失败Pod的重试最大时间，超过这个时间不会继续重试. （2）先删除之前的pod 1[root@master yaml]# kubectl delete jobs.batch test-job （3）执行一下 1[root@master yaml]# kubectl apply -f jop.yaml （4）查看一下 1[root@master yaml]# kubectl get pod ![image-20200115094519494](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\09 Job资源对象.assets\\image-20200115094519494.png) 可以看到pod是两个两个的启动的。 3. 如何定时执行Job （1）编写一个cronjob的yaml文件 12345678910111213141516[root@master yaml]# vim cronjop.yamlkind: CronJobapiVersion: batch/v1beta1metadata: name: hellospec: schedule: \"*/1 * * * *\" #限定时间 jobTemplate: spec: template: spec: containers: - name: hello image: busybox command: [\"echo\",\"hello\",\"cronjob\"] restartPolicy: OnFailure （2）先删除之前的pod 1[root@master yaml]# kubectl delete jobs.batch test-job （3）执行一下 1[root@master yaml]# kubectl apply -f jop.yaml （4）查看一下 1[root@master yaml]# kubectl get pod ![image-20200115095857428](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\09 Job资源对象.assets\\image-20200115095857428.png) 1[root@master yaml]# kubectl get cronjobs.batch ![image-20200115095920740](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\09 Job资源对象.assets\\image-20200115095920740.png) 此时查看Pod的状态，会发现，每分钟都会运行一个新的Pod来执行命令规定的任 务。 练习：规定2020.1.15.10.5分运行上面的crontab任务。 （1）编写一个cronjob的yaml文件 12345678910111213141516[root@master yaml]# vim cronjop.yamlkind: CronJobapiVersion: batch/v1beta1metadata: name: hellospec: schedule: \"5 10 15 1 *\" #限定时间 jobTemplate: spec: template: spec: containers: - name: hello image: busybox command: [\"echo\",\"hello\",\"cronjob\"] restartPolicy: OnFailure （2）先删除之前的pod 1[root@master yaml]# kubectl delete cronjobs.batch hello （3）执行一下 1[root@master yaml]# kubectl apply -f jop.yaml （4）查看一下 1[root@master yaml]# kubectl get pod ![image-20200115100855819](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\09 Job资源对象.assets\\image-20200115100855819.png) 这时会发现，如果规定具体时间，可能并不会执行任务。 （5）添加apiVersion库 123456[root@master yaml]# vim /etc/kubernetes/manifests/kube-apiserver.yaml spec: containers: - command: - kube-apiserver - --runtime-config=batch/v2alpha1=true #添加 ![image-20200115104218361](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\09 Job资源对象.assets\\image-20200115104218361.png) （6）重启kubelet 1[root@master yaml]# systemctl restart kubelet.service （7）查看api版本 1[root@master yaml]# kubectl api-versions ![image-20200115104521662](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\09 Job资源对象.assets\\image-20200115104521662.png) （8）编写一个cronjob的yaml文件 12345678910111213141516[root@master yaml]# vim cronjop.yamlkind: CronJobapiVersion: batch/v1beta1metadata: name: hellospec: schedule: \"47 10 15 1 *\" #限定时间 jobTemplate: spec: template: spec: containers: - name: hello image: busybox command: [\"echo\",\"hello\",\"cronjob\"] restartPolicy: OnFailure （9）执行一下 1[root@master yaml]# kubectl apply -f jop.yaml （4）查看一下 1[root@master yaml]# kubectl get pod -w ![image-20200115100855819](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\09 Job资源对象.assets\\image-20200115100855819.png) 注意：此时仍然不能正常运行指定时间的Job，这是因为K8s官方在cronjob这个资源对象的支持中还没有完善此功能，还待开发。 跟Job资源一样在cronjob.spec.jobTemplate.spec 下同样支持并发Job参数: parallelism，也支持完成Pod的总数参数: completionsr 总结 Job 作为 Kubernetes 中用于处理任务的资源，与其他的资源没有太多的区别，它也使用 Kubernetes 中常见的控制器模式，监听 Informer 中的事件并运行 syncHandler 同步任务 而 CronJob 由于其功能的特殊性，每隔 10s 会从 apiserver 中取出资源并进行检查是否应该触发调度创建新的资源，需要注意的是 CronJob 并不能保证在准确的目标时间执行，执行会有一定程度的滞后。 两个控制器的实现都比较清晰，只是边界条件比较多，分析其实现原理时一定要多注意。","path":"posts/e9be.html","date":"05-01","excerpt":"","tags":[]},{"title":"08 ReplicaSet、DaemonSet","text":"ReplicaSet简单介绍 1. RC：ReplicationController（老一代的pod控制器） 用来确保由其管控的Pod对象副本数量，能够满足用户期望，多则删除，少则通过模本创建 特点： ​ 确保Pod资源对象的数量精准。 ​ 确保pod健康运行。 ​ 弹性伸缩 同样，它也可以通过yaml或json格式的资源清单来创建。其中spec字段一般嵌套以下字段： ​ replicas：期望的Pod对象副本数量。 ​ selector：当前控制器匹配Pod对此项副本的标签选择器 ​ template：pod副本的模板 与RC相比而言，RS不仅支持基于等值的标签选择器，而且还支持基于集合的标签选择器。 2. 标签：解决同类型的资源对象，为了更好的管理，按照标签分组。 常用的标签分类： ​ release（版本）：stable（稳定版）、canary（金丝雀版本）、beta（测试版本） ​ environment（环境变量）：dev（开发）、qa（测试）、production（生产） ​ application（应用）：ui、as（application software应用软件）、pc、sc ​ tier（架构层级）：frontend（前端）、backend（后端）、cache（缓存） ​ partition（分区）：customerA（客户A）、customerB（客户B） ​ track（品控级别）：daily（每天）、weekly（每周） 标签要做到：见名知意。 3.测试 （1）编写一个pod的yaml文件 12345678910111213[root@master ~]# vim label.yaml kind: PodapiVersion: v1metadata: name: labels labels: env: qa tier: frontendspec: containers: - name: myapp image: httpd &lt;1&gt;执行一下 1[root@master ~]# kubectl apply -f label.yaml --record &lt;2&gt;查看一下 12[root@master ~]# kubectl get pod --show-labels //通过--show-labels显示资源对象的 12[root@master ~]# kubectl get po -L env,tier//显示某个键对应的值 12[root@master ~]# kubectl get po -l env,tier//通过-l 查看仅包含某个标签的资源。 （2）添加标签 12[root@master ~]# kubectl label pod labels app=pc//给pod资源添加标签 （3）修改标签 12[root@master ~]# kubectl label pod labels env=dev --overwrite//修改标签 12[root@master ~]# kubectl get pod -l tier --show-labels //查看标签 （4）编写一个service的yaml文件 1234567891011121314[root@master ~]# vim service.yamlkind: ServiceapiVersion: v1metadata: name: servicespec: type: NodePort selector: env: qa ports: - protocol: TCP port: 90 targetPort: 80 nodePort: 30123 &lt;1&gt;执行一下 1[root@master ~]# kubectl apply -f service.yaml &lt;2&gt;查看一下 1[root@master ~]# kubectl describe svc ![image-20200114101837151](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\08 ReplicaSet、DaemonSet.assets\\image-20200114101837151.png) &lt;3&gt;访问一下 1[root@master ~]# curl 127.0.0.1:30123 ![image-20200114101915248](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\08 ReplicaSet、DaemonSet.assets\\image-20200114101915248.png) 如果标签有多个，标签选择器选择其中一个，也可以关联成功。相反，如果选择器有多个，那么标签必须完全满足条件，才可以关联成功。 4. 标签选择器：标签的查询过滤条件。 基于等值关系的（equality-based）：“=”，“==”，“！ =”前面两个都是相等，最后一个是不等于。 基于集合关系（set-based）:in、notin、exists三种。选择器列表间为“逻辑与”关系，使用ln或者NotIn操作时，其valuas不强制要求为非空的字符串列表，而使用Exists或DostNotExist时，其values必须为空 使用标签选择器的逻辑： 同时指定的多个选择器之间的逻辑关系为“与”操作。 使用空值的标签选择器意味着每个资源对象都将把选中。 空的标签选择器无法选中任何资源。 （1）例子 ![image-20200114110334223](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\08 ReplicaSet、DaemonSet.assets\\image-20200114110334223.png) 编写一个selector的yaml’文件 1234567[root@master ~]# vim selector.yamlselector: matchLabels: app: nginx mathExpressions: - &#123;key: name,operator: In,values: [zhangsan,lisi]&#125; - &#123;key: age,operator: Exists,values:&#125; selector：当前控制器匹配Pod对此项副本的标签选择器 matchLabels: 指定键值对表示的标签选择器。 mathExpressions:：基于表达式来指定的标签选择器。 DaemonSet 它也是一种pod控制器。 RC，RS , deployment , daemonset.都是pod控制器。statfukSet，RBAC 1. 使用场景： 如果必须将pod运行在固定的某个或某几个节点，且要优先于其他的pod的启动。通常情况下，默认会将每一个节点都运行，并且只能运行一个pod。这种情况推荐使用DeamonSet资源对象。 监控程序； 日志收集程序； 集群存储程序； 12[root@master ~]# kubectl get ds -n kube-system //查看一下DaemonSet 2. DaemonSet 与 Deployment 的区别 Deployment 部署的副本 Pod 会分布在各个 Node 上，每个 Node 都可能运行好几个副本。 DaemonSet 的不同之处在于：每个 Node 上最多只能运行一个副本。 3. 运行一个web服务，在每一个节点运行一个pod。 123456789101112131415[root@master ~]# vim daemonset.yamlkind: DaemonSetapiVersion: extensions/v1beta1metadata: name: test-dsspec: template: metadata: labels: name: test-ds spec: containers: - name: test-ds image: httpd &lt;1&gt;执行一下 1[root@master ~]# kubectl apply -f daemonset.yaml &lt;2&gt;查看一下 1[root@master ~]# kubectl get ds ![image-20200114112936161](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\08 ReplicaSet、DaemonSet.assets\\image-20200114112936161.png) 总结 1）总结RC、RS、Deplyment、DaemonSet控制器的特点及使用场景。 &lt;1&gt;Replication Controller（RC） 介绍及使用场景 Replication Controller简称RC，RC是Kubernetes系统中的核心概念之一，简单来说，RC可以保证在任意时间运行Pod的副本数量，能够保证Pod总是可用的。如果实际Pod数量比指定的多那就结束掉多余的，如果实际数量比指定的少就新启动一些Pod，当Pod失败、被删除或者挂掉后，RC都会去自动创建新的Pod来保证副本数量，所以即使只有一个Pod，我们也应该使用RC来管理我们的Pod。 主要功能 确保pod数量：RC用来管理正常运行Pod数量，一个RC可以由一个或多个Pod组成，在RC被创建后，系统会根据定义好的副本数来创建Pod数量。在运行过程中，如果Pod数量小于定义的，就会重启停止的或重新分配Pod，反之则杀死多余的。 确保pod健康：当pod不健康，运行出错或者无法提供服务时，RC也会杀死不健康的pod，重新创建新的。 弹性伸缩 ：在业务高峰或者低峰期的时候，可以通过RC动态的调整pod的数量来提高资源的利用率。同时，配置相应的监控功能（Hroizontal Pod Autoscaler），会定时自动从监控平台获取RC关联pod的整体资源使用情况，做到自动伸缩。 滚动升级：滚动升级为一种平滑的升级方式，通过逐步替换的策略，保证整体系统的稳定，在初始化升级的时候就可以及时发现和解决问题，避免问题不断扩大。 &lt;2&gt;Replication Set（RS） 被认为 是“升级版”的RC。RS也是用于保证与label selector匹配的pod数量维持在期望状态。 实际上RS和RC的功能基本一致，目前唯一的一个区别就是RC只支持基于等式的selector（env=dev或app=nginx），但RS还支持基于集合的selector（version in (v1, v2)），这对复杂的运维管理就非常方便了。 kubectl命令行工具中关于RC的大部分命令同样适用于我们的RS资源对象。不过我们也很少会去单独使用RS，它主要被Deployment这个更加高层的资源对象使用，除非用户需要自定义升级功能或根本不需要升级Pod，在一般情况下，我们推荐使用Deployment而不直接使用Replica Set。 区别在于 1、RC只支持基于等式的selector（env=dev或environment!=qa），但RS还支持新的，基于集合的selector（version in (v1.0, v2.0)或env notin (dev, qa)），这对复杂的运维管理很方便。 2、升级方式 RS不能使用kubectlrolling-update进行升级 kubectl rolling-update专用于rc RS升级使用deployment或者kubectl replace命令 社区引入这一API的初衷是用于取代vl中的RC，也就是说当v1版本被废弃时，RC就完成了它的历史使命，而由RS来接管其工作 &lt;3&gt;DaemonSet 1. 特点： 如果必须将pod运行在固定的某个或某几个节点，且要优先于其他的pod的启动。通常情况下，默认会将每一个节点都运行，并且只能运行一个pod。这种情况推荐使用DeamonSet资源对象。 一个DaemonSet对象能确保其创建的Pod在集群中的每一台（或指定）Node上都运行一个副本。如果集群中动态加入了新的Node，DaemonSet中的Pod也会被添加在新加入Node上运行。删除一个DaemonSet也会级联删除所有其创建的Pod。 2. 使用环境 监控程序； 日志收集程序； 集群存储程序； &lt;4&gt;Deployment 1. 什么是Deployment Kubernetes Deployment提供了官方的用于更新Pod和Replica Set（下一代的Replication Controller）的方法，您可以在Deployment对象中只描述您所期望的理想状态（预期的运行状态），Deployment控制器为您将现在的实际状态转换成您期望的状态，例如，您想将所有的webapp:v1.0.9升级成webapp:v1.1.0，您只需创建一个Deployment，Kubernetes会按照Deployment自动进行升级。现在，您可以通过Deployment来创建新的资源（pod，rs，rc），替换已经存在的资源等。 你只需要在Deployment中描述你想要的目标状态是什么，Deployment controller就会帮你将Pod和Replica Set的实际状态改变到你的目标状态。你可以定义一个全新的Deployment，也可以创建一个新的替换旧的Deployment。 2. 典型的用例 使用Deployment来创建ReplicaSet。ReplicaSet在后台创建pod。检查启动状态，看它是成功还是失败。 然后，通过更新Deployment的PodTemplateSpec字段来声明Pod的新状态。这会创建一个新的ReplicaSet，Deployment会按照控制的速率将pod从旧的ReplicaSet移动到新的ReplicaSet中。 如果当前状态不稳定，回滚到之前的Deployment revision。每次回滚都会更新Deployment的revision。 扩容Deployment以满足更高的负载。 暂停Deployment来应用PodTemplateSpec的多个修复，然后恢复上线。 根据Deployment 的状态判断上线是否hang住了。 清除旧的不必要的ReplicaSet。 3. 使用环境 Deployment集成了上线部署、滚动升级、创建副本、暂停上线任务，恢复上线任务，回滚到以前某一版本（成功/稳定）的Deployment等功能，在某种程度上，Deployment可以帮我们实现无人值守的上线，大大降低我们的上线过程的复杂沟通、操作风险。 定义Deployment来创建Pod和ReplicaSet 滚动升级和回滚应用 扩容和缩容 暂停和继续Deployment 3. DaemonSet 与 Deployment 的区别 Deployment 部署的副本 Pod 会分布在各个 Node 上，每个 Node 都可能运行好几个副本。 DaemonSet 的不同之处在于：每个 Node 上最多只能运行一个副本。 2）使用DaemonSet控制器运行httpd服务，要求名称以自己的名称命名。标签为：tier=backend,env=dev. 123456789101112131415[root@master ~]# vim daemonset.yaml kind: DaemonSetapiVersion: extensions/v1beta1metadata: name: xgp-dsspec: template: metadata: labels: tier: backend env: dev spec: containers: - name: xgp-ds image: httpd 查看一下 1[root@master ~]# kubectl get pod --show-labels 1[root@master ~]# kubectl get pod -L env,tier ![image-20200114095943595](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\08 ReplicaSet、DaemonSet.assets\\image-20200114095943595.png) 3) 创建service资源对象与上述资源进行关联，要有验证。 1234567891011121314[root@master ~]# vim service.yaml kind: ServiceapiVersion: v1metadata: name: servicespec: type: NodePort selector: env: dev ports: - protocol: TCP port: 90 targetPort: 80 nodePort: 30123 执行一下 1[root@master ~]# kubectl apply -f service.yaml 查看一下 1[root@master ~]# kubectl describe svc ![image-20200114120345596](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\08 ReplicaSet、DaemonSet.assets\\image-20200114120345596.png) 访问一下 1[root@master ~]# curl 127.0.0.1:30123 ![image-20200114120444524](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\08 ReplicaSet、DaemonSet.assets\\image-20200114120444524.png) 4）整理关于标签和标签选择器都有什么作用？ &lt;1&gt;标签：解决同类型的资源对象，为了更好的管理，按照标签分组。 &lt;2&gt;标签选择器：标签的查询过滤条件。","path":"posts/7772.html","date":"05-01","excerpt":"","tags":[]},{"title":"07 pod健康检查","text":"pod健康检查 一、Pod的liveness和readiness探针 Kubelet使用liveness probe（存活探针）来确定何时重启容器。例如，当应用程序处于运行状态但无法做进一步操作，liveness探针将捕获到deadlock，重启处于该状态下的容器，使应用程序在存在bug的情况下依然能够继续运行下去 Kubelet使用readiness probe（就绪探针）来确定容器是否已经就绪可以接受流量。只有当Pod中的容器都处于就绪状态时kubelet才会认定该Pod处于就绪状态。该信号的作用是控制哪些Pod应该作为service的后端。如果Pod处于非就绪状态，那么它们将会被从service的load balancer中移除。 Probe支持以下三种检查方法： &lt;1&gt;exec-命令 在用户容器内执行一次命令，如果命令执行的退出码为0，则认为应用程序正常运行，其他任务应用程序运行不正常。 12345livenessProbe: exec: command: - cat - /home/laizy/test/hostpath/healthy &lt;2&gt;TCPSocket 将会尝试打开一个用户容器的Socket连接（就是IP地址：端口）。如果能够建立这条连接，则认为应用程序正常运行，否则认为应用程序运行不正常。 123livenessProbe:tcpSocket: port: 8080 &lt;3&gt;HTTPGet 调用容器内Web应用的web hook，如果返回的HTTP状态码在200和399之间，则认为应用程序正常运行，否则认为应用程序运行不正常。每进行一次HTTP健康检查都会访问一次指定的URL。 123456httpGet: #通过httpget检查健康，返回200-399之间，则认为容器正常 path: / #URI地址 port: 80 #端口号 #host: 127.0.0.1 #主机地址 scheme: HTTP #支持的协议，http或者httpshttpHeaders：’’ #自定义请求的header 参数说明 **initialDelaySeconds：**容器启动后第一次执行探测是需要等待多少秒。 **periodSeconds：**执行探测的频率。默认是10秒，最小1秒。 ** timeoutSeconds：**探测超时时间。默认1秒，最小1秒。 **successThreshold：**探测失败后，最少连续探测成功多少次才被认定为成功。默认是1。对于liveness必须是1。最小值是1。 探针探测的结果有以下三者之一： Success：Container通过了检查。 Failure：Container未通过检查。 Unknown：未能执行检查，因此不采取任何措施。 1. LivenessProbe（活跃度） （1）编写一个livenss的yaml文件 1234567891011121314151617181920212223[root@node02 ~]# vim livenss.yamlkind: PodapiVersion: v1metadata: name: liveness labels: test: livenessspec: restartPolicy: OnFailure containers: - name: liveness image: busybox args: - /bin/sh - -c - touch /tmp/test; sleep 60; rm -rf /tmp/test; sleep 300 livenessProbe: #存活探测 exec: #通过执行命令来检查服务是否正常 command: #命令模式 - cat - /tmp/test initialDelaySeconds: 10 #pod运行10秒后开始探测 periodSeconds: 5 #检查的频率，每5秒探测一次 （2）运行一下 1[root@master ~]# kubectl apply -f liveness.yaml （3）查看一下 1[root@master ~]# kubectl get pod -w Liveness活跃度探测，根据探测某个文件是否存在，来确定某个服务是否正常运行，如果存在则正常，负责，它会根据你设置的Pod的重启策略操作Pod。 2. Readiness（敏感探测、就绪性探测） （1）编写一个readiness的yaml文件 1234567891011121314151617181920212223[root@master ~]# vim readiness.yaml kind: PodapiVersion: v1metadata: name: readiness labels: test: readinessspec: restartPolicy: Never containers: - name: readiness image: busybox args: - /bin/sh - -c - touch /tmp/test; sleep 60; rm -rf /tmp/test; sleep 300 readinessProbe: exec: command: - cat - /tmp/test initialDelaySeconds: 10 periodSeconds: 5 （2）运行一下 1[root@master ~]# kubectl apply -f readiness.yaml （3）查看一下 1[root@master ~]# kubectl get pod -w ![image-20200113095301156](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\07 pod健康检查.assets\\image-20200113095301156.png) 3. 总结liveness和readiness探测 （1）liveness和readiness是两种健康检查机制，k8s将两种探测采取相同的默认行为，即通过判断容器启动进程的返回值是否为零，来判断探测是否成功。 （2）两种探测配置方法完全一样，不同之处在于探测失败后的行为。 ​ liveness探测是根据重启策略操作容器，大多数是重启容器。 ​ readiness则是将容器设置为不可用，不接收Service转发的请求。 （3）两种探测方法可建议独立存在，也可以同时存在。用livensess判断是否需要重启，实现自愈；用readiness判断容器是否已经准备好对外提供服务。 二、 检测的应用 1. 在scale(扩容/缩容) 中的应用。 （1）编写一个readiness的yaml文件 123456789101112131415161718192021222324252627282930313233343536373839[root@master ~]# vim hcscal.yamlkind: DeploymentapiVersion: extensions/v1beta1metadata: name: webspec: replicas: 3 template: metadata: labels: run: web spec: containers: - name: web image: httpd ports: - containerPort: 80 readinessProbe: httpGet: scheme: HTTP #探测的协议 path: /healthy #访问的目录 port: 80 initialDelaySeconds: 10 periodSeconds: 5---kind: ServiceapiVersion: v1metadata: name: web-svcspec: type: NodePort selector: run: web ports: - protocol: TCP port: 90 targetPort: 80 nodePort: 30321 （2）运行一下 1[root@master ~]# kubectl apply -f readiness.yaml （3）查看一下 1[root@master ~]# kubectl get pod -w ![image-20200113102400721](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\07 pod健康检查.assets\\image-20200113102400721.png) 1[root@master ~]# kubectl get pod -o wide ![image-20200113104819603](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\07 pod健康检查.assets\\image-20200113104819603.png) 1[root@master ~]# kubectl get service -o wide ![image-20200113104858861](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\07 pod健康检查.assets\\image-20200113104858861.png) （4）访问一下 1[root@master ~]# curl 10.244.1.21/healthy ![image-20200113104931451](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\07 pod健康检查.assets\\image-20200113104931451.png) （5）pod在指定目录创建一个文件 1[root@master ~]# kubectl exec web-69d659f974-7s9bc touch /usr/local/apache2/htdocs/healthy （6）查看一下 1[root@master ~]# kubectl get pod -w ![image-20200113105045616](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\07 pod健康检查.assets\\image-20200113105045616.png) 2. 在更新过程中的使用 （1）编写一个readiness的yaml文件 1234567891011121314151617181920212223242526[root@master ~]# vim app.v1.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: appspec: replicas: 10 template: metadata: labels: run: app spec: containers: - name: app image: busybox args: - /bin/sh - -c - sleep 10; touch /tmp/healthy; sleep 3000 readinessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 10 periodSeconds: 5 （2）运行一下并记录版本信息 1[root@master ~]# kubectl apply -f readiness.yaml --record 查看一下 1[root@master ~]# kubectl rollout history deployment app ![image-20200113110638083](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\07 pod健康检查.assets\\image-20200113110638083.png) （3）查看一下 1[root@master ~]# kubectl get pod -w ![image-20200113110659355](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\07 pod健康检查.assets\\image-20200113110659355.png) 3.升级一下Deployment （1）编写一个readiness的yaml文件 1[root@master ~]# cp app.v1.yaml app.v2.yaml 123456789101112131415161718192021222324252627[root@master ~]# vim app.v2.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: appspec: replicas: 10 template: metadata: labels: run: app spec: containers: - name: app image: busybox args: - /bin/sh - -c - sleep 3000 #修改命令 readinessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 10 periodSeconds: 5 （2）运行一下并记录版本信息 1[root@master ~]# kubectl apply -f readiness.yaml --record 查看一下 1[root@master ~]# kubectl rollout history deployment app ![image-20200113111024791](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\07 pod健康检查.assets\\image-20200113111024791.png) （3）查看一下 1[root@master ~]# kubectl get pod -w ![image-20200113111125387](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\07 pod健康检查.assets\\image-20200113111125387.png) （4）再次升级一下deployment &lt;1&gt; 编写一个readiness的yaml文件 1[root@master ~]# cp app.v1.yaml app.v3.yaml 1234567891011121314151617181920[root@master ~]# vim app.v2.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: appspec: replicas: 10 template: metadata: labels: run: app spec: containers: - name: app image: busybox args: - /bin/sh - -c - sleep 3000 #修改命令 &lt;2&gt; 运行一下并记录版本信息 1[root@master ~]# kubectl apply -f readiness.yaml --record 查看一下 1[root@master ~]# kubectl rollout history deployment app ![image-20200113111559864](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\07 pod健康检查.assets\\image-20200113111559864.png) &lt;3&gt; 查看一下 1[root@master ~]# kubectl get pod -w ![image-20200113111625947](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\07 pod健康检查.assets\\image-20200113111625947.png) 4. 回滚v2版本 1[root@master ~]# kubectl rollout undo deployment app --to-revision=2 查看一下 1[root@master ~]# kubectl get pod ![image-20200113112216777](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\07 pod健康检查.assets\\image-20200113112216777.png) （1）编写一个readiness的yaml文件 123456789101112131415161718192021222324252627282930[root@master ~]# vim app.v2.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: appspec: strategy: rollingUpdate: maxSurge: 2 maxUnavailable: 2 replicas: 10 template: metadata: labels: run: app spec: containers: - name: app image: busybox args: - /bin/sh - -c - sleep 3000 readinessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 10 periodSeconds: 5 maxSurge：此参数控制滚动更新过程中，副本总数超过预期数的值。可以是整数，也可以是百分比，默认是1。 maxUnavailable：不可用pod的值，默认为1，可以是整数，也可以是百分比。 （2） 运行一下并记录版本信息 1[root@master ~]# kubectl apply -f app.v2.yaml --record 查看一下 1[root@master ~]# kubectl rollout history deployment app ![image-20200113114726856](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\07 pod健康检查.assets\\image-20200113114726856.png) （3） 查看一下 1[root@master ~]# kubectl get pod -w ![image-20200113114755658](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\07 pod健康检查.assets\\image-20200113114755658.png) 三、小实验 1）写一个Deployment资源对象，要求2个副本，nginx镜像。使用Readiness探测，自定义文件/test是否存在，容器开启之后10秒开始探测，时间间隔为10秒。 （1）编写一个readiness的yaml文件 1234567891011121314151617181920212223[root@master yaml]# vim nginx.yamlkind: DeploymentapiVersion: extensions/v1beta1metadata: name: webspec: replicas: 2 template: metadata: labels: run: web spec: containers: - name: readiness image: 192.168.1.21:5000/nginx:v1 readinessProbe: exec: command: - cat - /usr/share/nginx/html/test initialDelaySeconds: 10 periodSeconds: 10 （2）运行一下并记录版本信息 1[root@master ~]# kubectl apply -f nginx.yaml --record 查看一下 1[root@master ~]# kubectl rollout history deployment web ![image-20200113122256692](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\07 pod健康检查.assets\\image-20200113122256692.png) （3）查看一下 1[root@master ~]# kubectl get pod -w ![image-20200113141908252](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\07 pod健康检查.assets\\image-20200113141908252.png) 2）在运行之后两个Pod里，进入一个Pod，创建文件/test。 12[root@master yaml]# kubectl exec -it web-864c7cf7fc-gpxq4 /bin/bashroot@web-68444bff8-xm22z:/# touch /usr/share/nginx/html/test 查看一下 1[root@master yaml]# kubectl get pod -w ![image-20200113142148105](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\07 pod健康检查.assets\\image-20200113142148105.png) 3）创建一个Service资源对象，跟上述Deployment进行关联，运行之后，查看Service资源详细信息，确认EndPoint负载均衡后端Pod。 （1）编写service的yaml文件 1234567891011121314[root@master yaml]# vim nginx-svc.yamlkind: ServiceapiVersion: v1metadata: name: web-svcspec: type: NodePort selector: run: web ports: - protocol: TCP port: 90 targetPort: 80 nodePort: 30321 （2）执行一下 1[root@master yaml]# kubectl apply -f nginx-svc.yaml （3）给两个pod刚更改页面 查看一下pod 1[root@master yaml]# kubectl get pod -o wide 更改页面 1234567[root@master yaml]# kubectl exec -it web-864c7cf7fc-gpxq4 /bin/bashroot@web-864c7cf7fc-gpxq4:/# echo \"123\"&gt;/usr/share/nginx/html/testroot@web-864c7cf7fc-gpxq4:/# exit[root@master yaml]# kubectl exec -it web-864c7cf7fc-pcrs9 /bin/bashroot@web-864c7cf7fc-pcrs9:/# echo \"321\"&gt;/usr/share/nginx/html/testroot@web-864c7cf7fc-pcrs9:/# exit 4）观察状态之后，尝试将另一个Pod也写入/test文件，然后再去查看SVC对应的EndPoint的负载均衡情况。 （1）查看一下service 1[root@master yaml]# kubectl get service ![image-20200113144624099](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\07 pod健康检查.assets\\image-20200113144624099.png) （2）访问一下 1[root@master ~]# curl 192.168.1.21:30321/test ![image-20200113144514174](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\07 pod健康检查.assets\\image-20200113144514174.png) 5）通过httpGet的探测方式，重新运行一下deployment资源，总结对比一下这两种Readiness探测方式。 （1）修改deployment的yaml文件 12345678910111213141516171819202122[root@master yaml]# vim nginx.yaml kind: DeploymentapiVersion: extensions/v1beta1metadata: name: webspec: replicas: 2 template: metadata: labels: run: web spec: containers: - name: readiness image: 192.168.1.21:5000/nginx:v1 readinessProbe: httpGet: scheme: HTTP path: /usr/share/nginx/html/test port: 80 initialDelaySeconds: 10 periodSeconds: 10 （2）执行一下 1[root@master yaml]# kubectl apply -f nginx.yaml （3）查看一下pod 1[root@master yaml]# kubectl get pod -w ![image-20200113151034766](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\07 pod健康检查.assets\\image-20200113151034766.png) maxSurge：此参数控制滚动更新过程中，副本总数超过预期数的值。可以是整数，也可以是百分比，默认是1。所以现在是3台pod （4）访问一下 1[root@master yaml]# curl 192.168.1.21:30321/test ![image-20200113151225572](G:\\四期\\虚拟化\\kubernetes\\k8s文档\\07 pod健康检查.assets\\image-20200113151225572.png) 6）总结对比liveness和readiness探测的相同和不同之处，以及它们的使用场景。","path":"posts/cffe.html","date":"05-01","excerpt":"","tags":[]},{"title":"06 pod资源对象","text":"一，k8s的资源对象 Deployment、Service、Pod是k8s最核心的3个资源对象 **Deployment：**最常见的无状态应用的控制器，支持应用的扩缩容、滚动升级等操作。 **Service：**为弹性变动且存在生命周期的Pod对象提供了一个固定的访问接口，用于服务发现和服务访问。 **Pod：**是运行容器以及调度的最小单位。同一个pod可以同时运行多个容器，这些容器共享net、UTS、IPC，除此之外还有USER、PID、MOUNT。 **ReplicationController：**用于确保每个Pod副本在任意时刻都能满足目标数量，简单来说，它用于每个容器或容器组总是运行并且可以访问的：老一代无状态的Pod应用控制器。 **RwplicatSet：**新一代的无状态的Pod应用控制器，它与RC的不同之处在于支持的标签选择器不同，RC只支持等值选择器（键值对），RS还额外支持基于集合的选择器。 **StatefulSet：**用于管理有状态的持久化应用，如database服务程序，它与Deployment不同之处在于，它会为每一个pod创建一个独有的持久性标识符，并确保每个pod之间的顺序性。 **DaemonSet：**用于确保每一个节点都运行了某个pod的一个副本，新增的节点一样会被添加到此类pod，在节点移除时，此pod会被回收。 **Job：**用于管理运行完成后即可终止的应用，例如批量处理做作业任务； **volume：**pv pvc ConfigMap： Secret： Role： ClusterRole： RoleBinding： cluster RoleBinding： service account： Helm： Pod的生命周期被定义为以下几个阶段。 Pending：Pod已经被创建，但是一个或者多个容器还未创建，这包括Pod调度阶段，以及容器镜像的下载过程。 Running：Pod已经被调度到Node，所有容器已经创建，并且至少一个容器在运行或者正在重启。 Succeeded：Pod中所有容器正常退出。 Failed：Pod中所有容器退出，至少有一个容器是一次退出的。 环境介绍 主机 IP地址 服务 master 192.168.1.21 k8s node01 192.168.1.22 k8s node02 192.168.1.23 k8s 二，Namespace：名称空间 默认的名称空间： Namespace（命名空间）是kubernetes系统中的另一个重要的概念，通过将系统内部的对象“分配”到不同的Namespace中，形成逻辑上分组的不同项目、小组或用户组，便于不同的分组在共享使用整个集群的资源的同时还能被分别管理。 Kubernetes集群在启动后，会创建一个名为“default”的Namespace，如果不特别指明Namespace，则用户创建的Pod、RC、Service都被系统创建到“default”的Namespace中。 1.查看名称空间 1[root@master ~]# kubectl get namespaces 2.查看名称空间详细信息 1[root@master ~]# kubectl describe ns default 3.创建名称空间 1[root@master ~]# kubectl create ns bdqn 查看一下 1[root@master ~]# kubectl get namespaces 4.创建namespace的yaml文件 （1）查看格式 12[root@master ~]# kubectl explain ns//查看nasespace的yaml文件的格式 （2）创建namespace的yaml文件 12345[root@master ~]# vim test-ns.yamlapiVersion: v1kind: Namespacemetadata: name: test （3）运行namespace的yaml文件 1[root@master ~]# kubectl apply -f test-ns.yaml （4）查看一下 1[root@master ~]# kubectl get ns 4.删除名称空间 12[root@master ~]# kubectl delete ns test [root@master ~]# kubectl delete -f test-ns.yaml 注意：namespace资源对象进用于资源对象的隔离，并不能隔绝不同名称空间的Pod之间的通信。那是网络策略资源的功能。 5.查看指定名称空间 可使用–namespace或-n选项 12[root@master ~]# kubectl get pod -n kube-system [root@master ~]# kubectl get pod --namespace kube-system 三，Pod 1.编写一个pod的yaml文件 123456789[root@master ~]# vim pod.yamlkind: PodapiVersion: v1metadata: name: test-podspec: containers: - name: test-app image: 192.168.1.21:5000/web:v1 pod的yaml文件不支持replicas字段 （1）运行一下 1[root@master ~]# kubectl apply -f pod.yaml （2）查看一下 1[root@master ~]# kubectl get pod ps：这个pod因为是自己创建的，所以删除之后k8s并不会自动生成，相当于docker中创建 2.指定pod的namespace名称空间 （1）修改pod的yaml文件 12345678910[root@master ~]# vim pod.yamlkind: Pod #资源类型apiVersion: v1 #api版本metadata: name: test-pod #指定控制器名称 namespace: bdqn #指定namespace（名称空间）spec: containers: #容器 - name: test-app #容器名称 image: 192.168.1.21:5000/web:v1 #镜像 执行一下 1[root@master ~]# kubectl apply -f pod.yaml （2）查看一下 12[root@master ~]# kubectl get pod -n bdqn //根据namespace名称查看 3.pod中镜像获取策略 **Always：**镜像标签为“laster”或镜像不存在时，总是从指定的仓库中获取镜像。 **IfNotPresent：**仅当本地镜像不存在时才从目标仓库下载。 **Never：**禁止从仓库中下载镜像，即只使用本地镜像。 注意：对于标签为“laster”或者标签不存在，其默认的镜像下载策略为“Always”，而对于其他的标签镜像，默认策略为“IfNotPresent”。 4.观察pod和service的不同并关联 （1）pod的yaml文件（指定端口） 1234567891011121314[root@master ~]# vim pod.yaml kind: Pod #资源类型apiVersion: v1 #api版本metadata: name: test-pod #指定控制器名称 namespace: bdqn #指定namespace（名称空间）spec: containers: #容器 - name: test-app #容器名称 image: 192.168.1.21:5000/web:v1 #镜像 imagePullPolicy: IfNotPresent #获取的策略 ports: - protocol: TCP containerPort: 80 &lt;1&gt;删除之前的pod 1[root@master ~]# kubectl delete pod -n bdqn test-pod &lt;2&gt;执行一下 1[root@master ~]# kubectl apply -f pod.yaml &lt;3&gt;查看一下 1[root@master ~]# kubectl get pod -n bdqn （2）pod的yaml文件（修改端口） 1234567891011121314[root@master ~]# vim pod.yaml kind: PodapiVersion: v1metadata: name: test-pod namespace: bdqnspec: containers: - name: test-app image: 192.168.1.21:5000/web:v1 imagePullPolicy: IfNotPresent ports: - protocol: TCP containerPort: 90 #改一下端口 &lt;1&gt;删除之前的pod 1[root@master ~]# kubectl delete pod -n bdqn test-pod &lt;2&gt;执行一下 1[root@master ~]# kubectl apply -f pod.yaml &lt;3&gt;查看一下 1[root@master ~]# kubectl get pod -n bdqn -o wide &lt;4&gt;访问一下 会发现修改的90端口并不生效，他只是一个提示字段并不生效。 （3）pod的yaml文件（添加标签） 12345678910111213141516[root@master ~]# vim pod.yaml kind: PodapiVersion: v1metadata: name: test-pod namespace: bdqn labels: #标签 app: test-web #标签名称spec: containers: - name: test-app image: 192.168.1.21:5000/web:v1 imagePullPolicy: IfNotPresent ports: - protocol: TCP containerPort: 90 #改一下端口 --------------------------------------pod--------------------------------------------- （4）编写一个service的yaml文件 123456789101112[root@master ~]# vim test-svc.yaml apiVersion: v1 #api版本kind: Service #资源类型metadata: name: test-svc #指定控制器名称 namespace: bdqn #指定namespace（名称空间）spec: selector: #标签 app: test-web #标签名称（须和pod的标签名称一致） ports: - port: 80 #宿主机端口 targetPort: 80 #容器端口 会发现添加的80端口生效了，所以不能乱改。 &lt;1&gt;执行一下 1[root@master ~]# kubectl apply -f test-svc.yaml &lt;2&gt;查看一下 1[root@master ~]# kubectl get svc -n bdqn 1[root@master ~]# kubectl describe svc -n bdqn test-svc &lt;4&gt;访问一下 1[root@master ~]# curl 10.98.57.97 --------------------------------------service--------------------------------------------- 四，容器的重启策略 Pod的重启策略（RestartPolicy）应用与Pod内所有容器，并且仅在Pod所处的Node上由kubelet进行判断和重启操作。当某个容器异常退出或者健康检查失败时，kubelet将根据RestartPolicy的设置来进行相应的操作。 Always：（默认情况下使用）但凡Pod对象终止就将其重启； **OnFailure：**仅在Pod对象出现错误时才将其重启； **Never：**从不重启； 五，pod的默认健康检查 每个容器启动时都会执行一个进程，此进程由 Dockerfile 的 CMD 或 ENTRYPOINT 指定。如果进程退出时返回码非零，则认为容器发生故障，Kubernetes 就会根据 restartPolicy 重启容器。 （1）编写健康检查的yaml文件 下面我们模拟一个容器发生故障的场景，Pod 配置文件如下： 12345678910111213141516[root@master ~]# vim healcheck.yaml apiVersion: v1kind: Podmetadata: labels: test: healcheck name: healcheckspec: restartPolicy: OnFailure #指定重启策略 containers: - name: healcheck image: busybox:latest args: #生成pod时运行的命令 - /bin/sh - -c - sleep 20; exit 1 &lt;1&gt;执行一下 1[root@master ~]# kubectl apply -f healcheck.yaml &lt;2&gt;查看一下 1[root@master ~]# kubectl get pod -o wide 1[root@master ~]# kubectl get pod -w | grep healcheck 在上面的例子中，容器进程返回值非零，Kubernetes 则认为容器发生故障，需要重启。但有不少情况是发生了故障，但进程并不会退出。 六，小实验 1）以自己的名称创建一个k8s名称空间，以下所有操作都在此名称空间中。 （1）创建名称空间 1[root@master ~]# kubectl create ns xgp （2）查看一下 1[root@master ~]# kubectl get ns xgp 2）创建一个Pod资源对象，使用的是私有仓库中私有镜像，其镜像的下载策略为：NEVER。 Pod的重启策略为： Never. 123456789101112131415161718192021[root@master ~]# vim pod.yamlkind: PodapiVersion: v1metadata: name: test-pod namespace: xgp labels: app: test-webspec: restartPolicy: Never containers: - name: www image: 192.168.1.21:5000/web:v1 imagePullPolicy: Never args: - /bin/sh - -c - sleep 90; exit 1 ports: - protocol: TCP containerPort: 80 3）创建出容器之后，执行非正常退出，查看Pod的最终状态。 （1）执行一下上面pod的yaml文件 1[root@master ~]# kubectl apply -f pod.yaml （2）动态查看ns中test-pod的信息 1[root@master ~]# kubectl get pod -n xgp -w | grep test-pod 删除test-pod 1[root@master ~]# kubectl delete pod -n xgp test-pod 4) 创建一个Service资源对象，与上述Pod对象关联，验证他们的关联性。 （1）修改pod的yaml文件 1234567891011121314151617[root@master ~]# vim pod.yamlkind: PodapiVersion: v1metadata: name: test-pod namespace: xgp labels: app: test-webspec: restartPolicy: Never containers: - name: www image: 192.168.1.21:5000/web:v1 imagePullPolicy: Never ports: - protocol: TCP containerPort: 80 （1）编写service的yaml文件 123456789101112[root@master ~]# vim svc.yaml apiVersion: v1kind: Servicemetadata: name: test-svc namespace: xgpspec: selector: app: test-web ports: - port: 80 targetPort: 80 （2）执行一下 1[root@master ~]# kubectl apply -f svc.yaml （3）查看一下 1[root@master ~]# kubectl get pod -o wide -n xgp （4）访问一下 1[root@master ~]# curl 10.244.1.21","path":"posts/cf38.html","date":"05-01","excerpt":"","tags":[]},{"title":"05 Delpoyment、service","text":"Deployment介绍 Deployment是kubernetes 1.2引入的概念，用来解决Pod的编排问题。Deployment可以理解为RC的升级版（RC+Reolicat Set）。特点在于可以随时知道Pod的部署进度，即对Pod的创建、调度、绑定节点、启动容器完整过程的进度展示。 使用场景 创建一个Deployment对象来生成对应的Replica Set并完成Pod副本的创建过程。 检查Deployment的状态来确认部署动作是否完成（Pod副本的数量是否达到预期值）。 更新Deployment以创建新的Pod(例如镜像升级的场景)。 如果当前Deployment不稳定，回退到上一个Deployment版本。 挂起或恢复一个Deployment。 Service介绍 Service定义了一个服务的访问入口地址，前端应用通过这个入口地址访问其背后的一组由Pod副本组成的集群实例，Service与其后端的Pod副本集群之间是通过Label Selector来实现“无缝对接”。RC保证Service的Pod副本实例数目保持预期水平。 外部系统访问Service的问题 IP类型 说明 Node IP Node节点的IP地址 Pod IP Pod的IP地址 Cluster IP Service的IP地址 环境介绍 主机 IP地址 服务 master 192.168.1.21 k8s node01 192.168.1.22 k8s node02 192.168.1.23 k8s 一，Delpoyment和service的简单使用 1.练习写一个yaml文件，要求使用自己的私有镜像，要求副本数量为三个。 123456789101112131415[root@master ~]# vim xgp.yamlkind: DeploymentapiVersion: extensions/v1beta1metadata: name: xgp-webspec: replicas: 3 template: metadata: labels: app: xgp-server spec: containers: - name: web image: 192.168.1.21:5000/web:v1 （1）执行一下 1[root@master ~]# kubectl apply -f xgp.yaml --recore （2）查看一下 1[root@master ~]# kubectl get pod （3）访问一下 1[root@master ~]# curl 10.244.2.16 （4）更新一下yaml文件，副本加一 123456789101112131415[root@master ~]# vim xgp.yamlkind: DeploymentapiVersion: extensions/v1beta1metadata: name: xgp-webspec: replicas: 4 template: metadata: labels: app: xgp-server spec: containers: - name: web image: 192.168.1.21:5000/web:v1 &lt;1&gt;执行一下 1[root@master ~]# kubectl apply -f xgp.yaml --recore &lt;2&gt;查看一下 1[root@master ~]# kubectl get pod 副本数量加一，如果yaml文件的副本为0，则副本数量还是之前的状态，并不会更新。 2.练习写一个service文件 123456789101112[root@master ~]# vim xgp-svc.yamlkind: ServiceapiVersion: v1metadata: name: xgp-svcspec: selector: app: xgp-server ports: - protocol: TCP port: 80 targetPort: 80 （1）执行一下 1[root@master ~]# kubectl apply -f xgp-svc.yaml （2）查看一下 1[root@master ~]# kubectl get svc （3）访问一下 1[root@master ~]# curl 10.107.119.49 3.修改yaml文件 1234567891011121314151617[root@master ~]# vim xgp.yaml kind: DeploymentapiVersion: extensions/v1beta1metadata: name: xgp-webspec: replicas: 3 template: metadata: labels: app: xgp-server spec: containers: - name: web image: 192.168.1.21:5000/web:v1 ports: - containerPort: 80 #提示端口 注意：在Delpoyment资源对象中，可以添加Port字段，但此字段仅供用户查看，并不实际生效 执行一下 1[root@master ~]# kubectl apply -f xgp.yaml --recore 4.service文件映射端口 1234567891011121314[root@master ~]# vim xgp-svc.yaml kind: ServiceapiVersion: v1metadata: name: xgp-svcspec: type: NodePort selector: app: xgp-server ports: - protocol: TCP port: 80 targetPort: 80 nodePort: 30123 执行一下 1[root@master ~]# kubectl apply -f xgp-svc.yaml 查看一下 1[root@master ~]# kubectl get svc 访问一下 1[root@master ~]# curl 127.0.0.1:30123 5.修改三个pod页面内容 （1）查看一下pod信息 1[root@master ~]# kubectl get pod -o wide （2）修改POD页面内容（三台不一样） 12[root@master ~]# kubectl exec -it xgp-web-8d5f9656f-8z7d9 /bin/bash//根据pod名称进入pod之中 进入容器后修改页面内容 12root@xgp-web-8d5f9656f-8z7d9:/usr/local/apache2# echo xgp-v1 &gt; htdocs/index.html root@xgp-web-8d5f9656f-8z7d9:/usr/local/apache2# exit 访问一下 1[root@master ~]# curl 127.0.0.1:30123 二.分析一下k8s负载均衡原理 （1）查看service的暴露IP 1[root@master ~]# kubectl get svc （2）查看一下iptabes规则 12[root@master ~]# iptables-save //查看已配置的规则 SNAT：Source NAT（源地址转换） DNAT：Destination NAT（目标地址转换） MASQ：动态的源地址转换 （3）根据service的暴露IP，查看对应的iptabes规则 1[root@master ~]# iptables-save | grep 10.107.119.49 1[root@master ~]# iptables-save | grep KUBE-SVC-ESI7C72YHAUGMG5S （4）对应一下IP是否一致 1[root@master ~]# iptables-save | grep KUBE-SEP-ZHDQ73ZKUBMELLJB 1[root@master ~]# kubectl get pod -o wide Service实现的负载均衡：默认使用的是iptables规则。IPVS 三.回滚到指定版本 （1）删除之前创建的delpoy和service 12[root@master ~]# kubectl delete -f xgp.yaml [root@master ~]# kubectl delete -f xgp-svc.yaml （2）准备三个版本所使用的私有镜像，来模拟每次升级不同的镜像 123456789101112131415161718[root@master ~]# vim xgp1.yaml （三个文件名不相同）kind: DeploymentapiVersion: extensions/v1beta1metadata: name: xgp-webspec: revisionHistoryLimit: 10 replicas: 3 template: metadata: labels: app: xgp-server spec: containers: - name: web image: 192.168.1.21:5000/web:v1 （三台版本不同） ports: - containerPort: 80 此处3个yaml文件 指定不同版本的镜像 （3）运行三个服务，并记录三个版本信息 123[root@master ~]# kubectl apply -f xgp-1.yaml --record [root@master ~]# kubectl apply -f xgp-2.yaml --record [root@master ~]# kubectl apply -f xgp-3.yaml --record （4）查看有哪些版本信息 1[root@master ~]# kubectl rollout history deployment xgp-web （5）运行之前的service文件 1[root@master ~]# kubectl apply -f xgp-svc.yaml （6）查看service暴露端口 1[root@master ~]# kubectl get svc （7）测试访问 1[root@master ~]# curl 127.0.0.1:30123 （8）回滚到指定版本 12[root@master ~]# kubectl rollout undo deployment xgp-web --to-revision=1//这里指定的是版本信息的编号 &lt;1&gt;访问一下 1[root@master ~]# curl 127.0.0.1:30123 &lt;2&gt;查看有哪些版本信息 1[root@master ~]# kubectl rollout history deployment xgp-web 编号1已经被编号2替代，从而生的是一个新的编号4 四.用label控制pod的位置 默认情况下，scheduler会将pod调度到所有可用的Node，不过有些情况我们希望将 Pod 部署到指定的 Node，比如将有大量磁盘 I/O 的 Pod 部署到配置了 SSD 的 Node；或者 Pod 需要 GPU，需要运行在配置了 GPU 的节点上。 kubernetes通过label来实现这个功能 label 是 key-value 对，各种资源都可以设置 label，灵活添加各种自定义属性。比如执行如下命令标注 k8s-node1 是配置了 SSD 的节点 首先我们给node1节点打上一个ssd的标签 1[root@master ~]# kubectl label nodes node02 disk=ssd （1）查看标签 1[root@master ~]# kubectl get nodes --show-labels | grep node02 （2）删除副本一 123[root@master ~]# kubectl delete -f xgp-1.yaml deployment.extensions \"xgp-web\" deleted[root@master ~]# kubectl delete svc xgp-svc （3）修改副本一的yaml文件 123456789101112131415161718192021[root@master ~]# vim xgp-1.yaml kind: DeploymentapiVersion: extensions/v1beta1metadata: name: xgp-webspec: revisionHistoryLimit: 10 replicas: 3 template: metadata: labels: app: xgp-server spec: containers: - name: web image: 192.168.1.21:5000/web:v1 ports: - containerPort: 80 nodeSelector: #添加节点选择器 disk: ssd #和标签内容一致 （4）执行一下 1[root@master ~]# kubectl apply -f xgp-1.yaml 查看一下 1[root@master ~]# kubectl get pod -o wide 现在pod都在node02上运行 （5）删除标签 1[root@master ~]# kubectl label nodes node02 disk- 查看一下 1[root@master ~]# kubectl get nodes --show-labels | grep node02 没有disk标签了 五，小实验 1）使用私有镜像v1版本部署一个Deployment资源对象，要求副本Pod数量为3个，并创建一个Service资源对象相互关联，指定要求3个副本Pod全部运行在node01节点上，记录一个版本。 （1）用label控制pod的位置 1[root@master ~]# kubectl label nodes node01 disk=ssd （2）编写源yaml文件 12345678910111213141516171819[root@master ~]# vim xgp.yamlkind: DeploymentapiVersion: extensions/v1beta1metadata: name: xgp-webspec: replicas: 3 template: metadata: labels: app: xgp-server spec: containers: - name: web image: 192.168.1.21:5000/web:v1 ports: - containerPort: 80 nodeSelector: disk: ssd （3）编写源service文件 1234567891011121314[root@master ~]# vim xgp-svc.yamlkind: ServiceapiVersion: v1metadata: name: xgp-svcspec: type: NodePort selector: app: xgp-server ports: - protocol: TCP port: 80 targetPort: 80 nodePort: 30123 （4）执行yaml文件，创建控制器。执行service文件创建映射端口 12[root@master ~]# kubectl apply -f xgp.yaml [root@master ~]# kubectl apply -f xgp-svc.yaml （5）查看一下pod节点 1[root@master ~]# kubectl get pod -o wide （6）记录一个版本 1[root@master ~]# kubectl rollout history deployment xgp-web &gt; pod.txt （7）访问一下 2）根据上述Deployment，升级为v2版本，记录一个版本。 （1）修改yaml文件镜像版本 12345678910111213141516171819[root@master ~]# vim xgp.yaml kind: DeploymentapiVersion: extensions/v1beta1metadata: name: xgp-webspec: replicas: 3 template: metadata: labels: app: xgp-server spec: containers: - name: web image: 192.168.1.21:5000/web:v2 #修改版本为二 ports: - containerPort: 80 nodeSelector: disk: ssd （2）刷新一下yaml文件 1[root@master ~]# kubectl apply -f xgp.yaml --recore （3）访问一下 （4）记录一个版本 1[root@master ~]# kubectl rollout history deployment xgp-web &gt; pod.txt 3）最后升级到v3版本，这时，查看Service关联，并且分析访问流量的负载均衡详细情况。 1）修改yaml文件镜像版本 12345678910111213141516171819[root@master ~]# vim xgp.yaml kind: DeploymentapiVersion: extensions/v1beta1metadata: name: xgp-webspec: replicas: 3 template: metadata: labels: app: xgp-server spec: containers: - name: web image: 192.168.1.21:5000/web:v3 #修改版本为二 ports: - containerPort: 80 nodeSelector: disk: ssd （2）刷新一下yaml文件 1[root@master ~]# kubectl apply -f xgp.yaml --recore （3）访问一下 （5）分析访问流量的负载均衡详细情况 &lt;1&gt;查看一下service映射端口 &lt;2&gt;以ip为起点，分析访问流量的负载均衡详细情况 Service实现的负载均衡：默认使用的是iptables规则。IPVS 12[root@master ~]# iptables-save | grep 10.107.27.229//根据service的暴露IP，查看对应的iptabes规则 1[root@master ~]# iptables-save | grep KUBE-SVC-ESI7C72YHAUGMG5S 这里显示了各节点的负载比例 &lt;3&gt;对应一下IP是否一致 1[root@master ~]# iptables-save | grep KUBE-SEP-VDKW5WQIWOLZMJ6G 1[root@master ~]# kubectl get pod -o wide 4）回滚到指定版本v1，并作验证。 &lt;1&gt;回滚到指定版本 12[root@master ~]# kubectl rollout undo deployment xgp-web --to-revision=1//这里指定的是版本信息的编号 &lt;2&gt;访问一下 1[root@master ~]# curl 127.0.0.1:30123 排错思路 123[root@master ~]# less /var/log/messages | grep kubelet[root@master ~]# kubectl logs -n kube-system kube-scheduler-master [root@master ~]# kubectl describe pod xgp-web-7d478f5bb7-bd4bj","path":"posts/936d.html","date":"05-01","excerpt":"","tags":[]},{"title":"04 配置清单","text":"一，两种创建资源的方法 1. 基于命令的方式： 简单直观快捷，上手快。 适合临时测试或实验。 2. 基于配置清单的方式： 配置文件描述了 What，即应用最终要达到的状态。 配置文件提供了创建资源的模板，能够重复部署。 可以像管理代码一样管理部署。 适合正式的、跨环境的、规模化部署。 这种方式要求熟悉配置文件的语法，有一定难度。 环境介绍 主机 IP地址 服务 master 192.168.1.21 k8s node01 192.168.1.22 k8s node02 192.168.1.23 k8s 二. 配置清单（yam，yaml） 在k8s中，一般使用yaml格式的文件来创建符合我们预期期望的pod，这样的yaml文件我们一般称为资源清单 /etc/kubernetes/manifests/ k8s存放（yam、yaml）文件的地方 **kubectl explain deployment（通过explain参数加上资源类别就能看到该资源应该怎么定义） kubectl explain deployment.metadata 通过资源类别加上带有Object标记的字段，我们就可以看到一级字段下二级字段的内容有那些怎么去定义等 kubectl explain deployment.metadata.ownerReferences 通过加上不同级别的字段名称来看下字段下的内容，而且前面的[]号代表对象列表 1.常见yaml文件写法，以及字段的作用 (1) apiVersion：api版本信息 （用来定义当前属于哪个组和那个版本，这个直接关系到最终提供使用的是那个版本） 12[root@master manifests]# kubectl api-versions//查看到当前所有api的版本 (2) kind: 资源对象的类别 (用来定义创建的对象是属于什么类别，是pod，service，还是deployment等对象，可以按照其固定的语法格式来自定义。) (3) metadata: 元数据 名称字段（必写） 提供以下几个字段： creationTimestamp: &quot;2019-06-24T12:18:48Z&quot; generateName: myweb-5b59c8b9d- labels: （对象标签） pod-template-hash: 5b59c8b9d run: myweb name: myweb-5b59c8b9d-gwzz5 （pods对象的名称，同一个类别当中的pod对象名称是唯一的，不能重复） namespace: default （对象所属的名称空间，同一名称空间内可以重复，这个名称空间也是k8s级别的名称空间，不和容器的名称空间混淆） ownerReferences: - apiVersion: apps/v1 blockOwnerDeletion: true controller: true kind: ReplicaSet name: myweb-5b59c8b9d uid: 37f38f64-967a-11e9-8b4b-000c291028e5 resourceVersion: &quot;943&quot; selfLink: /api/v1/namespaces/default/pods/myweb-5b59c8b9d-gwzz5 uid: 37f653a6-967a-11e9-8b4b-000c291028e5 annotations（资源注解，这个需要提前定义，默认是没有的） 通过这些标识定义了每个资源引用的path：即/api/group/version/namespaces/名称空间/资源类别/对象名称 (4) spec： 用户期望的状态 （这个字段最重要，因为spec是用来定义目标状态的‘disired state’，而且资源不通导致spec所嵌套的字段也各不相同，也就因为spec重要且字段不相同，k8s在内部自建了一个spec的说明用于查询） (5) status：资源现在处于什么样的状态 （当前状态，’current state‘，这个字段有k8s集群来生成和维护，不能自定义，属于一个只读字段） 2.编写一个yaml文件 123456789101112131415[root@master ~]# vim web.yamlkind: Deployment #资源对象是控制器apiVersion: extensions/v1beta1 #api的版本metadata: #描述kind（资源类型） name: web #定义控制器名称spec: replicas: 2 #副本数量 template: #模板 metadata: labels: #标签 app: web_server spec: containers: #指定容器 - name: nginx #容器名称 image: nginx #使用的镜像 执行一下 1[root@master ~]# kubectl apply -f web.yaml 查看一下 12[root@master ~]# kubectl get deployments. -o wide//查看控制器信息 12[root@master ~]# kubectl get pod -o wide//查看pod节点信息 3.编写一个service.yaml文件 123456789101112[root@master ~]# vim web-svc.yamlkind: Service #资源对象是副本apiVersion: v1 #api的版本metadata: name: web-svcspec: selector: #标签选择器 app: web-server #须和web.yaml的标签一致 ports: #端口 - protocol: TCP port: 80 #宿主机的端口 targetPort: 80 #容器的端口 使用相同标签和标签选择器内容，使两个资源对象相互关联。 创建的service资源对象，默认的type为ClusterIP，意味着集群内任意节点都可访问。它的作用是为后端真正服务的pod提供一个统一的接口。如果想要外网能够访问服务，应该把type改为NodePort （1）执行一下 1[root@master ~]# kubectl apply -f web-svc.yaml （2）查看一下 12[root@master ~]# kubectl get svc//查看控制器信息 （3）访问一下 1[root@master ~]# curl 10.111.193.168 4.外网能够访问服务 （1）修改web-svc.yaml文件 12345678910111213kind: Service #资源对象是副本apiVersion: v1 #api的版本metadata: name: web-svcspec: type: NodePort #添加 更改网络类型 selector: #标签选择器 app: web_server #须和web.yaml的标签一致 ports: #端口 - protocol: TCP port: 80 #宿主机的端口 targetPort: 80 #容器的端口 nodePort: 30086 #指定群集映射端口，范围是30000-32767 （2）刷新一下 1[root@master ~]# kubectl apply -f web-svc.yaml （3）查看一下 1[root@master ~]# kubectl get svc （4）浏览器测试 三、小实验 基于上一篇博客实验继续进行 1.使用yaml文件的方式创建一个Deployment资源对象，要求镜像使用个人私有镜像v1版本。replicas为3个。 编写yaml文件 123456789101112131415[root@master ~]# vim www.yamlkind: DeploymentapiVersion: extensions/v1beta1metadata: name: xgpspec: replicas: 3 template: metadata: labels: app: www_server spec: containers: - name: web image: 192.168.1.21:5000/web:v1 （1）执行一下 1[root@master ~]# kubectl apply -f web-svc.yaml （2）查看一下 12[root@master ~]# kubectl get deployments. -o wide//查看控制器信息 12[root@master ~]# kubectl get pod -o wide//查看pod节点信息 （3）访问一下 2. 使用yaml文件的方式创建一个Service资源对象，要与上述Deployment资源对象关联，type类型为： NodePort，端口为:30123. 编写service文件 1234567891011121314[root@master ~]# vim www-svc.yamlkind: ServiceapiVersion: v1metadata: name: www-svcspec: type: NodePort selector: app: www_server ports: - protocol: TCP port: 80 targetPort: 80 nodePort: 30123 执行一下 1[root@master ~]# kubectl apply -f www-svc.yaml 查看一下 1[root@master ~]# kubectl get svc 访问一下 四. 总结 1. Pod的作用 在k8s中pod是最小的管理单位，在一个pod中通常会包含一个或多个容器。大多数情况下，一个Pod内只有一个Container容器。 在每一个Pod中都有一个特殊的Pause容器和一个或多个业务容器，Pause来源于pause-amd64镜像,Pause容器在Pod中具有非常重要的作用： Pause容器作为Pod容器的根容器，其本地于业务容器无关，它的状态代表了整个pod的状态。 Pod里的多个业务容器共享Pause容器的IP，每个Pod被分配一个独立的IP地址，Pod中的每个容器共享网络命名空间，包括IP地址和网络端口。Pod内的容器可以使用localhost相互通信。k8s支持底层网络集群内任意两个Pod之间进行通信。 Pod中的所有容器都可以访问共享volumes，允许这些容器共享数据。volumes还用于Pod中的数据持久化，以防其中一个容器需要重新启动而丢失数据。 2. Service的作用 Service 是后端真实服务的抽象，一个 Service 可以代表多个相同的后端服务 Service 为 POD 控制器控制的 POD 集群提供一个固定的访问端点，Service 的工作还依赖于 K8s 中的一个附件，就是 CoreDNS ，它将 Service 地址提供一个域名解析。 NodePort 类型的 service clusterIP：指定 Service 处于 service 网络的哪个 IP，默认为动态分配 NodePort 是在 ClusterIP 类型上增加了一个暴露在了 node 的网络命名空间上的一个 nodePort，所以用户可以从集群外部访问到集群了，因而用户的请求流程是：Client -&gt; NodeIP:NodePort -&gt; ClusterIP:ServicePort -&gt; PodIP:ContainerPort。 可以理解为 NodePort 增强了 ClusterIP 的功能，让客户端可以在每个集群外部访问任意一个 nodeip 从而访问到 clusterIP，再由 clusterIP 进行负载均衡至 POD。 3.流量走向 我们在创建完成一个服务之后，用户首先应该访问的是nginx反向代理的ip，然后通过nginx访问到后端的k8s服务器（master节点）的“NodePort暴露IP 及 映射的端口“，master的apiserver接受到客户端发送来的访问指令，将访问指令通知Controller Manager控制器，Scheduler执行调度任务，将访问指令分发到各节点之上，通过”master节点“的“ip+映射端口”访问到后端k8s节点的信息，节点的Kubelet（pod代理）当Scheduler确定让那个节点返回访问信息之后，kube-proxy将访问信息负载均衡到该节点的容器上，各容器返回信息，并向Master报告运行状态","path":"posts/748b.html","date":"05-01","excerpt":"","tags":[]},{"title":"03 创建资源的两种方式","text":"主机 IP地址 master 192.168.1.21 node01 192.168.1.22 node02 192.168.1.23 两种创建资源的方法 基于命令的方式： 简单直观快捷，上手快。 适合临时测试或实验。 基于配置文件的方式： 配置文件描述了 What，即应用最终要达到的状态。 配置文件提供了创建资源的模板，能够重复部署。 可以像管理代码一样管理部署。 适合正式的、跨环境的、规模化部署。 这种方式要求熟悉配置文件的语法，有一定难度。 一，用命令行的方式创建资源 仅接受json格式 配置清单（yml、yaml） 12[root@master ~]# cd /etc/kubernetes/manifests///k8s的yml、yaml文件 1.node01和node02下载nginx镜像 12docker pull nginx//下载nginx镜像 2.master创建Pod控制器（test-web），deployment 12[root@master ~]# kubectl run test-web --image=nginx --replicas=5//创建Pod控制器，deployment 3.查看控制器情况 （1） 12[root@master ~]# kubectl get deployments.//查看控制器情况 12[root@master ~]# kubectl get pod --all-namespaces -o wide//显示pod的节点信息 （2） 12[root@master ~]# kubectl get namespaces //查看k8s名称空间 12[root@master ~]# kubectl describe deployments. test-web//查看资源详细信息 查看某种资源对象，没有指定名称空间，默认是在default名称空间。可以加上-n选项，查看指定名称空间的资源。 1[root@master ~]# kubectl get pod -n kube-system 3.删除test-web控制器 1[root@master ~]# kubectl delete deployments. test-web 4.master创建Pod控制器（web），deployment 1[root@master ~]# kubectl run web --image=nginx --replicas=5 查看一下pod信息 12[root@master ~]# kubectl get pod -o wide//查看一下pod的节点信息 12[root@master ~]# kubectl describe deployments. web //查看资源详细信息 注意：直接运行创建的deployment资源对象，是经常使用的一个控制器资源类型，除了deployment，还有rc、rs等等pod控制器，deployment是一个高级的pod控制器。 本机测试访问nginx 1[root@master ~]# curl 10.244.1.7 5.创建service资源类型 12[root@master ~]# kubectl expose deployment web --name=web-xgp --port=80 --type=NodePort//创建service资源类型，这里我们设置了映射端口 如果想要外网能够访问服务，可以暴露deployment资源，得到service资源，但svc资源的类型必须为NodePort。 映射端口范围：30000-32767 查看service信息 1[root@master ~]# kubectl get svc 浏览器测试访问http://192.168.1.21:30493/ 二、服务的扩容与缩容 1. 查看控制器信息 1[root@master ~]# kubectl get deployments. -o wide 2.扩容 1[root@master ~]# kubectl scale deployment web --replicas=8 查看一下 1[root@master ~]# kubectl get deployments. -o wide 3.缩容 1[root@master ~]# kubectl scale deployment web --replicas=4 查看一下 1[root@master ~]# kubectl get deployments. -o wide 3.通过修改web的yaml文件进行扩容缩容 备份web的yaml文件 1[root@master ~]# kubectl get deployments. -o yaml &gt; web.yaml 使用edit修改web的yaml文件 1[root@master ~]# kubectl edit deployments. web 查看一下 1[root@master ~]# kubectl get deployments. -o wide 三、服务的升级与回滚 node01和node02下载1.15版本的nginx 1[root@master ~]# docker pull nginx:1.15 1.master设置服务升级 1[root@master ~]# kubectl set image deployment web web=nginx:1.15 查看一下 2.master设置服务回滚 （1）修改配置文件回滚 使用edit修改web的yaml文件 1[root@master ~]# kubectl edit deployments. web 查看一下 1[root@master ~]# kubectl get deployments. -o wide （2）命令回滚 1[root@master ~]# kubectl rollout undo deployment web 注意:只能回滚到上一次操作的状态 四、实验环境 主机 IP地址 服务 master 192.168.1.21 registry+Deployment node01 192.168.1.22 node02 192.168.1.23 1.master 基于httpd制作自己的镜像，需要3个版本，v1,v2,v3.并且对应的版本镜像，访问的主目录内容不一样 （1）master下载httpd镜像 1[root@master ~]# docker pull httpd （2）编写Dockerfile 123[root@master xgp]# vim DockerfileFROM httpdCOPY index.html /usr/local/apache2/htdocs/index.html （3）创建测试网页v1 1[root@master xgp]#echo \"&lt;h1&gt;xgp | test-web | httpd:v1&lt;h1&gt;\" &gt; index.html （4）基于Dockerfile创建镜像 web1 1[root@master xgp]# docker build -t web1 . （5）创建测试网页v2 1[root@master xgp]#echo \"&lt;h1&gt;xgp | test-web | httpd:v1&lt;h1&gt;\" &gt; index.html （6）基于Dockerfile创建镜像 web2 1[root@master xgp]# docker build -t web2 . （7）创建测试网页v3 1[root@master xgp]# echo \"&lt;h1&gt;xgp | test-web | httpd:v3&lt;h1&gt;\" &gt; index.html （8）基于Dockerfile创建镜像 web3 1[root@master xgp]# docker build -t web3 . 2.master部署私有仓库 （1）master下载registry镜像 1[root@master ~]# docker pull registry （2）启动registry 1[root@master xgp]# docker run -itd --name registry -p 5000:5000 --restart=always registry:latest （3）修改docker配置文件，加入私有仓库（三台） 12[root@master xgp]# vim /usr/lib/systemd/system/docker.serviceExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.21:5000 （4）重启docker（三台） 12[root@master xgp]# systemctl daemon-reload [root@master xgp]# systemctl restart docker 3.上传之前创建的三个web镜像到私有仓库 （1）修改镜像标签 123[root@master xgp]# docker tag web1:latest 192.168.1.21:5000/web1:latest[root@master xgp]# docker tag web2:latest 192.168.1.21:5000/web2:latest[root@master xgp]# docker tag web3:latest 192.168.1.21:5000/web3:latest （2）将三个web镜像上传到私有仓库 123[root@master xgp]# docker push 192.168.1.21:5000/web1:latest [root@master xgp]# docker push 192.168.1.21:5000/web2:latest[root@master xgp]# docker push 192.168.1.21:5000/web3:latest 4.部署一个Deployment资源对象，要求镜像使用上述私有镜像v1版本。6个副本Pod。 1[root@master xgp]# kubectl run www1 --image=192.168.1.21:5000/web1:latest --replicas=6 查看一下 1[root@master xgp]# kubectl get pod 本地访问一下 5.将上述Deployment暴露一个service资源对象，使外网能否访问服务。 1[root@master xgp]# kubectl expose deployment www1 --name=web-xgp --port=80 --type=NodePort 查看一下 1[root@master xgp]# kubectl get svc 浏览器访问一下 6.将上述Deployment进行扩容和缩容操作，扩容为8个副本Pod，然后缩容为4个副本Pod。 （1）扩容 1[root@master xgp]# kubectl scale deployment www1 --replicas=8 查看一下 1[root@master xgp]# kubectl get deployments. -o wide （2）缩容 修改k8s配置文件 备份web的yaml文件 1[root@master ~]# kubectl get deployments. -o yaml &gt; www1.yaml 使用edit修改web的yaml文件 1[root@master ~]# kubectl edit deployments. www1 查看一下 1[root@master xgp]# kubectl get deployments. -o wide 7.将上述Deployment进行升级与回滚操作，将v1版本，升级到v2版本。 （1）升级版本为web2 1[root@master ~]# kubectl set image deployment www1 www1=192.168.1.21:5000/web2 本机测试访问 12[root@master ~]# curl 127.0.0.1:30996&lt;h1&gt;xgp | test-web | httpd:v2&lt;h1&gt; 浏览器测试访问 （2）回滚版本到web1 &lt;1&gt;修改配置文件回滚 使用edit修改web的yaml文件 1[root@master ~]# kubectl edit deployments. www1 查看一下 1[root@master ~]# kubectl get deployments. -o wide 访问一下 &lt;2&gt;命令回滚 1[root@master ~]# kubectl rollout undo deployment www1 注意:只能回滚到上一次操作的状态 访问一下","path":"posts/6989.html","date":"05-01","excerpt":"","tags":[]},{"title":"02 k8s架构，基本概念","text":"主机名 IP地址 服务 master 192.168.1.21 node01 192.168.1.22 node02 192.168.1.23 kubernetes架构 kubectl：k8s是命令行端，用来发送客户的操作指令。 master节点 1. API server[资源操作入口]：是k8s集群的前端接口，各种各样客户端工具以及k8s的其他组件可以通过它管理k8s集群的各种资源。它提供了HTTP/HTTPS RESTful API,即K8S API。 提供了资源对象的唯一操作入口，其他所有组件都必须通过它提供的API来操作资源数据，只有API Server与存储通信，其他模块通过API Server访问集群状态。 第一，是为了保证集群状态访问的安全。 第二，是为了隔离集群状态访问的方式和后端存储实现的方式：API Server是状态访问的方式，不会因为后端存储技术etcd的改变而改变。 作为kubernetes系统的入口，封装了核心对象的增删改查操作，以RESTFul接口方式提供给外部客户和内部组件调用。对相关的资源数据“全量查询”+“变化监听”，实时完成相关的业务功能。 2. Scheduler[集群分发调度器]：负责决定将Pod放在哪个Node上运行。在调度时，会充分考虑集群的拓扑结构，当前各个节点的负载情况，以及应对高可用、性能、数据亲和性和需求。 1.Scheduler收集和分析当前Kubernetes集群中所有Minion节点的资源(内存、CPU)负载情况，然后依此分发新建的Pod到Kubernetes集群中可用的节点。 2.实时监测Kubernetes集群中未分发和已分发的所有运行的Pod。 3.Scheduler也监测Minion节点信息，由于会频繁查找Minion节点，Scheduler会缓存一份最新的信息在本地。 4.最后，Scheduler在分发Pod到指定的Minion节点后，会把Pod相关的信息Binding写回API Server。 4. Controller Manager[内部管理控制中心]：负责管理集群的各种资源，保证资源处于预期的状态。它由多种Controller组成，包括Replication Controller、Endpoints Controller、Namespace Controller、Serviceaccounts Controller等。 实现集群故障检测和恢复的自动化工作，负责执行各种控制器，主要有： 1.endpoint-controller：定期关联service和pod(关联信息由endpoint对象维护)，保证service到pod的映射总是最新的。 2.replication-controller：定期关联replicationController和pod，保证replicationController定义的复制数量与实际运行pod的数量总是一致的。 5. Etcd：负责保存k8s集群的配置信息和各种资源的状态信息。当数据发生变化时，etcd会快速的通知k8s相关组件。（第三方组件）它有可替换方案。Consul、zookeeper 6. Pod: k8s集群的最小组成单位。一个Pod内，可以运行一个或多个容器。大多数情况下，一个Pod内只有一个Container容器。 7. Flanner：是k8s集群网络，可以保证Pod的跨主机通信。也有替换方案。 12[root@master ~]# kubectl get pod --all-namespaces//查看pod信息 12[root@master ~]# kubectl get pod --all-namespaces -o wide//显示pod的节点信息 Node节点 Kubelet[节点上的Pod管家]：它是Node的agent(代理)，当Scheduler确定某 个Node上运行Pod之后，会将Pod的具体配置信息发送给该节点的kubelet,kubelet会根据这些信息创建和运行容器，并向Master报告运行状态。 负责Node节点上pod的创建、修改、监控、删除等全生命周期的管理 定时上报本Node的状态信息给API Server。 kubelet是Master API Server和Minion之间的桥梁，接收Master API Server分配给它的commands和work，与持久性键值存储etcd、file、server和http进行交互，读取配置信息。 具体的工作如下： 设置容器的环境变量、给容器绑定Volume、给容器绑定Port、根据指定的Pod运行一个单一容器、给指定的Pod创建network 容器。 同步Pod的状态、同步Pod的状态、从cAdvisor获取Container info、 pod info、 root info、 machine info。 在容器中运行命令、杀死容器、删除Pod的所有容器。 **kube-proxy[负载均衡、路由转发]:**负责将访问service的TCP/UDP数据流转发到后端的容器。如果有多个副本，kube-proxy会实现负载均衡。 Proxy是为了解决外部网络能够访问跨机器集群中容器提供的应用服务而设计的，运行在每个Node上。Proxy提供TCP/UDP sockets的proxy，每创建一种Service，Proxy主要从etcd获取Services和Endpoints的配置信息（也可以从file获取），然后根据配置信息在Minion上启动一个Proxy的进程并监听相应的服务端口，当外部请求发生时，Proxy会根据Load Balancer将请求分发到后端正确的容器处理。 Proxy不但解决了同一主宿机相同服务端口冲突的问题，还提供了Service转发服务端口对外提供服务的能力，Proxy后端使用了随机、轮循负载均衡算法。 范例 分析各个组件的作用以及架构工作流程: 1) kubectl发送部署 请求到API server 2) APIserver通知Controller Manager创建一个Deployment资源。 3) Scheduler执行调度任务,将两个副本Pod分发到node01和node02. 上。 4) node01和node02, 上的kubelet在各自节点上创建并运行Pod。 补充 1.应用的配置和当前的状态信息保存在etcd中，执行kubectl get pod时API server会从etcd中读取这些数据。 2.flannel会为每个Pod分配一个IP。 但此时没有创建Service资源，目前kube-proxy还没有参与进来。 运行一个例子（创建一个deployment资源对象&lt;pod控制器&gt;） 12[root@master ~]# kubectl run test-web --image=httpd --replicas=2//创建一个deployment资源对象。 运行完成之后，如果有镜像可直接开启，没有的话需要等待一会儿，node节点要在docker hup上下载 查看一下 1[root@master ~]# kubectl get deployments.或 kubectl get deploy 1[root@master ~]# kubectl get pod 12[root@master ~]# kubectl get pod -o wide//显示pod的节点信息 如果，node节点没有运行test-web服务，需要在节点上重启一下 如果删除一个pod 1[root@master ~]# kubectl delete pod test-web-5b56bdff65-2njqf 查看一下 1[root@master ~]# kubectl get pod -o wide 现在发现容器还存在，因为控制器会自动发现，一旦与之前执行的命令有误差，他会自动补全。 https://blog.csdn.net/gongxsh00/article/details/79932136","path":"posts/cd85.html","date":"05-01","excerpt":"","tags":[]}],"categories":[],"tags":[]}