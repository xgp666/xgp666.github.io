{"meta":{"title":"Xgp & Blog","subtitle":"Today is still beautiful","description":"Small steel gun article","author":"Wu Shao Dong","url":"https://wsdlxgp.top","root":"/"},"pages":[{"title":"可爱的我","text":"","path":"about/index.html","date":"05-30","excerpt":""},{"title":"网站感想","text":"","path":"about/site.html","date":"05-30","excerpt":""},{"title":"categories","text":"","path":"categories/index.html","date":"03-11","excerpt":""},{"title":"tags","text":"","path":"tags/index.html","date":"03-11","excerpt":""},{"title":"contact","text":"","path":"contact/index.html","date":"03-30","excerpt":""},{"title":"","text":"Theme NexT Canvas Nest canvas-nest.js for NexT. Install Step 1 → Go to Hexo dir Change dir to Hexo directory. There must be scaffolds, source, themes and other directories: 123$ cd hexo$ lsscaffolds source themes _config.yml package.json Step 2 → Create footer.swig Create a file named footer.swig in hexo/source/_data directory (create _data directory if it does not exist). Edit this file and add the following content: 1&lt;script color=\"0,0,255\" opacity=\"0.5\" zIndex=\"-1\" count=\"99\" src=\"https://cdn.jsdelivr.net/npm/canvas-nest.js@1/dist/canvas-nest.js\"&gt;&lt;/script&gt; You can customize these options. Step 3 → Set it up In the NexT _config.yml, uncomment footer under the custom_file_path section. 12345678910111213# Define custom file paths.# Create your custom files in site directory `source/_data` and uncomment needed files below.custom_file_path: #head: source/_data/head.swig #header: source/_data/header.swig #sidebar: source/_data/sidebar.swig #postMeta: source/_data/post-meta.swig #postBodyEnd: source/_data/post-body-end.swig footer: source/_data/footer.swig #bodyEnd: source/_data/body-end.swig #variable: source/_data/variables.styl #mixin: source/_data/mixins.styl #style: source/_data/styles.styl","path":"lib/canvas-nest/README.html","date":"03-28","excerpt":""},{"title":"","text":"!function(){ var userAgentInfo = navigator.userAgent; var Agents = [\"iPad\", \"iPhone\", \"Android\", \"SymbianOS\", \"Windows Phone\", \"iPod\", \"webOS\", \"BlackBerry\", \"IEMobile\"]; for (var v = 0; v < Agents.length; v++) { if (userAgentInfo.indexOf(Agents[v]) > 0) { return; } } function o(w,v,i){return w.getAttribute(v)||i}function j(i){return document.getElementsByTagName(i)}function l(){var i=j(\"script\"),w=i.length,v=i[w-1];return{l:w,z:o(v,\"zIndex\",-1),o:o(v,\"opacity\",0.5),c:o(v,\"color\",\"0,0,0\"),n:o(v,\"count\",99)}}function k(){r=u.width=window.innerWidth||document.documentElement.clientWidth||document.body.clientWidth,n=u.height=window.innerHeight||document.documentElement.clientHeight||document.body.clientHeight}function b(){e.clearRect(0,0,r,n);var w=[f].concat(t);var x,v,A,B,z,y;t.forEach(function(i){i.x+=i.xa,i.y+=i.ya,i.xa*=i.x>r||i.xn||i.y","path":"lib/canvas-nest/canvas-nest-nomobile.min.js","date":"03-28","excerpt":""},{"title":"","text":"!function(){function o(w,v,i){return w.getAttribute(v)||i}function j(i){return document.getElementsByTagName(i)}function l(){var i=j(\"script\"),w=i.length,v=i[w-1];return{l:w,z:o(v,\"zIndex\",-1),o:o(v,\"opacity\",0.5),c:o(v,\"color\",\"0,0,0\"),n:o(v,\"count\",99)}}function k(){r=u.width=window.innerWidth||document.documentElement.clientWidth||document.body.clientWidth,n=u.height=window.innerHeight||document.documentElement.clientHeight||document.body.clientHeight}function b(){e.clearRect(0,0,r,n);var w=[f].concat(t);var x,v,A,B,z,y;t.forEach(function(i){i.x+=i.xa,i.y+=i.ya,i.xa*=i.x>r||i.xn||i.y","path":"lib/canvas-nest/canvas-nest.min.js","date":"03-28","excerpt":""}],"posts":[{"title":"企业级私有仓库镜像仓库Harbor","text":"企业级私有仓库镜像仓库Harbor 12curl -L https://github.com/docker/compose/releases/download/1.25.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-composechmod +x /usr/local/bin/docker-compose 在网上下载docker-compose工具。** https://github.com/docker/compose/releases/tag/1.25.1-rc1 123[root@docker02 ~]# tar -zxf docker-compose.tar.gz -C /usr/local/bin///解压到命令目录[root@docker02 ~]# chmod +x /usr/local/bin/docker-compose 12[root@docker02 ~]# yum -y install yum-utils device-mapper-persistent-data lvm2//安装依赖包 123[root@docker02 ~]# docker-compose -v//查看版本信息docker-compose version 1.24.0, build 0aa59064 在网上下载harbor并安装。 https://github.com/goharbor/harbor/releases 12[root@docker02 ~]# tar -zxf harbor-offline-installer-v1.7.4.tgz -C /usr/local///导入harbor离线安装包，解压到/usr/ 修改harbor配置文件，并执行自带的安装脚本 12[root@docker02 ~]# cd /usr/local/harbor/[root@docker02 harbor]#ls 12345[root@docker02 harbor]# vim harbor.cfg hostname = 192.168.1.13 #13 改为本机IP地址harbor_admin_password = Harbor12345 #harbor密码[root@docker02 harbor]# ./install.sh//执行一下自带的安装脚本 在浏览器登陆一下harbor http://192.168.1.13:80 用户名：admin，密码：Harbor12345 创建一个项目 修改docker配置文件，连接私有仓库 12345678[root@docker02 harbor]# vim /usr/lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.13 #13行添加[root@docker02 harbor]# systemctl daemon-reload [root@docker02 harbor]# systemctl restart docker//重启docker 12[root@docker02 harbor]# docker ps//发现运行的容器少了很多 12[root@docker02 harbor]# docker-compose start//启动harker的文件中的容器 登陆harbor 12[root@docker02 harbor]# docker login -u admin -p Harbor12345 192.168.1.13//登陆harbor 上传镜像到仓库 1234[root@docker02 harbor]# docker tag centos:7 192.168.1.13/xgp/centos:7//修改标签[root@docker02 harbor]# docker push 192.168.1.13/xgp/centos:7 //上传镜像 第二台加入仓库，测试下载 12345678[root@docker02 harbor]# vim /usr/lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.13 #13行添加[root@docker02 harbor]# systemctl daemon-reload [root@docker02 harbor]# systemctl restart docker//重启docker 登陆harbor 12[root@docker02 harbor]# docker login -u admin -p Harbor12345 192.168.1.13//登陆harbor 下载刚刚上传的镜像 1[root@docker01 xxx]# docker pull 192.168.1.13/xgp/centos:7 12[root@docker01 xxx]# docker images//查看本地镜像 下载成功","path":"posts/7597.html","date":"06-06","excerpt":"","tags":[]},{"title":"prometheus监控","text":"配置AlertManager AlertManager：用来接收prometheus发送来的报警信息，并且执行设置好的报警方式、报警内容。 下载镜像 12[root@docker01 ~]# docker pull alertmanager//下载alertmanager镜像 基于alertmanager运行一台容器 1[root@docker01 ~]# docker run -d --name alertmanager -p 9093:9093 prom/alertmanager:latest 配置路由转发 12[root@docker01 ~]# echo net.ipv4.ip_forward = 1 &gt;&gt; /etc/sysctl.conf [root@docker01 ~]# sysctl -p 在部署alertmanager之前，我们需要对它的配置文件进行修改,所以我们先运行一个容器，先将其配置文件拷贝出来。 12[root@docker01 ~]# docker cp alertmanager:/etc/alertmanager/alertmanager.yml .///拷贝alertmanager的配置文件到本地 修改alertmanager的配置文件 配置文件简单介绍 AlertManager：用来接收Prometheus发送的报警信息，并且执行设置好的报警方式，报警内容。 AlertManager.yml配置文件： global：全局配置，包括报警解决后的超时时间、SMTP相关配置、各种渠道通知的API地址等消息。 route：用来设置报警的分发策略。 receivers：配置报警信息接收者信息。 inhibit_rules：抑制规则配置，当存在与另一个匹配的报警时，抑制规则将禁用用于有匹配的警报。 修改配置文件 123456789101112131415161718192021222324252627[root@docker01 ~]# vim alertmanager.yml //修改alertmanager配置文件global: resolve_timeout: 5m smtp_from: '2877364346@qq.com' #自己邮箱地址 smtp_smarthost: 'smtp.qq.com:465' #qq的邮箱地址及端口 smtp_auth_username: '2877364346@qq.com' smtp_auth_password: 'osjppnjkbuhcdfff' #需要在qq邮箱获取授权码 smtp_require_tls: false smtp_hello: 'qq.com'route: group_by: ['alertname'] group_wait: 5s group_interval: 5s repeat_interval: 5m receiver: 'email' #接收者改为邮箱receivers:- name: 'email' email_configs: - to: '2877364346@qq.com' send_resolved: true inhibit_rules: - source_match: severity: 'critical' target_match: severity: 'warning' equal: ['alertname', 'dev', 'instance'] 12345678910111213141516171819202122232425global: resolve_timeout: 5m smtp_from: '2877364346@qq.com' smtp_smarthost: 'smtp.qq.com:465' smtp_auth_username: '2877364346@qq.com' smtp_auth_password: 'osjppnjkbuhcdfff' smtp_require_tls: false smtp_hello: 'qq.com'route: group_by: ['alertname'] group_wait: 5s group_interval: 5s repeat_interval: 5m receiver: 'email'receivers:- name: 'email' email_configs: - to: '2877364346@qq.com' send_resolved: trueinhibit_rules: - source_match: severity: 'critical' target_match: severity: 'warning' equal: ['alertname', 'dev', 'instance'] 重新运行 alertmanager 容器 1234[root@docker01 ~]# docker rm -f alertmanager//删除alertmanager容器[root@docker01 ~]# docker run -d --name alertmanager -v /root/alertmanager.yml:/etc/alertmanager/alertmanager.yml -p 9093:9093 prom/alertmanager:latest //运行一台新的alertmanager容器，记得挂载配置文件 Prometheus配置和alertmanager报警规则 创建存放规则的目录 123[root@docker01 ~]# mkdir -p prometheus/rules//创建规则目录[root@docker01 ~]# cd prometheus/rules/ 编写规则 123456789101112[root@docker01 rules]# vim node-up.rules groups:- name: node-up rules: - alert: node-up expr: up&#123;job=\"prometheus\"&#125; == 0 #&#123;job=\"prometheus\"&#125;中的prometheus需要和prometheus配置文件23行的相同 for: 15s labels: severity: 1 team: node annotations: summary: \"&#123;&#123; $labels.instance &#125;&#125; 已停止运行超过 15s！\" 修改 prometheus配置文件 123456789101112[root@docker01 ~]# vim prometheus.yml # Alertmanager configuration #7alerting: alertmanagers: - static_configs: - targets: - 192.168.1.11:9093 #去注释修改# Load rules once and periodically evaluate them according to the global 'evaluation_interval'. #14行rule_files: - \"/usr/local/prometheus/rules/*.rules\" #添加（这个路径是prometheus容器内的路径） 重新运行prometheus 容器 1234[root@docker01 ~]# docker rm -f prometheus //删除prometheus容器[root@docker01 ~]# docker run -d -p 9090:9090 --name prometheus --net=host -v /root/prometheus.yml:/etc/prometheus/prometheus.yml -v /root/prometheus/rules/node-up.rules:/usr/local/prometheus/rules/node-up.rules prom/prometheus//运行一台新的alertmanager容器，记得挂载规则文件 浏览器验证一下http://192.168.1.11:9090/rules 挂起docker02 会收到邮件","path":"posts/fe12.html","date":"06-06","excerpt":"","tags":[]},{"title":"prometheus","text":"Prometheus（普罗米修斯） 实验环境 docker01 docker02 docker03 192.168.1.11 192.168.1.13 192.168.1.20 NodeEXporter NodeEXporter NodeEXporter cAdvisor cAdvisor cAdvisor Prometheus Server 空 空 Grafana 空 空 全部关闭防火墙，禁用selinux 需要部署的组件： Prometheus Server:普罗米修斯的主服务器。 Prometheus是一个开源的服务监控系统，它通过HTTP协议从远程的机器收集数据并存储在本地的时序数据库上。 多维数据模型（时序列数据由metric名和一组key/value组成） 在多维度上灵活的查询语言(PromQl) 不依赖分布式存储，单主节点工作. 通过基于HTTP的pull方式采集时序数据 可以通过push gateway进行时序列数据推送(pushing) 可以通过服务发现或者静态配置去获取要采集的目标服务器 多种可视化图表及仪表盘支持 Prometheus通过安装在远程机器上的exporter来收集监控数据，后面我们将使用到node_exporter收集系统数据。 NodeEXporter:负责收集Host硬件信息和操作系统信息。 cAdvisor:负责收集Host.上运行的容器信息。 Grafana:负责展示普罗米修斯监控界面。 Grafana 是一个开箱即用的可视化工具，具有功能齐全的度量仪表盘和图形编辑器，有灵活丰富的图形化选项，可以混合多种风格，支持多个数据源特点。 这些可以直接docker pull下载镜像（现在是本地导入镜像） 本地上传镜像 docker01 1[09:05:42][docker01$ docker load -i node-exporter.tar &amp;&amp; docker load -i mycadvisor.tar &amp;&amp; docker load -i prometheus.tar &amp;&amp; docker load -i grafana.tar docker02和docker03 1[09:05:22]docker03]$ docker load -i node-exporter.tar &amp;&amp; docker load -i mycadvisor.tar 各主机部署 1) 3个节点，全部部署node-EXporter,和cAdvisor. 部署安装node-EXporter收集节点硬件和操作系统信息。 12[09:21:03[docker01]$ docker run -d -p 9100:9100 -v /proc:/host/proc -v /sys:/host/sys -v /:/rootfs --net=host prom/node-exporter --path.procfs /host/proc --path.sysfs /host/sys --collector.filesystem.ignored-mount-points \"^/(sys|proc|dev|host|etc)($|/)\"//部署node-EXporter,收集硬件和系统信息。 PS: 注意，这里使用了–net=host， 这样Prometheus Server可以直接与Node- EXporter通信。 验证:打开浏览器验证结果。http://192.168.1.11:9100/，http://192.168.1.13:9100/，http://192.168.1.20:9100/ 部署安装cAdvisor,收集节点容器信息。 1[09:39:10[docker01]$ docker run -v /:/rootfs:ro -v /var/run:/var/run/:rw -v /sys:/sys:ro -v /var/lib/docker:/var/lib/docker:ro --detach=true --name=cadvisor --net=host google/cadvisor 验证:打开浏览器验证结果。http://192.168.1.11:8080，http://192.168.1.13:8080，http://192.168.1.20:8080 2)在docker01上部署Prometheus Server服务。 在部署prometheus之前，我们需要对它的配置文件进行修改,所以我们先运行一个容器，先将其配置文件拷贝出来。 123409:51:22][docker01]$ docker run -d -p 9090:9090 --name prometheus --net=host prom/prometheus//打开一台Prometheus[09:51:00[docker01]$ docker cp prometheus:/etc/prometheus/prometheus.yml .///拷贝Prometheus的配置文件到本地 修改Prometheus的配置文件，添加监听端口（29行） 123[09:55:53][docker01][~]$ vim prometheus.yml //修改配置文件这里指定了prometheus的监控项，包括它也会监控自己手机到的数据。- targets: ['localhost:9090','localhost:8080','localhost:9100','192.168.1.13:8080','192.168.1.13:9100','192.168.1.20:8080','192.168.1.20:9100'] 重新运行prometheus容器 1234[10:00:27][docker01][~]$ docker rm -f prometheus //删除 prometheus容器[10:02:45][docker01][~]$ docker run -d -p 9090:9090 --name prometheus --net=host -v /root/prometheus.yml:/etc/prometheus/prometheus.yml prom/prometheus//运行一台新的 prometheus容器 浏览器访问，验证：http://192.168.1.11:9090/graph ps：这里能够查看到我们各个监控项。 如果现在挂起一台虚拟机（测试完之后继续运行） 3)在docker01.上,部署grafana服务,用来展示prometheus收集到的数据。 1234[root@docker01 ~]# mkdir grafana-storage//创建收集信息的目录[root@docker01 ~]# chmod 777 grafana-storage///给予777权限 1[root@docker01 ~]# docker run -d -p 3000:3000 --name grafana -v /root/grafana-storage:/var/lib/grafana -e \"GF_SECURITY_ADMIN_PASSWORD=123.com\" grafana/grafana **浏览器访问验证：**http://192.168.1.11:3000/login （&lt;默认&gt;用户名：admin，密码：123.com） 添加数据源 PS:看到这个提示， 说明prometheus和grafana服务的是 正常连接的。 此时，虽然grafana收集到了数据，但怎么显示它,仍然是个问题，grafana支持自定 义显示信息,不过要自定义起来非常麻烦，不过好在，grafana官方为我们提供了- -些模板，来供我们使用。 **grafana官网:**https://grafana.com/docs/grafana/latest/ 选中一款模板，然后，我们有2种方式可以套用这个模板。 第一种方式：通过JSON文件使用模板。 下载完成之后，来到grafana控制台 第二种导入模板的方式:** 可以直接通过模板的ID号。 //这个id不好用换成8321了 复制模板id之后，来到grafana控制台 排错思路 防火墙是否关闭，selinux是否禁用 主机名称是否更改 镜像是否正常 各服务启动时挂载目录是否正确 grafana服务，是否创建所需目录，目录是否有权限 Prometheus服务是否修改配置文件 总结 恭喜！您已经设置了Prometheus服务器，Node Exporter和Grafana 等所有这些都可以使用的Docker。尽管这些目前都在同一台机器上运行，但这仅用于演示目的。在生产设置中，通常会在每台受监控的计算机上运行节点导出器，多个Prometheus服务器（根据组织的需要），以及单个Grafana服务器来绘制来自这些服务器的数据。","path":"posts/d356.html","date":"06-06","excerpt":"","tags":[]},{"title":"nginx+docker+nfs部署","text":"nginx（两台都是） 安装nginx 12345[root@nginx01 ~]# tar zxf nginx-1.14.0.tar.gz //解压nginx安装包[root@nginx01 ~]# cd nginx-1.14.0/[root@nginx01 nginx-1.14.0]# yum -y install openssl-devel pcre-devel zlib-devel//安装nginx依赖包 12[root@nginx01 nginx-1.14.0]# ./configure --prefix=/usr/local/nginx1.14 --with-http_dav_module --with-http_stub_status_module --with-http_addition_module --with-http_sub_module --with-http_flv_module --with-http_mp4_module --with-pcre --with-http_ssl_module --with-http_gzip_static_module --user=nginx --group=nginx &amp;&amp; make &amp;&amp; make install//编译安装nginx 12345678[root@nginx01 nginx-1.14.0]# useradd nginx -s /sbin/nologin -M//创建所需用户[root@nginx01 nginx-1.14.0]# ln -s /usr/local/nginx1.14/sbin/nginx /usr/local/sbin///链接命令[root@nginx01 nginx-1.14.0]# nginx //开启nginx[root@nginx01 nginx-1.14.0]# netstat -anpt | grep nginx//查看nginx是否开启 部署nginx 12[root@nginx01 ~]# cd /usr/local/nginx1.14/conf/[root@nginx01 conf]# vim nginx.conf ​ http模块加 1234upstream backend &#123;server 192.168.1.11:90 weight=1 max_fails=2 fail_timeout=10s;server 192.168.1.13:90 weight=1 max_fails=2 fail_timeout=10s;&#125; location / { # root html; # index index.html index.htm; proxy_pass http://backend; #添加 } 高可用环境 安装keepalived [root@nginx02 nginx-1.14.0]# yum -y install keepalived 配置keepalived 修改主和备nginx服务器上的keepalived 配置文件 /etc/keepalived/keepalived.conf 文件 主nginx 修改主nginx下/etc/keepalived/keepalived.conf文件 123456789101112131415161718! Configuration File for keepalivedglobal_defs &#123; router_id LVS_DEVEL&#125; vrrp_instance VI_1 &#123; state MASTER interface ens33 virtual_router_id 51 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.1.40 &#125;&#125; 备nginx 修改备nginx下 /etc/keepalived /keepalived.conf文件 配置备nginx时需要注意：需要修改state为BACKUP , priority比MASTER低，virtual_router_id和master的值一致 12345678910111213141516171819! Configuration File for keepalivedglobal_defs &#123; router_id TWO&#125;vrrp_instance VI_1 &#123; state BACKUP interface ens33 virtual_router_id 1 priority 99 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.1.40 &#125;&#125; 测试（在做完docker的时候） 主备nginx都启动keepalived 1systemctl start keepalived 12[root@nginx01 conf]# curl 192.168.1.40wsd666 nfs（两台都是) nfs操作 12345678910[root@localhost ~]# yum -y install nfs-utils//下载nfs服务[root@nfs ~]# mkdir /database//创建共享目录[root@nfs02 ~]# chmod 777 /database///设置权限[root@nfs ~]# vim /etc/exports//设置权限如下/database *(rw,sync,no_root_squash) 开启各项服务 1234[root@nfs ~]# systemctl start rpcbind[root@nfs ~]# systemctl enable rpcbind[root@nfs ~]# systemctl start nfs-server[root@nfs ~]# systemctl enable nfs-server docker01和docker02测试nfs 1234567891011121314[root@nfs01 ~]# vim /etc/rsyncd.conf //建立rsync配置文件uid = nobodygid = nobodyuse chroot = yesaddress = 192.168.1.30port 873log file = /var/log/rsyncd.logpid file = /var/run/rsyncd.pidhosts allow = 192.168.1.0/24[wwwroot]path = /databaseread only = nodont compress = *.gz *.bz2 *.rar *.zip 123456[root@nfs01 ~]# mkdir /database//创建共享目录[root@nfs01 ~]# rsync --daemon//启动rsync[root@nfs01 ~]# netstat -anpt | grep rsync//查看端口 如果需要重启rsync服务，需要： 12345[root@localhost ~]# kill $(cat /var/run/rsyncd.pid)//停止服务[root@localhost ~]# rsync --daemon//启动服务[root@localhost ~]# kill -9 $(cat /var/run/rsyncd.pid) 或者直接使用“netstat -anpt | grep rsync”命令查出进程号，使用“kill 进程号”一样。 使用第一种方法停止rsync服务必须删除存放rsync服务进程的文件： 1[root@localhost ~]# rm -rf /var/run/rsyncd.pid 使用rsync备份工具 配置好rsync同步源服务器之后，客户端就可以使用rsync工具来执行远程同步了。 与rsync主机同步 123456789101112131415rsync命令的选项：-r：递归模式，包含目录及子目录中所有文件-l：对于符号链接文件仍然复制为符号链接文件-p：保留文件的权限标记-t：保留文件的时间标记-g：保留文件的属组标记（仅超级用户使用）-o：保留文件的属主标记（仅超级用户使用）-D：保留设备文件及其他特殊文件-a：归档模式，递归并保留对象属性，等同于 -rlptgoD-v：显示同步过程的详细（verbose）信息-z：在传输文件时进行压缩（compress）-H：保留硬连接文件-A：保留ACL属性信息--delete：删除目标位置有而原始位置没有的文件--checksum：根据对象的校验和来决定是否跳过文件 rsync是一款快速增量备份工具，支持： （1）本地复制； （2）与其他SSH同步； （3）与rsync主机同步。 手动与rsync主机同步 123[root@localhost ~]# rsync -avz 192.168.1.1::wwwroot /root或者[root@localhost ~]# rsync -avz rsync://192.168.1.1/wwwroot /root 123[root@nfs01 database]# vim index.htmlxgp666//创建测试目录 配置inotify+rsync实时同步（两台都是） (1)、软件安装 12rpm -q rsync //查询rsync是否安装，一般为系统自带安装yum install rsync -y //若没有安装，使用yum安装 安装inotify软件包 123[root@nfs02 ~]# tar zxf inotify-tools-3.14.tar.gz [root@nfs02 ~]# cd inotify-tools-3.14/[root@nfs02 inotify-tools-3.14]# ./configure &amp;&amp; make &amp;&amp; make install （2）调整inotify内核参数 1234567[root@nfs02 ~]# vim /etc/sysctl.conffs.inotify.max_queued_events = 16384fs.inotify.max_user_instances = 1024fs.inotify.max_user_watches = 1048576[root@nfs02 ~]# sysctl -p//生效 (3) 编写触发式同步脚本 123456789#!/bin/bashA=\"inotifywait -mrq -e modify,move,create,delete /database/\"B=\"rsync -avz /database/ 192.168.1.40::wwwroot\"$A | while read DIRECTORY EVENT FILEdo if [ $(pgrep rsync | wc -l) -gt 0 ] ; then $B fidone 此处需要注意，在两台服务器需要同步的目录之间，也需要将目录权限放到最大，避免因目录本身权限报错。 1[root@nfs01 inotify-tools-3.14]# chmod +x /opt/ino.sh 设置脚本开机自启 123[root@nfs01 database]# vim /etc/rc.d/rc.local /opt/ino.sh &amp;/usr/bin/rsync --daemon 源服务器端测试 执行脚本后，当前终端会变成实时监控界面，需要重新打开终端操作。 在源服务器端共享模块目录下进行文件操作，然后去备份服务器下，可观察到文件已经被实时同步。 docker(两台都是) 123[root@docker01 ~]# docker pull nginx[root@docker01 ~]# mkdir -p /www //创建挂载目录 nfs创建好之后docker上挂载目录 1[root@docker01 ~]# mount -t nfs 192.168.1.30:/database /www 1[root@docker01 ~]# docker run -itd --name nginx -p 90:80 -v /www/index.html:/usr/share/nginx/html/index.html nginx:latest","path":"posts/24f3.html","date":"06-06","excerpt":"","tags":[]},{"title":"docker数据持久化","text":"数据持久化 Storage Driver 数据存储方式 Centos7版本的docker，Storage Driver（数据存储方式）为：overlay2 ，Backing Filesystem（文件系统类型）: xfs Data Volume 持久化存储：本质上是DockerHost文件系统中的目录或文件，能够直接被Mount到容器的文件系统中。在运行容器时，可通过-v实现。 特点： 1. Data Volume是目录或文件，不能是没有格式化的磁盘（块设备）。 2. 容器可以读写volume中的数据。 3. Volume数据可以永久保存，即使使用它的容器已经被销毁。 4.默认挂载到容器内的文件，容器是有读写权限。可以在运行容器是-v 后边加“:ro”限制容器的写入权限 5并且还可以挂载单独的文件到容器内部，一般他的使用场景是：如果不想对整个目录进行覆盖，而只希望添加某个文件，就可以使用挂载单个文件。 6.删除容器的操作，默认不会对dockerhost上的源文件操作，如果想要在删除容器时把源文件也删除，可以在删除容器时添加-v选项（一般不推荐使用这种方式，因为文件有可能被其他容器使用）&quot;Data Volume不支持&quot; 注意：dockerhost上需要被挂载的源文件或目录，必须是已经存在，否则，会被当作一个目录挂载到容器中。 Docker Manager Volume 123[root@docker01 ~]# docker run -itd --name t1 -P -v /usr/share/nginx/html nginx:latest[root@docker01 ~]# docker ps 1[root@docker01 ~]# docker inspect t1 12345[root@docker01_data]#cd /var/lib/docker/volumes/17c50a065a6b10ccd01ca1ce8091fdf6282dc9dcb77a0f6695906257ecc03a63/_data[root@docker01 _data]# echo \"this is a testfile\" &gt; index.html[root@docker01 _data]# docker ps 1[root@docker01 _data]# curl 127.0.0.1:32777 1[root@docker01 _data]# docker volume ls 容器与容器的数据共享 Volume container：给其他容器提供volume存储卷的容器。并且它可以提供bind mount，也可以提供docker manager volume。 Volume container：给其他容器提供volume存储卷的容器。并且它可以提供bind mount，也可以提供docker manager volume。 创建一个vc_data容器 123[root@docker01 ~]# docker create --name vc_data -v ~/html:/usr/share/nginx/html -v /other/useful/tools busybox[root@docker01 ~]# docker inspect vc_data 123[root@docker01 ~]# docker run -itd --name t3 -P --volumes-from vc_data nginx:latest[root@docker01 ~]# docker ps 1[root@docker01 ~]# curl 127.0.0.1:32779 123456[root@docker01 ~]# mkdir htdocs//创建测试目录[root@docker01 ~]# cd htdocs/[root@docker01 htdocs]# echo The version one &gt; index.html[root@docker01 htdocs]# docker run -itd --name web1 -P -v ~/htdocs:/usr/local/apache2/htdocs httpd:latest [root@docker01 htdocs]# docker ps ![img](file:///C:\\Users\\huawei\\AppData\\Roaming\\Tencent\\Users\\2877364346\\QQ\\WinTemp\\RichOle\\26LA}X1ZTD80SPWLY_TNF.png) 1[root@docker01 htdocs]# curl 127.0.0.1:32768 12[root@docker01 htdocs]# echo The version TWO &gt; index.html [root@docker01 htdocs]# curl 127.0.0.1:32768 12[root@docker01 htdocs]# rm -f index.html [root@docker01 htdocs]# curl 127.0.0.1:32768 1234567891011121314151617181920[root@docker01 ~]# mkdir htdocs///创建一个测试目录[root@docker01 htdocs]# vim index.html//创建测试网页&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;[root@docker01 ~]# docker create --name vc_data -v ~/html:/usr/share/nginx/html -v /other/useful/tools busybox//创建一个共享容器[root@docker01 htdocs]# vim Dockerfile//编写DockerfileFROM busyboxADD index.html /usr/local/apache2/htdocs/index.htmlVOLUME /usr/local/apache/htdocs[root@docker01 htdocs]# docker build -t databack .//通过当前Dockfile创建一个镜像[root@docker01 htdocs]# docker create --name vc-new databack:latest//通过databack创建一个容器 12[root@docker01 htdocs]# docker inspect vc-new//查看vc-new的详细信息 1234 [root@docker01 htdocs]# docker run -itd --name new-web1 -P --volumes-from vc-new httpd:latest //通过volume卷和httpd运行一个容器[root@docker01 htdocs]# docker ps//查看容器信息 12[root@docker01 htdocs]# curl 127.0.0.1:32773//测试访问一下网页 1234[root@docker01 htdocs]# echo 12345 &gt;index.html //更改网页内容[root@docker01 htdocs]# curl 127.0.0.1:32773//测试访问网页还是原来的内容 容器的跨主机数据共享 实验环境 docker01 docker02 docker03 httpd httpd nfs 要求：docker01和docker02的主目录，是一样的。 准备工作 123[root@localhost ~]# hostnamectl set-hostname nfs[root@localhost ~]# hostnamectl set-hostname docker01[root@localhost ~]# hostnamectl set-hostname docker02 nfs操作 123456789[root@localhost ~]# yum -y install nfs-utils//下载nfs服务[root@nfs ~]# mkdir /datashare//创建共享目录[root@nfs ~]# vim /etc/exports//设置权限如下/datashare *(rw,sync,no_root_squash) 开启各项服务 1234[root@nfs ~]# systemctl start rpcbind[root@nfs ~]# systemctl enable rpcbind[root@nfs ~]# systemctl start nfs-server[root@nfs ~]# systemctl enable nfs-server docker01和docker02测试nfs 12[root@docker01 htdocs]# showmount -e 192.168.1.20[root@docker02 htdocs]# showmount -e 192.168.1.20 docker01的操作 12345[root@docker02 ~]# mkdir /xxx[root@docker01 ~]# mount -t nfs 192.168.1.10:/datashare /xxx//挂载nfs上的共享目录[root@docker01 ~]# mount | tail -1//查看是否挂载 nfs创建测试文件 12345678[root@nfs ~]# cd datashare/[root@nfs datashare]# vim index.html&lt;div id=\"datetime\"&gt; &lt;script&gt; setInterval(\"document.getElementById('datetime').innerHTML=new Date().toLocaleString();\", 1000); &lt;/script&gt;&lt;/div&gt;xgp666 docker01查看一下 docker02的操作与docker01上一样 这里先不考虑将代码写入镜像，先以这种方式，分别在docker01和docker02部署httpd服务 12[root@docker01 ~]# docker run -itd --name bdqn-web1 -P -v /xxx/:/usr/local/apache2/htdocs httpd:latest [root@docker02 ~]# docker run -itd --name bdqn-web2 -P -v /xxx/:/usr/local/apache2/htdocs httpd:latest 123456[root@docker01 ~]# docker ps //查看端口0.0.0.0:32775-&gt;80/tcp bdqn-web[root@docker02 ~]# docker ps//查看端口0.0.0.0:32769-&gt;80/tcp bdqn-web2 此时，用浏览器访问,两个WEB服务的主界面是一样。但如果，NFS服务器上的源文件丢失, 则两个web服务都会异常。 想办法将元数据写入镜像内，在基于镜像创建一个vc_data容器，这里因为没有接触到docker-compose和docker-swarm等docker编排工具，所以需手动创建镜像！ nfs操作 12[root@nfs datashare]# echo xgp666 &gt; index.html //更改测试文件 docker02操作 1234567[root@docker02 ~]# cd /xxx/[root@docker02 xxx]# vim Dockerfile//编写Dockerfile[root@docker02 xxx]# cat Dockerfile FROM busyboxADD index.html /usr/local/apache2/htdocs/index.htmlVOLUME /usr/local/apache2/htdocs 创建镜像并运行一个容器 1234[root@docker02 xxx]# docker build -t back_data .//基于Dockerfile创建镜像[root@docker02 xxx]# docker create --name back_container1 back_data:latest //基于刚刚创建的镜像创建容器 运行容器，并导出镜像 1234[root@docker02 xxx]# docker run -itd --name bdqn-web3 -P --volumes-from back_container1 httpd:latest //运行一台容器[root@docker02 xxx]# docker save &gt; back_data.tar back_data:latest//导出镜像，因为是在共享目录所以docker01也可以看到 docker01 123456[root@docker01 xxx]# docker load -i back_data.tar //去共享目录，导入镜像[root@docker01 xxx]# docker create --name back_container2 back_data:latest//基于刚刚创建的镜像运行容器[root@docker01 xxx]# docker run -itd --name bdqn-web4 -P --volumes-from back_container2 httpd:latest//运行一台容器 浏览器访问 123456[root@docker01 ~]# docker ps //查看端口 0.0.0.0:32776-&gt;80/tcp bdqn-web4[root@docker02 ~]# docker ps//查看端口0.0.0.0:32770-&gt;80/tcp bdqn-web3","path":"posts/f8c7.html","date":"06-06","excerpt":"","tags":[]},{"title":"Docker实现服务发现","text":"更改时间 mv /etc/localtime/etc/localtime. bk cp /usr/share/zoneinfo/Asia/Shanghai/etc/localtime 查看端口 [root@docker01 consul]# ss -lnt Consul:分布式、高可用的，服务发现和配置服务的工具。数据中心 Rigistrator:负责收集dockerhost_上,容器服务的信息，并且发送给consul Consul-tpmplate:根据编辑好的模板，生产新的nginx配置文件，并负责重新加载nginx配置文件 Docker + Consul + registrator实现服务发现 1.实验环境 上面示意图的大概流程如下： 1、docker01主机上以二进制包的方式部署consul服务并后台运行，其身份为leader； 2、docker02、docker03以容器的方式运行consul服务，并加入到docker01的consul群集中； 3、在主机docker02、docker03上后台运行registrator容器，使其自动发现docker容器提供的服务； 4、在docker01上部署Nginx，提供反向代理服务，docker02、docker03主机上基于Nginx镜像，各运行两个web容器，提供不同的网页文件，以便测试效果； 5、在docker01上安装consul-template命令，将收集到的信息（registrator收集到容器的信息）写入template模板中，并且最终写入Nginx的配置文件中。 6、至此，实现客户端通过访问Nginx反向代理服务器（docker01），获得docker02、docker03服务器上运行的Nginx容器提供的网页文件。 注：registrator是一个自动发现docker container提供的服务，并且在后端服务注册中心（数据中心）注册服务。主要用来收集容器运行服务的信息，并且发送给consul。数据中心除了consul外，还有etcd、zookeeper等。 主机 iP地址 服务 docker01 192.168.1.11 consul+consul-template+nginx docker02 192.168.1.13 consul+registrator docker03 192.168.1.20 consul+registrator 三台主机关闭防火墙，禁用selinux，更改主机名如上所述。 （1）docker01去官网https://www.consul.io/downloads.html下载consul服务 123456[root@docker01 ~]# unzip consul_1.5.1_linux_amd64.zip //现在是本地导入压缩包，需要解压 [root@docker01 ~]# mv consul /usr/local/bin///移动服务到bin目录[root@docker01 ~]# chmod +x /usr/local/bin/consul//给予一个可执行权限 （2）启动consul 1[root@docker01 ~]# consul agent -server -bootstrap -ui -data-dir=/var/lib/consul-data -bind=192.168.1.11 -client=0.0.0.0 -node=master PS: //-bootstrap: 加入这个选项时，一般都在server单节点的时候用，自选举为leader。 参数解释： -server：添加一个服务 -bootstrap：一般在server单节点的时候使用，自选举为leader。 -data-dir：key/volume指定数据存放的目录 -ui：开启内部的web界面 -bind：指定开启服务的ip -client：指定访问的客户端 -node：在集群内部通信使用的名称，默认是主机名。 现在这个ip是外部使用 PS:开启的端口 8300 集群节点 8301 集群内部的访问 8302 跨数据中心的通信 8500 web ui界面 8600 使用dns协议查看节点信息的端口 这时，这条命令会占用终端，可以使用nohup命令让它保持后台运行。 1[root@docker01 ~]# nohup consul agent -server -bootstrap -ui -data-dir=/var/lib/consule-data -bind=192.168.1.11 -client=0.0.0.0 -node=master &amp; （3）查看consul端口的信息** 1[root@docker01 ~]# consul info （4）查看consul集群成员的信息 1[root@docker01 ~]# consul members 现在这个ip是内部使用 2. docker02，docker03，加入consul集群 这里我们采用容器的方式去运行consul服务。 （1）下载consu所需的l镜像 1[root@docker02 ~]# docker pull consul （2）基于consul镜像开启一台容器 1[root@docker02 ~]# docker run -d --name consul -p 8301:8301 -p 8301:8301/udp -p 8500:8500 -p 8600:8600 -p 8600:8600/udp --restart always progrium/consul -join 192.168.1.11 -advertise 192.168.1.13 -client 0.0.0.0 -node=node01 参数解释： -d：守护进程 –name：容器名称 –restart：容器随着docker服务一直运行 -advertise:声明本机地址 -join:声明服务端地址 -node:consul集群中的名称 （3）docker查看consul集群成员的信息 1[root@docker01 ~]# consul members （4）两台docker开启容器后，docker01查看 （5）浏览器访问http://192.168.1.11:8500 3. docker01下载部署consul-template 在https://github.com/hashicorp/consul-template上，下载consul-template 12345678910111213141516171819[root@docker01 ~]# unzip consul-template_0.19.5_linux_amd64.zip//解压安装好的consul-template包[root@docker01 ~]# mv consul-template /usr/local/bin///移动到命令目录[root@docker01 ~]# chmod +x /usr/local/bin/consul-template //给予一个可执行权限​```shell ## 4. docker02、docker03_ 上部署registrator服务&gt; **registrator是一个能自动发现docker container提供的服务,并在后端服务注册中心注册服务或取消服务的工具，后端注册中心支持conusl、etcd、 skydns2、zookeeper等。**### （1）下载registrator镜像​```shell [root@docker02 ~]# docker pull registrator//下载registrator镜像 （2）基于registrator镜像，开启一台容器 1[root@docker02 ~]# docker run -d --name registrator -v /var/run/docker.sock:/tmp/docker.sock --restart always gliderlabs/registrator consul://192.168.1.13:8500 （3）开启一台nginx容器 1[root@docker02 ~]# docker run -d —P --name nginx nginx:latest （4）浏览器查看一下http://192.168.1.11:8500/ui/dc1/nodes 5.docker01部署一个nginx服务 （1）安装开启nginx服务 安装nginx依赖包 1[root@docker01 ~]# yum -y install pcre pcre-devel openssl openssl-devel zlib zlib-devel 编译安装nginx 12[root@docker01 ~]# cd nginx-1.14.0/[root@docker01 nginx-1.14.0]# ./configure --user=nginx --group=nginx --with-http_stub_status_module --with-http_realip_module --with-pcre --with-http_ssl_module &amp;&amp; make &amp;&amp; make install 创建所需用户和链接命令目录 12[root@docker01 nginx-1.14.0]# useradd -M -s /sbin/nologin nginx[root@docker01 nginx-1.14.0]# ln -s /usr/local/nginx/sbin/* /usr/local/bin/ 检查nginx是否有问题，并开启nginx 1234[root@docker01 nginx-1.14.0]# nginx -tnginx: the configuration file /usr/local/nginx/conf/nginx.conf syntax is oknginx: configuration file /usr/local/nginx/conf/nginx.conf test is successful[root@docker01 nginx-1.14.0]# nginx PS:这里nginx作为反向代理，代理后端docker02、 docker03 上nginx的容器服务,所以我们先去docker02、docker03. 上部署一些服务， 为了方便等会看到负载的效果，所以，我们运行完成容器之后，做一个主界面内容的区分。 （2）安装完成之后，本机测试访问 1[root@docker01 nginx-1.14.0]# curl 127.0.0.1 （3）docker02和docker03部署环境 主机 服务 docker02 nginx web01，web02 docker03 nginx web03，web04 &lt;1&gt;下载nginx镜像（docker02，docker03都要） 12[root@docker02 ~]# docker pull nginx//下载nginx镜像 &lt;2&gt;docker01操作 基于nginx镜像运行上述所说的容器并设置测试页面 1234567891011web01[root@docker02 ~]# docker run -itd --name web01 -P nginx:latest[root@docker02 ~]# docker exec -it web01 /bin/bashroot@44b59d07202f:/# cd /usr/share/nginx/html/root@44b59d07202f:/usr/share/nginx/html# echo web01 &gt; index.htmlweb02[root@docker02 ~]# docker run -itd --name web02 -P nginx:latest[root@docker02 ~]# docker exec -it web02 /bin/bashroot@44b59d07202f:/# cd /usr/share/nginx/html/root@44b59d07202f:/usr/share/nginx/html# echo web02 &gt; index.html &lt;3&gt;docker02操作 基于nginx镜像运行上述所说的容器并设置测试页面 12345678910111213web03[root@docker03 ~]# docker run -itd --name web03 -P nginx:latest[root@docker03 ~]# docker exec -it web03 /bin/bashroot@fd8e8b2df136:/# cd /usr/share/nginx/html/root@fd8e8b2df136:/usr/share/nginx/html# echo web03 &gt; index.htmlroot@fd8e8b2df136:/usr/share/nginx/html# exittrueweb04[root@docker03 ~]# docker run -itd --name web04 -P nginx:latest[root@docker03 ~]# docker exec -it web04 /bin/bashroot@fd8e8b2df136:/# cd /usr/share/nginx/html/root@fd8e8b2df136:/usr/share/nginx/html# echo web04 &gt; index.htmlroot@fd8e8b2df136:/usr/share/nginx/html# exit （4）docker01更改nginx配置文件 123456[root@docker01 ~]# cd /usr/local/nginx///进入nginx配置文件目录[root@docker01 nginx]# mkdir consul//创建consul目录[root@docker01 nginx]# cd consul///进入consul目录 &lt;1&gt;创建nginx.ctmpl模板 1234567891011121314[root@docker01 consul]# vim nginx.ctmplupstream http_backend &#123; &#123;&#123;range service \"nginx\"&#125;&#125; server &#123;&#123; .Address &#125;&#125;:&#123;&#123; .Port &#125;&#125;; &#123;&#123; end &#125;&#125;&#125;server &#123; listen 8000; server_name localhost; location / &#123; proxy_pass http://http_backend; &#125;&#125; &lt;2&gt;修改nginx配置文件，通过 include 参数包含刚刚创建的文件 123[root@docker01 consul]# cd /usr/local/nginx/conf/[root@docker01 conf]# vim nginx.conf include /usr/local/nginx/consul/*.conf; #文件最后添加（要在大括号里面） &lt;3&gt; 生成一个vhost.conf配置文件，并重启nginx（会占用终端) 使用consul-template命令，根据模板生产新的配置文件，并重新加载nginx的配置文件。 1[root@docker01 conf]# consul-template -consul-addr 192.168.1.11:8500 -template \"/usr/local/nginx/consul/nginx.ctmpl:/usr/local/nginx/consul/vhost.conf:/usr/local/bin/nginx -s reload\" 这时，这条命令会占用终端，可以使用nohup命令让它保持后台运行,并重启nginx服务。 1[root@docker01 conf]# nohup consul-template -consul-addr 192.168.1.11:8500 -template \"/usr/local/nginx/consul/nginx.ctmpl:/usr/local/nginx/consul/vhost.conf:/usr/local/sbin/nginx -s reload\" &amp; 查看一下文件是否生成，里面是否有内容 123[root@docker01 ~]# cd /usr/local/nginx/consul/[root@docker01 consul]# lsnginx.ctmpl vhost.conf 1[root@docker01 consul]# cat vhost.conf 此时，应该能够看到，新生产的vhost.conf配置文件已经生效，访问本机8000端口可以得到不同容器提供的服务。 &lt;4&gt;测试访问 12[root@docker01 consul]# curl 127.0.0.1:8000web01 此时可以看到负载均衡的效果！ &lt;5&gt;如果访问不成功 查看端口8000是否开启 1[root@docker01 consul]# ss -lnt 检查nginx配置文件 123[root@docker01 consul]# nginx -tnginx: the configuration file /usr/local/nginx/conf/nginx.conf syntax is oknginx: configuration file /usr/local/nginx/conf/nginx.conf test is successful 检查自己编写的nginx配置文件 123456789101112131415[root@docker01 consul]# cd /usr/local/nginx/consul/[root@docker01 consul]# cat nginx.ctmpl upstream http_backend &#123;true&#123;&#123;range service \"nginx\"&#125;&#125;trueserver &#123;&#123; .Address &#125;&#125;:&#123;&#123; .Port &#125;&#125;;true&#123;&#123; end &#125;&#125;&#125;server &#123;truelisten 8000;trueserver_name localhost;truelocation / &#123;truetrueproxy_pass http://http_backend;true&#125;&#125; 如果nginx配置文件没问题，重启nginx 1[root@docker01 consul]# nginx -s reload &lt;6&gt;测试自动发现 docker02 创建测试容器 12345[root@docker02 ~]# docker run -itd --name web05 -P nginx:latest[root@docker02 ~]# docker exec -it web05 /bin/bashroot@44b59d07202f:/# cd /usr/share/nginx/html/root@44b59d07202f:/usr/share/nginx/html# echo web02 &gt; index.html[root@docker02 ~]# docker ps docker01查看 12[root@docker01 consul]# cd /usr/local/nginx/consul/[root@docker01 consul]# cat vhost.conf docker01测试访问 1[root@docker01 consul]# curl 127.0.0.1:8000 //同上 此时可以看到负载均衡的效果！ 这时不需要考虑后端的web服务器添加还是删除都会自动更新的，这是因为在运行consul-template这条命令后添加的/usr/local/sbin/nginx -s reload的作用！","path":"posts/b5a0.html","date":"06-06","excerpt":"","tags":[]},{"title":"Docker的监控","text":"Docker的监控 Docker自带的监控命令 12[root@docker01 ~]# docker ps//查看容器信息 123[root@docker01 ~]# docker top 容器名称[root@docker01 ~]# docker top wordpress_wordpress_1 //查看容器中运行的进程信息，支持 ps 命令参数。 12[root@docker01 ~]# docker stats wordpress_wordpress_1 //实时查看容器统计信息，查看容器的CPU利用率、内存的使用量以及可用内存总量。 123[root@docker01 ~]# docker logs 容器名称[root@docker01 ~]# docker logs wordpress_wordpress_1 //查看容器的日志 用 Sysdig 监控服务器和 Docker 容器 12[root@docker01 ~]# docker pull sysdig//下载sysdig镜像 通过sysdig运行容器 1[root@docker01 ~]# docker run -it --rm --name sysdig --privileged=true --volume=/var/run/docker.sock:/host/var/run/docker.sock --volume=/dev:/host/dev --volume=/proc:/host/proc:ro --volume=/boot:/host/boot:ro --volume=/lib/modules:/host/lib/modules:ro --volume=/usr:/host/usr:ro sysdig/sysdig 下载插件失败后可以运行下边命令，重新下载 12root@10ccab83a512:/# system-sysdig-loader//下载插件失败后可以运行下边命令，重新下载 下载成功后，可以运行sysdig命令，查看监控项 12root@10ccab83a512:/# sysdig//运行sysdig命令，查看监控项，它会动态查看 使用 csysdig csysdig 就是运 ncurses 库的用户界面的 sysdig 软件包，Ncurses 是一个能提供功能键定义 ( 快捷键 ), 屏幕绘制以及基于文本终端的图形互动功能的动态库。在 sysdig 软件包里还提供了一个工具 csysdig，该工具执行后，运行界面和 top 命令类似。csysdig 工作界面如图 5。 运行csysdig命令，查看监控项 12root@10ccab83a512:/# csysdig//运行csysdig命令，图形化界面查看监控项，它会动态查看 csysdig 使用如下快捷键： P：暂停屏幕输出信息 Enter：进入当前突出显示的条目。 Ctrl+F：列表搜索。 F1- 帮助信息 F2- 显示视图选择器。这将让你切换到另一个视图。 F4- 使用过滤器 F5- 查看 IO 输出信息 F7 显示帮助页面当前显示的视图。 F8 打开视图的操作面板。 F9，打开列排序面板。 Q 放弃退出。 Arrows, PgUP, PgDn, Home, End：图标上下左右的移动控制。 Docker监控方案之Weave Scope 12[root@docker01 ~]# docker pull scope//下载scope镜像 执行如下脚本安装运行Weave Scope 123[root@docker01 ~]# curl -L git.io/scope -o /usr/local/bin/scope[root@docker01 ~]# chmod +x /usr/local/bin/scope[root@docker01 ~]# scope launch 浏览器访问http://192.168.1.11:4040/ 然后就可以更好的监控，管理docker中的容器了 开启第docker02，加入docker01监控项 docker01 删除weavescope容器 1234[root@docker01 ~]# docker stop weavescope weavescope[root@docker01 ~]# docker rm weavescope weavescope docker02 12[root@docker01 ~]# docker pull scope//下载scope镜像 123[root@docker01 ~]# curl -L git.io/scope -o /usr/local/bin/scope[root@docker01 ~]# chmod +x /usr/local/bin/scope[root@docker01 ~]# scope launch docker01 1[root@docker01 ~]# scope launch 192.168.1.11 192.168.1.13 docker02 1[root@docker02 ~]# scope launch 192.168.1.13 192.168.1.11 浏览器访问http://192.168.1.11:4040/ 浏览器访问http://192.168.1.13:4040/也是可以的","path":"posts/bdbf.html","date":"06-06","excerpt":"","tags":[]},{"title":"docker部署LNMP环境","text":"12 ifdown ens33;ifup ens33//重启网卡 首先要有确认环境中有需要的tar包，可以使用docker pull来下载这些镜像 现在我们是使用已经下载好的镜像，所以需要导入一下 12[root@docker01 ~]# docker load -i nginx.tar &amp;&amp; docker load -i wordpress.tar &amp;&amp; docker load -i mysql-5.7.tar &amp;&amp; docker load -i php.7.2-fpm.tar//导入nginx,wordpress,mysql,php镜像 部署LNMP 172.16.10.0/24 Nginx：172.16.10.10 Mysql：172.16.10.20 Php ：172.16.10.30 网站的访问主目录：/wwwroot Nginx的配置文件：/docker /etc/nginx/conf.d #nginx配置文件 12345678[root@docker01 ~]# docker run -itd --name test nginx:latest //先启动一台nginx，用来拷贝配置文件和访问主目录[root@docker01 ~]# mkdir -p /wwwroot /docker//创建挂载目录[root@docker01 ~]# docker cp test:/etc/nginx /docker///拷贝配置文件到挂载目录[root@docker01 ~]# ls /docker/nginx /usr/share/nginx/html #nginx主目录 1234[root@docker01 ~]# docker cp test:/usr/share/nginx/html /wwwroot///拷贝访问目录到挂载目录[root@docker01 ~]# ls /wwwroot/html 1）创建一个自定义网络 1[root@docker01 ~]# docker network create -d bridge --subnet 172.16.10.0/24 --gateway 172.16.10.1 lnmp 2)运行nginx容器 12[root@docker01 ~]# netstat -anpt | grep 80//查看80端口是否被占用 12[root@docker01 ~]# docker run -itd --name nginx -v /docker/nginx:/etc/nginx -v /wwwroot/html:/usr/share/nginx/html -p 80:80 --network lnmp --ip 172.16.10.10 nginx//运行一台nginx服务，并指明ip，映射端口，挂载目录 12[root@docker01 ~]# docker ps//查看容器是否存在 12345678[root@docker01 ~]# cd /wwwroot/html[root@docker01 wwwroot]# vim index.htmlhello lnmp!//创建测试网页[root@docker01 wwwroot]# curl 127.0.0.1hello lnmp!//测试访问 3)运行mysql容器 12[root@docker01 html]# docker run --name mysql -e MYSQL_ROOT_PASSWORD=123.com -d -p 3306:3306 --network lnmp --ip 172.16.10.20 mysql:5.7//运行一台nginx服务，并指明ip，映射端口 -e：设置环境变量 1[root@docker02 ~]# docker ps 安装mysql，并设置密码 123[root@docker01 html]# yum -y install mysql//安装mysql[root@docker01 ~]# mysql -u root -p123.com -h 127.0.0.1 -P 3306 随便新建一个库做验证： 1MySQL [(none)]&gt; create database name; 再查看有没有刚创建的库： 1MySQL [(none)]&gt; show databases; 4)运行php容器，并创建php页面 1[root@docker01 html]# docker run -itd --name phpfpm -p 9000:9000 -v /wwwroot/html:/usr/share/nginx/html --network lnmp --ip 172.16.10.30 php:7.2-fpm 123456[root@docker01 ~]# cd /wwwroot/html[root@docker01 wwwroot]# vim test.php&lt;?phpphpinfo();?&gt;//添加php测试界面 1[root@docker02 ~]# docker ps 5)修改nginx配置文件，nginx和php连接 12[root@docker01 html]# cd /docker/nginx/conf.d/[root@docker01 conf.d]# vim default.conf location / { root /usr/share/nginx/html; index index.html index.htm index.php; #10添加index.php } location ~ \\.php$ { root /usr/share/nginx/html; fastcgi_pass 172.16.10.30:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } 设置完毕后重启nginx 123[root@docker01 conf.d]# docker restart nginx//重启nginx[root@docker01 conf.d]# docker ps 浏览器测试访问nginx和php 说明是nginx和php的连接，没有问题，接下来是php和MySQL的连接。这里我们使用一个phpmyadmin的数据库管理工具 6)修改nginx配置文件，php和mysql连接 1[root@docker01 html]# cd /wwwroot/html 上传phpMyAdmin包如果没有请在https://github.com/phpmyadmin/phpmyadmin/releases下载 123456789[root@docker01 html]# unzip phpMyAdmin-4.9.1-all-languages.zip //解压phpmyadmin包[root@docker01 html]# mv phpMyAdmin-4.9.1-all-languages phpmyadmin//更改刚刚解压文件的名称[root@docker01 html]# cd /docker/nginx/conf.d/[root@docker01 conf.d]# vim default.conf //修改nginx配置文件[root@docker01 conf.d]# docker restart nginx //重启nginx location /phpmyadmin { root /usr/share/nginx/html; index index.html index.htm index.php; } location ~ /phpmyadmin/(?&lt;after_ali&gt;(.*)\\.(php|php5)?$) { root /usr/share/nginx/html; fastcgi_pass 172.16.10.30:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } 12[root@docker01 conf.d]# docker restart nginx [root@docker01 conf.d]# docker ps 浏览器访问 http://192.168.1.11/phpmyadmin/index.php 报红框属于正常现象，不要惊慌，接下来就解决它 需要我们对php镜像做出更改，添加php和MySQL连接模块 编写一个Dockerfile 1234567891011[root@docker01 conf.d]# cd [root@docker01 ~]# vim DockerfileFROM php:7.2-fpmRUN apt-get update &amp;&amp; apt-get install -y \\ libfreetype6-dev \\ libjpeg62-turbo-dev \\ libpng-dev \\ &amp;&amp; docker-php-ext-install -j$(nproc) iconv \\ &amp;&amp; docker-php-ext-configure gd --with-freetype-dir=/usr/include/ --with-jpeg-dir=/usr/include/ \\ &amp;&amp; docker-php-ext-install -j$(nproc) gd \\ &amp;&amp; docker-php-ext-install mysqli pdo pdo_mysql 基于dockerfile创建php镜像 12[root@docker01 ~]# docker build -t phpmysql .//基于Dockerfiler创建一个镜像 删除之前的php容器 123[root@docker01 ~]# docker stop phpfpm[root@docker01 ~]# docker rm phpfpm //关闭并删除php容器 用新的php镜像运行容器 12[root@docker01 ~]# docker run -itd --name phpfpm -p 9000:9000 -v /wwwroot/html:/usr/share/nginx/html --network lnmp --ip 172.16.10.30 phpmysql//用新做的php镜像重新运行 //修改phpmyadmin的配置文件，指定连接的数据库的IP，然后重启php容器 12345678[root@docker01 html]# cd /wwwroot/html/phpmyadmin/[root@docker01 phpmyadmin]# cp config.sample.inc.php config.inc.php[root@docker01 phpmyadmin]# vim config.inc.php$cfg['Servers'][$i]['auth_type'] = 'cookie';/* Server parameters */$cfg['Servers'][$i]['host'] = '172.16.10.20'; #31写mysql数据库的IP地址$cfg['Servers'][$i]['compress'] = false;$cfg['Servers'][$i]['AllowNoPassword'] = false; 12[root@docker01 phpmyadmin]# docker restart phpfpm //重启phpfpm容器 浏览器测试访问http://192.168.1.11/phpmyadmin/index.php 用户名：root 密码：123.com 登陆成功之后可以看到之前mysql创建的数据库","path":"posts/32f5.html","date":"06-06","excerpt":"","tags":[]},{"title":"docker-compose","text":"docker三剑客之docker-compose docker容器的编排工具: 解决相互有依赖关系的多个容器的管理。 12[root@docker01 ~]# docker-compose -v//验证已有docker-compose命令 docker-compose的配置文件实例 通过识别一个docker-compose.yml的配置文件，去管理容器。 设置tab键的空格数量 12345[root@docker01 ~]# vim .vimrcset tabstop=2//设置tab键的空格数量[root@docker01 ~]# source .vimrc //刷新一下 创建一个docker-compose.yml测试文件 123456789101112131415[root@docker01 ~]# mkdir compose_test//创建测试目录[root@docker01 ~]# cd compose_test/[root@docker01 compose_test]# vim docker-compose.yml//创建测试文件docker-compose.ymlversion: \"3\"services: nginx: container_name: web-nginx image: nginx restart: always ports: - 90:80 volumes: - ./webserver:/usr/share/nginx/html docker-compose.yml文件的解释 第一部分: version: 指定语法格式的版本。 第二部分: service: 定义服务,(想要运行什么样的容器) 通过docker-compose.yml文件运行容器 12[root@docker01 compose_test]# docker-compose up -d//后台运行docker-compose规定的容器。（在执行这条命令的当前目录下，也需要一个docker-compose.yml的配置文件，并且通常只有一个。） 12[root@docker01 compose_test]# docker ps//查看容器信息 12[root@docker01 compose_test]# curl 127.0.0.1:90//访问nginx会失败，因为挂载目录没有页面内容 123456[root@docker01 compose_test]# vim webserver/index.html//创建测试网页xgp666[root@docker01 compose_test]# curl 127.0.0.1:90//再次访问，是成功的xgp666 通过docker-compose.yml文件停止运行容器 1[root@docker01 compose_test]# docker-compose stop 通过docker-compose.yml文件重启容器 1[root@docker01 compose_test]# docker-compose restart 不在docker-compose.yml文件所在目录，要使用-f指定目录 1[root@docker01 ~]# docker-compose -f compose_test/docker-compose.yml stop 并且，在运行container（docker-compose.yml）的过程中，还支持Dockerfile 1234[root@docker01 compose_test]# vim Dockerfile//编写dockerfileFROM nginxADD webserver /usr/share/nginx/html 1234567891011[root@docker01 compose_test]# vim docker-compose.yml //修改docker-compose.yml文件version: \"3\"services: nginx: build: . #添加 container_name: web-nginx image: new-nginx:v1.0 #修改镜像名称 restart: always ports: - 90:80 通过docker-compose.yml文件停止并删除容器 123[root@docker01 compose_test]# docker-compose stopStopping web-nginx ... done[root@docker01 compose_test]# docker-compose rm 通过docker-compose.yml文件运行容器 1234[root@docker01 compose_test]# docker-compose up -d//通过docker-compose.yml文件[运行]()容器[root@docker01 compose_test]# docker ps//查看容器信息 测试nginx访问页面 123[root@docker01 compose_test]# curl 127.0.0.1:90//测试访问nginx页面，成功xgp666 搭建wordpress的博客 下载wordpress和mysql:5.7容器 1234[root@docker01 ~]# docker pull wordpress//下载wordpress容器[root@docker01 ~]# docker pull mysql：5.7//下载mysql：5.7容器 编写一个docker-ccompose.yml 1234567891011121314151617181920212223242526[root@docker01 ~]# mkdir wordpress//创建wordpress测试文件[root@docker01 ~]# cd wordpress/[root@docker01 wordpress]# vim docker-compose.yml//编写docker-compose.ymlversion: \"3.1\"services: wordpress: image: wordpress restart: always ports: - 8080:80 environment: WORDPRESS_DB_HOST: db WORDPRESS_DB_USER: wordpress WORDPRESS_DB_PASSWORD: 123.com WORDPRESS_DB_NAME: wordpress db: image: mysql:5.7 restart: always environment: MYSQL_DATABASE: wordpress MYSQL_USER: wordpress MYSQL_PASSWORD: 123.com MYSQL_ROOT_PASSWORD: 123.com 通过docker-compose.yml文件运行容器 1[root@docker01 wordpress]# docker-compose up -d 12[root@docker01 wordpress]# docker ps//查看容器信息 12[root@docker01 wordpress]# docker logs 容器名称//查看容器日志 浏览器访问一下 http://192.168.1.11:8080/ 选择语言 安装wordpress 登陆wordpress 登陆成功后，自己就可以进行设置了","path":"posts/713b.html","date":"06-06","excerpt":"","tags":[]},{"title":"Docker swarm集群","text":"Docker swarm docker swarm集群：三剑客之一 一. 实验环境 主机 IP地址 服务 docker01 192.168.1.11 swarm+overlay+webUI docker02 192.168.1.13 docker docker03 192.168.1.20 docker 三台主机都关闭防火墙，禁用selinux，修改主机名，时间同步，并添加域名解析。 docker版本必须是：v1.12版本开始（可使用docker version查看版本） 1.关闭防火墙，禁用selinux 123[root@localhost ~]# systemctl stop firewalld[root@localhost ~]# hostnamectl set-hostname docker03[root@localhost ~]# su - 2.时间同步 12mv /etc/localtime /etc/localtime.bkcp /usr/share/zoneinfo/Asia/Shanghai/etc/localtime 3.修改主机名（三台都要） 12[root@localhost ~]# hostnamectl set-hostname docker01[root@localhost ~]# su - 4.添加域名解析 1234567[root@docker01 ~]# vim /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.1.11 docker01192.168.1.13 docker02192.168.1.20 docker03 二. swarm原理 **swarm：**作用运行docker engin的多个主机组成的集群 **node：**每一个docker engin都是一个node（节点），分为manager和worker。 **manager node：**负责执行容器的编排和集群的管理工作，保持并维护swarm处于期望的状态。swarm可以有多个manager node，他们会自动协调并选举一个leader执行编排任务。但相反，不能没有manager node。 **worker node：**接受并执行由manager node派发的任务，并且默认manager node也是一个worker node，不过可以将它设置为manager-only node，让他只负责编排和管理工作。 **service：**用来定义worker上执行的命令。 基本命令操作 **docker swarm leave：**申请离开一个集群，之后查看节点状态会变成down，然后可通过manager node 将其删除 **docker node rm xxx：**删除某个节点 docker swarm join-token [manager|worker]：生成令牌，可以是manager或worker身份。 docker node demote（降级）：将swarm节点的为manager降级为worker docker node promote（升级）：将swarm节点的work升级为manager **docker node ls:**查看群集的信息（只可以在manager角色的主机上查看） docker service scale web05=6:容器的动态扩容及缩容 docker service ps web01: 查看创建的容器运行在哪些节点 docker service ls: 查看创建的服务 docker swarm leave: 脱离这个群集 docker node rm docker03: 在manager角色的服务器上移除docker03 docker node update --availability drain docker01: 设置主机docker01以后不运行容器，但已经运行的容器并不会停止 docker node update --label-add mem=max docker03: 更改docker03主机的标签为mem=max docker service update --replicas 8 --image 192.168.20.6:5000/lvjianzhao:v2.0 --container-label-add ‘node.labels.mem==max’ lvjianzhao05: 将服务升级为8个容器，并且指定在mem=max标签的主机上运行 三. docker01 初始化集群 1[root@docker01 ~]# docker swarm init --advertise-addr 192.168.1.11 **–advertise-addr：**指定与其它docker通信的地址。 上边返回的结果告诉我们：初始化成功，并且，如果想要添加work节点运行下面的命令： 注意：token令牌只有24小时的有效期 如果想要添加manager节点：运行下面命令 四.swarm集群的简单操作 1.docker02和docker03以worker加入集群 1[root@docker03 ~]# docker swarm join --token SWMTKN-1-5kxn9wloh7npnytklwbfciesr9di7uvu521gwnqm9h1n0pbokj-1e60wt0yr5583e4mzwbxnn3a8 192.168.1.11:2377 docker01查看集群 1[root@docker01 ~]# docker node ls 注意：这里的”*****“代表的是当前所属的节点 2.删除集群中节点 docker02和docker03申请离开一个集群 1[root@docker02 ~]# docker swarm leave docker删除docker02和docker03节点 12[root@docker01 ~]# docker node rm docker02 [root@docker01 ~]# docker node rm docker03 docker01查看集群 1[root@docker01 ~]# docker node ls 3.docker02和docker03以manager加入集群 docker01生成manager令牌 1[root@docker01 ~]# docker swarm join-token manager docker02和docker03加入集群 1docker swarm join --token SWMTKN-1-5kxn9wloh7npnytklwbfciesr9di7uvu521gwnqm9h1n0pbokj-cz6hbyv9r5htyqwj5tfol65aa 192.168.1.11:2377 docker01查看集群 1[root@docker01 ~]# docker node ls 4.docker02和docker03降级 docker01（manager）把docker02和docker03降级成worker 12[root@docker01 ~]# docker node demote docker02[root@docker01 ~]# docker node demote docker03 查看集群 1[root@docker01 ~]# docker node ls 五.部署docker swarm集群网络 overlay:覆盖型网络 overlay networks 管理Swarm中docker守护进程间的通信。可以将容器附加到一个或多个已存在的overlay网络上，使容器与容器之间能够通信； 12[root@docker01 ~]# docker network create -d overlay --attachable docker//attachable：这个参数必须要加，否则不能用于容器。 在创建网络的时候，我们并没有部署一个存储服务，比如consul，那是因为docker swarm自带存储。 docker01查看网络 但是会发现其他两台并不会发现此网络，需等基于此网络创建service服务就可以看到了 1[root@docker01 ~]# docker network ls 六. docker01部署一个图形化webUI界面 1.docker01 导入镜像 1[root@docker01~]# docker pull dockersamples/visualizer 2.基于镜像启动一台容器 1[root@docker01 ~]# docker run -d -p 8080:8080 -e HOST=192.168.1.100 -e PORT=8080 -v /var/run/docker.sock:/var/run/docker.sock --name visualiaer dockersamples/visualizer 3.通过浏览器访问验证http://192.168.1.11:8080/ 如果访问不到网页，需开启路由转发 12[root@docker01 ~]# echo net.ipv4.ip_forward = 1 &gt;&gt; /etc/sysctl.conf [root@docker01 ~]# sysctl -p 七. 创建service（服务） 1. 基于nginx容器创建一个service服务 1234[root@docker01 ~]#docker pull nginx//下载nginx镜像（三台都要）[root@docker01 ~]# docker service create --replicas 1 --network docker --name web1 -p 80:80 nginx:latest [root@docker01 ~]# docker service create --replicas 1 --network docker --name web2 -p 80 nginx:latest //–replicas：副本数量 大概可以理解为一个副本等于一个容器 2. 查看创建的service服务 1[root@docker01 ~]# docker service ls 单独查看一个servicefuw 1[root@docker01 ~]# docker service ps web1 1[root@docker01 ~]# docker service ps web2 3. web界面查看 4. 基于nginx容器创建五个service服务 1[root@docker01 ~]# docker service create --replicas 5 --network docker --name web -p 80 nginx:latest web界面查看 5. 挂起docker02 web查看（发现服务都分配到其他服务器了） 6. 恢复docker02 web查看（发现服务没有回到docker02） 八、实现docker容器的扩容及缩容 1. 删除web1和web2服务 1[root@docker01 ~]# docker service rm web1 web2 2. 容器的扩容和缩减 （1）扩容 1[root@docker01 ~]# docker service scale web=8 （2）缩减 1[root@docker01 ~]# docker service scale web=3 3.设置manager node不参加工作 1[root@docker01 ~]# docker node update docker01 --availability drain 设置主机docker01以后不运行容器，但已经运行的容器并不会停止 “–availability”选项后面共有三个选项可配置，如下： “active”：工作；“pause”：暂时不工作；“drain”：永久性的不工作 1[root@docker01 ~]# docker node ls web界面查看 九、docker Swarm总结 在我对docker Swarm群集进行一定了解后，得出的结论如下： 参与群集的主机名一定不能冲突，并且可以互相解析对方的主机名； 集群内的所有节点可以都是manager角色，但是不可以都是worker角色； 当指定运行的镜像时，如果群集中的节点本地没有该镜像，那么它将会自动下载对应的镜像； 当群集正常工作时，若一个运行着容器的docker服务器发生宕机，那么，其所运行的所有容器，都将转移到其他正常运行的节点之上，而且，就算发生宕机的服务器恢复正常运行，也不会再接管之前运行的容器；","path":"posts/ca45.html","date":"06-06","excerpt":"","tags":[]},{"title":"docker swarm版本回滚","text":"Docker swarm docker swarm集群：三剑客之一 一. 实验环境 主机 IP地址 服务 docker01 192.168.1.11 swarm+service+webUI+registry docker02 192.168.1.13 docker docker03 192.168.1.20 docker 三台主机都关闭防火墙，禁用selinux，修改主机名，时间同步，并添加域名解析。 docker版本必须是：v1.12版本开始（可使用docker version查看版本） 1.关闭防火墙，禁用selinux 123[root@localhost ~]# systemctl stop firewalld[root@localhost ~]# hostnamectl set-hostname docker03[root@localhost ~]# su - 2.时间同步 12mv /etc/localtime /etc/localtime.bkcp /usr/share/zoneinfo/Asia/Shanghai/etc/localtime 3.修改主机名（三台都要） 12[root@localhost ~]# hostnamectl set-hostname docker01[root@localhost ~]# su - 4.添加域名解析 123[root@docker01 ~]# echo 192.168.1.11 docker01 &gt;&gt; /etc/hosts[root@docker01 ~]# echo 192.168.1.13 docker02 &gt;&gt; /etc/hosts[root@docker01 ~]# echo 192.168.1.20 docker03 &gt;&gt; /etc/hosts 三. docker01 初始化集群 1[root@docker01 ~]# docker swarm init --advertise-addr 192.168.1.11 **–advertise-addr：**指定与其它docker通信的地址。 上边返回的结果告诉我们：初始化成功，并且，如果想要添加work节点运行下面的命令： 注意：token令牌只有24小时的有效期 如果想要添加manager节点：运行下面命令 四，docker02和docker03以worker加入集群 1[root@docker03 ~]# docker swarm join --token SWMTKN-1-5kxn9wloh7npnytklwbfciesr9di7uvu521gwnqm9h1n0pbokj-1e60wt0yr5583e4mzwbxnn3a8 192.168.1.11:2377 docker01查看集群 1[root@docker01 ~]# docker node ls 注意：这里的”*****“代表的是当前所属的节点 五.设置manager node（docker01）不参加工作 1[root@docker01 ~]# docker node update docker01 --availability drain 设置主机docker01以后不运行容器，但已经运行的容器并不会停止 “–availability”选项后面共有三个选项可配置，如下： “active”：工作；“pause”：暂时不工作；“drain”：永久性的不工作 1[root@docker01 ~]# docker node ls 六. docker01部署一个图形化webUI界面 1.docker01 导入镜像 1[root@docker01~]# docker pull dockersamples/visualizer 2.基于镜像启动一台容器 1[root@docker01 ~]# docker run -d -p 8080:8080 -e HOST=192.168.1.100 -e PORT=8080 -v /var/run/docker.sock:/var/run/docker.sock --name visualiaer dockersamples/visualizer 3.通过浏览器访问验证http://192.168.1.11:8080/ 如果访问不到网页，需开启路由转发 12[root@docker01 ~]# echo net.ipv4.ip_forward = 1 &gt;&gt; /etc/sysctl.conf [root@docker01 ~]# sysctl -p 一. Docker01部署一个私有仓库 Docker01部署 123456789101112131415161772 docker pull registry//下载registry镜像73 docker run -itd --name registry -p 5000:5000 --restart=always registry:latest//基于registry镜像，启动一台容器78 vim /usr/lib/systemd/system/docker.service #13行修改ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.11:5000 80 systemctl daemon-reload81 systemctl restart docker.service//重启docker76 docker tag busybox:latest 192.168.1.11:5000/busybox:v1 //把容器重命名一个标签77 docker ps 1234567891078 vim /usr/lib/systemd/system/docker.service #13行修改ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.11:5000 80 systemctl daemon-reload81 systemctl restart docker.service//重启docker100 docker push 192.168.1.11:5000/busybox:v1//上传容器到私有仓库 Docker02和docker03加入私有仓库 12345678978 vim /usr/lib/systemd/system/docker.service #13行修改ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.11:5000 80 systemctl daemon-reload81 systemctl restart docker.service//重启docker99 docker pull 192.168.1.11/busybox:v1//测试下载 2. 自定义镜像 要求：基于httpd镜像，更改访问界面内容。镜像tag版本为v1，v2，v3，对应主机面内容为v1，xgp666、v2，xgp666、v2，xgp666 12[root@docker01 ~]# docker pull httpd//下载httpd镜像 创建三个测试目录 12[root@docker01 ~]# mkdir &#123;v1,v2,v3&#125;//创建测试目录 docker01，v1目录操作 1234567891011121314[root@docker01 ~]# cd v1[root@docker01 v1]# echo v1,xgp666 &gt; index.html//创建测试网页[root@docker01 v1]# vim Dockerfile//编写DockerfileFROM httpdADD index.html /c[root@docker01 v1]# docker build -t 192.168.1.11:5000/httpd:v1 .//基于dockerfile创建镜像[root@docker01 v1]# docker push 192.168.1.11:5000/httpd:v1//上传刚刚创建镜像到私有仓库 docker01，v2目录操作 12345678910111213[root@docker01 v1]# cd ../v2[root@docker01 v2]# echo v2,xgp666 &gt; index.html[root@docker01 v2]# vim Dockerfile //编写DockerfileFROM httpdADD index.html /usr/local/apache2/htdocs/index.html[root@docker01 v2]# docker build -t 192.168.1.11:5000/httpd:v2 .//基于dockerfile创建镜像[root@docker01 v2]# docker push 192.168.1.11:5000/httpd:v2//上传刚刚创建镜像到私有仓库 docker01，v3目录操作 12345678910111213[root@docker01 v1]# cd ../v3[root@docker01 v2]# echo v3,xgp666 &gt; index.html[root@docker01 v2]# vim Dockerfile //编写DockerfileFROM httpdADD index.html /usr/local/apache2/htdocs/index.html[root@docker01 v2]# docker build -t 192.168.1.11:5000/httpd:v3 .//基于dockerfile创建镜像[root@docker01 v2]# docker push 192.168.1.11:5000/httpd:v3//上传刚刚创建镜像到私有仓库 3. 发布一个服务，基于上述镜像 要求:副本数量为3个。服务的名称为: bdqn 1[root@docker01 v3]# docker service create --replicas 3 --name bdqn -p 80:80 192.168.1.11:5000/httpd:v1 查看一下网络 1[root@docker03 ~]# docker network ls 默认的Ingress网络，包括创建的自定义overlay网络, 为后端真正为用户提供服务的container,提供了一个统一的入口。 查看一下创建的副本 1[root@docker01 v3]# docker service ps bdqn 浏览器测试访问http://192.168.1.11:80,http://192.168.1.13:80,http://192.168.1.20:80 修改docker02和docker03测试网页内容 docker02 123[root@docker02 ~]# docker exec -it 388f3bd9dd33 /bin/bashroot@388f3bd9dd33:/usr/local/apache2# cd htdocs/root@388f3bd9dd33:/usr/local/apache2/htdocs# echo 123 &gt; index.html docker03 12[root@docker03 ~]# docker exec -it 281454867fac /bin/bashroot@281454867fac:/usr/local/apache2# echo 321 &gt; htdocs/index.html 测试访问（每一台都会显示，会负载均衡） 要求:副本数量为3个。服务的名称为:test 1[root@docker01 v3]# docker service create --replicas 3 --name test -p 80 192.168.1.11:5000/httpd:v1 查看创建的服务映射端口 1[root@docker01 v3]# docker service ls 默认映射端口30000-32767 4. 服务的扩容与缩容 扩容 1[root@docker01 v3]# docker service scale bdqn=6 缩容 1[root@docker01 v3]# docker service scale bdqn=4 扩容与缩容直接直接通过scale进行设置副本数量。 5.服务的升级与回滚 （1）升级 12[root@docker01 ~]# docker service update --image 192.168.1.11:5000/httpd:v2 bdqn//把bdqn服务升级成v2的版本 测试访问一下 （2）平滑的更新 12[root@docker01 ~]# docker service update --image 192.168.1.11:5000/httpd:v3 --update-parallelism 2 --update-delay 1m bdqn //两个服务一起更新，然后，隔一分钟，继续更新 默认情况下, swarm-次只更新-个副本,并且两个副本之间没有等待时间，我们可以通过 –update-parallelism;设置并行更新的副本数量。 –update-delay：指定滚动更新的时间间隔。 测试访问一下 (3) 回滚操作 1[root@docker01 ~]# docker service rollback bdqn 注意，docker swarm的回滚操作，默认只能回滚到上一-次操作的状态，并不能连续回滚到指定操作。 测试访问一下","path":"posts/4890.html","date":"06-06","excerpt":"","tags":[]},{"title":"AlertManager自定义邮件模板","text":"AlertManager自定义邮件模板 创建模板目录 1234[root@docker01 ~]# cd prometheus//进入之前创建的prometheus目录[root@docker01 prometheus]# mkdir alertmanager-tmpl//创建AlertManager模板目录 编写模板规则 123456789101112131415[root@docker01 prometheus]# vim email.tmpl &#123;&#123; define \"email.from\" &#125;&#125;2877364346@qq.com&#123;&#123; end &#125;&#125;&#123;&#123; define \"email.to\" &#125;&#125;2877364346@qq.com&#123;&#123; end &#125;&#125;&#123;&#123; define \"email.to.html\" &#125;&#125;&#123;&#123; range .Alerts &#125;&#125;=========start==========&lt;br&gt;告警程序: prometheus_alert&lt;br&gt;告警级别: &#123;&#123; .Labels.severity &#125;&#125; 级&lt;br&gt;告警类型: &#123;&#123; .Labels.alertname &#125;&#125;&lt;br&gt;故障主机: &#123;&#123; .Labels.instance &#125;&#125;&lt;br&gt;告警主题: &#123;&#123; .Annotations.summary &#125;&#125;&lt;br&gt;触发时间: &#123;&#123; .StartsAt.Format \"2019-08-04 16:58:15\" &#125;&#125; &lt;br&gt;=========end==========&lt;br&gt;&#123;&#123; end &#125;&#125;&#123;&#123; end &#125;&#125; 修改alertmanager的配置文件 1234567891011121314151617181920212223242526272829[root@docker01 ~]# vim alertmanager.yml global: resolve_timeout: 5m smtp_from: '2877364346@qq.com' smtp_smarthost: 'smtp.qq.com:465' smtp_auth_username: '2877364346@qq.com' smtp_auth_password: 'evjmqipqezlbdfij' smtp_require_tls: false smtp_hello: 'qq.com'templates: #添加模板 - '/etc/alertmanager-tmpl/*.tmpl' #添加路径 route: group_by: ['alertname'] group_wait: 5s group_interval: 5s repeat_interval: 5m receiver: 'email' receivers:- name: 'email' email_configs: - to: '&#123;&#123; template \"email to\" &#125;&#125;' #修改 html: '&#123;&#123; template \"email.to.html\" .&#125;&#125;' #添加 send_resolved: true #删除 inhibit_rules: - source_match: severity: 'critical' target_match: severity: 'warning' equal: ['alertname', 'dev', 'instance'] 重新运行 alertmanager 容器 1234[root@docker01 ~]# docker rm -f alertmanager//删除alertmanager容器[root@docker01 ~]# docker run -itd --name alertmanager -p 9093:9093 -v /root/alertmanager.yml:/etc/alertmanager/alertmanager.yml -v /root/prometheus/alertmanager-tmpl:/etc/alertmanager-tmpl prom/alertmanager:latest//运行一台新的alertmanager容器，记得挂载配置文件 挂起docker02 收到邮件","path":"posts/e914.html","date":"06-06","excerpt":"","tags":[]}],"categories":[],"tags":[]}