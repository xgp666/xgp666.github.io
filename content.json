{"meta":{"title":"Xgp & Blog","subtitle":"Today is still beautiful","description":"Small steel gun article","author":"Wu Shao Dong","url":"https://wsdlxgp.top","root":"/"},"pages":[{"title":"categories","text":"","path":"categories/index.html","date":"03-11","excerpt":""},{"title":"可爱的我","text":"","path":"about/index.html","date":"05-30","excerpt":""},{"title":"网站感想","text":"","path":"about/site.html","date":"05-30","excerpt":""},{"title":"contact","text":"","path":"contact/index.html","date":"03-30","excerpt":""},{"title":"tags","text":"","path":"tags/index.html","date":"03-11","excerpt":""},{"title":"","text":"!function(){ var userAgentInfo = navigator.userAgent; var Agents = [\"iPad\", \"iPhone\", \"Android\", \"SymbianOS\", \"Windows Phone\", \"iPod\", \"webOS\", \"BlackBerry\", \"IEMobile\"]; for (var v = 0; v < Agents.length; v++) { if (userAgentInfo.indexOf(Agents[v]) > 0) { return; } } function o(w,v,i){return w.getAttribute(v)||i}function j(i){return document.getElementsByTagName(i)}function l(){var i=j(\"script\"),w=i.length,v=i[w-1];return{l:w,z:o(v,\"zIndex\",-1),o:o(v,\"opacity\",0.5),c:o(v,\"color\",\"0,0,0\"),n:o(v,\"count\",99)}}function k(){r=u.width=window.innerWidth||document.documentElement.clientWidth||document.body.clientWidth,n=u.height=window.innerHeight||document.documentElement.clientHeight||document.body.clientHeight}function b(){e.clearRect(0,0,r,n);var w=[f].concat(t);var x,v,A,B,z,y;t.forEach(function(i){i.x+=i.xa,i.y+=i.ya,i.xa*=i.x>r||i.xn||i.y","path":"lib/canvas-nest/canvas-nest-nomobile.min.js","date":"03-28","excerpt":""},{"title":"","text":"Theme NexT Canvas Nest canvas-nest.js for NexT. Install Step 1 → Go to Hexo dir Change dir to Hexo directory. There must be scaffolds, source, themes and other directories: 123$ cd hexo$ lsscaffolds source themes _config.yml package.json Step 2 → Create footer.swig Create a file named footer.swig in hexo/source/_data directory (create _data directory if it does not exist). Edit this file and add the following content: 1&lt;script color=\"0,0,255\" opacity=\"0.5\" zIndex=\"-1\" count=\"99\" src=\"https://cdn.jsdelivr.net/npm/canvas-nest.js@1/dist/canvas-nest.js\"&gt;&lt;/script&gt; You can customize these options. Step 3 → Set it up In the NexT _config.yml, uncomment footer under the custom_file_path section. 12345678910111213# Define custom file paths.# Create your custom files in site directory `source/_data` and uncomment needed files below.custom_file_path: #head: source/_data/head.swig #header: source/_data/header.swig #sidebar: source/_data/sidebar.swig #postMeta: source/_data/post-meta.swig #postBodyEnd: source/_data/post-body-end.swig footer: source/_data/footer.swig #bodyEnd: source/_data/body-end.swig #variable: source/_data/variables.styl #mixin: source/_data/mixins.styl #style: source/_data/styles.styl","path":"lib/canvas-nest/README.html","date":"03-28","excerpt":""},{"title":"","text":"!function(){function o(w,v,i){return w.getAttribute(v)||i}function j(i){return document.getElementsByTagName(i)}function l(){var i=j(\"script\"),w=i.length,v=i[w-1];return{l:w,z:o(v,\"zIndex\",-1),o:o(v,\"opacity\",0.5),c:o(v,\"color\",\"0,0,0\"),n:o(v,\"count\",99)}}function k(){r=u.width=window.innerWidth||document.documentElement.clientWidth||document.body.clientWidth,n=u.height=window.innerHeight||document.documentElement.clientHeight||document.body.clientHeight}function b(){e.clearRect(0,0,r,n);var w=[f].concat(t);var x,v,A,B,z,y;t.forEach(function(i){i.x+=i.xa,i.y+=i.ya,i.xa*=i.x>r||i.xn||i.y","path":"lib/canvas-nest/canvas-nest.min.js","date":"03-28","excerpt":""}],"posts":[{"title":"25 k8s架构，基本概念","text":"主机名 IP地址 服务 master 192.168.1.21 node01 192.168.1.22 node02 192.168.1.23 kubernetes架构 在这张系统架构图中，我们把服务分为运行在工作节点上的服务和组成集群级别控制板的服务。 Kubernetes节点有运行应用容器必备的服务，而这些都是受Master的控制。 每次个节点上当然都要运行Docker。Docker来负责所有具体的映像下载和容器运行。 Kubernetes主要由以下几个核心组件组成： kubectl：k8s是命令行端，用来发送客户的操作指令。 master节点 1. API server[资源操作入口]：是k8s集群的前端接口，各种各样客户端工具以及k8s的其他组件可以通过它管理k8s集群的各种资源。它提供了HTTP/HTTPS RESTful API,即K8S API。 提供了资源对象的唯一操作入口，其他所有组件都必须通过它提供的API来操作资源数据，只有API Server与存储通信，其他模块通过API Server访问集群状态。 第一，是为了保证集群状态访问的安全。 第二，是为了隔离集群状态访问的方式和后端存储实现的方式：API Server是状态访问的方式，不会因为后端存储技术etcd的改变而改变。 作为kubernetes系统的入口，封装了核心对象的增删改查操作，以RESTFul接口方式提供给外部客户和内部组件调用。对相关的资源数据“全量查询”+“变化监听”，实时完成相关的业务功能。 2. Scheduler[集群分发调度器]：负责决定将Pod放在哪个Node上运行。在调度时，会充分考虑集群的拓扑结构，当前各个节点的负载情况，以及应对高可用、性能、数据亲和性和需求。 1.Scheduler收集和分析当前Kubernetes集群中所有Minion节点的资源(内存、CPU)负载情况，然后依此分发新建的Pod到Kubernetes集群中可用的节点。 2.实时监测Kubernetes集群中未分发和已分发的所有运行的Pod。 3.Scheduler也监测Minion节点信息，由于会频繁查找Minion节点，Scheduler会缓存一份最新的信息在本地。 4.最后，Scheduler在分发Pod到指定的Minion节点后，会把Pod相关的信息Binding写回API Server。 4. Controller Manager[内部管理控制中心]：负责管理集群的各种资源，保证资源处于预期的状态。它由多种Controller组成，包括Replication Controller、Endpoints Controller、Namespace Controller、Serviceaccounts Controller等。 实现集群故障检测和恢复的自动化工作，负责执行各种控制器，主要有： 1.endpoint-controller：定期关联service和pod(关联信息由endpoint对象维护)，保证service到pod的映射总是最新的。 2.replication-controller：定期关联replicationController和pod，保证replicationController定义的复制数量与实际运行pod的数量总是一致的。 5. Etcd：负责保存k8s集群的配置信息和各种资源的状态信息。当数据发生变化时，etcd会快速的通知k8s相关组件。（第三方组件）它有可替换方案。Consul、zookeeper 6. Pod: k8s集群的最小组成单位。一个Pod内，可以运行一个或多个容器。大多数情况下，一个Pod内只有一个Container容器。 7. Flanner：是k8s集群网络，可以保证Pod的跨主机通信。也有替换方案。 12[root@master ~]# kubectl get pod --all-namespaces//查看pod信息 12[root@master ~]# kubectl get pod --all-namespaces -o wide//显示pod的节点信息 Node节点 Kubelet[节点上的Pod管家]：它是Node的agent(代理)，当Scheduler确定某 个Node上运行Pod之后，会将Pod的具体配置信息发送给该节点的kubelet,kubelet会根据这些信息创建和运行容器，并向Master报告运行状态。 负责Node节点上pod的创建、修改、监控、删除等全生命周期的管理 定时上报本Node的状态信息给API Server。 kubelet是Master API Server和Minion之间的桥梁，接收Master API Server分配给它的commands和work，与持久性键值存储etcd、file、server和http进行交互，读取配置信息。 具体的工作如下： 设置容器的环境变量、给容器绑定Volume、给容器绑定Port、根据指定的Pod运行一个单一容器、给指定的Pod创建network 容器。 同步Pod的状态、同步Pod的状态、从cAdvisor获取Container info、 pod info、 root info、 machine info。 在容器中运行命令、杀死容器、删除Pod的所有容器。 **kube-proxy[负载均衡、路由转发]:**负责将访问service的TCP/UDP数据流转发到后端的容器。如果有多个副本，kube-proxy会实现负载均衡。 Proxy是为了解决外部网络能够访问跨机器集群中容器提供的应用服务而设计的，运行在每个Node上。Proxy提供TCP/UDP sockets的proxy，每创建一种Service，Proxy主要从etcd获取Services和Endpoints的配置信息（也可以从file获取），然后根据配置信息在Minion上启动一个Proxy的进程并监听相应的服务端口，当外部请求发生时，Proxy会根据Load Balancer将请求分发到后端正确的容器处理。 Proxy不但解决了同一主宿机相同服务端口冲突的问题，还提供了Service转发服务端口对外提供服务的能力，Proxy后端使用了随机、轮循负载均衡算法。 除了核心组件，还有一些推荐的Add-ons： kube-dns负责为整个集群提供DNS服务 Ingress Controller为服务提供外网入口 Heapster提供资源监控 Dashboard提供GUI Federation提供跨可用区的集群 Fluentd-elasticsearch提供集群日志采集、存储与查询 一. 分层架构 Kubernetes设计理念和功能其实就是一个类似Linux的分层架构，如下图所示。 核心层：Kubernetes最核心的功能，对外提供API构建高层的应用，对内提供插件式应用执行环境 应用层：部署（无状态应用、有状态应用、批处理任务、集群应用等）和路由（服务发现、DNS解析等） 管理层：系统度量（如基础设施、容器和网络的度量），自动化（如自动扩展、动态Provision等）以及策略管理（RBAC、Quota、PSP、NetworkPolicy等） 接口层：kubectl命令行工具、客户端SDK以及集群联邦 生态系统：在接口层之上的庞大容器集群管理调度的生态系统，可以划分为两个范畴 Kubernetes外部：日志、监控、配置管理、CI、CD、Workflow、FaaS、OTS应用、ChatOps等 Kubernetes内部：CRI、CNI、CVI、镜像仓库、Cloud Provider、集群自身的配置和管理等 二. 在K8s中运行一个容器应用 下面通过运行一个容器应用的过程，来一起理解一下K8s组件是如何协作的。 开发者开发一个应用后，打包Docker镜像，上传到Docker registry；然后编写一个yaml部署描述文件，以描述应用的结构和资源需求。开发者通过kubectl（或其它应用），将部署描述文件提交到API server，API server将部署需求更新到etcd。etcd在K8s管理结点中的作用相当于数据库，其它组件提交到API server的数据都存储于etcd。API server非常轻量，并不会直接去创建或管理Pod等资源，在多数场景下甚至不会去主动调用其它的K8s组件发出指令。其它组件通过建立和API server的长连接，监视关心的对象，监视到变化后，执行所负责的操作。 继续我们的启动应用之旅，如图所示，Controller Manager中的控制器监视到新的部署描述后，根据部署描述，创建ReplicaSet、Pod等资源。Scheduler监视到新的Pod资源后，结合集群的资源情况，选定一或多个工作结点运行Pod。工作结点上的Kubelet监视到有Pod被计划在自己的结点后，向Docker等Container runtime发出启动容器的指令，Docker engineer将按照指令从Docker registy拉取镜像，然后启动并运行容器。 三. K8s集群的高可用部署 通过之前的介绍，我们看到K8s可以在多个工作结点上启动并管理容器，下面来学习一下，如何实现管理结点的高可用部署。 上图的K8s高可用部署中有3个管理结点。etcd自身是一个分布式数据存储系统，按照其多实例部署方案，结点只需在启动时知道其它结点的IP和端口号即可组成高可用环境。和通常的应用服务器一样，API Server是无状态的，可以运行任意多个实例，且彼此之间无需互相知道。为了能使kubectl等客户端和Kubelet等组件连接到健康的API Server、减轻单台API Server的压力，需使用基础架构提供的负载均衡器作为多个API Server实例的入口。如上图的部署方法，每个主结点上都运行了一个etcd实例，这样API Server只需连接本地的etcd实例即可，无需再使用负载均衡器作为etcd的入口。 Controller Manager和Scheduler需要修改K8s集群，同时修改时可能引发并发问题。假设两个ReplicaSet Controller同时监视到需创建一个Pod，然后同时进行创建操作，就会创建出两个Pod。K8s为了避免这个问题，一组此类组件的实例将选举出一个leader，仅有leader处于活动状态，其它实例处于待命状态。Controller Manager和Scheduler也可以独立于API server部署，通过负载均衡器连接到多个API server实例。 范例 分析各个组件的作用以及架构工作流程: 1) kubectl发送部署 请求到API server 2) APIserver通知Controller Manager创建一个Deployment资源。 3) Scheduler执行调度任务,将两个副本Pod分发到node01和node02. 上。 4) node01和node02, 上的kubelet在各自节点上创建并运行Pod。 补充 1.应用的配置和当前的状态信息保存在etcd中，执行kubectl get pod时API server会从etcd中读取这些数据。 2.flannel会为每个Pod分配一个IP。 但此时没有创建Service资源，目前kube-proxy还没有参与进来。 运行一个例子（创建一个deployment资源对象&lt;pod控制器&gt;） 12[root@master ~]# kubectl run test-web --image=httpd --replicas=2//创建一个deployment资源对象。 运行完成之后，如果有镜像可直接开启，没有的话需要等待一会儿，node节点要在docker hup上下载 查看一下 1[root@master ~]# kubectl get deployments.或 kubectl get deploy 1[root@master ~]# kubectl get pod 12[root@master ~]# kubectl get pod -o wide//显示pod的节点信息 如果，node节点没有运行test-web服务，需要在节点上重启一下 如果删除一个pod 1[root@master ~]# kubectl delete pod test-web-5b56bdff65-2njqf 查看一下 1[root@master ~]# kubectl get pod -o wide 现在发现容器还存在，因为控制器会自动发现，一旦与之前执行的命令有误差，他会自动补全。 https://blog.csdn.net/gongxsh00/article/details/79932136 https://www.jianshu.com/p/18edac81c718","path":"posts/e863.html","date":"06-07","excerpt":"","tags":[{"name":"docker","slug":"docker","permalink":"https://wsdlxgp.top/tags/docker/"},{"name":"kubeadml","slug":"kubeadml","permalink":"https://wsdlxgp.top/tags/kubeadml/"}]},{"title":"45 k8s复习","text":"创建镜像的方法 1234567891011121314151617[root@master xgp]# vim DockerfileFROM nginxADD index.htm /usr/share/nginx/html///创建Dockerfile[root@master test]# echo \"&lt;h1&gt;version 01 wsd&lt;/h1&gt;\" &gt; index.html[root@master test]# docker build -t 192.168.1.1:5000/nginx .[root@master test]# echo \"&lt;h1&gt;version 02 wsd&lt;/h1&gt;\" &gt; index.html [root@master test]# docker build -t 192.168.1.1:5000/nginx:v1.14 [root@master test]# echo \"&lt;h1&gt;version 03 wsd&lt;/h1&gt;\" &gt; index.html .[root@master test]# docker build -t 192.168.1.1:5000/nginx:v1.15 .//创建不同index.html文件，生成测试镜像[root@master test]# docker push 192.168.1.1:5000/nginx[root@master test]# docker push 192.168.1.1:5000/nginx:v1.14[root@master test]# docker push 192.168.1.1:5000/nginx:v1.15//上传镜像 2) deployment名字为:nginx,保证运行3个Pod.service名字为：nginx-svc。映射到主机端口：31234.（10分） 12345678910111213141516171819202122232425262728293031[root@master yaml]# docker pull nginx//下载nginx镜像[root@master yaml]# vim deployment.yaml //编写deployment和service的yaml文件apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginxspec: replicas: 3 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx---apiVersion: v1kind: Servicemetadata: name: nginx-svcspec: type: NodePort selector: app: nginx ports: - port: 80 targetPort: 80 nodePort: 31234 执行一下 1[root@master yaml]# kubectl apply -f deployment.yaml 查看一下 1[root@master yaml]# kubectl get pod 1[root@master yaml]# kubectl get svc 访问一下http://192.168.1.21:31234/ 3) 共有3个版本，版本1对应image镜像为：nginx，版本2对应的image为：nginx:1.14.版本3对应的版本为:nginx:1.15.分别运行各版本，每个版本要有在浏览器的访问验证。（10分） 1234[root@master yaml]# docker pull nginx[root@master yaml]# docker pull nginx:1.14[root@master yaml]# docker pull nginx:1.15//下载所需镜像 编写deployment的yaml文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051[root@master yaml]# vim banben1.yaml//编写deployment和service的yaml文件apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginxspec: replicas: 3 template: metadata: labels: app: nginx-svc spec: containers: - name: nginx image: nginx #更改一下镜像（1.14和1.15的）[root@master yaml]# vim banben2.yaml//编写deployment和service的yaml文件apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginx2spec: replicas: 3 template: metadata: labels: app: nginx-svc spec: containers: - name: nginx image: nginx:1.14 #更改一下镜像（1.14和1.15的）[root@master yaml]# vim banben3.yaml//编写deployment和service的yaml文件apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginx3spec: replicas: 3 template: metadata: labels: app: nginx-svc spec: containers: - name: nginx image: nginx:1.15 #更改一下镜像（1.14和1.15的） 编写service的yaml文件 1234567891011121314[root@master yaml]# vim ngnix-svc.yaml apiVersion: v1kind: Servicemetadata: name: nginx-svcspec: type: NodePort selector: app: nginx-svc ports: - port: 80 targetPort: 80 nodePort: 31235 执行一下（记录版本信息） 1234[root@master yaml]# kubectl apply -f banben1.yaml --record [root@master yaml]# kubectl apply -f banben2.yaml --record [root@master yaml]# kubectl apply -f banben3.yaml --record [root@master yaml]# kubectl apply -f ngnix-svc.yaml 查看一下 1[root@master yaml]# kubectl get pod 1[root@master yaml]# kubectl get svc 访问一下 http://192.168.1.21:31235/ 4)运行到版本3之后，进行回滚操作回滚到版本4.（5分） 查看记录的版本信息 1[root@master yaml]# kubectl rollout history deployment nginx 回滚到指定版本 12[root@master ~]# kubectl rollout undo deployment nginx --to-revision=4//这里指定的是版本信息的编号 访问一下 5) 此时更改默认的3个Pod的访问界面,.版本1的访问界面内容为：考生名称+version:No1.版本2的访问界面:考生名称+version:No2,以此类推。（5分） 修改POD页面内容（三台不一样） 12[root@master ~]# kubectl exec -it xgp-web-8d5f9656f-8z7d9 /bin/bash//根据pod名称进入pod之中 进入容器后修改页面内容 12345678910111213141[root@master yaml]# kubectl exec -it nginx-d6c5c85cb-8vcvt /bin/bashroot@nginx-d6c5c85cb-8vcvt:/# echo \"&lt;h1&gt;version 01 wushaodong&lt;/h1&gt;\" &gt; /usr/share/nginx/html/index.html root@nginx-d6c5c85cb-8vcvt:/# exit2[root@master yaml]# kubectl exec -it nginx-d6c5c85cb-bxvvt /bin/bashroot@nginx-d6c5c85cb-bxvvt:/# echo \"&lt;h1&gt;version 02 wushaodong&lt;/h1&gt;\" &gt; /usr/share/nginx/html/index.htmlroot@nginx-d6c5c85cb-bxvvt:/# exit3[root@master yaml]# kubectl exec -it nginx-d6c5c85cb-lhlz9 /bin/bashroot@nginx-d6c5c85cb-lhlz9:/# echo \"&lt;h1&gt;version 03 wushaodong&lt;/h1&gt;\" &gt; /usr/share/nginx/html/index.htmlroot@nginx-d6c5c85cb-lhlz9:/# exit 6) 验证界面是否会会有轮训效果，并加以分析论述。（5分） 不要在浏览器里测试轮询，有缓存 1[root@master ~]# curl 127.0.0.1:31235 答：会有轮询的效果，kubernetes 内部的负载均衡是通过 iptables 的 probability 特性来做到的，kube-proxy通过iptables 将访问 Service 的流量转发到后端 Pod，而且使用类似轮询的负载均衡策略。 7) 创建一个NFS PV，NFS共享目录为：考生名称。PV名称为：new-pv。创建一个PVC，名称为new-pvc。单独创建一个pod，使用new-pv，运行之后，验证nfs是否使用成功。（10分） 12345678910111213[root@master ~]# yum -y install nfs-utils rpcbind[root@master yaml]# mkdir /wushaodong//创建指定名称的共享目录[root@master yaml]# echo \"/wushaodong *(rw,sync,no_root_squash)\" &gt; /etc/exports//编写共享目录的权限[root@master ~]# systemctl start nfs-server[root@master ~]# systemctl start rpcbind//启动服务[root@master yaml]# showmount -e//测试一下 1、创建一个NFS PV的yaml文件 12345678910111213141516171819[root@master yaml]# vim new-pv.yamlapiVersion: v1kind: PersistentVolumemetadata: name: new-xgpspec: capacity: storage: 1Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: nfs nfs: path: /wushaodong/new-pv server: 192.168.1.21[root@master yaml]# mkdir /wushaodong/new-pv//创建指定目录 执行一下 1[root@master yaml]# kubectl apply -f new-pv.yaml 查看一下 1[root@master yaml]# kubectl get pv 2、创建一个PVC的yaml文件 123456789101112[root@master yaml]# vim new-pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: new-pvcspec: accessModes: #要和pv的一直否则关联不成功 - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: nfs #要和pv的一直否则关联不成功 执行一下 1[root@master yaml]# kubectl apply -f new-pvc.yaml 查看一下 1[root@master yaml]# kubectl get pvc 3、单独创建一个pod，使用new-pv 1234567891011121314151617181920[root@master yaml]# vim pod.yamlapiVersion: v1kind: Podmetadata: name: xgp-podspec: containers: - name: xgp-pod image: busybox args: - /bin/sh - -c - sleep 300000 volumeMounts: - mountPath: /wushaodong #容器的被挂载目录 name: volumedata volumes: - name: volumedata persistentVolumeClaim: claimName: new-pvc 执行一下 1[root@master yaml]# kubectl apply -f pod.yaml 查看一下 1[root@master yaml]# kubectl get pod 4、测试一下 12345[root@master yaml]# kubectl exec -it xgp-pod /bin/sh//进入pod# echo \"xgpIwsd\" &gt; /wushaodong/xgp.txt//添加内容到挂载目录# exit 查看一下，挂载目录是否有添加内容 1[root@master yaml]# cat /wushaodong/new-pv/xgp.txt 8）请简述k8s集群中，master节点有哪些组件，node节点有哪些组件，作用分别有什么作用，各组件又是怎么交互的。（5分） master节点 1. API server[资源操作入口]：是k8s集群的前端接口，各种各样客户端工具以及k8s的其他组件可以通过它管理k8s集群的各种资源。它提供了HTTP/HTTPS RESTful API,即K8S API。 提供了资源对象的唯一操作入口，其他所有组件都必须通过它提供的API来操作资源数据，只有API Server与存储通信，其他模块通过API Server访问集群状态。 第一，是为了保证集群状态访问的安全。 第二，是为了隔离集群状态访问的方式和后端存储实现的方式：API Server是状态访问的方式，不会因为后端存储技术etcd的改变而改变。 作为kubernetes系统的入口，封装了核心对象的增删改查操作，以RESTFul接口方式提供给外部客户和内部组件调用。对相关的资源数据“全量查询”+“变化监听”，实时完成相关的业务功能。 2. Scheduler[集群分发调度器]：负责决定将Pod放在哪个Node上运行。在调度时，会充分考虑集群的拓扑结构，当前各个节点的负载情况，以及应对高可用、性能、数据亲和性和需求。 1.Scheduler收集和分析当前Kubernetes集群中所有Minion节点的资源(内存、CPU)负载情况，然后依此分发新建的Pod到Kubernetes集群中可用的节点。 2.实时监测Kubernetes集群中未分发和已分发的所有运行的Pod。 3.Scheduler也监测Minion节点信息，由于会频繁查找Minion节点，Scheduler会缓存一份最新的信息在本地。 4.最后，Scheduler在分发Pod到指定的Minion节点后，会把Pod相关的信息Binding写回API Server。 4. Controller Manager[内部管理控制中心]：负责管理集群的各种资源，保证资源处于预期的状态。它由多种Controller组成，包括Replication Controller、Endpoints Controller、Namespace Controller、Serviceaccounts Controller等。 实现集群故障检测和恢复的自动化工作，负责执行各种控制器，主要有： 1.endpoint-controller：定期关联service和pod(关联信息由endpoint对象维护)，保证service到pod的映射总是最新的。 2.replication-controller：定期关联replicationController和pod，保证replicationController定义的复制数量与实际运行pod的数量总是一致的。 **5. Etcd：**负责保存k8s集群的配置信息和各种资源的状态信息。当数据发生变化时，etcd会快速的通知k8s相关组件。（第三方组件）它有可替换方案。Consul、zookeeper 6. Pod: k8s集群的最小组成单位。一个Pod内，可以运行一个或多个容器。大多数情况下，一个Pod内只有一个Container容器。 **7. Flanner：**是k8s集群网络，可以保证Pod的跨主机通信。也有替换方案。 Node节点 Kubelet[节点上的Pod管家]：它是Node的agent(代理)，当Scheduler确定某 个Node上运行Pod之后，会将Pod的具体配置信息发送给该节点的kubelet,kubelet会根据这些信息创建和运行容器，并向Master报告运行状态。 负责Node节点上pod的创建、修改、监控、删除等全生命周期的管理 定时上报本Node的状态信息给API Server。 kubelet是Master API Server和Minion之间的桥梁，接收Master API Server分配给它的commands和work，与持久性键值存储etcd、file、server和http进行交互，读取配置信息。 具体的工作如下： 设置容器的环境变量、给容器绑定Volume、给容器绑定Port、根据指定的Pod运行一个单一容器、给指定的Pod创建network 容器。 同步Pod的状态、同步Pod的状态、从cAdvisor获取Container info、 pod info、 root info、 machine info。 在容器中运行命令、杀死容器、删除Pod的所有容器。 **kube-proxy[负载均衡、路由转发]:**负责将访问service的TCP/UDP数据流转发到后端的容器。如果有多个 副本，kube-proxy会实现负载均衡。 Proxy是为了解决外部网络能够访问跨机器集群中容器提供的应用服务而设计的，运行在每个Node上。Proxy提供TCP/UDP sockets的proxy，每创建一种Service，Proxy主要从etcd获取Services和Endpoints的配置信息（也可以从file获取），然后根据配置信息在Minion上启动一个Proxy的进程并监听相应的服务端口，当外部请求发生时，Proxy会根据Load Balancer将请求分发到后端正确的容器处理。 Proxy不但解决了同一主宿机相同服务端口冲突的问题，还提供了Service转发服务端口对外提供服务的能力，Proxy后端使用了随机、轮循负载均衡算法。 各个组件的作用以及架构工作流程: 1) kubectl发送部署 请求到API server 2) APIserver通知Controller Manager创建一个Deployment资源。 3) Scheduler执行调度任务,将两个副本Pod分发到node01和node02. 上。 4) node01和node02, 上的kubelet在各自节点上创建并运行Pod。 补充 1.应用的配置和当前的状态信息保存在etcd中，执行kubectl get pod时API server会从etcd中读取这些数据。 2.flannel会为每个Pod分配一个IP。 但此时没有创建Service资源，目前kube-proxy还没有参与进来。 9）部署一个dashboard。（5分） 1、下载所需yaml文件和镜像 12[root@master https]# wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-rc5/aio/deploy/recommended.yaml[root@master https]# docker pull kubernetesui/dashboard:v2.0.0-rc5 2、修改 recommended.yaml 12345678910111213141516[root@master https]#vim recommended.yaml ---kind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboardspec: type: NodePort #添加40 ports: - port: 443 targetPort: 8443 selector: k8s-app: kubernetes-dashboard 执行一下 1[root@master https]# kubectl apply -f recommended.yaml 查看一下 1[root@master https]# kubectl get svc -n kubernetes-dashboard 3、浏览器访问https://192.168.1.21:30949/ PS:如果是使用的旧版本的dashboard, 使用Google浏览器登录，可能是不成功的，需要换成其他的浏览器，比如:火狐。 4、基于token的方法登录dashboard &lt;1&gt;创建一个dashboard的管理用户 1[root@master https]# kubectl create serviceaccount dashboard-admin -n kube-system &lt;2&gt;绑定用户为集群管理用户 1[root@master https]# kubectl create clusterrolebinding dashboard-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin &lt;3&gt;获取Token 12[root@master https]# kubectl get secrets -n kube-system | grep dashboard-admin//先得到Token的名称 12[root@master https]# kubectl describe secrets -n kube-system dashboard-admin-token-j874n//查看上述得到的secret资源的详细信息，会得到token &lt;4&gt;在浏览器上使用token登录。 成功界面 10）使用helm的方式，部署mysql服务，要求使用storageclass作为持久化存储，服务运行之后，进入数据库，创建一个test库，库中一张test表，内容为： 9527. 然后模拟数据库Pod失败，待Pod重启后，查看对应数据是否还存在？（10分） 1、安装部署helm工具 （1）下载helm的包 12[root@master ~]#docker pull gcr.io/kubernetes-helm/tiller:v2.14.3[root@master ~]# wget https://get.helm.sh/helm-v2.14.3-linux-amd64.tar.gz （2）把helm包的命令，复制到本地 123456[root@master helm]# mv linux-amd64/helm /usr/local/bin///移动命令目录到/usr/local/bin/[root@master helm]# chmod +x /usr/local/bin/helm //给予执行权限[root@master helm]# helm help//验证是否安装成功 （3）设置命令自动补全 123[root@master helm]# echo 'source &lt;(helm completion bash)' &gt;&gt; /etc/profile[root@master helm]# . /etc/profile//刷新一下 2、安装Tiller server（服务端，需要创建授权用户） 12345678910111213141516171819[root@master ~]# vim tiller-rbac.yaml #创建授权用户apiVersion: v1kind: ServiceAccountmetadata: name: tiller namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: tillerroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects: - kind: ServiceAccount name: tiller namespace: kube-system 执行一下 1[root@master ~]# kubectl apply -f tiller-rbac.yaml （1）Tiller server的环境初始化 12[root@master helm]# helm init --service-account=tiller//helm的服务端就是Tiller（因为是访问外国的网站，可能需要多次执行） 查看一下 1[root@master helm]# kubectl get deployment. -n kube-system 现在发现没有开启，那是因为默认下载的Google的镜像，下载不下来 （2）设置镜像源改为阿里云的 1[root@master helm]# helm repo add stable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts 查看一下 1[root@master helm]# helm version 3、基于NFS服务，创建共享。 因为上面已经做过了，所以现在只需创建目录和设置权限即可 123456789[root@master heml]# mkdir /xgpwsd//创建目录[root@master heml]# echo '/xgpwsd *(rw,sync,no_root_squash)' &gt;&gt; /etc/exports//设置共享目录权限[root@master heml]# systemctl restart nfs-server[root@master heml]# systemctl restart rpcbind//重启nfs服务[root@master heml]# showmount -e//测试一下 4、创建pv 12345678910111213141516[root@master xgp]# vim nfs-pv1.yml apiVersion: v1kind: PersistentVolumemetadata: name: mysqlpvspec: capacity: storage: 8Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle nfs: path: /xgpwsd/xgp server: 192.168.1.21[root@master xgp]# mkdir /xgpwsd/xgp//创建所需目录 执行一下 1[root@master xgp]# kubectl apply -f nfs-pv1.yml 查看一下 1[root@master xgp]# kubectl get pv 5、创建StorageClass资源对象。 （1）创建rbac权限。 123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@master yaml]# vim rbac.yaml apiVersion: v1kind: ServiceAccountmetadata: name: nfs-provisioner namespace: default---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: nfs-provisioner-runner namespace: defaultrules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"watch\", \"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\", \"endpoints\"] verbs: [\"get\",\"create\",\"list\", \"watch\",\"update\"] - apiGroups: [\"extensions\"] resources: [\"podsecuritypolicies\"] resourceNames: [\"nfs-provisioner\"] verbs: [\"use\"]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: run-nfs-provisionersubjects: - kind: ServiceAccount name: nfs-provisioner namespace: default #必写字段roleRef: kind: ClusterRole name: nfs-provisioner-runner apiGroup: rbac.authorization.k8s.io 执行一下 1[root@master yaml]# kubectl apply -f rbac.yaml （2）创建Deployment资源对象，用Pod代替 真正的NFS服务。 123456789101112131415161718192021222324252627282930313233343536[root@master yaml]# vim nfs-deployment.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nfs-client-provisionerspec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-client-provisioner spec: serviceAccount: nfs-provisioner containers: - name: nfs-client-provisioner image: registry.cn-hangzhou.aliyuncs.com/open-ali/nfs-client-provisioner volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: xgp - name: NFS_SERVER value: 192.168.1.21 - name: NFS_PATH value: /xgpwsd/wsd volumes: - name: nfs-client-root nfs: server: 192.168.1.21 path: /xgpwsd/wsd [root@master heml]# mkdir /xgpwsd/wsd//创建指定目录 执行一下 1[root@master yaml]# kubectl apply -f nfs-deployment.yaml 查看一下 1[root@master yaml]# kubectl get pod （3）创建storageclass的yaml文件 1234567[root@master yaml]# vim xgp-storageclass.yaml apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: xgp-nfsprovisioner: xgp #通过provisioner字段关联到上述DeployreclaimPolicy: Retain 执行一下 1[root@master yaml]# kubectl apply -f xgp-storageclass.yaml 查看一下 1[root@master yaml]# kubectl get sc 6、创建一个mysql服务 123456789[root@master ~]# docker pull mysql:5.7.14//下载所需镜像[root@master yaml]# helm fetch stable/mysql//直接下载stable/mysql的chart包[root@master yaml]# tar -zxf mysql-0.3.5.tgz //解压mysql包[root@master yaml]# cd mysql/[root@master mysql]# vim values.yaml //修改values.yaml文件，添加storageClass存储卷 12[root@master mysql]# helm install stable/mysql -n xgp-mysql --set mysqlRootPassword=123.com -f values.yaml //基于values.yaml和stable/mysql开启一个密码为123.com的mysqlpod 查看一下 1[root@master mysql]# kubectl get svc 1[root@master mysql]# kubectl get pod -o wide 7、进入mysql数据库，创建一个test库，库中一张test表，内容为： 9527。 1[root@master xgp]# kubectl exec -it bdqn-mysql-mysql-7b89c7b99-8ff2r -- mysql -u root -p123.com 创建数据库 1mysql&gt; create database test; 切换数据库 1mysql&gt; use test; 创建表 1mysql&gt; create table test( id int(4))； 在表中插入数据 1mysql&gt; insert test values(9527); 查看表 1mysql&gt; select * from test; 8、模拟数据库Pod失败，待Pod重启后，查看对应数据是否还存在？ 123[root@master mysql]# kubectl delete pod xgp-mysql-mysql-67c6fb5f9-4h4kz//删除这个pod让他重新生成[root@master mysql]# kubectl get pod 进入新的pod查看 12345678910111213[root@master mysql]# kubectl exec -it xgp-mysql-mysql-67c6fb5f9-k4c29 -- mysql -u root -p123.commysql&gt; use test;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; select * from test;+------+| id |+------+| 9527 |+------+1 row in set (0.00 sec)","path":"posts/h8er.html","date":"06-07","excerpt":"","tags":[{"name":"nfs","slug":"nfs","permalink":"https://wsdlxgp.top/tags/nfs/"},{"name":"pv","slug":"pv","permalink":"https://wsdlxgp.top/tags/pv/"},{"name":"pvc","slug":"pvc","permalink":"https://wsdlxgp.top/tags/pvc/"},{"name":"StorageClass","slug":"StorageClass","permalink":"https://wsdlxgp.top/tags/StorageClass/"},{"name":"dashboard","slug":"dashboard","permalink":"https://wsdlxgp.top/tags/dashboard/"},{"name":"helm","slug":"helm","permalink":"https://wsdlxgp.top/tags/helm/"},{"name":"deployment","slug":"deployment","permalink":"https://wsdlxgp.top/tags/deployment/"}]},{"title":"44 k8s的持续集成（jenkins+gitlab+k8s）","text":"应用场景： 问题项目分为app和后台两种，为了保证再同一个环境下面测试，所以不可能链接开发本地服务进行测试，所以需要搭建一个测试环境，供app进行开发测试。这个时候就有一个问题，如果开发新增加功能或者app调试的时候发现问题，这个时候就需要提交新的代码或者修复bug，然后重新发布到测试环境中去。但是后台人员又不能进入Linux服务器中，只能通过Linux运维人员来重新部署，这样的效率就会极低。 方案：基于这种模式下面的，我们引入了Jenkins工具，通过Jenkins来拉取svn/git代码到服务器中，再Jenkins中编写Linux运行脚本，通过脚本我们就可以对代码进行编译运行，然后重新发布到服务器中运行。后端人员也不需要通知Linux运维人员来执行这个操作，直接再Jenkins的控制台就可以执行了。 实验环境 IP 主机名称 服务 192.168.1.21 master k8s 192.168.1.22 node01 k8s 192.168.1.10 git gitlab 192.168.1.13 jenkins jenkins 总体流程： 在开发机开发代码后提交到gitlab 之后通过webhook插件触发jenkins进行构建，jenkins将代码打成docker镜像，push到docker-registry 之后将在k8s-master上执行rc、service的创建，进而创建Pod，从私服拉取镜像，根据该镜像启动容器 应用构建和发布流程说明。 用户向Gitlab提交代码，代码中必须包含Dockerfile 将代码提交到远程仓库 用户在发布应用时需要填写git仓库地址和分支、服务类型、服务名称、资源数量、实例个数，确定后触发Jenkins自动构建 Jenkins的CI流水线自动编译代码并打包成docker镜像推送到Harbor镜像仓库 Jenkins的CI流水线中包括了自定义脚本，根据我们已准备好的kubernetes的YAML模板，将其中的变量替换成用户输入的选项 生成应用的kubernetes YAML配置文件 更新Ingress的配置，根据新部署的应用的名称，在ingress的配置文件中增加一条路由信息 更新PowerDNS，向其中插入一条DNS记录，IP地址是边缘节点的IP地址。关于边缘节点，请查看边缘节点配置 Jenkins调用kubernetes的API，部署应用 一、前期工作 1、先验证k8s集群（1.21和1.22） 1[root@master ~]# kubectl get nodes 2、master部署私有仓库 Docker01部署 12345678910111213141516171872 docker pull registry//下载registry镜像73 docker run -itd --name registry -p 5000:5000 --restart=always registry:latest//基于registry镜像，启动一台容器78 vim /usr/lib/systemd/system/docker.service #13行修改ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.21:5000 80 systemctl daemon-reload81 systemctl restart docker.service//重启docker76 docker tag httpd:latest 192.168.1.11:5000/web:v1 76 docker tag httpd:latest 192.168.1.11:5000/web:v2//把容器重命名一个标签77 docker ps 123456789101178 vim /usr/lib/systemd/system/docker.service #13行修改ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.11:5000 80 systemctl daemon-reload81 systemctl restart docker.service//重启docker100 docker push 192.168.1.11:5000/web:v1100 docker push 192.168.1.11:5000/web:v2//上传容器到私有仓库 Docker02和docker03加入私有仓库 12345678978 vim /usr/lib/systemd/system/docker.service #13行修改ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.11:5000 80 systemctl daemon-reload81 systemctl restart docker.service//重启docker99 docker pull 192.168.1.21:5000/web:v1//测试下载 3、然后重要的地方到了，建立 yaml配置文件让kubernetes自己控制容器集群。 用来模拟我们部署的服务 12345678910111213141516171819[root@master app]# vim deploy.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: webspec: replicas: 2 template: metadata: labels: name: web spec: containers: - name: web image: 192.168.1.21:5000/web:v1 imagePullPolicy: Always #改为本地仓库下载 ports: - containerPort: 80 执行一下 1[root@master app]# kubectl apply -f deploy.yaml 查看一下 1[root@master app]# kubectl get pod 可是容器的ip只能在容器本机上访问，集群内的其他主机和集群外的主机都没办法访问，这个时候就需要将容器的端口映射到服务器上的端口了，所以需要做一个service的模板。service 模板可以将容器的端口映射到服务器的端口上，并且可以固定映射在服务器上的端口。 12345678910111213141516[root@master app]# vim deploy-svc.yamlapiVersion: v1kind: Servicemetadata: labels: name: web name: webspec: type: NodePort ports: - port: 80 targetPort: 80 nodePort: 31234 selector: name: web 执行一下 1[root@master app]# kubectl apply -f deploy-svc.yaml 查看一下 1[root@master app]# kubectl get svc 访问一下http://192.168.1.21:31234/ 《ok kubernetes 完毕， 开始配置 jenkins+gitlab联动》 4、git和jenkins加入私有仓库 12345678978 vim /usr/lib/systemd/system/docker.service #13行修改ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.11:5000 80 systemctl daemon-reload81 systemctl restart docker.service//重启docker99 docker pull 192.168.1.11/busybox:v1//测试下载 5、jenkins服务器向k8smaster做免密登录 1100 ssh-copy-id 192.168.1.21 二、安装jenkins（1.13） 安装java环境 123456789101112131415[root@jenkins ~]# tar -zxf jdk-8u231-linux-x64.tar.gz[root@jenkins ~]# mv jdk1.8.0_131 /usr/java#注意 这里有位置敏感，不要多一个“/”[root@jenkins ~]# vim /etc/profile #在最下面写export JAVA_HOME=/usr/javaexport JRE_HOME=/usr/java/jreexport PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$PATHexport CLASSPATH=$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar[root@jenkins ~]# source /etc/profile//环境变量生效[root@jenkins ~]# java -version//验证环境变量 安装tomcat 1234567[root@jenkins ~]# tar -zxf apache-tomcat-7.0.54.tar.gz [root@jenkins ~]# mv apache-tomcat-7.0.54 /usr/tomcat7[root@jenkins ~]# cd /usr/tomcat7/webapps/[root@jenkins webapps]# rm -rf *[root@jenkins webapps]# cp /root/jenkins.war . #这几步是jenkins的包放进了tomcat里[root@jenkins webapps]# vim /usr/tomcat7/conf/server.xml //修改tomcat的字符集 12345678910[root@jenkins webapps]# cd /usr/tomcat7/bin/[root@jenkins bin]# vim catalina.sh #!/bin/shexport CATALINA_OPTS=\"-DJENKINS_HOME=/data/jenkins\"export JENKINS_JAVA_OPTIONS=\"-Djava.awt.headless=true -Dhudson.ClassicPluginStrategy.noBytecodeTransformer=true\"//这两行添加的是jenkins的家目录位置，这个很重要[root@jenkins bin]# ./catalina.sh start //启动tomcat 1[root@jenkins bin]# netstat -anput | grep 8080 浏览器安装jenkins http://192.168.1.11:8080/jenkins 12[root@jenkins bin]# cat /data/jenkins/secrets/initialAdminPasswordc577cbf75d934878a94b0f9e00ada328 //复制密码 （1）推荐安装 #左边是自动安装， 右边是自定义安装，我们选左边的，如果不是这个画面则说明网络很卡或者没有网(推荐使用右边的，然后选择不安装插件，之后可以自定义安装） （2）这个是自定义安装（自己上传的包） 12345678[root@autoweb bin]# ./catalina.sh stop[root@autoweb ~]# cd /data/jenkins/plugins/[root@autoweb jenkins]# mv plugins plugins/.bk然后上传plugins.tar.gz包：[root@autoweb jenkins]# tar -zxf plugins.tar.gz [root@autoweb ~]# cd /usr/tomcat7/bin/[root@autoweb bin]# ./catalina.sh stop[root@autoweb bin]# ./catalina.sh start 输入密码后断网 （3）两个剩下的方法一样 下载中文插件 系统管理-----&gt;插件管理-----&gt;avalilable(可选)然后搜索localization-zh-cn 然后还需要3个插件 三、安装gitlab（1.10） GitLab CI 是 GitLab 默认集成的 CI 功能，GitLab CI 通过在项目内 .gitlab-ci.yaml 配置文件读取 CI 任务并进行相应处理；GitLab CI 通过其称为 GitLab Runner 的 Agent 端进行 build 操作；Runner 本身可以使用多种方式安装，比如使用 Docker 镜像启动等；Runner 在进行 build 操作时也可以选择多种 build 环境提供者；比如直接在 Runner 所在宿主机 build、通过新创建虚拟机(vmware、virtualbox)进行 build等；同时 Runner 支持 Docker 作为 build 提供者，即每次 build 新启动容器进行 build；GitLab CI 其大致架构如下 12345# yum -y install curl policycoreutils openssh-server openssh-clients postfix git# systemctl enable sshd# systemctl start sshd# systemctl enable postfix# systemctl start postfix 安装gitlab-ce 1[root@git ~]# curl -sS https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.rpm.sh | sudo bash 注：由于网络问题，国内用户，使用清华大学的镜像源进行安装： 1234567891011121314151617181920212223242526272829[root@git ~]# vim /etc/yum.repos.d/gitlab-ce.repo[gitlab-ce]name=gitlab-cebaseurl=http://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7repo_gpgcheck=0gpgcheck=0enabled=1gpgkey=https://packages.gitlab.com/gpg.key[root@git ~]# yum makecache//保存到本地[root@git ~]# yum -y install gitlab-ce #这两条命令是把gitlab源先加入了yum，然后yum下载gitlab[root@git ~]# vim /etc/gitlab/gitlab.rb //修改端口是为了防止端口冲突，因为80默认是http服务的 external_url 'http://192.168.1.21:90' #端口， unicorn默认是8080 也是tomcat的端口 unicorn['listen'] = '127.0.0.1'unicorn['port'] = 3000 [root@git ~]# gitlab-ctl reconfigure //启动gitlab，这个过程可能会有点慢[root@git ~]# ls /etc/yum.repos.d///查看一下 访问192.168.1.10:90 在网页配置用户密码后则安装完毕。用户默认root，这里让设置一个密码再登录，这里设置12345.com（相对较短的密码不让设置） 四、jenkins和gitlab相互关联 jenkins：工具集成平台 gitlab: 软件托管平台 部署这两个服务的联动，需要经过ssh验证。 1、首先我们需要在gitlab上绑定jenkins服务器的ssh公钥，这里我们使用的是root用户的公私钥，切记生产环境是不允许随便用root的 （1）jenkins 12[root@jenkins ~]# ssh-keygen -t rsa //然后不输入只回车会生成一对公私钥 默认在/root/.ssh/目录里 123[root@jenkins ~]# cat /root/.ssh/id_rsa.pub //查看公钥并复制ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDMA4+je3NsxZrF2v8TPLXJp1ejwy1YokXipEFyGVNo5IbtkiBDwBLOAl5i7yromY8YGgoNNriE2g89IM/44BGC5UDCokQ69Ze9Ta9Kynv3/1PDFXIABJJG0f6LsUqt0nKFaFoGz3ZuYAnl6AzLpXEic8DBDrsFk+UGrxvMfSEqHlYO2b7jRXE1HGRnqI/IcVB190cLT1kmBKi7hSqUNBc1cY6t3a6gGiBpp9tc8PW4r/RcLblhAL1LKx8x37NOZkqox8IMh3eM/wtWwAVFlI8XU+sz9akzJOVmd1ArT5Q4w8WA/uVHCDUGVI/fli/ZRv+mNZyF3EH26runctb5LkCT root@jenkins （2）gitlab 在这里放刚才拷贝的公钥保存就行了。 我们先在gitlab上创建一个代码仓库 点击 new project 输入一个仓库的名字，权限选择公共的（public）然后直接点击创建 点击新建一个new.file 写入代码，起一个名字然后保存 创建好了，然后在本地测试一下是否可用 12345678910[root@git ~]# mkdir xgp[root@git ~]# cd xgp/[root@git xgp]# git clone git@192.168.1.10:root/xgp-demo.git//克隆xgp-demo仓库到本地[root@git xgp]# ls xgp-demo/index.html[root@git xgp]# cat xgp-demo/index.html print: \"hello word!!!\"//查看一下 （3）自动构建 安装插件 先进入到之前查看插件的地方 系统设置----插件管理----高级_—上传插件gitlab-oauth、gitlab-plugin、 windows-slaves、ruby-runt ime、gitlab-hook （4）如果可以用，则打开jenkins 点击新建 地址粘贴进去以后没有报错则没错 但是很伤心它报错了，那是因为jenkins和git没有关联上 解决 git主机生成ssh密钥 1234[root@jenkins ~]# ssh-keygen -t rsa //然后不输入只回车会生成一对公私钥[root@jenkins ~]# cat /root/.ssh/id_rsa //查看密钥并复制 下面的这个插件很重要，就是他实现自动化更新的webhook插件，安装过了就会有这条，然后点击这条下面出来的这些东西保持默认就行。同时注意复制 这个里面写的是jenkins构建时候会执行的shell脚本，这个是最重要的，就是他实现了下端kubernetes自动更新容器的操作。 12345678910111213#!/bin/bashbackupcode=\"/data/backcode/$JOB_NAME/$BUILD_NUMBER\" mkdir -p $backupcode #jenkins创建上述目录chmod 644 \"$JENKINS_HOME\"/workspace/\"$JOB_NAME\"/*rsync -acP \"$JENKINS_HOME\"/workspace/\"$JOB_NAME\"/* $backupcode #$JENKINS_HOME和$JOB_NAME同步最新消息#ssh root@192.168.1.21 sed -i 's/v1/v2/g' /root/app/deploy.yaml #更改镜像版本echo From 192.168.1.21:5000/web:v1 &gt; \"$JENKINS_HOME\"/workspace/Dockerfileecho COPY ./\"$JOB_NAME\"/* /usr/local/apache2/htdocs/ &gt;&gt; \"$JENKINS_HOME\"/workspace/Dockerfiledocker rmi 192.168.1.21:5000/web:v1docker build -t 192.168.1.21:5000/web:v1 /\"$JENKINS_HOME\"/workspace/.docker push 192.168.1.21:5000/web:v1ssh root@192.168.1.21 kubectl delete deployment webssh root@192.168.1.21 kubectl apply -f /root/app/deploy.yaml $JOB_NAME：项目名称 $BUILD_NUMBER：第几次构建 $JENKINS_HOME：jenkins的家目录 完事以后先别保存，首先复制一下上面的jenkins地址，然后去gitlab上绑定webhook 保存，登陆gitlab，点击下图这个设置 测试显示下图 的蓝条说明jenkins 已经连通了gitlab 回到Jenkins开启匿名访问权限 测试显示下图 的蓝条说明jenkins 已经连通了gitlab 好了，jenkins和gitlab 都已经互相的ssh通过了，然后我们最后需要做的一个ssh是关于jenkins ///注意，这里是从git和jenkins向master节点做免密登录。 12[root@git ~]# ssh-copy-id root@192.168.1.21[root@jenkins ~]# ssh-copy-id root@192.168.1.21 好了，环境全部部署完毕！！！。开始测试 五、测试 测试的方法很简单，就是在gitlab上新建代码，删除代码，修改代码，都会触发webhook进行自动部署。最终会作用在所有的nginx容器中，也就是我们的web服务器。 这里我修改了之前建立的 index.html文件 保存以后，就打开浏览器 一直访问kubernetes-node 里面的容器了 访问一下http://192.168.1.21:31234/ 如果没有变，应该注意查看是否在jenkins上构建完成，等以小会就可以了。 构建成功 六、GitLab CI 总结 CS 架构 GitLab 作为 Server 端，控制 Runner 端执行一系列的 CI 任务；代码 clone 等无需关心，GitLab 会自动处理好一切；Runner 每次都会启动新的容器执行 CI 任务 容器即环境 在 Runner 使用 Docker build 的前提下；所有依赖切换、环境切换应当由切换不同镜像实现，即 build 那就使用 build 的镜像，deploy 就用带有 deploy 功能的镜像；通过不同镜像容器实现完整的环境隔离 CI即脚本 不同的 CI 任务实际上就是在使用不同镜像的容器中执行 SHELL 命令，自动化 CI 就是执行预先写好的一些小脚本 敏感信息走环境变量 一切重要的敏感信息，如账户密码等，不要写到 CI 配置中，直接放到 GitLab 的环境变量中；GitLab 会保证将其推送到远端 Runner 的 SHELL 变量中","path":"posts/b04b.html","date":"06-07","excerpt":"","tags":[{"name":"jenkins","slug":"jenkins","permalink":"https://wsdlxgp.top/tags/jenkins/"},{"name":"gitlab","slug":"gitlab","permalink":"https://wsdlxgp.top/tags/gitlab/"}]},{"title":"43 k8s的charts的四种安装方式及helm私有仓库","text":"自定义helm模板 https://hub.helm.sh/ 1、开发自己的chare包 12345678910111213141516[root@master ~]# helm create mychare//创建一个名为mychare的chare包[root@master ~]# tree -C mychare///以树状图查看一下chare包mychare/├── charts├── Chart.yaml├── templates│ ├── deployment.yaml│ ├── _helpers.tpl│ ├── ingress.yaml│ ├── NOTES.txt│ ├── service.yaml│ └── tests│ └── test-connection.yaml└── values.yaml 2、调试chart 123[root@master mychare]# cd[root@master ~]# helm install --dry-run --debug mychare//检查这个mychare是否有问题 3、安装chart 1[root@node02 ~]# docker pull nginx:stable （1）通过仓库安装 123456[root@master mychare]# helm search redis//搜索chare包[root@master mychare]# helm repo list//查看是否有能访问仓库[root@master mychare]# helm install stable/redis//安装 （2）通过tar包安装 1234567891011121314151617[root@master ~]# helm fetch stable/redis//直接下载chare包[root@master ~]# tar -zxf redis-1.1.15.tgz//解压下载的chare包[root@master ~]# tree -C redisredis├── Chart.yaml├── README.md├── templates│ ├── deployment.yaml│ ├── _helpers.tpl│ ├── networkpolicy.yaml│ ├── NOTES.txt│ ├── pvc.yaml│ ├── secrets.yaml│ └── svc.yaml└── values.yaml （3）通过chare本地目录安装 12345[root@master ~]# helm fetch stable/redis//直接下载chare包[root@master ~]# tar -zxf redis-1.1.15.tgz//解压下载的chare包[root@master ~]# helm install redis （4）通过URL安装 1[root@master ~]# helm install https://example.com/charts/foo-1.2.3.tgz （5）使用本地目录安装： 12[root@master ~]# cd mychare/[root@master mychare]# vim values.yaml 12[root@master mychare]# cd templates/[root@master templates]# vim service.yaml 123[root@master templates]# cd ..[root@master mychare]# helm install -n test ../mychare/[root@master ~]# helm upgrade test mychare/ -f mychare/values.yaml 4、例子 使用mychart部署一个实例: xgp。使用镜像为私有镜像v1 版本。 完成之后，镜像版本。 全部成功之后，将实例做一个升级，将镜像改为v2版本。 更改镜像为私有镜像 1[root@master ~]# vim mychare/values.yaml 12[root@master ~]# helm install -n xgp mychare/ -f mychare/values.yaml[root@master ~]# kubectl get deployments. -o wide 1[root@master ~]# vim mychare/values.yaml 12[root@master ~]# helm upgrade xgp mychare/ -f mychare/values.yaml [root@master ~]# kubectl get deployments. -o wide 1[root@master ~]# kubectl edit deployments. xgp-mychare 1[root@master ~]# kubectl get deployments. -o wide 创建自己的Repo仓库 1、node01启动一个httpd的容器 123456[root@node01 ~]# mkdir /var/xgp//创建一个目录[root@node01 ~]# docker pull httpd//下载httpd镜像[root@node02 ~]# docker run -d -p 8080:80 -v /var/xgp:/usr/local/apache2/htdocs httpd//启动一个httpd的容器 2、master节点上，将mychart目录打包。 12[root@master ~]# helm package mychare/Successfully packaged chart and saved it to: /root/mychare-0.1.0.tgz 3、生成仓库的index文件。 12345678[root@master ~]# mkdir myrepo//创建一个目录存放打包的chare[root@master ~]# mv mychare-0.1.0.tgz myrepo///移动打包好的文件[root@master ~]# helm repo index myrepo/ --url http://192.168.1.22:8080/charts//生成仓库的index文件[root@master ~]# ls myrepo/index.yaml mychare-0.1.0.tgz 4、将生成的tar包和index.yaml上传到node01的/var/www/charts目录下. node01创建目录 1[root@node01 ~]# mkdir /var/xgp/charts master移动动到 1[root@master ~]# scp myrepo/* node01:/var/xgp/charts/ node01查看一下 12[root@node01 ~]# ls /var/xgp/charts/index.yaml mychare-0.1.0.tgz 5、添加新的repo仓库。 12[root@master ~]# helm repo add newrepo http://192.168.1.22:8080/charts[root@master ~]# helm repo list 1[root@master ~]# helm search mychare 6、我们就可以直接使用新的repo仓库部署实例了。 12[root@master ~]# helm install newrepo/mychare -n wsd[root@master ~]# helm list 7.如果以后仓库中新添加了chart包,需要用helm repo update命玲更新本地的index文件。 练习： 新创建一个bdqn.的chart包。然后将chart包上传到上述repo源中。 12345678[root@master ~]# helm create bdqn[root@master ~]# helm package bdqn/[root@master ~]# mv bdqn-0.1.0.tgz myrepo/[root@master ~]# helm repo index myrepo/ --url http://192.168.1.22:8080/charts[root@master myrepo]# scp bdqn-0.1.0.tgz index.yaml node01:/var/xgp/charts[root@master myrepo]# helm repo update[root@master myrepo]# helm search bdqn[root@master myrepo]# helm install http://192.168.1.22:8080/charts/bdqn-0.1.0.tgz 1）创建helm的私有仓库，以自己的名字命名。 1、node01启动一个httpd的容器 123456[root@node01 ~]# mkdir /var/xgp//创建一个目录[root@node01 ~]# docker pull httpd//下载httpd镜像[root@node02 ~]# docker run -d -p 8080:80 -v /var/xgp:/usr/local/apache2/htdocs httpd//启动一个httpd的容器 3、生成仓库的index文件。 1234[root@master ~]# mkdir xgprepo//创建一个目录存放打包的chare[root@master ~]# helm repo index xgprepo/ --url http://192.168.1.22:8080/charts//生成仓库的index文件 4、将生成的index.yaml上传到node01的/var/www/charts目录下. node01创建目录 1[root@node01 ~]# mkdir /var/xgp/charts master移动动到 1[root@master ~]# scp xgprepo/* node01:/var/xgp/charts/ node01查看一下 12[root@node01 ~]# ls /var/xgp/charts/index.yaml 5、添加新的repo仓库 12[root@master ~]# helm repo add xgp http://192.168.1.22:8080/charts[root@master ~]# helm repo list 2） 自定义一个chart包，要求这个包运行一个httpd的服务，使用私有镜像v1版本。3个副本Pod，service类型更改为NodePort，端口指定为:30000 自定义一个chart包 12[root@master ~]# helm create wsd//创建一个名为wsd的chares包 按照要求修改配置文件 123456789101112131415161718192021222324252627282930313233343536373839[root@master ~]# cd wsd///进入这个chart包[root@master wsd]# vim values.yaml//修改wsd的配置文件replicaCount: 3 #三个副本image: repository: 192.168.1.21:5000/web #更改镜像为私有镜像 tag: v1 #镜像标签v1 pullPolicy: IfNotPresent imagePullSecrets: []nameOverride: \"\"fullnameOverride: \"\"service: type: NodePort #修改模式为映射端口 port: 80 nodePort: 30000 #添加端口[root@master wsd]# vim templates/service.yaml apiVersion: v1kind: Servicemetadata: name: &#123;&#123; include \"wsd.fullname\" . &#125;&#125; labels:&#123;&#123; include \"wsd.labels\" . | indent 4 &#125;&#125;spec: type: &#123;&#123; .Values.service.type &#125;&#125; ports: - port: &#123;&#123; .Values.service.port &#125;&#125; targetPort: http protocol: TCP name: http nodePort: &#123;&#123; .Values.service.nodePort &#125;&#125; #“添加”能让服务识别到nodePort的端口 selector: app.kubernetes.io/name: &#123;&#123; include \"wsd.name\" . &#125;&#125; app.kubernetes.io/instance: &#123;&#123; .Release.Name &#125;&#125; 测试一下 1[root@master ~]# helm install -n wsd wsd/ -f wsd/values.yaml 查看一下镜像版本 1[root@master ~]# kubectl get deployments. -o wide 访问一下 1[root@master ~]# curl 127.0.0.1:30000 3) 将实例进行更新，要求镜像生产v2版本。 私有镜像和官方镜像升级有所不同，官方的只需通过 （helm upgrade --set imageTag=“标签” 服务名称 charts包名 ）进行更改标签即可，而私有镜像需通过更改values.yaml中的标签才行比较麻烦一点。 1、修改values.yaml 1234567891011121314[root@master ~]# vim wsd/values.yaml # Default values for wsd.# This is a YAML-formatted file.# Declare variables to be passed into your templates.replicaCount: 3image: repository: 192.168.1.21:5000/web tag: v2 #修改标签为v2 pullPolicy: IfNotPresent[root@master ~]# helm upgrade wsd wsd/ -f wsd/values.yaml//基于配置文件刷新一下wsd服务 查看一下 1[root@master ~]# kubectl get deployments. -o wide 访问一下 1[root@master ~]# curl 127.0.0.1:30000 2、使用edit进行版本更新 确定wsd这个服务开启 1[root@master ~]# kubectl edit deployments. wsd 查看一下 1[root@master ~]# kubectl get deployments. -o wide 访问一下 1[root@master ~]# curl 127.0.0.1:30000 4）重新定义一个chart包，名称为: new-test,将这个包上传到上述私有仓库中。 1[root@master ~]# helm repo list 1234567891011121314151617[root@master ~]# helm create xgp-wsd//创建一个名为xgp-wsd的charts包[root@master ~]# helm package xgp-wsd///将xgp-wsd打包在当前目录[root@master ~]# mv xgp-wsd-0.1.0.tgz xgprepo///把打包文件放到仓库目录[root@master ~]# helm repo index xgprepo/ --url http://192.168.1.22:8080/charts//把仓库目录新加入的charts包信息记录在index.yaml中，使得其他加入的主机可以识别到，仓库的charts包[root@master ~]# scp xgprepo/* node01:/var/xgp/charts//将仓库目录的文件移动到httpd服务上，使各个主机可以访问，下载仓库的charts包[root@master ~]# helm repo update //更新一下chart存储库 查看一下 1[root@master ~]# helm search xgp-wsd","path":"posts/e7d.html","date":"06-07","excerpt":"","tags":[{"name":"helm","slug":"helm","permalink":"https://wsdlxgp.top/tags/helm/"},{"name":"chares","slug":"chares","permalink":"https://wsdlxgp.top/tags/chares/"},{"name":"url","slug":"url","permalink":"https://wsdlxgp.top/tags/url/"}]},{"title":"42 k8s中helm安装部署，升级和回滚（chart，helm，tiller，StorageClass）","text":"一、Helm介绍 helm是基于kubernetes 的包管理器。它之于 kubernetes 就如 yum 之于 centos，pip 之于 python，npm 之于 javascript 那 helm 的引入对于管理集群有哪些帮助呢？ 更方便地部署基础设施，如 gitlab，postgres，prometheus，grafana 等 更方便地部署自己的应用，为公司内部的项目配置 Chart，使用 helm 结合 CI，在 k8s 中部署应用一行命令般简单 1、Helm用途 Helm把Kubernetes资源(比如deployments、services或 ingress等) 打包到一个chart中，而chart被保存到chart仓库。通过chart仓库可用来存储和分享chart。Helm使发布可配置，支持发布应用配置的版本管理，简化了Kubernetes部署应用的版本控制、打包、发布、删除、更新等操作。 做为Kubernetes的一个包管理工具，用来管理charts——预先配置好的安装包资源，有点类似于Ubuntu的APT和CentOS中的yum。 Helm具有如下功能： 创建新的chart chart打包成tgz格式 上传chart到chart仓库或从仓库中下载chart 在Kubernetes集群中安装或卸载chart 管理用Helm安装的chart的发布周期 使用Helm可以完成以下事情： 管理Kubernetes manifest files 管理Helm安装包charts 基于chart的Kubernetes应用分发 2、Helm组件及相关术语 开始接触Helm时遇到的一个常见问题就是Helm中的一些概念和术语非常让人迷惑，我开始学习Helm就遇到这个问题。 因此我们先了解一下Helm的这些相关概念和术语。 包管理工具: Helm: Kubernetes的应用打包工具，也是命令行工具的名称。 Helm CLI：是 Helm 客户端，可以在本地执行 Tiller: Helm的服务端，部署在Kubernetes集群中，用于处理Helm的相关命令。 helm的作用：像centos7中的yum命令一样，管理软件包，只不过helm这儿管理的是在k8s上安装的各种容器。 tiller的作用：像centos7的软件仓库一样，简单说类似于/etc/yum.repos.d目录下的xxx.repo。 Repoistory: Helm的软件仓库，repository本质上是一个web服务器，该服务器保存了chart软件包以供下载，并有提供一个该repository的chart包的清单文件以供查询。在使用时，Helm可以对接多个不同的Repository。 Charts：是一个Helm的程序包，它包含了运行一个kubernetes应用程序所需要的镜像、依赖关系和资源定义等。 Release：应用程序运行Charts之后，得到的一个实例。 需要特别注意的是， Helm中提到的Release和我们通常概念中的版本有所不同，这里的Release可以理解为Helm使用Chart包部署的一个应用实例。 其实Helm中的Release叫做Deployment更合适。估计因为Deployment这个概念已经被Kubernetes使用了，因此Helm才采用了Release这个术语。 命令介绍 123456789101112131415161718[root@master ~]# helm search//查看可用的Charts包[root@master ~]# helm inspect stable/redis//查看stable/redis包的详细信息[root@master mysql]# helm fetch stable/mysql//直接下载stable/mysql的chart包[root@master ~]# helm install stable/redis -n redis --dry-run //基于stable/redis包运行一个名为redis的服务（把--dry-run去掉之后相当于安装了一个服务）[root@master ~]# helm list//查看安装的服务[root@master ~]# helm delete redis//删除这个服务[root@master mysql]# helm upgrade --set imageTag=5.7.15 xgp-mysql stable/mysql -f values.yaml //mysql服务的升级[root@master mysql]# helm history xgp-mysql//查看历史版本[root@master mysql]# helm rollback xgp-mysql 1 //回滚到版本一 123456789101112131415161718192021222324252627282930http://hub.kubeapps.com/completion # 为指定的shell生成自动完成脚本（bash或zsh）create # 创建一个具有给定名称的新 chartdelete # 从 Kubernetes 删除指定名称的 releasedependency # 管理 chart 的依赖关系fetch # 从存储库下载 chart 并（可选）将其解压缩到本地目录中get # 下载一个命名 releasehelp # 列出所有帮助信息history # 获取 release 历史home # 显示 HELM_HOME 的位置init # 在客户端和服务器上初始化Helminspect # 检查 chart 详细信息install # 安装 chart 存档lint # 对 chart 进行语法检查list # releases 列表package # 将 chart 目录打包成 chart 档案plugin # 添加列表或删除 helm 插件repo # 添加列表删除更新和索引 chart 存储库reset # 从集群中卸载 Tillerrollback # 将版本回滚到以前的版本search # 在 chart 存储库中搜索关键字serve # 启动本地http网络服务器status # 显示指定 release 的状态template # 本地渲染模板test # 测试一个 releaseupgrade # 升级一个 releaseverify # 验证给定路径上的 chart 是否已签名且有效version # 打印客户端/服务器版本信息dep # 分析 Chart 并下载依赖 3、组件架构 Helm Client 是用户命令行工具，其主要负责如下： 本地 chart 开发 仓库管理 与 Tiller sever 交互 发送预安装的 chart 查询 release 信息 要求升级或卸载已存在的 release Tiller Server是一个部署在Kubernetes集群内部的 server，其与 Helm client、Kubernetes API server 进行交互。Tiller server 主要负责如下： 监听来自 Helm client 的请求 通过 chart 及其配置构建一次发布 安装 chart 到Kubernetes集群，并跟踪随后的发布 通过与Kubernetes交互升级或卸载 chart 简单的说，client 管理 charts，而 server 管理发布 release helm客户端 helm客户端是一个命令行工具，负责管理charts、reprepository和release。它通过gPRC API（使用kubectl port-forward将tiller的端口映射到本地，然后再通过映射后的端口跟tiller通信）向tiller发送请求，并由tiller来管理对应的Kubernetes资源。 tiller服务端 tiller接收来自helm客户端的请求，并把相关资源的操作发送到Kubernetes，负责管理（安装、查询、升级或删除等）和跟踪Kubernetes资源。为了方便管理，tiller把release的相关信息保存在kubernetes的ConfigMap中。 tiller对外暴露gRPC API，供helm客户端调用。 4、工作原理 Chart Install 过程： Helm从指定的目录或者tgz文件中解析出Chart结构信息 Helm将指定的Chart结构和Values信息通过gRPC传递给Tiller Tiller根据Chart和Values生成一个Release Tiller将Release发送给Kubernetes运行。 Chart Update过程： Helm从指定的目录或者tgz文件中解析出Chart结构信息 Helm将要更新的Release的名称和Chart结构，Values信息传递给Tiller Tiller生成Release并更新指定名称的Release的History Tiller将Release发送给Kubernetes运行 Chart Rollback helm将会滚的release名称传递给tiller tiller根据release名称查找history tiller从history中获取到上一个release tiller将上一个release发送给kubernetes用于替换当前release Chart处理依赖 Tiller 在处理 Chart 时，直接将 Chart 以及其依赖的所有 Charts 合并为一个 Release，同时传递给 Kubernetes。因此 Tiller 并不负责管理依赖之间的启动顺序。Chart 中的应用需要能够自行处理依赖关系。 二、安装部署helm工具（客户端） 前提要求 Kubernetes1.5以上版本 集群可访问到的镜像仓库 执行helm命令的主机可以访问到kubernetes集群 （1）下载helm的包 12[root@master ~]#docker pull gcr.io/kubernetes-helm/tiller:v2.14.3[root@master ~]# wget https://get.helm.sh/helm-v2.14.3-linux-amd64.tar.gz （2）把helm包的命令，复制到本地 123456[root@master helm]# mv linux-amd64/helm /usr/local/bin///移动命令目录到/usr/local/bin/[root@master helm]# chmod +x /usr/local/bin/helm //给予执行权限[root@master helm]# helm help//验证是否安装成功 （3）设置命令自动补全 123[root@master helm]# echo 'source &lt;(helm completion bash)' &gt;&gt; /etc/profile[root@master helm]# . /etc/profile//刷新一下 2、安装Tiller server（服务端，需要创建授权用户） 12345678910111213141516171819[root@master ~]# vim tiller-rbac.yaml #创建授权用户apiVersion: v1kind: ServiceAccountmetadata: name: tiller namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: tillerroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects: - kind: ServiceAccount name: tiller namespace: kube-system 执行一下 1[root@master ~]# kubectl apply -f tiller-rbac.yaml （1）Tiller server的环境初始化 12[root@master helm]# helm init --service-account=tiller//helm的服务端就是Tiller（因为是访问外国的网站，可能需要多次执行） 查看一下 1[root@master helm]# kubectl get deployment. -n kube-system 现在发现没有开启，那是因为默认下载的Google的镜像，下载不下来 （2）设置镜像源改为阿里云的 1[root@master helm]# helm repo add stable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts 查看一下 1[root@master helm]# helm version 3、部署一个实例helm install + charts -n Release名称。 1、关于这个Release的描述。 2、关于这个Release资源的描述。 3、怎么使用这个Release。 （1）Helm部署安装一个Mysql服务。 12[root@master ~]# helm search mysql//查看关于mysqk的Charts包 12[root@master ~]# helm install stable/mysql -n mysql //基于stable/mysql包安装一个名为MySQL的服务 查看一下 1[root@master ~]# helm list （2）Charts包解压过后的目录: 123[root@master ~]# cd .helm/cache/archive//查看helm缓存[root@master archive]# ls 123456[root@master mysql]# helm fetch stable/mysql//直接下载stable/mysql的chart包[root@master archive]# tar -zxf mysql-0.3.5.tgz //解压一下MySQL包[root@master archive]# tree -C mysql //树状图查看解压出来的mysql目录，-C:显示颜色 Chart.yaml：这个chart包的概要信息。（name和version 这两是必填项，其他可选。） README md：是这个chart包的一个使用帮助文档。 templates：chart包内各种资源对象的模板。 deployment.yaml：deployment 控制器的 Go 模板文件 _helpers.tpl：以 _ 开头的文件不会部署到 k8s 上，可用于定制通用信息 NOTES.txt：Chart 部署到集群后的一些信息 service.yaml：service 的 Go 模板文件 values.yaml：是这个chart包的默认的值，可以被templet内的yaml文件使用。 （3）Helm部署安装-个Mysql服务。 123456[root@master ~]# docker pull mysql:5.7.14[root@master ~]# docker pull mysql:5.7.15[root@master ~]# docker pull busybox:1.25.0下载所需的mysql镜像[root@master ~]# helm delete mysql --purge //删除之前的MySQL服务并清除缓存 （4）设置共享目录 12345678910111213[root@master ~]# yum -y install rpcbind nfs-utils//安装nfs[root@master ~]# mkdir /data//创建共享目录[root@master ~]# vim /etc/exports/data *(rw,sync,no_root_squash)//设置共享目录权限[root@master ~]# systemctl restart rpcbind[root@master ~]# systemctl restart nfs-server//重启nfs服务测试一下[root@master ~]# showmount -e （5）创建pv 12345678910111213141516[root@master xgp]# vim nfs-pv1.yml apiVersion: v1kind: PersistentVolumemetadata: name: mysqlpvspec: capacity: storage: 8Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle nfs: path: /data/mysqlpv server: 192.168.1.21[root@master xgp]# mkdir /data/mysqlpv//创建所需目录 执行一下 1[root@master xgp]# kubectl apply -f nfs-pv1.yml 查看一下 1[root@master xgp]# kubectl get pv （6）创建一个mysql服务 1[root@master xgp]# helm install stable/mysql -n bdqn-mysql --set mysqlRootPassword=123.com 查看一下 1[root@master xgp]# kubectl get pod （7）进入pod并查看一下 1234567891011[root@master xgp]# kubectl exec -it bdqn-mysql-mysql-7b89c7b99-8ff2r -- mysql -u root -p123.commysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || sys |+--------------------+4 rows in set (0.01 sec) 4、mysql服务的升级与回滚 （1）mysql服务的升级 1[root@master mysql]# helm upgrade --set imageTag=5.7.15 bdqn-mysql stable/mysql -f values.yaml 查看一下 1[root@master mysql]# kubectl get deployments. -o wide （2）mysql服务的回滚 12[root@master mysql]# helm history bdqn-mysql//查看历史版本 回滚到版本一 1[root@master mysql]# helm rollback bdqn-mysql 1 查看一下 1[root@master mysql]# kubectl get deployments. -o wide 三、小实验 在部署mysql的时候，如何开启storageclass，以及如何将service资源对象的类型更改为NodePort, 如何使用? 将上述部署的实例进行升级回滚操作。升级的时候镜像改为： mysql:5.7.15版本。回滚到最初的版本。 1、基于NFS服务，创建NFS服务。 下载nfs所需安装包 1[root@node02 ~]# yum -y install nfs-utils rpcbind 创建共享目录 1[root@master ~]# mkdir -p /xgp/wsd 创建共享目录的权限 12[root@master ~]# vim /etc/exports/xgp *(rw,sync,no_root_squash) 开启nfs和rpcbind（三台都要） 12[root@master ~]# systemctl start nfs-server.service [root@master ~]# systemctl start rpcbind 测试一下 1[root@master ~]# showmount -e 2、创建StorageClass资源对象。 （1）创建rbac权限。 123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@master yaml]# vim rbac.yaml apiVersion: v1kind: ServiceAccountmetadata: name: nfs-provisioner namespace: default---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: nfs-provisioner-runner namespace: defaultrules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"watch\", \"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\", \"endpoints\"] verbs: [\"get\",\"create\",\"list\", \"watch\",\"update\"] - apiGroups: [\"extensions\"] resources: [\"podsecuritypolicies\"] resourceNames: [\"nfs-provisioner\"] verbs: [\"use\"]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: run-nfs-provisionersubjects: - kind: ServiceAccount name: nfs-provisioner namespace: default #必写字段roleRef: kind: ClusterRole name: nfs-provisioner-runner apiGroup: rbac.authorization.k8s.io 执行一下 1[root@master yaml]# kubectl apply -f rbac.yaml （2）创建Deployment资源对象，用Pod代替 真正的NFS服务。 123456789101112131415161718192021222324252627282930313233[root@master yaml]# vim nfs-deployment.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nfs-client-provisionerspec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-client-provisioner spec: serviceAccount: nfs-provisioner containers: - name: nfs-client-provisioner image: registry.cn-hangzhou.aliyuncs.com/open-ali/nfs-client-provisioner volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: xgp - name: NFS_SERVER value: 192.168.1.21 - name: NFS_PATH value: /xgp/wsd volumes: - name: nfs-client-root nfs: server: 192.168.1.21 path: /xgp/wsd 执行一下 1[root@master yaml]# kubectl apply -f nfs-deployment.yaml 查看一下 1[root@master yaml]# kubectl get pod （3）创建storageclass的yaml文件 1234567[root@master yaml]# vim xgp-storageclass.yaml apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: xgp-nfsprovisioner: xgp #通过provisioner字段关联到上述DeployreclaimPolicy: Retain 执行一下 1[root@master yaml]# kubectl apply -f test-storageclass.yaml 查看一下 1[root@master yaml]# kubectl get sc 3、创建一个mysql服务 1234567891011[root@master ~]# docker pull mysql:5.7.14[root@master ~]# docker pull mysql:5.7.15[root@master ~]# docker pull busybox:1.25.0//下载所需镜像[root@master yaml]# helm fetch stable/mysql//直接下载stable/mysql的chart包[root@master yaml]# tar -zxf mysql-0.3.5.tgz //解压mysql包[root@master yaml]# cd mysql/[root@master mysql]# vim values.yaml //修改values.yaml文件，添加storageClass存储卷和更改svc的模式为NodePort 12[root@master mysql]# helm install stable/mysql -n xgp-mysql --set mysqlRootPassword=123.com -f values.yaml //基于values.yaml和stable/mysql开启一个密码为123.com的mysqlpod 查看一下 1[root@master mysql]# kubectl get svc 1[root@master mysql]# kubectl get pod -o wide 4、进入pod并查看一下 1234567891011[root@master mysql]# kubectl exec -it xgp-mysql-mysql-67c6fb5f9-dn7s2 -- mysql -u root -p123.commysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || sys |+--------------------+4 rows in set (0.01 sec) 5、mysql服务的升级与回滚 （1）mysql服务的升级 1[root@master mysql]# helm upgrade --set imageTag=5.7.15 xgp-mysql stable/mysql -f values.yaml 查看一下 1[root@master mysql]# kubectl get deployments. -o wide （2）服务的回滚 12[root@master mysql]# helm history xgp-mysql//查看历史版本 回滚到版本一 1[root@master mysql]# helm rollback xgp-mysql 1 查看一下 1[root@master mysql]# kubectl get deployments. -o wide 6、进入pod并查看一下 1234567891011[root@master mysql]# kubectl exec -it xgp-mysql-mysql-67c6fb5f9-dn7s2 -- mysql -u root -p123.commysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || sys |+--------------------+4 rows in set (0.01 sec) 四、总结 Helm作为kubernetes应用的包管理以及部署工具，提供了应用打包，发布，版本管理以及部署，升级，回退等功能。Helm以Chart软件包的形式简化Kubernetes的应用管理，提高了对用户的友好性。 使用心得 helm 客户端的功能非常简单，直接参考官网文档即可。 列一下相关使用心得： Helm 的所有功能都是围绕着 chart、release 和 repository 的； 仅初始化客户端相关配置且仅建立本地仓库，可执行 helm init --client-only --skip-refresh； 查找 chart 的方式是通过 HELM_HOME（默认是 ~/.helm 目录）下的 repositories 目录进行的，几个重要文件或目录为 cache、repositories/cache； 修改 chart index.yaml 的 url，可执行 helm serve --url http://demo.com 来重新 reindex； 依赖关系管理，requirements定义，子 chart 值定义； install 、 update 的方式管理不方便，这样需要维护 chart 的版本关系，集成 install 和 update ，组成类似 k8s 中的 apply 命令； package 命令 -u 可以更新依赖，建议推到 repositiories 前先 package ，否则后期可能出现依赖检测不全的错误； release 相关的信息存储在 k8s 的 configmap 中，命名形式为 release_name.v1 的格式。 rollback 相关功能就是通过存储在 configmap 中的信息进行回滚的； Helm 客户端与 k8s 中的 TillerServer 是通过 k8s 提供的 port-forward 来实现的，而 port-forward 需要在指定节点上部署 socat； TillerServer 可以不部署在 k8s 中， 此时 Helm 客户端需要通过 HELM_HOST 环境变量来指定 TillerServer 的地址和端口； 建议 TillerServer 部署在 k8s 中，既然 Helm 为 CNCF 的一员，那么就尽量把云原生做到极致吧； 写 chart 时多参考官方最佳实践，The Chart Best Practices Guide； 不足 Helm 虽然提供了 install、update 命令来安装或更新对应的 release，但这给使用者带来了需要维护 release 状态的压力。举个例子，在还没安装 release 之前，release 是不存在的，update 操作是会失败的。反之已经存在的 release，install 操作也会失败。其实大部分情况下我是不需要知道 release 的状态的，不管它存在还是不存在，我执行的命令就是我希望的意图，我希望 release 能成为我执行命令后的状态。这一点上 k8s 的 apply 命令就非常好，不需要用户来维护资源的状态。","path":"posts/5bc1.html","date":"06-07","excerpt":"","tags":[{"name":"chart","slug":"chart","permalink":"https://wsdlxgp.top/tags/chart/"},{"name":"helm","slug":"helm","permalink":"https://wsdlxgp.top/tags/helm/"},{"name":"tiller","slug":"tiller","permalink":"https://wsdlxgp.top/tags/tiller/"}]},{"title":"41 k8s的HPA自动扩容与缩容","text":"HPA介绍 Kubernetes HPA（水平Pod自动缩放）Pod水平自动伸缩，通过此功能，只需简单的配置，即可便可以利用监控指标（cpu使用率、磁盘、内存等）自动的扩容或缩容服务中Pod数量，当业务需求增加时，系统将为您无缝地自动增加适量容器，提高系统稳定性。此处将详细讲解HPA的核心设计原理和基于Hepaster的使用方法。 前提条件 系统应该能否获取到当前Pod的资源使用情况 (意思是可以执行kubectl top pod命令,并且能够得到反馈信息)。 若要实现自动扩缩容的功能，还需要部署heapster服务，用来收集及统计资源的利用率，支持kubectl top命令，heapster服务集成在prometheus（普罗米修斯） MertricServer服务中，所以说，为了方便，我这里基于prometheus服务的环境上进行部署HPA（动态扩缩容）的服务。 实验环境 主机 IP地址 服务 master 192.168.1.21 k8s node01 192.168.1.22 k8s node02 192.168.1.23 k8s 基于https://blog.51cto.com/14320361/2473879 的实验继续进行 heapster：这个组件之前是集成在k8s集群的,不过在1.12版本之后被移除了。如果还想使用此功能，应该部署metricServer, 这个k8s集群资源使用情况的聚合器。 Cousom：同样处于beta阶段(autoscaling/v2beta1)，但是涉及到自定义的REST API的开发，复杂度会大一些，并且当需要从自定义的监控中获取数据时，只能设置绝对值，无法设置使用率。 自动扩展主要分为两种： 水平扩展(scale out)，针对于实例数目的增减。 垂直扩展(scal up)，即单个实例可以使用的资源的增减, 比如增加cpu和增大内存。 HPA属于前者。它可以根据CPU使用率或应用自定义metrics自动扩展Pod数量(支持 replication controller、deployment 和 replica set)。 工作流程 创建HPA资源，设定目标CPU使用率限额，以及最大/最小实例数，一定要设置Pod的资源限制参数: request，否则HPA不会工作。 控制管理器每隔30s(在kube-controller-manager.service中可以通过–-horizontal-pod-autoscaler-sync-period修改)查询metrics的资源使用情况。 然后与创建时设定的值和指标做对比(平均值之和/限额)，求出目标调整的实例个数。 目标调整的实例数不能超过第一条中设定的最大/最小实例数。如果没有超过，则扩容；超过，则扩容至最大的实例个数。 重复第2-4步。 这里，我们使用一个测试镜像， 这个镜像基于php-apache制作的docker镜像，包含了一些可以运行cpu密集计算任务的代码。 1、创建一个deployment控制器 12345[root@master ~]#docker pull mirrorgooglecontainers/hpa-example:latest//下载hpa-example镜像[root@master ~]# kubectl run php-apache --image=mirrorgooglecontainers/hpa-example --requests=cpu=200m --expose --port=80//基于hpa-example镜像，运行一个deployment控制器，请求CPU的资源为200m，暴露一个80端口 查看一下 1[root@master ~]# kubectl get deployments. 2、创建HPA控制器 12[root@master ~]# kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10//当deployment资源对象的CPU使用率达到50%时，就进行扩容，最多可以扩容到10个 查看一下 1[root@master ~]# kubectl get hpa 3、测试（master开启三个端口） 新开启多个终端，对pod进行死循环请求php-apache的pod 端口一 （1）创建一个应用，用来不停的访问我们刚刚创建的php-apache的svc资源。 1[root@master ~]# kubectl run -i --tty load-generator --image=busybox /bin/sh （2）进入Pod内，执行以下这条命令.用来模拟访问php-apache的svc资源。 12[root@master ~]# while true; do wget -q -O- http://php-apache.default.svc.cluster.local ; done//不停地向php-apache的svc资源，发送ok 端口二 12[root@master ~]# kubectl get hpa -w//实时查看pod的cpu状态 可以看到php-apache的cpu使用情况已经超过了50% 端口三 12[root@master images]# kubectl get pod -w//实时查看pod的状态 可以看到当php-apache的cpu使用情况超过50%后，就会不断生成新的php-apache来进行负载均衡（目前设置的上线时10个），当然，如果cpu使用情况下降到50%，master就会陆续地删除php-apache，这样的使用可以减少不必要的资源浪费、资源分配不均等情况。 二、资源限制 1、基于Pod Kubernetes对资源的限制实际上是通过cgroup来控制的，cgroup 是容器的一组用来控制内核如何运行进程的相关属性集合。针对内存、CPU 和各种设备都有对应的cgroup 默认情况下，Pod运行没有CPU和内存的限额。这意味着系统中的任何 Pod将能够像执行该Pod所在的节点一样，消耗足够多的CPU和内存。一般会针对某些应用的pod资源进行资源限制，这个资源限制是通过 resources的requests和limits来实现 1[root@master ~]# vim cgroup-pod.yaml requests: 要分配的资源，limits为最高请求的资源值。可以简单的理解为初始值和最大值。 2、基于名称空间 1） 计算资源配额 1[root@master ~]# vim compute-resources.yaml 2）配置对象数量配额限制 1[root@master ~]# vim object-counts.yaml 3） 配置CPU和内存的LimitRange 1[root@master ~]# vim limitRange.yaml default 即 limit的值。 defaultRequest 即 request的值。","path":"posts/5f70.html","date":"06-07","excerpt":"","tags":[{"name":"HPA","slug":"HPA","permalink":"https://wsdlxgp.top/tags/HPA/"},{"name":"heapster","slug":"heapster","permalink":"https://wsdlxgp.top/tags/heapster/"},{"name":"top","slug":"top","permalink":"https://wsdlxgp.top/tags/top/"}]},{"title":"40 k8s群集的三种的Web-UI界面部署（dashboard、weave-scope、Prometheus）","text":"一、k8s的UI访问界面-dashboard 在dashboard中，虽然可以做到创建、删除、修改资源等操作，但通常情况下，我们会把它当做健康k8s集群的软件。 作为Kubernetes的Web用户界面，用户可以通过Dashboard在Kubernetes集群中部署容器化的应用，对应用进行问题处理和管理，并对集群本身进行管理。通过Dashboard，用户可以查看集群中应用的运行情况，同时也能够基于Dashboard创建或修改部署、任务、服务等Kubernetes的资源。通过部署向导，用户能够对部署进行扩缩容，进行滚动更新、重启Pod和部署新应用。当然，通过Dashboard也能够查看Kubernetes资源的状态。 1、Dashboard提供的功能 在默认情况下，Dashboard显示默认(default)命名空间下的对象，也可以通过命名空间选择器选择其他的命名空间。在Dashboard用户界面中能够显示集群大部分的对象类型。 1）集群管理 集群管理视图用于对节点、命名空间、持久化存储卷、角色和存储类进行管理。 节点视图显示CPU和内存的使用情况，以及此节点的创建时间和运行状态。 命名空间视图会显示集群中存在哪些命名空间，以及这些命名空间的运行状态。角色视图以列表形式展示集群中存在哪些角色，这些角色的类型和所在的命名空间。 持久化存储卷以列表的方式进行展示，可以看到每一个持久化存储卷的存储总量、访问模式、使用状态等信息；管理员也能够删除和编辑持久化存储卷的YAML文件。 2） 工作负载 工作负载视图显示部署、副本集、有状态副本集等所有的工作负载类型。在此视图中，各种工作负载会按照各自的类型进行组织。 工作负载的详细信息视图能够显示应用的详细信息和状态信息，以及对象之间的关系。 3） 服务发现和负载均衡 服务发现视图能够将集群内容的服务暴露给集群外的应用，集群内外的应用可以通过暴露的服务调用应用，外部的应用使用外部的端点，内部的应用使用内部端点。 4） 存储 存储视图显示被应用用来存储数据的持久化存储卷申明资源。 5） 配置 配置视图显示集群中应用运行时所使用配置信息，Kubernetes提供了配置字典（ConfigMaps）和秘密字典（Secrets），通过配置视图，能够编辑和管理配置对象，以及查看隐藏的敏感信息。 6） 日志视图 Pod列表和详细信息页面提供了查看日志视图的链接，通过日志视图不但能够查看Pod的日志信息，也能够查看Pod容器的日志信息。通过Dashboard能够根据向导创建和部署一个容器化的应用，当然也可以通过手工的方式输入指定应用信息，或者通过上传YAML和JSON文件来创建和不受应用。 2、下载所需yaml文件和镜像 12[root@master https]# wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-rc5/aio/deploy/recommended.yaml[root@master https]# docker pull kubernetesui/dashboard:v2.0.0-rc5 3、修改 recommended.yaml 12345678910111213141516[root@master https]#vim recommended.yaml ---kind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboardspec: type: NodePort #添加40 ports: - port: 443 targetPort: 8443 selector: k8s-app: kubernetes-dashboard 执行一下 1[root@master https]# kubectl apply -f recommended.yaml 查看一下 1[root@master https]# kubectl get svc -n kubernetes-dashboard 3、浏览器访问https://192.168.1.21:32306 PS:如果是使用的旧版本的dashboard, 使用谷歌浏览器登录，可能是不成功的，需要换成其他的浏览器，比如:火狐。 4、基于token的方法登录dashboard &lt;1&gt;创建一个dashboard的管理用户 1[root@master https]# kubectl create serviceaccount dashboard-admin -n kube-system &lt;2&gt;绑定用户为集群管理用户 1[root@master https]# kubectl create clusterrolebinding dashboard-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin &lt;3&gt;获取Token 12[root@master https]# kubectl get secrets -n kube-system | grep dashboard-admin//先得到Token的名称 12[root@master https]# kubectl describe secrets -n kube-system dashboard-admin-token-62bh9//查看上述得到的secret资源的详细信息，会得到token &lt;4&gt;在浏览器上使用token登录。 创建一个资源 查看是否创建成功 5、基于kubeconfig配置文件的方法登录dashboard &lt;1&gt;获取Token 12[root@master https]# kubectl get secrets -n kube-system | grep dashboard-admin//先得到Token的名称 12[root@master https]# kubectl describe secrets -n kube-system dashboard-admin-token-62bh9//查看上述得到的secret资源的详细信息，会得到token &lt;2&gt;生成kubeconfig配置文件。 设置一个环境变量代表获取的token 1[root@master https]# DASH_TOKEN=$(kubectl get secrets -n kube-system dashboard-admin-token-62bh9 -o jsonpath=&#123;.data.token&#125; | base64 -d) 将k8s集群的配置信息写入kubeconfig配置文件中。 1234[root@master https]# kubectl config set-cluster kubernetes --server=192.168.1.21:6443 --kubeconfig=/root/.dashboard-admin.conf[root@master https]# kubectl config set-credentials dashboard-admin --token=$DASH_TOKEN --kubeconfig=/root/.dashboard-admin.conf[root@master https]# kubectl config set-context dashboard-admin@kubernetes --cluster=kubernetes --user=dashboard-admin --kubeconfig=/root/.dashboard-admin.conf[root@master https]# kubectl config use-context dashboard-admin@kubernetes --kubeconfig=/root/.dashboard-admin.conf &lt;3&gt;将生成的/root/.dashboard-admin.conf的配置文件，导出并做保存。 12[root@master https]# sz /root/.dashboard-admin.conf //导出到自己习惯的位置即可 &lt;4&gt;从浏览器选择kubeconfig的登录方式，然后导入配置文件即可。 二、部署weave-scope监控k8s集群 Weave Scope 是 Docker 和 Kubernetes 可视化监控工具。Scope 提供了至上而下的集群基础设施和应用的完整视图，用户可以轻松对分布式的容器化应用进行实时监控和问题诊断。 使用scope Scope 会自动构建应用和集群的逻辑拓扑。比如点击顶部 PODS，会显示所有 Pod 以及 Pod 之间的依赖关系。 点击 HOSTS，会显示各个节点之间的关系。 实时资源监控 可以在 Scope 中查看资源的 CPU 和内存使用情况。 支持的资源有 Host、Pod 和 Container。** 在线操作 Scope 还提供了便捷的在线操作功能，比如选中某个 Host，点击 &gt;_ 按钮可以直接在浏览器中打开节点的命令行终端 点击 Deployment 的 + 可以执行 Scale Up 操作 可以查看 Pod 的日志 可以 attach、restart、stop 容器，以及直接在 Scope 中排查问题 强大的搜索功能 Scope 支持关键字搜索和定位资源。 还可以进行条件搜索，比如查找和定位 MEMORY &gt; 100M 的 Pod。 1、在github上查找scope的yaml文件 （1）github上搜索scope （2）进入k8s的部署scope的说明 （3）选择k8s的部署 （4）复制上面的链接，并下载yaml文件 1[root@master https]# wget https://cloud.weave.works/k8s/scope.yaml 2、修改下载的yaml文件并运行 123456789[root@master ~]# vim scope.yaml #编辑yaml文件#跳转至213行，修改其service的端口类型 spec: type: NodePort #修改类型为NodePort ports: - name: app port: 80 protocol: TCP targetPort: 4040 （1）执行一下 1[root@master https]# kubectl apply -f scope.yaml （2）查看容器的运行情况，确定处于正常运行 1[root@master https]# kubectl get pod -o wide -n weave DaemonSet weave-scope-agent，集群每个节点上都会运行的 scope agent 程序，负责收集数据。 Deployment weave-scope-app，scope 应用，从 agent 获取数据，通过 Web UI 展示并与用户交互。 Service weave-scope-app，默认是 ClusterIP 类型，我们已经在上面的命令中添加了参数k8s-service-type=NodePort修改为 NodePort。 1[root@master https]# kubectl get svc -n weave #DaemonSet资源对象：weave-scope-agent（代理）：负责收集节点的信息； #deployment资源对象:weave-scope-app(应用)：从agent获取数据，通过web UI展示并与用户交互； #DaemonSet资源对象的特性和deployment相比，就是DaemonSet资源对象会在每个节点上都运行且只能运行一个pod。 #由于每个节点都需要监控，所以用到了DaemonSet这种资源对象 3、浏览器访问一下http://192.168.1.21:31841/ 在scope的web界面中，可以查看很多的东西，pod、node节点等详细信息，包括打开容器的终端，查看其日志信息等等 总结 • weave scope可以以其简洁的可视化为我们更生动形象的展现出service/controller/pod等资源对象的管理及简单的web ui操作，方便故障排除及时定位 • weave scope作为web ui目前缺少登录验证，可以利用其他方式里面web服务器的验证做安全管控。 三、部署Prometheus服务 PS:在这里部署的prometheus,并不是Prometheus官网提供的，而是使用的coreos提供的prometheus项目。 在部署之前，先来了解一下Prometheus各个组件的作用吧！ MetricsServer: 是k8s集群资源使用情况的聚合器，收集数据给k8s集群内使用，如kubectl,hpa,scheduler等。 Prometheus Operator : 是一个系统检测和警报工具箱，用来存储监控数据。 Prometheus node-exporter ：收集k8s集群资源的数据，指定告警规则。 Prometheus ：收集apiserver，scheduler，controller-manager，kubelet组件的数据，通过http协议传输。 Grafana: 可视化数据统计和监控平台。 特征 Prometheus 相比于其他传统监控工具主要有以下几个特点： 具有由 metric 名称和键/值对标识的时间序列数据的多维数据模型 有一个灵活的查询语言 不依赖分布式存储，只和本地磁盘有关 通过 HTTP 的服务拉取时间序列数据 也支持推送的方式来添加时间序列数据 还支持通过服务发现或静态配置发现目标 多种图形和仪表板支持 1、在github上搜索coreos/prometheus 复制链接 2、克隆github上的promethes项目 1234[root@master promethes]# yum -y install git//下载git命令[root@master promethes]# git clone https://github.com/coreos/kube-prometheus.git//克隆github上的项目 3、修改grafapa-service.yaml文件, 更改为nodePort的暴露方式，暴露端口为31001.。 1234567891011121314151617181920[root@master promethes]# cd kube-prometheus/manifests///进入kube-prometheus的manifests目录[root@master manifests]# vim grafana-service.yaml #修改grafana的yaml文件apiVersion: v1kind: Servicemetadata: labels: app: grafana name: grafana namespace: monitoringspec: type: NodePort #改为NodePort类型 ports: - name: http port: 3000 targetPort: http nodePort: 31001 #映射到宿主机31001端口 selector: app: grafana 3.修改prometheus-service.yaml文件， 更改为nodePort的暴露方式，暴露端口为31002. 1234567891011121314151617181920[root@master manifests]# vim prometheus-service.yaml #修改prometheus的yaml文件apiVersion: v1kind: Servicemetadata: labels: prometheus: k8s name: prometheus-k8s namespace: monitoringspec: type: NodePort #改为NodePort类型 ports: - name: web port: 9090 targetPort: web nodePort: 31002 #映射到宿主机31002端口 selector: app: prometheus prometheus: k8s sessionAffinity: ClientIP 4、修改alertmanager-service.yaml文件， 更改为nodePort的暴露方式，暴露端口为31003 1234567891011121314151617181920[root@master manifests]# vim alertmanager-service.yaml #修改alertmanager的yaml文件apiVersion: v1kind: Servicemetadata: labels: alertmanager: main name: alertmanager-main namespace: monitoringspec: type: NodePort #改为NodePort类型 ports: - name: web port: 9093 targetPort: web nodePort: 31003 #映射到宿主机31003端口 selector: alertmanager: main app: alertmanager sessionAffinity: ClientIP 5、将setup目录中所有的yaml文件,全部运行。是运行以上yaml文件的基础环境配置。 1234[root@master manifests]# cd setup///进入setup/目录[root@master manifests]# kubectl apply -f setup///运行setup目录中所有的yaml文件 6、将主目录(kube-prometheus)中所有的yaml文件,全部运行。 1234[root@master manifests]# cd ..//返回上一级目录（kube-prometheus）[root@master kube-prometheus]# kubectl apply -f manifests///运行kube-prometheus目录中所有的yaml文件 查看一下 1[root@master ~]# kubectl get pod -n monitoring 部署成功之后，可以运行一条命令， 查看资源使用情况(MetricsServer必须部署成功) 1[root@master images]# kubectl top node 7、浏览器访问一下http://192.168.1.21:31001 客户端访问群集中任意节点的IP+30100端口，即可看到以下界面（默认用户名和密码都是admin） 根据提示更改密码： （1）添加模板 依次点击“import”进行导入下面三个模板： （2）进行以下点击，即可查看群集内的监控状态 以下可看到监控状态 8、导入监控模板 从grafana的官网搜索https://grafana.com/ 复制以下这个模板的id 现在可以看到监控画面了","path":"posts/4f99.html","date":"06-07","excerpt":"","tags":[{"name":"dashboard","slug":"dashboard","permalink":"https://wsdlxgp.top/tags/dashboard/"},{"name":"weave-scope","slug":"weave-scope","permalink":"https://wsdlxgp.top/tags/weave-scope/"},{"name":"Prometheus","slug":"Prometheus","permalink":"https://wsdlxgp.top/tags/Prometheus/"}]},{"title":"39 k8s中ingress资源的应用","text":"Ingress实现虚拟主机的方案 环境介绍 主机 IP地址 服务 master 192.168.1.21 k8s node01 192.168.1.22 k8s node02 192.168.1.23 k8s 基于 https://blog.51cto.com/14320361/2464655 的实验继续进行 1、首先确定要运行ingress-nginx-controller服务。 在gitbub上找到所需的ingress的yaml文件 4. master下载 1[root@master ingress]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.29.0/deploy/static/mandatory.yaml 5. 修改 mandatory.yaml 文件 12[root@master ingress]# vim mandatory.yaml hostNetwork: true #213 （1）执行一下 1[root@master ingress]# kubectl apply -f mandatory.yaml （2）查看一下 1[root@master ingress]# kubectl get pod -n ingress-nginx 2、将ingress-nginx-controller暴露为一个Service资源对象。 1234567891011121314151617181920212223242526[root@master yaml]# vim service-nodeport.yaml apiVersion: v1kind: Servicemetadata: name: ingress-nginx namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxspec: type: NodePort ports: - name: http port: 80 targetPort: 80 protocol: TCP - name: https port: 443 targetPort: 443 protocol: TCP selector: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx--- （1）执行一下 1[root@master ingress]# kubectl apply -f service-nodeport.yaml （2）查看一下 1[root@master ingress]# kubectl get svc -n ingress-nginx 3、创建一个deployment资源，和一个service资源， 并相互关联。 123456789101112131415161718192021222324252627[root@master yaml]# vim deploy1.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: deploy1spec: replicas: 2 template: metadata: labels: app: nginx1 spec: containers: - name: nginx1 image: nginx---apiVersion: v1kind: Servicemetadata: name: svc-1spec: selector: app: nginx1 ports: - port: 80 targetPort: 80 执行一下 1[root@master yaml]# kubectl apply -f deploy1.yaml 查看一下 1[root@master yaml]# kubectl get pod 1[root@master yaml]# kubectl get svc 然后复制deploy1.yaml资源工创建另外”一对“服务。 123456789101112131415161718192021222324252627[root@master yaml]# vim deploy2.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: deploy2spec: replicas: 2 template: metadata: labels: app: nginx2 spec: containers: - name: nginx2 image: nginx---apiVersion: v1kind: Servicemetadata: name: svc-2spec: selector: app: nginx2 ports: - port: 80 targetPort: 80 执行一下 1[root@master yaml]# kubectl apply -f deploy2.yaml 查看一下 1[root@master yaml]# kubectl get deployments. 4. 创建ingress的yaml文件，关联是svc1和svc2 12345678910111213141516171819202122232425262728[root@master yaml]# vim ingress.yamlapiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-1spec: rules: - host: www1.bdqn.com http: paths: - path: / backend: serviceName: svc-1 servicePort: 80---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-2spec: rules: - host: www2.bdqn.com http: paths: - path: / backend: serviceName: svc-2 servicePort: 80 执行一下 1[root@master yaml]# kubectl apply -f ingress.yaml 查看一下 1[root@master yaml]# kubectl get ingresses. 1[root@master yaml]# kubectl describe ingresses. ingress-1 1[root@master yaml]# kubectl describe ingresses. ingress-2 5、由于实验环境限制，所以自己用来模拟-一个域名。 进入本机的 C:\\Windows\\System32\\drivers\\etc ， 修改hosts文件，添加Pod（ingress-controller）运行所在的节点IP。 访问一下 12[root@master yaml]# kubectl get svc -n ingress-nginx //查看映射的端口 http://www1.bdqn.com:30817/ http://www2.bdqn.com:30817/ 总结上述示例的pod是如何一步一步可以使client访问到的，总结如下： 后端pod===》service====》ingress规则====》写入Ingress-nginx-controller配置文件并自动重载使更改生效===》对本机进行域名解析====》实现client通过域名的IP+端口都可以访问到后端pod Ingress资源实现https代理安全访问。 在上面的操作中，实现了使用ingress-nginx为后端所有pod提供一个统一的入口，那么，有一个非常严肃的问题需要考虑，就是如何为我们的pod配置CA证书来实现HTTPS访问？在pod中直接配置CA么？那需要进行多少重复性的操作？而且，pod是随时可能被kubelet杀死再创建的。当然这些问题有很多解决方法，比如直接将CA配置到镜像中，但是这样又需要很多个CA证书。 这里有更简便的一种方法，就拿上面的情况来说，后端有多个pod，pod与service进行关联，service又被ingress规则发现并动态写入到ingress-nginx-controller容器中，然后又为ingress-nginx-controller创建了一个Service映射到群集节点上的端口，来供client来访问。 在上面的一系列流程中，关键的点就在于ingress规则，我们只需要在ingress的yaml文件中，为域名配置CA证书即可，只要可以通过HTTPS访问到域名，至于这个域名是怎么关联到后端提供服务的pod，这就是属于k8s群集内部的通信了，即便是使用http来通信，也无伤大雅。 1. 生成证书 12345[root@master yaml]# mkdir https//创建一个放置证书的目录[root@master yaml]# cd https/[root@master https]# openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \"/CN=testsvc /O=testsvc\"//生成证书 2. 创建secret资源， 保存证书。 1[root@master https]# kubectl create secret tls tls-secret --key=tls.key --cert tls.crt 3、创建一个deploy3.yaml文件，模拟一个web服务。 123456789101112131415161718192021222324252627[root@master yaml]# vim deploy3.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: deploy3spec: replicas: 2 template: metadata: labels: app: nginx3 spec: containers: - name: nginx3 image: nginx---apiVersion: v1kind: Servicemetadata: name: svc-3spec: selector: app: nginx3 ports: - port: 80 targetPort: 80 执行一下 1[root@master https]# kubectl apply -f deploy3.yaml 查看一下 1[root@master https]# kubectl get pod 1[root@master https]# kubectl get svc 4、创建对应的ingress规则。 12345678910111213141516171819[root@master https]# vim ingress.yamlapiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-3spec: tls: - hosts: - www3.bdqn.com #域名 secretName: tls-secret #保存的证书 rules: - host: www3.bdqn.com http: paths: - path: / backend: serviceName: svc-3 servicePort: 80 执行一下 1[root@master https]# kubectl apply -f ingress.yaml 查看一下 1[root@master https]# kubectl get ingresses. 5.查找对应service nodePort的443端口映射的端口，直接用浏览器访问即可。 进入本机的 C:\\Windows\\System32\\drivers\\etc ， 修改hosts文件，添加Pod（ingress-controller）运行所在的节点IP。 查看映射端口 1[root@master https]# kubectl get svc -n ingress-nginx https://www3.bdqn.com:31372/ k8s集群利用了“一切皆为资源”的原理，把生成的ca证书当成一个公共的资源来使用，使用时只需绑定保存的ca证书即可，不像之前一样，需要一个一个的创建ca证书，然后在关联起来，方便好用又快捷。","path":"posts/7f86.html","date":"06-07","excerpt":"","tags":[{"name":"ingress-nginx","slug":"ingress-nginx","permalink":"https://wsdlxgp.top/tags/ingress-nginx/"},{"name":"https","slug":"https","permalink":"https://wsdlxgp.top/tags/https/"},{"name":"ca","slug":"ca","permalink":"https://wsdlxgp.top/tags/ca/"}]},{"title":"38 K8S的inress-nginx","text":"一、Ingress 及 Ingress Controller 简介 Ingress简单的理解: 原先暴露的service,现在给定个统一的访问入口。 Ingress 是 k8s 资源对象，用于对外暴露服务，该资源对象定义了不同主机名（域名）及 URL 和对应后端 Service（k8s Service）的绑定，根据不同的路径路由 http 和 https 流量。而 Ingress Contoller 是一个 pod 服务，封装了一个 web 前端负载均衡器，同时在其基础上实现了动态感知 Ingress 并根据 Ingress 的定义动态生成 前端 web 负载均衡器的配置文件，比如 Nginx Ingress Controller 本质上就是一个 Nginx，只不过它能根据 Ingress 资源的定义动态生成 Nginx 的配置文件，然后动态 Reload。个人觉得 Ingress Controller 的重大作用是将前端负载均衡器和 Kubernetes 完美地结合了起来，一方面在云、容器平台下方便配置的管理，另一方面实现了集群统一的流量入口，而不是像 nodePort 那样给集群打多个孔。。 所以，总的来说要使用 Ingress，得先部署 Ingress Controller 实体（相当于前端 Nginx），然后再创建 Ingress （相当于 Nginx 配置的 k8s 资源体现），Ingress Controller 部署好后会动态检测 Ingress 的创建情况生成相应配置。Ingress Controller 的实现有很多种：有基于 Nginx 的，也有基于 HAProxy的，还有基于 OpenResty 的 Kong Ingress Controller 等，更多 Controller 见：https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/，本文使用基于 Nginx 的 Ingress Controller：ingress-nginx。 二、Ingress 组成 将Nginx的配置抽象成一个Ingress对象，每添加一个新的服务只需写一个新的Ingress的yaml文件即可 将新加入的Ingress转化成Nginx的配置文件并使之生效 ingress controller ingress服务 三、ingress的工作原理 ingress具体的工作原理如下: ingress contronler通过与k8s的api进行交互，动态的去感知k8s集群中ingress服务规则的变化，然后读取它，并按照定义的ingress规则，转发到k8s集群中对应的service。 而这个ingress规则写明了哪个域名对应k8s集群中的哪个service，然后再根据ingress-controller中的nginx配置模板，生成一段对应的nginx配置。 然后再把该配置动态的写到ingress-controller的pod里，该ingress-controller的pod里面运行着一个nginx服务，控制器会把生成的nginx配置写入到nginx的配置文件中，然后reload一下，使其配置生效。以此来达到域名分配置及动态更新的效果。 四、Ingress 可以解决什么问题？ 动态配置服务 如果按照传统方式, 当新增加一个服务时, 我们可能需要在流量入口加一个反向代理指向我们新的k8s服务. 而如果用了Ingress, 只需要配置好这个服务, 当服务启动时, 会自动注册到Ingress的中, 不需要而外的操作. 减少不必要的暴露端口 配置过k8s的都清楚, 第一步是要关闭防火墙的, 主要原因是k8s的很多服务会以NodePort方式映射出去, 这样就相当于给宿主机打了很多孔, 既不安全也不优雅. 而Ingress可以避免这个问题, 除了Ingress自身服务可能需要映射出去, 其他服务都不要用NodePort方式 五、Ingress-nginx配置示例 1) 创建一个web服务，用deployment资源， 用httpd镜像，然后创建一个service资源与之关联。 1234567891011121314151617181920212223242526272829303132333435363738[root@master ingress]# vim deploy_1.yamlapiVersion: v1kind: Namespacemetadata: name: bdqn-ns labels: name: bdqn-ns---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: httpd-deploy namespace: bdqn-nsspec: replicas: 2 template: metadata: labels: app: bdqn-ns spec: containers: - name: httpd image: httpd---apiVersion: v1kind: Servicemetadata: name: httpd-svc namespace: bdqn-nsspec: type: NodePort selector: app: bdqn-ns ports: - name: http-port port: 80 targetPort: 80 nodePort: 31033 执行一下 1[root@master ingress]# kubectl apply -f deploy_1.yaml 查看一下 1[root@master ingress]# kubectl get svc -n bdqn-ns 1[root@master ingress]# kubectl get pod -n bdqn-ns 访问一下 2) 创建一个web服务，用deployment 资源，用tomcat:8.5.45镜像。 1234567891011121314151617181920212223242526272829303132[root@master ingress]# vim deploy_2.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: tomcat-deploy namespace: bdqn-nsspec: replicas: 2 template: metadata: labels: app: bdqn-tomcat spec: containers: - name: tomcat image: tomcat:8.5.45---apiVersion: v1kind: Servicemetadata: name: tomcat-svc namespace: bdqn-nsspec: type: NodePort selector: app: bdqn-tomcat ports: - name: tomcat-port port: 8080 targetPort: 8080 nodePort: 32033 执行一下 1[root@master ingress]# kubectl apply -f deploy_2.yaml 查看一下 1[root@master ingress]# kubectl get pod -n bdqn-ns 1[root@master ingress]# kubectl get svc -n bdqn-ns 访问一下 3) 在k8s集群前边部署一个反向代理服务器，这个服务器代理这k8s集群内部的service资源。 1. Ingress: （1）Ingress controller: 将新加入的Ingress转化为反向代理服务器的配置文件，并使之生效。(动态的感知k8s集群内Ingress资源的变化。） （2）Ingress : Ingress:将反向代理服务器的配置抽象成一个Ingress对象，每添加一个新的服务，只需要写一个新的Ingress的yaml文件即可。 2. Nginx :反向代理服务器。 需要解决了两个问题: 1、动态的配置服务。 2、减少不必要的端口暴露。 基于nginx的ingress controller根据不同的开发公司，又分为两种: ​ 1、k8s社区版的: Ingerss - nginx. ​ 2、nginx公司自己开发的: nginx- ingress . 3. 在gitbub上找到所需的ingress的yaml文件 4. master下载 1[root@master ingress]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.29.0/deploy/static/mandatory.yaml 5. 修改 mandatory.yaml 文件 12[root@master ingress]# vim mandatory.yaml hostNetwork: true #213 ---------如果ingress-controller镜像下载不成功，可以直接使用下边的镜像。 docker pull registry.cn-hangzhou.aliyuncs.com/ilanni/nginx-ingress-controller:0.22.0 需要注意的是，如果使用上述镜像，需要将deployment资源指定的镜像名称进行修改。 修改的是madatory.yaml文件里的deployment资源。 在deployment资源中，如果添加了此字段，意味着Pod中运行的应用可以直接使用node节点的端口，这样node节 点主机所在网络的其他主机，就可以通过访问该端口访问此应用。(类似于docker映射到宿主机 上的端口。) （1）执行一下 1[root@master ingress]# kubectl apply -f mandatory.yaml （2）查看一下 1[root@master ingress]# kubectl get pod -n ingress-nginx 6. 创建一个service的yaml文件 （1）执行一下 1[root@master ingress]# kubectl apply -f mandatory.yaml （2）查看一下 1234567891011121314151617[root@master ingress]# vim mandatory-svc.yaml apiVersion: v1kind: Servicemetadata: name: ingress-nginx namespace: ingress-nginxspec: type: NodePort ports: - name: httpd port: 80 targetPort: 80 - name: https port: 443 selector: app: ingress-nginx （1）执行一下 1[root@master ingress]# kubectl apply -f mandatory-svc.yaml （2）查看一下 1[root@master ingress]# kubectl get svc -n ingress-nginx 4）创建Ingress资源。 ingress ： ingress-nginx-controller: 动态感知ingress 资源的变化 ingress: 创建svc与ingress-nginx-controller 关联的规则 （1）编写ingress的yaml文件 123456789101112131415161718192021[root@master yaml]# vim ingress.yaml apiVersion: extensions/v1beta1kind: Ingressmetadata: name: bdqn-ingress namespace: bdqn-ns annotations: nginx.ingress.kubernetes.io/rewrite-target: /spec: rules: #规则 - host: ingress.bdqn.com #域名 http: paths: - path: / backend: serviceName: httpd-svc #关联service servicePort: 80 #关联service的映射端口 - path: /tomcat backend: serviceName: tomcat-svc #关联service servicePort: 8080 #关联service的映射端口 执行一下 1[root@master yaml]# kubectl apply -f ingress.yaml 查看一下 1[root@master yaml]# kubectl get pod -n ingress-nginx -o wide 1[root@master yaml]# kubectl get ingresses. -n bdqn-ns 1[root@master yaml]# kubectl describe ingresses. -n bdqn-ns 进入pod查看一下 12[root@master yaml]# kubectl exec -it -n ingress-nginx nginx-ingress-controller-5954d475b6-24k92 /bin/sh/etc/nginx $ cat nginx.conf （2）访问一下 进入本机的 C:\\Windows\\System32\\drivers\\etc ， 修改hosts文件，添加Pod（ingress-controller）运行所在的节点IP。 访问http://ingress.bdqn.com/ 访问http://ingress.bdqn.com/tomcat 5）为ingress-nginx创建一个service（使用官网的service文件就可以） 复制上面的网址 12[root@master yaml]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.29.0/deploy/static/provider/baremetal/service-nodeport.yaml//下载文件到master节点 执行一下，下载的service文件 1[root@master yaml]# kubectl apply -f service-nodeport.yaml 查看一下 1[root@master yaml]# kubectl get service -n ingress-nginx 访问一下 进入本机的 C:\\Windows\\System32\\drivers\\etc ， 修改hosts文件，添加Pod（ingress-controller）运行所在的节点IP。 访问http://ingress.bdqn.com:30817/ 访问http://ingress.bdqn.com:30817/tomcat Service -Nodeport:因为ingress - nginx - controller运行在了集群内的其中一个节点，为了保证即使这个节点宕机，我们对应的域名仍然能够正常访问服务，所以我们将ingress -nginx- controller也暴露为一个service资源。 六、练习: 创建一个deploymen资源，基于nginx镜像，repolicas：2个.然后创建一个service资源关联这个deployment资源。最后创建一个ingress资源，将上述svc关联到ingress.bdqn.com/nginx 目录下。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455[root@master yaml]# vim lianxi.yamlapiVersion: v1kind: Namespacemetadata: name: xgp-666 labels: name: xgp-666---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: xgp namespace: xgp-666spec: replicas: 2 template: metadata: labels: app: xgp-nginx spec: containers: - name: xgp-nginx image: nginx---apiVersion: v1kind: Servicemetadata: name: xgp-svc namespace: xgp-666spec: type: NodePort selector: app: xgp-nginx ports: - name: xgp-port port: 80 targetPort: 80 nodePort: 30000---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: xgp-ingress namespace: xgp-666 annotations: nginx.ingress.kubernetes.io/rewrite-target: /spec: rules: - host: ingress.xgp.com http: paths: - path: / backend: serviceName: xgp-svc servicePort: 80 执行一下 1[root@master yaml]# kubectl apply -f lianxi.yaml 查看一下 1[root@master yaml]# kubectl describe ingresses. -n xgp-666 进入本机的 C:\\Windows\\System32\\drivers\\etc ， 修改hosts文件，添加Pod（ingress-controller）运行所在的节点IP。 添加完之后访问一下http://ingress.xgp.com/","path":"posts/c92f.html","date":"06-07","excerpt":"","tags":[{"name":"nginx","slug":"nginx","permalink":"https://wsdlxgp.top/tags/nginx/"},{"name":"ingress","slug":"ingress","permalink":"https://wsdlxgp.top/tags/ingress/"},{"name":"ingress controller","slug":"ingress-controller","permalink":"https://wsdlxgp.top/tags/ingress-controller/"}]},{"title":"37 k8s的Secret（密文）和configmap（明文）的使用教程","text":"一、Secret Secret :用来保存一些敏感信息，比如数据库的用户名密码或者秘钥。 概览 Secret是用来保存小片敏感数据的k8s资源，例如密码，token，或者秘钥。这类数据当然也可以存放在Pod或者镜像中，但是放在Secret中是为了更方便的控制如何使用数据，并减少暴露的风险。 用户可以创建自己的secret，系统也会有自己的secret。 Pod需要先引用才能使用某个secret，Pod有2种方式来使用secret：作为volume的一个域被一个或多个容器挂载；在拉取镜像的时候被kubelet引用。 內建的Secrets 由ServiceAccount创建的API证书附加的秘钥 k8s自动生成的用来访问apiserver的Secret，所有Pod会默认使用这个Secret与apiserver通信 1. Secret类型 Secret有三种类型： Opaque：使用base64编码存储信息，可以通过base64 --decode解码获得原始数据，因此安全性弱。 kubernetes.io/dockerconfigjson：用于存储docker registry的认证信息。 kubernetes.io/service-account-token：用于被 serviceaccount 引用。serviceaccout 创建时 Kubernetes 会默认创建对应的 secret。Pod 如果使用了 serviceaccount，对应的 secret 会自动挂载到 Pod 的 /run/secrets/kubernetes.io/serviceaccount 目录中。 举例:保存数据库的用户名和密码 用户名： root 密码： 123.com 1、通过–from-literal（文字的） 1[root@master secret]# kubectl create secret generic mysecret1 --from-literal=username=root --from-literal=password=123.com generic：通用的，一般的加密方式 查看一下 1[root@master secret]# kubectl get secrets 类型是Opaque（不透明的） 2、通过from-file（文件） 新建两个文件并分别写入用户名和密码 12[root@master secret]# echo root &gt; username[root@master secret]# echo 123.com &gt; password 创建一个secret 1[root@master secret]# kubectl create secret generic mysecret2 --from-file=username --from-file=password 查看一下 1[root@master secret]# kubectl get secrets 3、通过-- from- env-file: 创建一个文件写入用户名和密码 123[root@master secret]#vim env.txt username=rootpassword=123.com 创建一个secret 1[root@master secret]# kubectl create secret generic mysecret3 --from-env-file=env.txt 查看一下 1[root@master secret]# kubectl get secrets 4、通过yaml配置文件 （1）把需要保存的数据加密（”base64“的方式） 1234[root@master secret]# echo root | base64cm9vdAo=[root@master secret]# echo 123.com | base64MTIzLmNvbQo= 解码： 1234[root@master secret]# echo -n cm9vdAo | base64 --decode root[root@master secret]# echo -n MTIzLmNvbQo | base64 --decode 123.com （2）编写secre4的yaml文件 12345678[root@master secret]# vim secret4.yamlapiVersion: v1kind: Secretmetadata: name: mysecret4data: username: cm9vdAo= password: MTIzLmNvbQo= 执行一下 1[root@master secret]# kubectl apply -f secret4.yaml （3）查看一下 1[root@master secret]# kubectl get secrets 如果来使用Secret资源 1. 以Volume挂载的方式 使用Secret secret可以作为数据卷挂载或者作为环境变量暴露给Pod中的容器使用，也可以被系统中的其他资源使用。比如可以用secret导入与外部系统交互需要的证书文件等。 在Pod中以文件的形式使用secret 创建一个Secret，多个Pod可以引用同一个Secret 修改Pod的定义，在spec.volumes[]加一个volume，给这个volume起个名字，spec.volumes[].secret.secretName记录的是要引用的Secret名字 在每个需要使用Secret的容器中添加一项spec.containers[].volumeMounts[]，指定spec.containers[].volumeMounts[].readOnly = true，spec.containers[].volumeMounts[].mountPath要指向一个未被使用的系统路径。 修改镜像或者命令行使系统可以找到上一步指定的路径。此时Secret中data字段的每一个key都是指定路径下面的一个文件名 编写pod的yaml文件 12345678910111213141516171819202122[root@master secret]# vim pod.yaml apiVersion: v1kind: Podmetadata: name: mypodspec: containers: - name: mypod image: busybox args: - /bin/sh - -c - sleep 300000 volumeMounts: - name: secret-test mountPath: \"/etc/secret-test\" #pod中的路径 readOnly: true #是否只读 volumes: - name: secret-test secret: secretName: mysecret1 还可以自定义存放数据的文件名 执行一下 1[root@master secret]# kubectl apply -f pod.yaml Secret文件权限 可以指定secret文件的权限，类似linux系统文件权限，如果不指定默认权限是0644，等同于linux文件的-rw-r–r--权限 进入容器查看保存的数据 12345678[root@master secret]# kubectl exec -it mypod /bin/sh/ # cd /etc/secret-test//etc/secret-test # lspasword username/etc/secret-test # cat username root/etc/secret-test # cat pasword 123.com 测试是否有只读权限 12123.com/etc/secret-test # echo admin &gt; username/bin/sh: can't create username: Read-only file system 1.1 自定义存放数据的文件名的yaml文件 1234567891011121314151617181920212223242526[root@master yaml]# vim pod.yaml apiVersion: v1kind: Podmetadata: name: mypodspec: containers: - name: mypod image: busybox args: - /bin/sh - -c - sleep 300000 volumeMounts: - name: secret-test mountPath: \"/etc/secret-test\" #pod中的路径 readOnly: true #是否只读 volumes: - name: secret-test secret: secretName: mysecret1 items: - key: username path: my-group/my-username #自定义的容器中的目录 - key: password path: my-group/my-password #自定义的容器中的目录 执行一下 1[root@master yaml]# kubectl apply -f pod.yaml 查看一下 123456[root@master secret]# kubectl exec -it mypod /bin/sh//进入容器查看 # cat /etc/secret-test/my-group/my-password 123.com # cat /etc/secret-test/my-group/my-username root 1.2 如果，现在将secret资源内保存的数据进行更新，请问，使用此数据的应用内，数据是是否也会更新? 会实时更新(这里引用数据，是以volumes挂 载使用数据的方式)。 更新mysecret1的数据: password —&gt; admin YWRtaW4K (base64) 可以通过edit 命令，直接修改。 1[root@master secret]# kubectl edit secrets mysecret1 查看一下 123456[root@master secret]# kubectl exec -it mypod /bin/sh//进入容器查看 # cat /etc/secret-test/my-group/my-password admin # cat /etc/secret-test/my-group/my-username root 数据已经成功更新了 2、以环境变量的方式 创建一个Secret，多个Pod可以引用同一个Secret 修改pod的定义，定义环境变量并使用env[].valueFrom.secretKeyRef指定secret和相应的key 修改镜像或命令行，让它们可以读到环境变量 编写pod的yaml文件 123456789101112131415161718192021222324[root@master secret]# vim pod-env.yaml apiVersion: v1kind: Podmetadata: name: mypod2spec: containers: - name: mypod image: busybox args: - /bin/sh - -c - sleep 300000 env: - name: SECRET_USERNAME valueFrom: secretKeyRef: name: mysecret2 key: username - name: SECRET_PASSWORD valueFrom: secretKeyRef: name: mysecret2 key: password 执行一下 1[root@master secret]# kubectl apply -f pod-env.yaml 查看一下 1[root@master secret]# kubectl get pod 进入容器查看保存的数据 12345[root@master secret]# kubectl exec -it mypod2 /bin/sh/ # echo $SECRET_USERNAMEroot/ # echo $SECRET_PASSWORD123.com 2.1 更新sevret文件的内容 12[root@master yaml]# kubectl edit secrets mysecret2//修改保存文件的内容 查看一下 12345[root@master secret]# kubectl exec -it mypod2 /bin/sh/ # echo $SECRET_USERNAMEroot/ # echo $SECRET_PASSWORD123.com 等待了一定时间后，可以看到这个数据并没有没有改变 总结 如果引用secret数据的应用， 要求会随着secret资源对象内保存的数据的更新，而实时更新，那么应该使用volumes挂载的方式引用资源因为用环境变量的方式引用不会实时更新数据。 二、ConfigMap 和Secret资源类似，不同之处在于，secret 资源保存的是敏感信息，而Configmap保存的是以明文方式存放的数据。 Configmap的创建与使用方式与Secret非常类似，不同点只在于数据以明文形式存放（不过，我觉得Secret的密文形式也并不密文，只能算得上是简单编码）。 和Secret资源类似，不同之处在于，secret 资源保存的是敏感信息，而Configmap保存的是以明文方式存放的数据。 username：adam age：18 创建的四种方式 1、通过-- from- literal(文字的): 1[root@master yaml]# kubectl create configmap myconfigmap1 --from-literal=username=adam --from-literal=age=18 查看一下 1[root@master yaml]# kubectl get cm 1[root@master yaml]# kubectl describe cm 2、通过–from-file (文件) : 12[root@master yaml]# echo adam &gt; username[root@master yaml]# echo 18 &gt; age 创建 1[root@master yaml]# kubectl create configmap myconfigmap2 --from-file=username --from-file=age 查看一下 1[root@master yaml]# kubectl describe cm 3、通过–from- env-file: 123[root@master yaml]# vim env.txt username=adamage=18 创建 1[root@master yaml]# kubectl create configmap myconfigmap3 --from-env-file=env.txt 查看一下 1[root@master configmap]# kubectl describe cm 4、通过yaml配置文件: 12345678[root@master yaml]# vim configmap.yamlapiVersion: v1kind: ConfigMapmetadata: name: myconfigmap4data: username: 'adam' age: '18' 创建 1[root@master yaml]# kubectl apply -f configmap.yaml 查看一下 1[root@master yaml]# kubectl describe cm 如何来使用configmap资源 1. 以Volume挂载的方式 123456789101112131415161718192021[root@master yaml]# vim v-pod.yaml apiVersion: v1kind: Podmetadata: name: pod1spec: containers: - name: mypod image: busybox args: - /bin/sh - -c - sleep 300000 volumeMounts: - name: cmp-test mountPath: \"/etc/cmp-test\" readOnly: true volumes: - name: cmp-test configMap: name: myconfigmap1 执行一下 1[root@master configmap]# kubectl apply -f v-pod.yaml 查看一下 123456[root@master configmap]# kubectl exec -it pod1 /bin/sh//进入容器查看一下 # cat /etc/cmp-test/age 18/ # cat /etc/cmp-test/username adam/ 1.1 自定义存放数据的文件名的yaml文件 1234567891011121314151617181920212223242526[root@master configmap]# vim v-pod2.yaml apiVersion: v1kind: Podmetadata: name: pod3spec: containers: - name: mypod image: busybox args: - /bin/sh - -c - sleep 300000 volumeMounts: - name: cmp-test mountPath: \"/etc/cmp-test\" readOnly: true volumes: - name: cmp-test configMap: name: myconfigmap1 items: - key: username path: my-group/my-username #自定义的容器中的目录 - key: age path: my-group/my-age #自定义的容器中的目录 执行一下 1[root@master configmap]# kubectl apply -f v-pod2.yaml 查看一下 123456[root@master configmap]# kubectl exec -it pod3 /bin/sh//进入容器查看# cat /etc/cmp-test/my-group/my-username adam/ # cat /etc/cmp-test/my-group/my-age 18/ 1.2 如果，现在将secret资源内保存的数据进行更新，请问，使用此数据的应用内，数据是是否也会更新? 1[root@master configmap]# kubectl edit cm myconfigmap1 查看一下 123456[root@master configmap]# kubectl exec -it pod3 /bin/sh//进入容器查看# cat /etc/cmp-test/my-group/my-username adam/ # cat /etc/cmp-test/my-group/my-age 10 可以看到更新成功 2.以环境变量的方式 123456789101112131415161718192021222324[root@master configmap]# vim e-pod.yaml apiVersion: v1kind: Podmetadata: name: pod2spec: containers: - name: mypod image: busybox args: - /bin/sh - -c - sleep 300000 env: - name: CONFIGMAP_NAME valueFrom: configMapKeyRef: name: myconfigmap2 key: username - name: CONFIGMAP_AGE valueFrom: configMapKeyRef: name: myconfigmap2 key: age 执行一下 1[root@master configmap]# kubectl apply -f e-pod.yaml 查看一下 123456[root@master configmap]# kubectl exec -it pod2 /bin/sh//进入容器查看一下 # echo $CONFIGMAP_NAMEadam # echo $CONFIGMAP_AGE18 2.1 更新sevret文件的内容 12[root@master configmap]# kubectl edit cm myconfigmap2 //修改保存文件的内容 查看一下 123456[root@master configmap]# kubectl exec -it pod2 /bin/sh//进入容器查看一下 # echo $CONFIGMAP_NAMEadam # echo $CONFIGMAP_AGE18 等待了一定时间后，可以看到这个数据并没有没有改变 可以看出这个configmap和secret的更新效果基本没有区别。 总结configmap、与secret资源有什么相同和不同之处。 Secret 与 ConfigMap 对比 相同点： key/value的形式 属于某个特定的namespace 可以导出到环境变量 可以通过目录/文件形式挂载 通过 volume 挂载的配置信息均可热更新 不同点： Secret 可以被 ServerAccount 关联 Secret 可以存储 docker register 的鉴权信息，用在 ImagePullSecret 参数中，用于拉取私有仓库的镜像 Secret 支持 Base64 加密 Secret 分为 kubernetes.io/service-account-token、kubernetes.io/dockerconfigjson、Opaque 三种类型，而 Configmap 不区分类型 总结以volumes挂载、和环境变量方式引用资源的相同和不同之处。 volumes挂载(可根据更改数据更新)：引用自己创建的secret（密文）或configmap（明文），挂载到容器中指定的目录下。查看保存的文件时，根据自己所填路径和secret或configmap创建的文件，进行查看。 环境变量(不因更改数据更新)：引用自己创建的secret（密文）或configmap（明文），挂载到容器中指定的目录下。查看保存的文件时，根据自己环境变量，进行查看。","path":"posts/a387.html","date":"06-07","excerpt":"","tags":[{"name":"secret","slug":"secret","permalink":"https://wsdlxgp.top/tags/secret/"},{"name":"pod","slug":"pod","permalink":"https://wsdlxgp.top/tags/pod/"},{"name":"configmap","slug":"configmap","permalink":"https://wsdlxgp.top/tags/configmap/"}]},{"title":"36 k8s的StatefulSet（有状态服务）实现","text":"StatefulSet介绍 遇到的问题： 使用Deployment创建的Pod是无状态的，当挂在Volume之后，如果该Pod挂了，Replication Controller会再run一个来保证可用性，但是由于是无状态的，Pod挂了的时候与之前的Volume的关系就已经断开了，新起来的Pod无法找到之前的Pod。但是对于用户而言，他们对底层的Pod挂了没有感知，但是当Pod挂了之后就无法再使用之前挂载的磁盘了。 StatefulSet: 是一种给Pod提供唯一标志的控制器，它可以保证部署和扩展的顺序。 Pod一致性：包含次序（启动、停止次序）、网络一致性。此一致性与Pod相关，与被调度到哪个node节点无关。 稳定的次序：对于N个副本的StatefulSet，每个Pod都在[0，N)的范围内分配一个数字序号，且是唯一的。 稳定的网络：Pod的hostname模式为(statefulset名称)- (序号)。 稳定的存储：通过VolumeClaimTemplate为每个Pod创建一个PV。删除、减少副本，不会删除相关的卷。 (1) RC、 RS、Deployment、DS。-----&gt; 无状态服务 template(模板):根据模板 创建出来的Pod,它们J的状态都是一模一样的(除了名称，IP, 域名之外) 可以理解为:任何一个Pod, 都可以被删除，然后用新生成的Pod进行替换。 (2) 有状态的服务: 需要记录前一 次或者多次通信中的相关事件，以作为一下通信的分类标准。比如: mysql等数据库服务。(Pod的名称，不能随意变化。数据持久化的目录也是不一样，每一个Pod都有自己独有的数据持久化存储目录。) mysql:主从关系。 如果把之前无状态的服务比喻为牛、羊等牲畜，因为，这些到一定时候就可以出售。那么，有状态就比喻为:宠物，而宠物不像牲畜一样到达一定时候出售，人们往往会照顾宠物的一生。 (3) 每一个Pod----&gt;对应一个PVC----&gt;每一个PVC对应一个PV。 storageclass:自动创建PV 需要解决:自动创建PVC。 实现原理 与 ReplicaSet 和 Deployment 资源一样，StatefulSet 也使用控制器的方式实现，它主要由 StatefulSetController、StatefulSetControl 和 StatefulPodControl 三个组件协作来完成 StatefulSet 的管理，StatefulSetController 会同时从 PodInformer 和 ReplicaSetInformer 中接受增删改事件并将事件推送到队列中： 控制器 StatefulSetController 会在 Run 方法中启动多个 Goroutine 协程，这些协程会从队列中获取待处理的 StatefulSet 资源进行同步，接下来我们会先介绍 Kubernetes 同步 StatefulSet 的过程。 1，例子 （1）创建一个statefulset的yaml文件 12345678910111213141516171819202122232425262728293031323334[root@master yaml]# vim statefulset.yamlapiVersion: v1kind: Servicemetadata: name: headless-svc labels: app: headless-svcspec: ports: - port: 80 selector: app: headless-pod clusterIP: None #没有同一的ip---apiVersion: apps/v1kind: StatefulSetmetadata: name: statefulset-testspec: serviceName: headless-svc replicas: 3 selector: matchLabels: app: headless-pod template: metadata: labels: app: headless-pod spec: containers: - name: myhttpd image: httpd ports: - containerPort: 80 Deployment : Deploy+RS+随机字符串(Pod的名称。)没有顺序的，可 以没随意替代的。 1、headless-svc :无头服务。因为没有IP地址，所以它不具备负载均衡的功能了。因为statefulset要求Pod的名称是有顺序的，每一个Pod都不能被随意取代，也就是即使Pod重建之后，名称依然不变。为后端的每一个Pod去命名。 2、statefulSet:定义具体的应用 3、volumeClaimT emplates:自动创建PVC，为后端的Pod提供专有的存储。 执行一下 1[root@master yaml]# kubectl apply -f statefulset.yaml 查看一下 1[root@master yaml]# kubectl get svc 12[root@master yaml]# kubectl get pod//可看到这些pod是有顺序的 一、创建StorageClass资源对象。 1、基于NFS服务，创建NFS服务。 下载nfs所需安装包 1[root@node02 ~]# yum -y install nfs-utils rpcbind 创建共享目录 1[root@master ~]# mkdir /nfsdata 创建共享目录的权限 12[root@master ~]# vim /etc/exports/nfsdata *(rw,sync,no_root_squash) 开启nfs和rpcbind 12[root@master ~]# systemctl start nfs-server.service [root@master ~]# systemctl start rpcbind 测试一下 1[root@master ~]# showmount -e 2、创建rbac权限。 123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@master yaml]# vim rbac-rolebind.yaml apiVersion: v1kind: ServiceAccountmetadata: name: nfs-provisioner namespace: default---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: nfs-provisioner-runner namespace: defaultrules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"watch\", \"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\", \"endpoints\"] verbs: [\"get\",\"create\",\"list\", \"watch\",\"update\"] - apiGroups: [\"extensions\"] resources: [\"podsecuritypolicies\"] resourceNames: [\"nfs-provisioner\"] verbs: [\"use\"]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: run-nfs-provisionersubjects: - kind: ServiceAccount name: nfs-provisioner namespace: default #必写字段roleRef: kind: ClusterRole name: nfs-provisioner-runner apiGroup: rbac.authorization.k8s.io 执行一下 1[root@master yaml]# kubectl apply -f rbac-rolebind.yaml 3、创建Deployment资源对象，用Pod代替 真正的NFS服务。 123456789101112131415161718192021222324252627282930313233[root@master yaml]# vim nfs-deployment.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nfs-client-provisionerspec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-client-provisioner spec: serviceAccount: nfs-provisioner containers: - name: nfs-client-provisioner image: registry.cn-hangzhou.aliyuncs.com/open-ali/nfs-client-provisioner volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: bdqn - name: NFS_SERVER value: 192.168.1.21 - name: NFS_PATH value: /nfsdata volumes: - name: nfs-client-root nfs: server: 192.168.1.21 path: /nfsdata 执行一下 1[root@master yaml]# kubectl apply -f nfs-deployment.yaml 查看一下 1[root@master yaml]# kubectl get pod 4、创建storageclass的yaml文件 1234567[root@master yaml]# vim test-storageclass.yaml apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: stateful-nfsprovisioner: bdqn #通过provisioner字段关联到上述DeployreclaimPolicy: Retain 执行一下 1[root@master yaml]# kubectl apply -f test-storageclass.yaml 查看一下 1[root@master yaml]# kubectl get sc 二，解决自动创建pvc 1、创建statefulset的yaml文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[root@master yaml]# vim statefulset.yaml apiVersion: v1kind: Servicemetadata: name: headless-svc labels: app: headless-svcspec: ports: - port: 80 name: myweb selector: app: headless-pod clusterIP: None---apiVersion: apps/v1kind: StatefulSetmetadata: name: statefulset-testspec: serviceName: headless-svc replicas: 3 selector: matchLabels: app: headless-pod template: metadata: labels: app: headless-pod spec: containers: - image: httpd name: myhttpd ports: - containerPort: 80 name: httpd volumeMounts: - mountPath: /mnt name: test volumeClaimTemplates: #&gt; 自动创建PVC，为后端的Pod提供专有的存储。** - metadata: name: test annotations: #这是指定storageclass volume.beta.kubernetes.io/storage-class: stateful-nfs spec: accessModes: - ReadWriteOnce resources: requests: storage: 100Mi 在此示例中： 创建了一个名为 headless-svc 的 Service 对象，由 metadata: name 字段指示。该 Service 会定位一个名为 headless-svc 的应用，由 labels: app: headless-svc 和 selector: app: headless-pod 指示。该 Service 会公开端口 80 并将其命名为 web。而且该 Service 会控制网域并将互联网流量路由到 StatefulSet 部署的容器化应用。 使用三个副本 Pod (replicas: 3) 创建了一个名为 web 的 StatefulSet。 Pod 模板 (spec: template) 指示其 Pod 标记为 app: headless-pod。 Pod 规范 (template: spec) 指示 StatefulSet 的 Pod 运行一个容器 myhttpd，该容器运行版本为 httpd 映像。容器映像由 Container Registry 托管。 Pod 规范使用由 Service 打开的 web 端口。 template: spec: volumeMounts 指定一个名为 test 的 mountPath。mountPath 是容器中应装载存储卷的路径。 StatefulSet 预配了一个具有 100mb 预配存储空间的 PersistentVolumeClaim：test。 执行一下 1[root@master yaml]# kubectl apply -f statefulset.yaml 查看一下 1[root@master yaml]# kubectl get pod 如果第一个pod出现了问题，后面的pod就不会生成。 1[root@master yaml]# kubectl get statefulsets 2、 验证一下数据存储 容器中创建文件 1234[root@master yaml]# kubectl exec -it statefulset-test-0 /bin/sh# cd /mnt# touch testfile# exit 宿主机查看一下 12[root@master yaml]# ls /nfsdata/default-test-statefulset-test-0-pvc-bf1ae1d0-f496-4d69-b33b-39e8aa0a6e8d/testfile 三、小实验 以自己的名称创建一个名称空间，以下所有资源都运行在此空间中。用statefuset资源运行一个httpd web服务，要求3个Pod，但是每个Pod的主界面内容不一样，并且都要做专有的数据持久化，尝试删除其中一个Pod，查看新生成的Pod，总结对比与之前Deployment资源控制器控制的Pod有什么不同之处？ （一）创建StorageClass资源对象。 注意：nfs服务要开启 1、创建namespace的yaml文件 12345[root@master yaml]# vim namespace.yaml kind: NamespaceapiVersion: v1metadata: name: xgp-lll #namespave的名称 执行一下 1[root@master yaml]# kubectl apply -f namespace.yaml 查看一下 1[root@master yaml]# kubectl get namespaces 2. 创建rbac权限。 123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@master yaml]# vim rbac-rolebind.yamlapiVersion: v1kind: ServiceAccountmetadata: name: nfs-provisioner namespace: xgp-lll---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: nfs-provisioner-runner namespace: xgp-lllrules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"watch\", \"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\", \"endpoints\"] verbs: [\"get\",\"create\",\"list\", \"watch\",\"update\"] - apiGroups: [\"extensions\"] resources: [\"podsecuritypolicies\"] resourceNames: [\"nfs-provisioner\"] verbs: [\"use\"]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: run-nfs-provisionersubjects: - kind: ServiceAccount name: nfs-provisioner namespace: xgp-lllroleRef: kind: ClusterRole name: nfs-provisioner-runner apiGroup: rbac.authorization.k8s.io 执行一下 1[root@master yaml]# kubectl apply -f rbac-rolebind.yaml 3、创建Deployment资源对象，用Pod代替 真正的NFS服务。 1234567891011121314151617181920212223242526272829303132333435[root@master yaml]# vim nfs-deployment.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nfs-client-provisioner namespace: xgp-lllspec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-client-provisioner spec: serviceAccount: nfs-provisioner containers: - name: nfs-client-provisioner image: registry.cn-hangzhou.aliyuncs.com/open-ali/nfs-client-provisioner volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: xgp - name: NFS_SERVER value: 192.168.1.21 - name: NFS_PATH value: /nfsdata volumes: - name: nfs-client-root nfs: server: 192.168.1.21 path: /nfsdata 执行一下 1[root@master yaml]# kubectl apply -f nfs-deployment.yaml 查看一下 1[root@master yaml]# kubectl get pod -n xgp-lll 4、创建storageclass的yaml文件 12345678[root@master yaml]# vim test-storageclass.yaml apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: stateful-nfs namespace: xgp-lllprovisioner: xgp #通过provisioner字段关联到上述DeployreclaimPolicy: Retain 执行一下 1[root@master yaml]# kubectl apply -f test-storageclass.yaml 查看一下 1[root@master yaml]# kubectl get sc -n xgp-lll （二）解决自动创建pvc 1、创建statefulset的yaml文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051apiVersion: v1kind: Servicemetadata: name: headless-svc namespace: xgp-lll labels: app: headless-svcspec: ports: - port: 80 name: myweb selector: app: headless-pod clusterIP: None---apiVersion: apps/v1kind: StatefulSetmetadata: name: statefulset-test namespace: xgp-lllspec: serviceName: headless-svc replicas: 3 selector: matchLabels: app: headless-pod template: metadata: labels: app: headless-pod spec: containers: - image: httpd name: myhttpd ports: - containerPort: 80 name: httpd volumeMounts: - mountPath: /usr/local/apache2/htdocs name: test volumeClaimTemplates: #&gt; 自动创建PVC，为后端的Pod提供专有的存储。** - metadata: name: test annotations: #这是指定storageclass volume.beta.kubernetes.io/storage-class: stateful-nfs spec: accessModes: - ReadWriteOnce resources: requests: storage: 100Mi 执行一下 1[root@master yaml]# kubectl apply -f statefulset.yaml 查看一下 1[root@master yaml]# kubectl get pod -n xgp-lll 2、 验证一下数据存储 容器中创建文件 1234567891011第一个[root@master yaml]# kubectl exec -it -n xgp-lll statefulset-test-0 /bin/bash root@statefulset-test-0:/usr/local/apache2# echo 123 &gt; /usr/local/apache2/htdocs/index.html第二个[root@master yaml]# kubectl exec -it -n xgp-lll statefulset-test-1 /bin/bash root@statefulset-test-2:/usr/local/apache2# echo 456 &gt; /usr/local/apache2/htdocs/index.html第三个[root@master yaml]# kubectl exec -it -n xgp-lll statefulset-test-2 /bin/bash root@statefulset-test-1:/usr/local/apache2# echo 789 &gt; /usr/local/apache2/htdocs/index.html 宿主机查看一下 123456789101112第一个[root@master yaml]# cat /nfsdata/xgp-lll-test-statefulset-test-0-pvc-ccaa02df-4721-4453-a6ec-4f2c928221d7/index.html 123第二个[root@master yaml]# cat /nfsdata/xgp-lll-test-statefulset-test-1-pvc-88e60a58-97ea-4986-91d5-a3a6e907deac/index.html 456第三个[root@master yaml]# cat /nfsdata/xgp-lll-test-statefulset-test-2-pvc-4eb2bbe2-63d2-431a-ba3e-b7b8d7e068d3/index.html 789 访问一下 扩容、缩容:在此过程中，Pod的生成或删除操作也是有顺序性的。 升级操作 1kubectl explain sts.spec.updateStrategy.rollingUpdate.partition partition：如果partition后面的值等于N, N+的都会更新。默认值为0（所有都会更新）。","path":"posts/af4b.html","date":"06-07","excerpt":"","tags":[{"name":"StatefulSet","slug":"StatefulSet","permalink":"https://wsdlxgp.top/tags/StatefulSet/"},{"name":"nfs-deployment","slug":"nfs-deployment","permalink":"https://wsdlxgp.top/tags/nfs-deployment/"},{"name":"StorageClass","slug":"StorageClass","permalink":"https://wsdlxgp.top/tags/StorageClass/"}]},{"title":"35 k8s的存储类","text":"k8s有很多的服务，很多的资源对象。 如果要去创建服务，做数据持久化，需要预先知道可用PV有哪些? 如果为了这个服务去提前创建PV，那么我们还需要知道，这个服务，大概需要多大的空间? 环境介绍 主机 IP地址 服务 master 192.168.1.21 k8s node01 192.168.1.22 k8s node02 192.168.1.23 k8s 基于 https://blog.51cto.com/14320361/2464655 的实验继续进行 存储类介绍 Kubernetes集群管理员通过提供不同的存储类，可以满足用户不同的服务质量级别、备份策略和任意策略要求的存储需求。动态存储卷供应使用StorageClass进行实现，其允许存储卷按需被创建。如果没有动态存储供应，Kubernetes集群的管理员将不得不通过手工的方式类创建新的存储卷。通过动态存储卷，Kubernetes将能够按照用户的需要，自动创建其需要的存储。 基于StorageClass的动态存储供应整体过程如下图所示： 1）集群管理员预先创建存储类（StorageClass）； 2）用户创建使用存储类的持久化存储声明(PVC：PersistentVolumeClaim)； 3）存储持久化声明通知系统，它需要一个持久化存储(PV: PersistentVolume)； 4）系统读取存储类的信息； 5）系统基于存储类的信息，在后台自动创建PVC需要的PV； 6）用户创建一个使用PVC的Pod； 7）Pod中的应用通过PVC进行数据的持久化； 8）而PVC使用PV进行数据的最终持久化处理。 先来简单看一下这张图实现的过程，然后我们再来研究一下 说在前面的话，静态供给的话，会需要我们手动去创建pv，如果没有足够的资源，找不到合适的pv，那么pod就会处于pending等待的状态，就是说找不到合适的伴侣了，所以解决这两种问题，就给出了这种动态供给，主要是能够自动帮你创建pv ，就是你需要多大的容量，就自动给你创建多大的容量，也就是pv，k8s帮你创建了，创建pvc的时候就需要找pv了，这个时候就交给这个存储类了，而存储类呢，去帮你创建这些pv,存储类呢，就是实现了对指定存储的一个支持，直接帮你去调用api去创建存储类，所以就不需要人工的去帮你创建pv了。 而你去想想，当节点比较多，业务比较多的时候，再去人工手动创建pv，量还是很大的，而且也不是很好去维护。 而动态供给主要的一个实现就是StorageClass存储对象，其实它就是声明你使用哪个存储，然后呢帮你去连接，再帮你去自动创建pv。 举个例子更好去理解 话不多说下图 其实它是一个基于NFS实现的一个pv供给，它大概流程是这样的，我们可能会创建一个statefulset有状态的应用存储，然后有一个管理的nfs-storageClass，因为nfs目前是不支持这个自动的创建pv的，我们可以利用社区实现的插件来完成这个pv的自动创建，也就是StorageClass这一块，创建完之后，然后pod再去引用。 一，Storage Class（存储类） 作用：它可以动态的自动的创建所需要的PV Provisioner（供给方，提供者）：及提供了存储资源的存储系统。k8s内建有多重供给方，这些供给方的名字都以“kubernetes.io”为前缀。并且还可以自定义。 Parameters（参数）：存储类使用参数描述要关联到的存储卷，注意不同的供给方参数也不同。 ReclaimPlicy: PV的回收策略，可用值有Delete(默认)和Retain （1）确定基于NFS服务来做的SC。NFS开启 1[root@master yaml]# showmount -e （2）需要RBAC权限。 RBAC：rbac是k8s的API的安全策略，是基于用户的访问权限的控制。规定了谁，可以有什么样的权限。 为了给SC资源操作k8s集群的权限。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[root@master yaml]# vim rbac-rolebind.yamlkind: NamespaceapiVersion: v1metadata: name: bdqn-test---apiVersion: v1kind: ServiceAccountmetadata: name: nfs-provisioner namespace: bdqn-test---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: nfs-provisioner-runner namespace: bdqn-testrules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"watch\", \"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\", \"endpoints\"] verbs: [\"get\",\"create\",\"list\", \"watch\",\"update\"] - apiGroups: [\"extensions\"] resources: [\"podsecuritypolicies\"] resourceNames: [\"nfs-provisioner\"] verbs: [\"use\"]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: run-nfs-provisionersubjects: - kind: ServiceAccount name: nfs-provisioner namespace: bdqn-testroleRef: kind: ClusterRole name: nfs-provisioner-runner apiGroup: rbac.authorization.k8s.io 运行一下 1[root@master yaml]# kubectl apply -f rbac-rolebind.yaml （3）nfs-deployment 作用：其实它是一个NFS客户端。但它通过K8S的内置的NFS驱动挂载远端的NFS服务器到本地目录；然后将自身作为storage provider，关联storage class。 1234567891011121314151617181920212223242526272829303132333435[root@master yaml]# vim nfs-deployment.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nfs-client-provisioner namespace: bdqn-testspec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-client-provisioner spec: serviceAccount: nfs-provisioner #指定账户 containers: - name: nfs-client-provisioner image: registry.cn-hangzhou.aliyuncs.com/open-ali/nfs-client-provisioner volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes #指定容器内的挂载目录 env: - name: PROVISIONER_NAME #这是这个容器内置的变量 value: bdqn-test #这是上面变量的值（名字） - name: NFS_SERVER #内置变量，用于指定nfs服务的IP value: 192.168.1.21 - name: NFS_PATH #内置变量，指定的是nfs共享的目录 value: /nfsdata volumes: #这下面是指定上面挂载到容器内的nfs的路径及IP - name: nfs-client-root nfs: server: 192.168.1.21 path: /nfsdata 执行一下 1[root@master yaml]# kubectl apply -f nfs-deployment.yaml （4）创建storageclass 123456789[root@master yaml]# vim test-storageclass.yamlapiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: stateful-nfs namespace: bdqn-testprovisioner: bdqn-test #这里要和第三个nfs-client-provisioner的env环境变量中的value值对应。reclaimPolicy: Retain #回收策略为：retain，还有一个默认的值为“default” 执行一下 1[root@master yaml]# kubectl apply -f test-storageclass.yaml （5）创建PVC 1234567891011121314[root@master yaml]# vim test-pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: test-claim namespace: bdqn-testspec: storageClassName: stateful-nfs #定义存储类的名字，要和SC的名字对应 accessModes: - ReadWriteMany #访问模式为RWM resources: requests: storage: 500Mi 执行一下 1[root@master yaml]# kubectl apply -f test-pvc.yaml 查看一下 1[root@master yaml]# kubectl get pvc （6）创建一个Pod 12345678910111213141516171819202122[root@master yaml]# vim test-pod.yamlkind: PodapiVersion: v1metadata: name: test-pod namespace: bdqn-testspec: containers: - name: test-pod image: busybox args: - /bin/sh - -c - sleep 30000 volumeMounts: - name: nfs-pvc mountPath: /test restartPolicy: OnFailure volumes: - name: nfs-pvc persistentVolumeClaim: claimName: test-claim #这的名字要和PVC的名字一致 执行一下 1[root@master yaml]# kubectl apply -f test-pod.yaml 查看一下 1[root@master yaml]# kubectl get pod -n bdqn-test （7）容器中添加内容，并查看挂载目录 进入容器修改页面内容 123456[root@master yaml]# kubectl exec -it test-pod -n bdqn-test /bin/sh/ # cd test//test # touch test-file/test # echo 123456 &gt; test-file /test # cat test-file 123456 查看挂载目录 123456[root@master yaml]# ls /nfsdata/bdqn-test-test-claim-pvc-79ddfcf1-65ae-455f-9e03-5bcfe6c6ce15web1web2[root@master yaml]# cat /nfsdata/bdqn-test-test-claim-pvc-79ddfcf1-65ae-455f-9e03-5bcfe6c6ce15/test-file 123456 二，如果，K8S集群中， 有很多类似的PV, PVC在去向PV申请空间的时候，不仅会考虑名称以及访问控制模式，还会考虑你申请空间的大小，会分配给你最合适大小的PV。 运行一个web服务，采用Deployment资源，基于nginx镜像，replicas为3个。数据持久化目录为nginx服务的主访问目录：/usr/share/nginx/html 创建一个PVC,与上述资源进行关联。 1. 基于nfs服务来做的PV和pvc 下载nfs所需安装包 1[root@node02 ~]# yum -y install nfs-utils rpcbind 创建共享目录 1[root@master ~]# mkdir /nfsdata 创建共享目录的权限 12[root@master ~]# vim /etc/exports/nfsdata *(rw,sync,no_root_squash) 开启nfs和rpcbind 12[root@master ~]# systemctl start nfs-server.service [root@master ~]# systemctl start rpcbind 测试一下 1[root@master ~]# showmount -e 2.先创建两个PV, web- pV1(1G) ,web-pv2 (2G) web1 12345678910111213141516[root@master yaml]# vim web.yaml apiVersion: v1kind: PersistentVolumemetadata: name: web-pvspec : capacity: storage: 1Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: nfs nfs: path: /nfsdata/web1 server: 192.168.1.21 web2 12345678910111213141516[root@master yaml]# vim web2.yaml apiVersion: v1kind: PersistentVolumemetadata: name: web-pv2spec : capacity : storage: 2Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: nfs nfs: path: /nfsdata/web2 server: 192.168.1.21 3.创建所需文件夹 12[root@master yaml]# mkdir /nfsdata/web1[root@master yaml]# mkdir /nfsdata/web2 4.执行一下web和web2 12[root@master yaml]# kubectl apply -f web.yaml [root@master yaml]# kubectl apply -f web2.yaml 5.查看一下 1[root@master yaml]# kubectl get pv 6.创建web的pvc的yaml文件 12345678910111213[root@master yaml]# vim web-pvc.yaml apiVersion: v1kind: PersistentVolumeClaimmetadata: name: web-pvcspec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: nfs 执行一下 1[root@master yaml]# kubectl apply -f web-pvc.yaml 查看一下 1[root@master yaml]# kubectl get pvc 系统会自动给pvc一个相近内存的pv，所以选择了1G的那个 7.创建pod的yaml文件 123456789101112131415161718192021222324[root@master yaml]# vim web-pod.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: web-podspec: selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - image: nginx name: nginx volumeMounts: - name: web-test mountPath: /usr/share/nginx/html volumes: - name: web-test persistentVolumeClaim: claimName: web-pvc 执行一下 1[root@master yaml]# kubectl apply -f web-pod.yaml 查看一下 1[root@master yaml]# kubectl get pod 8. 访问一下nginx的网页 查看一下nginx的ip 1[root@master yaml]# kubectl get pod -o wide 进入容器设置网页内容 12345root@master yaml]# kubectl exec -it web-pod-8686d9c594-qxhr9 /bin/bashroot@web-pod-8686d9c594-qxhr9:/# cd /usr/share/nginx/html/root@web-pod-8686d9c594-qxhr9:/usr/share/nginx/html# lsroot@web-pod-8686d9c594-qxhr9:/usr/share/nginx/html# echo 123456 &gt; index.htmlroot@web-pod-8686d9c594-qxhr9:/usr/share/nginx/html# exit 访问一下 1[root@master yaml]# curl 10.244.2.17 三，如果两个PV，大小一样，名称一样，访问控制模式不一样，PVC会关联哪一个? (验证PV和PVC 关联的时候，访问模式必须一样) 两个PV，大小一样，名称一样，访问控制模式不一样 &lt;1&gt;创建两个pv web1 123456789101112131415[root@master yaml]# vim web1.yaml apiVersion: v1kind: PersistentVolumemetadata: name: web-pvspec : capacity: storage: 1Gi accessModes: - ReadWriteOnce #能以读-写mount到单个的节点 persistentVolumeReclaimPolicy: Recycle storageClassName: nfs nfs: path: /nfsdata/web1 server: 192.168.1.21 web2 123456789101112131415[root@master yaml]# vim web2.yaml apiVersion: v1kind: PersistentVolumemetadata: name: web-pvspec : capacity: storage: 1Gi accessModes: - ReadWriteMany #能以读-写mount到多个的节点 persistentVolumeReclaimPolicy: Recycle storageClassName: nfs nfs: path: /nfsdata/web1 server: 192.168.1.21 创建所需文件 1[root@master yaml]# mkdir /nfsdata/web1 执行一下 12[root@master yaml]# kubectl apply -f web1.yaml [root@master yaml]# kubectl apply -f web2.yaml &lt;2&gt;创建pvc 123456789101112[root@master yaml]# vim web-pvc.yaml apiVersion: v1kind: PersistentVolumeClaimmetadata: name: web-pvcspec: accessModes: - ReadWriteMany #能以读-写mount到多个的节点 resources: requests: storage: 1Gi storageClassName: nfs 执行一下 1[root@master yaml]# kubectl apply -f web-pvc.yaml &lt;3&gt;查看一下 1[root@master yaml]# kubectl get pv 1[root@master yaml]# kubectl get pvc 现在可以看到pv和pvc关联成功，但是为什么只有一个pv呢？（pv挂载的目录要相同） 那是因为当创建了两个相同名字的pv时它并不会认为这是两个不同的pv，而会把他们当成是同一个pv，后创建的pv会刷新前面创建的pv。然后，当创建了pvc，并且pvc的访问模式和后面创建pv的访问模式一样，他们就会关联成功，反之不成功。（当然这些条件下还需要考虑，pv的内存） 三，小实验 （1）以自己的名称创建一个名称空间。以下所有资源都在此名称空间之下。 &lt;1&gt;编写namespace的yam文件 12345[root@master yaml]# vim namespace.yaml kind: NamespaceapiVersion: v1metadata: name: xgp-znb &lt;2&gt;执行一下 1[root@master yaml]# kubectl apply -f namespace.yaml &lt;3&gt;查看一下 1[root@master yaml]# kubectl get ns （2）设置rbac权限。 下载所需镜像 1docker pull registry.cn-hangzhou.aliyuncs.com/open-ali/nfs-client-provisioner &lt;1&gt;编写rbac的yam文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[root@master yaml]# vim rbac-rolebind.yamlkind: NamespaceapiVersion: v1metadata: name: xgp-znb---apiVersion: v1kind: ServiceAccountmetadata: name: nfs-provisioner namespace: xgp-znb---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: nfs-provisioner-runner namespace: xgp-znbrules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"watch\", \"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\", \"endpoints\"] verbs: [\"get\",\"create\",\"list\", \"watch\",\"update\"] - apiGroups: [\"extensions\"] resources: [\"podsecuritypolicies\"] resourceNames: [\"nfs-provisioner\"] verbs: [\"use\"]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: run-nfs-provisionersubjects: - kind: ServiceAccount name: nfs-provisioner namespace: xgp-znbroleRef: kind: ClusterRole name: nfs-provisioner-runner apiGroup: rbac.authorization.k8s.io &lt;2&gt;执行一下 1[root@master yaml]# kubectl apply -f rbac-rolebind.yaml （3）创建nfs-deployment.yaml &lt;1&gt;编写deployment的yam文件 1234567891011121314151617181920212223242526272829303132333435[root@master yaml]# vim nfs-deployment.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nfs-client-provisioner namespace: xgp-znbspec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-client-provisioner spec: serviceAccount: nfs-provisioner containers: - name: nfs-client-provisioner image: registry.cn-hangzhou.aliyuncs.com/open-ali/nfs-client-provisioner volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: xgp-znb - name: NFS_SERVER value: 192.168.1.21 - name: NFS_PATH value: /nfsdata volumes: - name: nfs-client-root nfs: server: 192.168.1.21 path: /nfsdata &lt;2&gt;执行一下 1[root@master yaml]# kubectl apply -f nfs-deployment.yaml （4）创建storageclass自动创建PV。 &lt;1&gt;编写storageclass的yam文件 1234567[root@master yaml]# vim storageclass.yamlapiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: test-scprovisioner: xgp-znb #通过provisioner字段关联到上述DeployreclaimPolicy: Retain &lt;2&gt;执行一下 1[root@master yaml]# kubectl apply -f storageclass.yaml （5）创建PVC &lt;1&gt;编写PVC的yaml文件 12345678910111213[root@master yaml]# vim pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: test-claim namespace: xgp-znbspec: storageClassName: test-sc accessModes: - ReadWriteMany resources: requests: storage: 500Mi &lt;2&gt;执行一下 1[root@master yaml]# kubectl apply -f pvc.yaml &lt;3&gt;查看一下 1[root@master yaml]# kubectl get pvc -n xgp-znb （6）创建一个Pod, 基于nginx运行一个web服务，使用Deployment资源对象，replicas=3.持久化存储目录为默认主目录 &lt;1&gt;编写deployment的yam文件 1234567891011121314151617181920212223242526[root@master yaml]# vim pod.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: web-pod namespace: xgp-znbspec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - image: nginx name: nginx volumeMounts: - name: web-test mountPath: /usr/share/nginx/html volumes: - name: web-test persistentVolumeClaim: claimName: test-claim &lt;2&gt;执行一下 1[root@master yaml]# kubectl apply -f pvc.yaml &lt;3&gt;查看一下 1[root@master yaml]# kubectl get pod -n xgp-znb （7）访问nginx页面 修改nginx主页 1234[root@master yaml]# kubectl exec -it web-pod-8cd956cc7-6szjb -n xgp-znb /bin/bash//进入容器之中root@web-pod-8cd956cc7-6szjb:/# echo xgp-znb &gt; /usr/share/nginx/html/index.html//添加自定义内容主机 访问一下 1[root@master yaml]# curl 10.244.2.18 四，五个可移植性建议 把你的 pvc，和 其它一系列配置放一起， 比如说deployment，configmap 不要把你的pv放在其它配置里， 因为用户可能没有权限创建pv 初始化pvc 模版的时候， 提供一个storageclass 在你的工具软件中，watch那些没有bound的pvc，并呈现给用户 集群启动的时候启用DefaultStorageClass， 但是不要指定某一类特定的class， 因为不同provisioner的class，参数很难一致 五，四个阶段(volumn phase) 1. 在PVC中绑定一个PV，可以根据下面几种条件组合选择 Access Modes， 按照访问模式选择pv Resources， 按照资源属性选择， 比如说请求存储大小为8个G的pv Selector， 按照pv的label选择 Class， 根据StorageClass的class名称选择, 通过annotation指定了Storage Class的名字, 来绑定特定类型的后端存储 2. 关于根据class过滤出pv的说明： 所有的 PVC 都可以在不使用 StorageClass 注解的情况下，直接使用某个动态存储。把一个StorageClass 对象标记为 “default” 就可以了。StorageClass 用注解http://storageclass.beta.kubernetes.io/is-default-class 就可以成为缺省存储。有了缺省的 StorageClass，用户创建 PVC 就不用 storage-class 的注解了，1.4 中新加入的DefaultStorageClass 准入控制器会自动把这个标注指向缺省存储类。PVC 指定特定storageClassName，如fast时， 绑定名称为fast的storageClassPVC中指定storageClassName为“”时， 绑定no class的pv（pv中无class annotation， 或者其值为“”）PVC不指定storageClassName时， DefaultStorageClass admission plugin 开启与否（在apiserver启动时可以指定）， 对default class的解析行为是不同的。当DefaultStorageClass admission plugin启用时， 针对没有storageClass annotation的pvc，DefaultStorageClass会分配一个默认的class， 这个默认的class需要用户指定，比如在创建storageclass对象时加入annotation,如 http://storageclass.beta.kubernetes.io/is-default-class: “true” 。如果有多个默认的class， 则pvc会被拒绝创建， 如果用户没有指定默认的class， 则这个DefaultStorageClass admission plugin不会起任何作用。 pvc会找那些no class的pv做绑定。当DefaultStorageClass admission plugin没有启用时， 针对没有storageClass annotation的pvc， 会绑定no class的pv（pv中无class annotation， 或者其值为“”）","path":"posts/15ab.html","date":"06-07","excerpt":"","tags":[{"name":"pv","slug":"pv","permalink":"https://wsdlxgp.top/tags/pv/"},{"name":"Storage Class","slug":"Storage-Class","permalink":"https://wsdlxgp.top/tags/Storage-Class/"}]},{"title":"34 k8s存储方式的介绍及应用 （持久化，mysql对数据持久化的应用）","text":"k8s存储: (持久化) docker容器是有生命周期的。 volume 1，存储类（Storage class）是k8s资源类型的一种，它是有管理员为管理PV更加方便创建的一个逻辑组，可以按照存储系统的性能高低，或者综合服务质量，备份策略等分类。不过k8s本身不知道类别到底是什么，它这是作为一个描述。 2，存储类的好处之一就是支持PV的动态创建，当用户用到持久性存储时，不必再去提前创建PV，而是直接创建PVC就可以了，非常的方便。 3，存储类对象的名称很重要，并且出了名称之外，还有3个关键字段 Provisioner（供给方）: 及提供了存储资源的存储系统。k8s内建有多重供给方，这些供给方的名字都以“kubernetes.io”为前缀。并且还可以自定义。 Parameters(参数)：存储类使用参数描述要关联到的存储卷，注意不同的供给方参数也不同。 reclaimPolicy:PV的回收策略，可用值有Delete(默认)和Retain 简介 1, 由于容器本身是非持久化的，因此需要解决在容器中运行应用程序遇到的一些问题。首先，当容器崩溃时，kubelet将重新启动容器，但是写入容器的文件将会丢失，容器将会以镜像的初始状态重新开始；第二，在通过一个Pod中一起运行的容器，通常需要共享容器之间一些文件。Kubernetes通过存储卷解决上述的两个问题。 2, 在Docker有存储卷的概念卷，但Docker中存储卷只是磁盘的或另一个容器中的目录，并没有对其生命周期进行管理。Kubernetes的存储卷有自己的生命周期，它的生命周期与使用的它Pod生命周期一致。因此，相比于在Pod中运行的容器来说，存储卷的存在时间会比的其中的任何容器都长，并且在容器重新启动时会保留数据。当然，当Pod停止存在时，存储卷也将不再存在。在Kubernetes支持多种类型的卷，而Pod可以同时使用各种类型和任意数量的存储卷。在Pod中通过指定下面的字段来使用存储卷： spec.volumes：通过此字段提供指定的存储卷 spec.containers.volumeMounts：通过此字段将存储卷挂接到容器中 环境介绍 主机 IP地址 服务 master 192.168.1.21 k8s node01 192.168.1.22 k8s node02 192.168.1.23 k8s 1.emptyDir（空目录）:类似docker 数据持久化的:docer manager volume 使用场景:在同一 个Pod里，不同的容器，共享数据卷。 如果容器被删除，数据仍然存在，如果Pod被 删除，数据也会被删除。 &lt;1&gt; 介绍 一个emptyDir 第一次创建是在一个pod被指定到具体node的时候，并且会一直存在在pod的生命周期当中，正如它的名字一样，它初始化是一个空的目录，pod中的容器都可以读写这个目录，这个目录可以被挂在到各个容器相同或者不相同的的路径下。当一个pod因为任何原因被移除的时候，这些数据会被永久删除。注意：一个容器崩溃了不会导致数据的丢失，因为容器的崩溃并不移除pod. emptyDir的使用场景如下： 空白的初始空间，例如合并/排序算法中，临时将数据保存在磁盘上。 长时间计算中存储检查点（中间结果），以便容器崩溃时，可以从上一次存储的检查点（中间结果）继续进行，而不是从头开始。 作为两个容器的共享存储，使得第一个内容管理的容器可以将生成的数据存入其中，同时由一个webserver容器对外提供这些页面。 默认情况下，emptyDir数据卷存储在node节点的存储介质（机械硬盘、SSD或网络存储）上。 &lt;2&gt;emptyDir 磁盘的作用： （1）普通空间，基于磁盘的数据存储 （2）作为从崩溃中恢复的备份点 （3）存储那些那些需要长久保存的数据，例web服务中的数据 默认的，emptyDir 磁盘会存储在主机所使用的媒介上，可能是SSD，或者网络硬盘，这主要取决于你的环境。当然，我们也可以将emptyDir.medium的值设置为Memory来告诉Kubernetes 来挂在一个基于内存的目录tmpfs，因为 tmpfs速度会比硬盘块度了，但是，当主机重启的时候所有的数据都会丢失。 测试编写一个yaml文件 12345678910111213141516171819202122232425262728[root@master yaml]# vim emptyDir.yamlapiVersion: v1kind: Podmetadata: name: producer-consumerspec: containers: - image: busybox name: producer volumeMounts: - mountPath: /producer_dir name: shared-volume args: - /bin/sh - -c - echo \"hello k8s\" &gt; /producer_dir/hello; sleep 30000 - image: busybox name: consumer volumeMounts: - mountPath: /consumer_dir name: shared-volume args: - /bin/sh - -c - cat /consumer_dir/hello; sleep 30000 volumes: - name: shared-volume emptyDir: &#123;&#125; 执行一下 1[root@master yaml]# kubectl apply -f emptyDir.yaml 查看一下 1[root@master yaml]# kubectl get pod 查看日志 12[root@master yaml]# kubectl logs producer-consumer producer[root@master yaml]# kubectl logs producer-consumer consumer 查看挂载的目录 node节点查看容器名，并通过容器名查看挂载的目录 1[root@node01 shared-volume]# docker ps 1[root@node01 shared-volume]# docker inspect k8s_consumer_producer-consumer_default_9ec83f9e-e58b-4bf8-8e16-85b0f83febf9_0 进入挂载目录查看一下 2.hostPath Volume:类似docker 数据持久化的:bind mount 如果Pod被删除，数据会保留，相比较emptyDir要好一点。不过一旦host崩溃，hostPath也无法访问 了。 docker或者k8s集群本身的存储会采用hostPath这种方式。 &lt;1&gt; 介绍 hostPath宿主机路径，就是把pod所在的宿主机之上的脱离pod中的容器名称空间的之外的宿主机的文件系统的某一目录和pod建立关联关系，在pod删除时，存储数据不会丢失。 &lt;2&gt; 作用 如果Pod被删除，数据会保留，相比较emptyDir要好一点。不过一旦host崩溃，hostPath也无法访问 了。 docker或者k8s集群本身的存储会采用hostPath这种方式。 适用场景如下： 某容器需要访问 Docker，可使用 hostPath 挂载宿主节点的 /var/lib/docker 在容器中运行 cAdvisor，使用 hostPath 挂载宿主节点的 /sys 3.Persistent Volume| PV(持久卷) 提前做好的，数据持久化的数据存放目录。 Psesistent Volume Claim| PVC( 持久卷使用声明|申请) Psesistent Volume Claim| PVC( 持久卷使用声明|申请) PersistentVolume（PV）是集群中已由管理员配置的一段网络存储。 集群中的资源就像一个节点是一个集群资源。 PV是诸如卷之类的卷插件，但是具有独立于使用PV的任何单个pod的生命周期。 该API对象捕获存储的实现细节，即NFS，iSCSI或云提供商特定的存储系统。 PVC和PV的概念 我们前面提到kubernetes提供那么多存储接口，但是首先kubernetes的各个Node节点能管理这些存储，但是各种存储参数也需要专业的存储工程师才能了解，由此我们的kubernetes管理变的更加复杂的。由此kubernetes提出了PV和PVC的概念，这样开发人员和使用者就不需要关注后端存储是什么，使用什么参数等问题。如下图： PersistentVolume（PV）是集群中已由管理员配置的一段网络存储。 集群中的资源就像一个节点是一个集群资源。 PV是诸如卷之类的卷插件，但是具有独立于使用PV的任何单个pod的生命周期。 该API对象捕获存储的实现细节，即NFS，iSCSI或云提供商特定的存储系统。 PersistentVolumeClaim（PVC）是用户存储的请求。PVC的使用逻辑：在pod中定义一个存储卷（该存储卷类型为PVC），定义的时候直接指定大小，pvc必须与对应的pv建立关系，pvc会根据定义去pv申请，而pv是由存储空间创建出来的。pv和pvc是kubernetes抽象出来的一种存储资源。 虽然PersistentVolumeClaims允许用户使用抽象存储资源，但是常见的需求是，用户需要根据不同的需求去创建PV，用于不同的场景。而此时需要集群管理员提供不同需求的PV，而不仅仅是PV的大小和访问模式，但又不需要用户了解这些卷的实现细节。 对于这样的需求，此时可以采用StorageClass资源。这个在前面就已经提到过此方案。 PV是集群中的资源。 PVC是对这些资源的请求，也是对资源的索赔检查。 PV和PVC之间的相互作用遵循这个生命周期： 1Provisioning（配置）---&gt; Binding（绑定）---&gt;Using（使用）---&gt; Releasing（释放） ---&gt; Recycling（回收） （1）基于nfs服务来做的PV和pvc nfs使的我们可以挂在已经存在的共享到的我们的Pod中，和emptyDir不同的是，emptyDir会被删除当我们的Pod被删除的时候，但是nfs不会被删除，仅仅是解除挂在状态而已，这就意味着NFS能够允许我们提前对数据进行处理，而且这些数据可以在Pod之间相互传递.并且，nfs可以同时被多个pod挂在并进行读写 注意：必须先保证NFS服务器正常运行在我们进行挂在nfs的时候 下载nfs所需安装包 1[root@node02 ~]# yum -y install nfs-utils rpcbind 创建共享目录 1[root@master ~]# mkdir /nfsdata 创建共享目录的权限 12[root@master ~]# vim /etc/exports/nfsdata *(rw,sync,no_root_squash) 开启nfs和rpcbind 12[root@master ~]# systemctl start nfs-server.service [root@master ~]# systemctl start rpcbind 测试一下 1[root@master ~]# showmount -e &lt;1&gt;创建nfs-pv的yaml文件 12345678910111213141516[root@master yaml]# cd yaml/[root@master yaml]# vim nfs-pv.yamlapiVersion: v1kind: PersistentVolumemetadata: name: test-pvspec: capacity: #pv容量的大小 storage: 1Gi accessModes: #访问pv的模式 - ReadWriteOnce #能以读-写mount到单个的节点 persistentVolumeReclaimPolicy: Recycle storageClassName: nfs nfs: path: /nfsdata/pv1 server: 192.168.1.21 1234 accessModes:(PV支持的访问模式) - ReadWriteOnce: 能以读-写mount到单个的节点 - ReadWriteMany: 能以读-写mount到多个的节点。- ReadOnlyMnce: 能以只读的方式mount到多个节点。 1234persistentVolumeReclaimPolicy : (PV存储空间的回收策略是什么)trueRecycle: 自动清除数据。trueRetain: 需要管理员手动回收。trueDelete: 云存储专用。 &lt;2&gt;执行一下 1[root@master yaml]# kubectl apply -f nfs-pv.yaml &lt;3&gt;查看一下 1[root@master yaml]# kubectl get pv &lt;1&gt;创建nfs-pvc的yaml文件 PersistentVolumeClaim（PVC）是用户存储的请求。PVC的使用逻辑：在pod中定义一个存储卷（该存储卷类型为PVC），定义的时候直接指定大小，pvc必须与对应的pv建立关系，pvc会根据定义去pv申请，而pv是由存储空间创建出来的。pv和pvc是kubernetes抽象出来的一种存储资源。 12345678910111213[root@master yaml]# vim nfs-pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: test-pvcspec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: nfs &lt;2&gt;执行一下 1[root@master yaml]# kubectl apply -f nfs-pvc.yaml &lt;3&gt;查看一下 1[root@master yaml]# kubectl get pvc 1[root@master yaml]# kubectl get pv （2）创建一个pod资源 1234567891011121314151617181920[root@master yaml]# vim pod.yamlkind: PodapiVersion: v1metadata: name: test-podspec: containers: - name: pod1 image: busybox args: - /bin/sh - -c - sleep 30000 volumeMounts: - mountPath: \"/mydata\" name: mydata volumes: - name: mydata persistentVolumeClaim: claimName: test-pvc &lt;1&gt; 执行一下 1[root@master yaml]# kubectl apply -f pod.yaml &lt;2&gt;查看一下 1[root@master yaml]# kubectl get pod -o wide 可以看到现在没有开启成功 查看一下test-pod的信息看看是哪里的问题 1[root@master yaml]# kubectl describe pod test-pod 那是因为pv的本地挂载目录没有创建好 12[root@master yaml]# mkdir /nfsdata/pv1///要和nfs-pv.yaml的名字一样 重新创建一下pod 123[root@master yaml]# kubectl delete -f pod.yaml [root@master yaml]# kubectl apply -f pod.yaml [root@master yaml]# kubectl get pod -o wide （3）test-pod创建hello创建文件并添加内容 1[root@master yaml]# kubectl exec test-pod touch /mydata/hello 进入容器 123[root@master yaml]# kubectl exec -it test-pod /bin/sh/ # echo 123 &gt; /mydata/hello/ # exit 挂载目录查看一下 1[root@master yaml]# cat /nfsdata/pv1/hello 和刚刚的一样 （4）测试回收策略 删除pod和pvc，pv 123[root@master yaml]# kubectl delete pod test-pod [root@master yaml]# kubectl delete pvc test-pvc [root@master yaml]# kubectl delete pv test-pv 查看一下 1[root@master yaml]# kubectl get pv 1[root@master yaml]# cat /nfsdata/pv1/hello 文件已被回收 （5）修改pv的回收策略为手动 修改 123456789101112131415[root@master yaml]# vim nfs-pv.yaml apiVersion: v1kind: PersistentVolumemetadata: name: test-pvspec : capacity : storage: 1Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain #修改 storageClassName: nfs nfs: path: /nfsdata/pv1 server: 192.168.1.21 执行一下 1[root@master yaml]# kubectl apply -f nfs-pv.yaml 创建pod 1[root@master yaml]# kubectl apply -f pod.yaml 查看一下 1[root@master yaml]# kubectl describe pod test-pod 创建pvc 1[root@master yaml]# kubectl apply -f nfs-pvc.yaml 查看一下pod 1[root@master yaml]# kubectl get pod （6）test-pod创建hello创建文件并添加内容 1[root@master yaml]# kubectl exec test-pod touch /mydata/k8s 查看一下挂载目录 1[root@master yaml]# ls /nfsdata/pv1/ 删除pod和pvc，pv，再次查看挂载目录 123[root@master yaml]# kubectl delete pod test-pod [root@master yaml]# kubectl delete pvc test-pvc[root@master yaml]# kubectl delete pv test-pv 查看挂载目录 1[root@master yaml]# ls /nfsdata/pv1/ 内容还在 4.mysql对数据持久化的应用 下面演示如何为 MySQL 数据库提供持久化存储，步骤为： 创建 PV 和 PVC。 部署 MySQL。 向 MySQL 添加数据。 模拟节点宕机故障，Kubernetes 将 MySQL 自动迁移到其他节点。 验证数据一致性。 最小化安装系统需要 1yum -y install mariadb （1）通过之前的yaml文件，创建pv和pvc 12[root@master yaml]# kubectl apply -f nfs-pv.yaml [root@master yaml]# kubectl apply -f nfs-pvc.yaml 查看一下 1[root@master yaml]# kubectl get pv 1[root@master yaml]# kubectl get pvc （2）编写一个mysql的yaml文件 123456789101112131415161718192021222324252627282930313233343536[root@master yaml]# vim mysql.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: test-mysqlspec: selector: matchLabels: #支持等值的标签 app: mysqlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: test-mysqlspec: selector: matchLabels: app: mysql template: metadata: labels: app: mysql spec: containers: - image: mysql:5.6 name: mysql env: - name: MYSQL_ROOT_PASSWORD value: 123.com volumeMounts: - name: mysql-storage mountPath: /var/lib/mysql volumes: - name: mysql-storage persistentVolumeClaim: claimName: test-pvc 执行一下 1[root@master yaml]# kubectl apply -f mysql.yaml 查看一下 1[root@master yaml]# kubectl get pod （3）进入mysql容器 ① 切换到数据库 mysql。 ② 创建数据库表 my_id。 ③ 插入一条数据。 ④ 确认数据已经写入。 关闭 k8s-node2，模拟节点宕机故障。 1[root@master yaml]# kubectl exec -it test-mysql-569f8df4db-rkpwm -- mysql -u root -p123.com 创建数据库 1mysql&gt; create database yun33; 切换数据库 1mysql&gt; use yun33; 创建表 1mysql&gt; create table my_id( id int(4))； 在表中插入数据 1mysql&gt; insert my_id values(9527); 查看表 1mysql&gt; select * from my_id; （4）查看本地的挂载目录 1[root@master yaml]# ls /nfsdata/pv1/ 查看一下pod 1[root@master yaml]# kubectl get pod -o wide -w 挂起node01 （5）查看node02上面数据是否和刚才一样（验证数据的一致性） 进入数据库 1[root@master yaml]# kubectl exec -it test-mysql-569f8df4db-nsdnz -- mysql -u root -p123.com 查看数据库 1mysql&gt; show databases; 查看表 1mysql&gt; show tables; 1mysql&gt; select * from my_id; 可以看到数据还在 5. 排错方法 kubectl describe //查看详细信息，找出问题 kubectl logs //查看日志，找出问题 /var/ log/messages //查看该节点的kubelet的日志。 5. 总结 本章我们讨论了 Kubernetes 如何管理存储资源。 emptyDir 和 hostPath 类型的 Volume 很方便，但可持久性不强，Kubernetes 支持多种外部存储系统的 Volume。 PV 和 PVC 分离了管理员和普通用户的职责，更适合生产环境。我们还学习了如何通过 StorageClass 实现更高效的动态供给。 最后，我们演示了如何在 MySQL 中使用 PersistentVolume 实现数据持久性。 PV的访问控制类型 accessModes:(PV支持的访问模式) ReadWriteOnce: 能以读-写mount到单个的节点 ReadWriteMany: 能以读-写mount到多个的节点。 ReadOnlyOnce: 能以只读的方式mount到单个节点。 PV的空间回收策略 persistentVolumeReclaimPolicy : (PV存储空间的回收策略是什么) Recycle: 自动清除数据。 Retain: 需要管理员手动回收。 Delete: 云存储专用。 PV和PVC相互关联 是通过accessModes和storageClassName模块关联的 Pod不断的重启: 1、swap,没有关闭，导致集群运行不正常。 2、内存不足，运行服务也会重后。 kubectl describe kubectl logs /var/ log/messages 查看该节点的kubelet的日志。","path":"posts/ba49.html","date":"06-07","excerpt":"","tags":[{"name":"pv","slug":"pv","permalink":"https://wsdlxgp.top/tags/pv/"},{"name":"pvc","slug":"pvc","permalink":"https://wsdlxgp.top/tags/pvc/"},{"name":"emptyDir","slug":"emptyDir","permalink":"https://wsdlxgp.top/tags/emptyDir/"}]},{"title":"33 k8s复习","text":"虚拟化 云计算的分类: 基础及服务: laas 平台及服务: paas 软件及服务: saas **docker虚拟化的底层原理: ** Namespace + Cgroup **Namespace六项隔离: ** IPC: 共享内存,消息列队 MNT: 挂载点 文件系统 NET: 网络栈 PID: 进程编号 USER: 用户 组 UTS: 主机名 域名 namespace 六项隔离 实现了容器与宿主机 容器与容器之间的隔离 **Cgroup 四项作用: ** **1） 资源的限制: **cgroup可以对进程组使用的资源总额进行限制 **2） 优先级分配: **通过分配的cpu时间片数量以及硬盘IO带宽的大小，实际上相当于控制了进程运行的优先级别 **3） 资源统计: ** group可以统计系统资源使用量，比如gpu使用时间，内存使用量等，用于按量计费。同时还支持挂起动能，也就是说通过cgroup把所有 资源限制起来,对资源都不能使用，注意着并不是说我们的程序不能使用了,知识不能使用资源，处于等待状态。 **4） 进程控制: **可以对进程组执行挂起、恢复等操作。 镜像是容器运行的核心，容器是镜像运行的后的实例。 DockerHub| registry ----&gt; pull image : save &gt; | load &lt; run ----&gt; Container ----&gt; commit* Dockerfile Docker 三剑客。 docker machine: 自动化部署多台dockerHost 。 Docker-compose: 它可以同时控制多个容器。 yaml。 **Docker Swarm: ** 从单个的服务向集群的形势发展。 高可用、高性能、高并发 : 为了防止单点故障。 Service: 服务 ----&gt; 包括运行什么服务，需要多个 rep1icas（副本）, 外网如何访问。 k8s 关闭防火墙、禁用selinux、修改主机名并加入域名解析、关闭swap 、时间同步、免密登录、打开iptables桥接 对硬件的基本要求: CPU: 2核 MEM: 2G 主机名: master node01 node02 时间必须同步 kubctl: k8s客户端 kubeadm: 工具 kubelet: 客户端代理 **组件: ** 三层网络: DockerHost &gt; Pod &gt; Service **Deployment: Service: ** **master组件: ** kube- api( application interface) k8s的前端接口 **Scheduler[集群分发调度器]**负责决定将Pod放在哪个Node上运行。在调度时，会充分考虑集群的拓扑结构，当前各个节点的负载情况，以及应对高可用、性能、数据亲和性和需求。 Controller Manager[内部管理控制中心]: 负责管理集群的各种资源，保证资源处于预期的状态。它由多种Controller组成，包括Replication Controller、Endpoints Controller、Namespace Controller、Serviceaccounts Controller等。 **Etcd: **负责保存k8s集群的配置信息和各种资源的状态信息。当数据发生变化时，etcd会快速的通知k8s相关组件。[（第三方组件）它有可替换方案。Consul、zookeeper](https: //wsdlxgp.top/posts/1b18.html) **Flanner: **是k8s集群网络，可以保证Pod的跨主机通信。也有替换方案。 Node组件: Kubelet[节点上的Pod管家]: 它是Node的agent(代理)，当Scheduler确定某 个Node上运行Pod之后，会将Pod的具体配置信息发送给该节点的kubelet,kubelet会根据这些信息创建和运行容器，并向Master报告运行状态。 **kube-proxy[负载均衡、路由转发]: **负责将访问service的TCP/UDP数据流转发到后端的容器。如果有多个副本，kube-proxy会实现负载均衡。 yaml文件的一级字段: **VERSION: ** ​ **KIND: ** ​ **METADATA: ** ​ **SPEC : ** 12345678910111213141516[root@master ~]# vim web.yamlkind: Deployment #资源对象是控制器apiVersion: extensions/v1beta1 #api的版本metadata: #描述kind（资源类型） name: web #定义控制器名称 namespace: #名称空间spec: replicas: 2 #副本数量 template: #模板 metadata: labels: #标签 app: web_server spec: containers: #指定容器 - name: nginx #容器名称 image: nginx #使用的镜像 **Deployment（控制器): ** **ReplicationController: **用来确保由其管控的Pod对象副本数量，能够满足用户期望，多则删除，少则通过模本创建 **RS（RpelicaSet）: **RS也是用于保证与label selector匹配的pod数量维持在期望状态 **Service: ** type: 默认Cluster IP NodePort: 30000-32767 Deployment和Service关联: 标签和标签选择器 **Namespace: ** Pod: 最小单位 **镜像的下载策略: ** **Always: **镜像标签为“laster”或镜像不存在时，总是从指定的仓库中获取镜像。 **IfNotPresent: **仅当本地镜像不存在时才从目标仓库下载。 **Never: **禁止从仓库中下载镜像，即只使用本地镜像。 默认的标签 为latest: always **Pod的重启策略: ** **Always: **（默认情况下使用）但凡Pod对象终止就将其重启； ​ **OnFailure: **仅在Pod对象出现错误时才将其重启； ​ **Never: **从不重启； **Pod的健康检查: ** ​ Liveness: 探测失败重启pod ​ Readiness: 探测失败将pod设置为不可用 kubelet: 控制pod DaemonSet : 会在每一个节点都会运行，并且只运行一个Pod","path":"posts/fehv.html","date":"06-07","excerpt":"","tags":[{"name":"swarm","slug":"swarm","permalink":"https://wsdlxgp.top/tags/swarm/"}]},{"title":"32 k8s的Job/CronJob资源对象及添加api版本","text":"Job资源对象 **服务类的Pod容器：**RC、RS、DS、Deployment **工作类的Pod容器：**Job—&gt;执行一次，或者批量执行处理程序，完成之后退出容器。 注意： 如果容器内执行任务有误，会根据容器的重启策略操作容器，不过这里 的容器重启策略只能是: Never和 OnFailure。 概念 在有些场景下，是想要运行一些容器执行某种特定的任务，任务一旦执行完成，容器也就没有存在的必要了。在这种场景下，创建pod就显得不那么合适。于是就是了Job，Job指的就是那些一次性任务。通过Job运行一个容器，当其任务执行完以后，就自动退出，集群也不再重新将其唤醒。 从程序的运行形态上来区分，可以将Pod分为两类：长时运行服务（jboss、mysql等）和一次性任务（数据计算、测试）。RC创建的Pod都是长时运行的服务，Job多用于执行一次性任务、批处理工作等，执行完成后便会停止（status.phase变为Succeeded）。 环境介绍 主机 IP地址 服务 master 192.168.1.21 k8s node01 192.168.1.22 k8s node02 192.168.1.23 k8s 基于 https://blog.51cto.com/14320361/2464655 的实验继续进行 一、kubernetes支持以下几种job 非并行job：通常创建一个pod直至其成功结束。 固定结束次数的job：设置spec.completions,创建多个pod，直到.spec.completions个pod成功结束。 带有工作队列的并行job：设置.spec.Parallelism但不设置.spec.completions,当所有pod结束并且至少一个成功时，job就认为是成功。 Job Controller Job Controller负责根据Job Spec创建pod，并持续监控pod的状态，直至其成功结束，如果失败，则根据restartPolicy（只支持OnFailure和Never，不支持Always）决定是否创建新的pod再次重试任务。 例子 （1）编写一个job的yaml文件 123456789101112131415[root@master yaml]# vim jop.yamlkind: JobapiVersion: batch/v1metadata: name: test-jobspec: template: metadata: name: test-job spec: containers: - name: hello image: busybox command: [\"echo\",\"hello k8s job!\"] restartPolicy: Never （2）执行一下 1[root@master yaml]# kubectl apply -f jop.yaml （3）查看一下 1[root@master yaml]# kubectl get pod 查看日志 1[root@master yaml]# kubectl logs test-job-gs45w 我们可以看到job与其他资源对象不同，仅执行一次性任务，默认pod借宿运行后job即结束，状态为Completed。 （4）修改一下jop的yaml文件，把echo命令换成乱码 123456789101112131415[root@master yaml]# vim jop.yamlkind: JobapiVersion: batch/v1metadata: name: test-jobspec: template: metadata: name: test-job spec: containers: - name: hello image: busybox command: [\"asdasxsddwefew\",\"hello k8s job!\"] #修改 restartPolicy: Never （5）先删除之前的pod 1[root@master yaml]# kubectl delete jobs.batch test-job （6）执行一下 1[root@master yaml]# kubectl apply -f jop.yaml （7）查看一下 1[root@master yaml]# kubectl get pod -w 它会一直创建pod直到完成命令。 （8）修改一下jop的yaml文件，修改重启策略 123456789101112131415[root@master yaml]# vim jop.yaml kind: JobapiVersion: batch/v1metadata: name: test-jobspec: template: metadata: name: test-job spec: containers: - name: hello image: busybox command: [\"asdasxsddwefew\",\"hello k8s job!\"] restartPolicy: OnFailure （9）先删除之前的pod 1[root@master yaml]# kubectl delete jobs.batch test-job （10）执行一下 1[root@master yaml]# kubectl apply -f jop.yaml （11）查看一下 1[root@master yaml]# kubectl get pod -w 它会一直重启pod完成命令，直到重启到一定次数就会删除job。 二、提高Job的执行效率 1. 我们可以在Job.spec字段下加上parallelism选项。表示同时运行多少个Pod执行任务。 （1）编写一个job的yaml文件 12345678910111213141516[root@master yaml]# vim jop.yamlkind: JobapiVersion: batch/v1metadata: name: test-jobspec: parallelism: 2 #同时启用几个pod template: metadata: name: test-job spec: containers: - name: hello image: busybox command: [\"echo\",\"hello k8s job!\"] restartPolicy: OnFailure （3）执行一下 1[root@master yaml]# kubectl apply -f jop.yaml （4）查看一下 1[root@master yaml]# kubectl get pod 查看日志 2. 我们可以在Job.spec字段下加上complations选项。表示总共需要完成Pod的数量 （1）编写一个job的yaml文件 1234567891011121314151617[root@master yaml]# vim jop.yamlkind: JobapiVersion: batch/v1metadata: name: test-jobspec: complations: 8 #运行pod的总数量8个 parallelism: 2 #同时运行2个pod template: metadata: name: test-job spec: containers: - name: hello image: busybox command: [\"echo\",\"hello k8s job!\"] restartPolicy: OnFailure job 字段解释： 标志Job结束需要成功运行的Pod个数，默认为1 parallelism：标志并行运行的Pod的个数，默认为1 activeDeadlineSeconds：标志失败Pod的重试最大时间，超过这个时间不会继续重试. （2）先删除之前的pod 1[root@master yaml]# kubectl delete jobs.batch test-job （3）执行一下 1[root@master yaml]# kubectl apply -f jop.yaml （4）查看一下 1[root@master yaml]# kubectl get pod 可以看到pod是两个两个的启动的。 3. 如何定时执行Job （1）编写一个cronjob的yaml文件 12345678910111213141516[root@master yaml]# vim cronjop.yamlkind: CronJobapiVersion: batch/v1beta1metadata: name: hellospec: schedule: \"*/1 * * * *\" #限定时间 jobTemplate: spec: template: spec: containers: - name: hello image: busybox command: [\"echo\",\"hello\",\"cronjob\"] restartPolicy: OnFailure （2）先删除之前的pod 1[root@master yaml]# kubectl delete jobs.batch test-job （3）执行一下 1[root@master yaml]# kubectl apply -f jop.yaml （4）查看一下 1[root@master yaml]# kubectl get pod 1[root@master yaml]# kubectl get cronjobs.batch 此时查看Pod的状态，会发现，每分钟都会运行一个新的Pod来执行命令规定的任 务。 练习：规定2020.1.15.10.5分运行上面的crontab任务。 （1）编写一个cronjob的yaml文件 12345678910111213141516[root@master yaml]# vim cronjop.yamlkind: CronJobapiVersion: batch/v1beta1metadata: name: hellospec: schedule: \"5 10 15 1 *\" #限定时间 jobTemplate: spec: template: spec: containers: - name: hello image: busybox command: [\"echo\",\"hello\",\"cronjob\"] restartPolicy: OnFailure （2）先删除之前的pod 1[root@master yaml]# kubectl delete cronjobs.batch hello （3）执行一下 1[root@master yaml]# kubectl apply -f jop.yaml （4）查看一下 1[root@master yaml]# kubectl get pod 这时会发现，如果规定具体时间，可能并不会执行任务。 （5）添加apiVersion库 123456[root@master yaml]# vim /etc/kubernetes/manifests/kube-apiserver.yaml spec: containers: - command: - kube-apiserver - --runtime-config=batch/v2alpha1=true #添加 （6）重启kubelet 1[root@master yaml]# systemctl restart kubelet.service （7）查看api版本 1[root@master yaml]# kubectl api-versions （8）编写一个cronjob的yaml文件 12345678910111213141516[root@master yaml]# vim cronjop.yamlkind: CronJobapiVersion: batch/v1beta1metadata: name: hellospec: schedule: \"47 10 15 1 *\" #限定时间 jobTemplate: spec: template: spec: containers: - name: hello image: busybox command: [\"echo\",\"hello\",\"cronjob\"] restartPolicy: OnFailure （9）执行一下 1[root@master yaml]# kubectl apply -f jop.yaml （4）查看一下 1[root@master yaml]# kubectl get pod -w 注意：此时仍然不能正常运行指定时间的Job，这是因为K8s官方在cronjob这个资源对象的支持中还没有完善此功能，还待开发。 跟Job资源一样在cronjob.spec.jobTemplate.spec 下同样支持并发Job参数: parallelism，也支持完成Pod的总数参数: completionsr 总结 Job 作为 Kubernetes 中用于处理任务的资源，与其他的资源没有太多的区别，它也使用 Kubernetes 中常见的控制器模式，监听 Informer 中的事件并运行 syncHandler 同步任务 而 CronJob 由于其功能的特殊性，每隔 10s 会从 apiserver 中取出资源并进行检查是否应该触发调度创建新的资源，需要注意的是 CronJob 并不能保证在准确的目标时间执行，执行会有一定程度的滞后。 两个控制器的实现都比较清晰，只是边界条件比较多，分析其实现原理时一定要多注意。","path":"posts/fbf7.html","date":"06-07","excerpt":"","tags":[{"name":"Job","slug":"Job","permalink":"https://wsdlxgp.top/tags/Job/"},{"name":"apiVersion","slug":"apiVersion","permalink":"https://wsdlxgp.top/tags/apiVersion/"},{"name":"CronJob","slug":"CronJob","permalink":"https://wsdlxgp.top/tags/CronJob/"}]},{"title":"31 k8s的ReplicaSet，DaemonSet及标签","text":"环境介绍 主机 IP地址 服务 master 192.168.1.21 k8s node01 192.168.1.22 k8s node02 192.168.1.23 k8s 基于 https://blog.51cto.com/14320361/2464655 的实验继续进行 ReplicaSet简单介绍 1. RC：ReplicationController（老一代的pod控制器） 用来确保由其管控的Pod对象副本数量，能够满足用户期望，多则删除，少则通过模本创建 特点： 确保Pod资源对象的数量精准。 确保pod健康运行。 弹性伸缩 同样，它也可以通过yaml或json格式的资源清单来创建。其中spec字段一般嵌套以下字段： replicas：期望的Pod对象副本数量。 selector：当前控制器匹配Pod对此项副本的标签选择器 template：pod副本的模板 与RC相比而言，RS不仅支持*基于等值*的标签选择器，而且还支持*基于集合*的标签选择器。 2. 标签：解决同类型的资源对象，为了更好的管理，按照标签分组。 常用的标签分类： release（版本）：stable（稳定版）、canary（金丝雀版本）、beta（测试版本） environment（环境变量）：dev（开发）、qa（测试）、production（生产） application（应用）：ui、as（application software应用软件）、pc、sc tier（架构层级）：frontend（前端）、backend（后端）、cache（缓存） partition（分区）：customerA（客户A）、customerB（客户B） track（品控级别）：daily（每天）、weekly（每周） 标签要做到：见名知意。 3.测试 （1）编写一个pod的yaml文件 12345678910111213[root@master ~]# vim label.yaml kind: PodapiVersion: v1metadata: name: labels labels: env: qa tier: frontendspec: containers: - name: myapp image: httpd &lt;1&gt;执行一下 1[root@master ~]# kubectl apply -f label.yaml --record &lt;2&gt;查看一下 12[root@master ~]# kubectl get pod --show-labels //通过--show-labels显示资源对象的 12[root@master ~]# kubectl get po -L env,tier//显示某个键对应的值 12[root@master ~]# kubectl get po -l env,tier//通过-l 查看仅包含某个标签的资源。 （2）添加标签 12[root@master ~]# kubectl label pod labels app=pc//给pod资源添加标签 （3）修改标签 1234[root@master ~]# kubectl label pod labels env=dev --overwrite//修改标签[root@master ~]# kubectl get pod -l tier --show-labels //查看标签 （4）编写一个service的yaml文件 1234567891011121314[root@master ~]# vim service.yamlkind: ServiceapiVersion: v1metadata: name: servicespec: type: NodePort selector: env: qa ports: - protocol: TCP port: 90 targetPort: 80 nodePort: 30123 &lt;1&gt;执行一下 1[root@master ~]# kubectl apply -f service.yaml &lt;2&gt;查看一下 1[root@master ~]# kubectl describe svc &lt;3&gt;访问一下 1[root@master ~]# curl 127.0.0.1:30123 如果标签有多个，标签选择器选择其中一个，也可以关联成功。相反，如果选择器有多个，那么标签必须完全满足条件，才可以关联成功。 4. 标签选择器：标签的查询过滤条件。 基于等值关系的（equality-based）：“=”，“==”，“！ =”前面两个都是相等，最后一个是不等于。 基于集合关系（set-based）:in、notin、exists三种。选择器列表间为“逻辑与”关系，使用ln或者NotIn操作时，其valuas不强制要求为非空的字符串列表，而使用Exists或DostNotExist时，其values必须为空 使用标签选择器的逻辑： 同时指定的多个选择器之间的逻辑关系为“与”操作。 使用空值的标签选择器意味着每个资源对象都将把选中。 空的标签选择器无法选中任何资源。 （1）例子 编写一个selector的yaml’文件 1234567[root@master ~]# vim selector.yamlselector: matchLabels: app: nginx mathExpressions: - &#123;key: name,operator: In,values: [zhangsan,lisi]&#125; - &#123;key: age,operator: Exists,values:&#125; selector：当前控制器匹配Pod对此项副本的标签选择器 matchLabels: 指定键值对表示的标签选择器。 mathExpressions:：基于表达式来指定的标签选择器。 DaemonSet 它也是一种pod控制器。 RC，RS , deployment , daemonset.都是pod控制器。statfukSet，RBAC 1. 使用场景： 如果必须将pod运行在固定的某个或某几个节点，且要优先于其他的pod的启动。通常情况下，默认会将每一个节点都运行，并且只能运行一个pod。这种情况推荐使用DeamonSet资源对象。 监控程序； 日志收集程序； 集群存储程序； 12[root@master ~]# kubectl get ds -n kube-system //查看一下DaemonSet 2. DaemonSet 与 Deployment 的区别 Deployment 部署的副本 Pod 会分布在各个 Node 上，每个 Node 都可能运行好几个副本。 DaemonSet 的不同之处在于：每个 Node 上最多只能运行一个副本。 3. 运行一个web服务，在每一个节点运行一个pod。 123456789101112131415[root@master ~]# vim daemonset.yamlkind: DaemonSetapiVersion: extensions/v1beta1metadata: name: test-dsspec: template: metadata: labels: name: test-ds spec: containers: - name: test-ds image: httpd &lt;1&gt;执行一下 1[root@master ~]# kubectl apply -f daemonset.yaml &lt;2&gt;查看一下 1[root@master ~]# kubectl get ds 总结 1）总结RC、RS、Deplyment、DaemonSet控制器的特点及使用场景。 &lt;1&gt;Replication Controller（RC） 介绍及使用场景 Replication Controller简称RC，RC是Kubernetes系统中的核心概念之一，简单来说，RC可以保证在任意时间运行Pod的副本数量，能够保证Pod总是可用的。如果实际Pod数量比指定的多那就结束掉多余的，如果实际数量比指定的少就新启动一些Pod，当Pod失败、被删除或者挂掉后，RC都会去自动创建新的Pod来保证副本数量，所以即使只有一个Pod，我们也应该使用RC来管理我们的Pod。 主要功能 确保pod数量：RC用来管理正常运行Pod数量，一个RC可以由一个或多个Pod组成，在RC被创建后，系统会根据定义好的副本数来创建Pod数量。在运行过程中，如果Pod数量小于定义的，就会重启停止的或重新分配Pod，反之则杀死多余的。 确保pod健康：当pod不健康，运行出错或者无法提供服务时，RC也会杀死不健康的pod，重新创建新的。 弹性伸缩 ：在业务高峰或者低峰期的时候，可以通过RC动态的调整pod的数量来提高资源的利用率。同时，配置相应的监控功能（Hroizontal Pod Autoscaler），会定时自动从监控平台获取RC关联pod的整体资源使用情况，做到自动伸缩。 滚动升级：滚动升级为一种平滑的升级方式，通过逐步替换的策略，保证整体系统的稳定，在初始化升级的时候就可以及时发现和解决问题，避免问题不断扩大。 &lt;2&gt;Replication Set（RS） 被认为 是“升级版”的RC。RS也是用于保证与label selector匹配的pod数量维持在期望状态。 实际上RS和RC的功能基本一致，目前唯一的一个区别就是RC只支持基于等式的selector（env=dev或app=nginx），但RS还支持基于集合的selector（version in (v1, v2)），这对复杂的运维管理就非常方便了。 kubectl命令行工具中关于RC的大部分命令同样适用于我们的RS资源对象。不过我们也很少会去单独使用RS，它主要被Deployment这个更加高层的资源对象使用，除非用户需要自定义升级功能或根本不需要升级Pod，在一般情况下，我们推荐使用Deployment而不直接使用Replica Set。 区别在于 1、RC只支持基于等式的selector（env=dev或environment!=qa），但RS还支持新的，基于集合的selector（version in (v1.0, v2.0)或env notin (dev, qa)），这对复杂的运维管理很方便。 2、升级方式 RS不能使用kubectlrolling-update进行升级 kubectl rolling-update专用于rc RS升级使用deployment或者kubectl replace命令 社区引入这一API的初衷是用于取代vl中的RC，也就是说当v1版本被废弃时，RC就完成了它的历史使命，而由RS来接管其工作 &lt;3&gt;DaemonSet 1. 特点： 如果必须将pod运行在固定的某个或某几个节点，且要优先于其他的pod的启动。通常情况下，默认会将每一个节点都运行，并且只能运行一个pod。这种情况推荐使用DeamonSet资源对象。 一个DaemonSet对象能确保其创建的Pod在集群中的每一台（或指定）Node上都运行一个副本。如果集群中动态加入了新的Node，DaemonSet中的Pod也会被添加在新加入Node上运行。删除一个DaemonSet也会级联删除所有其创建的Pod。 2. 使用环境 监控程序； 日志收集程序； 集群存储程序； &lt;4&gt;Deployment 1. 什么是Deployment Kubernetes Deployment提供了官方的用于更新Pod和Replica Set（下一代的Replication Controller）的方法，您可以在Deployment对象中只描述您所期望的理想状态（预期的运行状态），Deployment控制器为您将现在的实际状态转换成您期望的状态，例如，您想将所有的webapp:v1.0.9升级成webapp:v1.1.0，您只需创建一个Deployment，Kubernetes会按照Deployment自动进行升级。现在，您可以通过Deployment来创建新的资源（pod，rs，rc），替换已经存在的资源等。 你只需要在Deployment中描述你想要的目标状态是什么，Deployment controller就会帮你将Pod和Replica Set的实际状态改变到你的目标状态。你可以定义一个全新的Deployment，也可以创建一个新的替换旧的Deployment。 2. 典型的用例 使用Deployment来创建ReplicaSet。ReplicaSet在后台创建pod。检查启动状态，看它是成功还是失败。 然后，通过更新Deployment的PodTemplateSpec字段来声明Pod的新状态。这会创建一个新的ReplicaSet，Deployment会按照控制的速率将pod从旧的ReplicaSet移动到新的ReplicaSet中。 如果当前状态不稳定，回滚到之前的Deployment revision。每次回滚都会更新Deployment的revision。 扩容Deployment以满足更高的负载。 暂停Deployment来应用PodTemplateSpec的多个修复，然后恢复上线。 根据Deployment 的状态判断上线是否hang住了。 清除旧的不必要的ReplicaSet。 3. 使用环境 Deployment集成了上线部署、滚动升级、创建副本、暂停上线任务，恢复上线任务，回滚到以前某一版本（成功/稳定）的Deployment等功能，在某种程度上，Deployment可以帮我们实现无人值守的上线，大大降低我们的上线过程的复杂沟通、操作风险。 定义Deployment来创建Pod和ReplicaSet 滚动升级和回滚应用 扩容和缩容 暂停和继续Deployment 3. DaemonSet 与 Deployment 的区别 Deployment 部署的副本 Pod 会分布在各个 Node 上，每个 Node 都可能运行好几个副本。 DaemonSet 的不同之处在于：每个 Node 上最多只能运行一个副本。 2）使用DaemonSet控制器运行httpd服务，要求名称以自己的名称命名。标签为：tier=backend,env=dev. 123456789101112131415[root@master ~]# vim daemonset.yaml kind: DaemonSetapiVersion: extensions/v1beta1metadata: name: xgp-dsspec: template: metadata: labels: tier: backend env: dev spec: containers: - name: xgp-ds image: httpd 查看一下 1[root@master ~]# kubectl get pod --show-labels 1[root@master ~]# kubectl get pod -L env,tier 3) 创建service资源对象与上述资源进行关联，要有验证。 1234567891011121314[root@master ~]# vim service.yaml kind: ServiceapiVersion: v1metadata: name: servicespec: type: NodePort selector: env: dev ports: - protocol: TCP port: 90 targetPort: 80 nodePort: 30123 执行一下 1[root@master ~]# kubectl apply -f service.yaml 查看一下 1[root@master ~]# kubectl describe svc 访问一下 1[root@master ~]# curl 127.0.0.1:30123 4）整理关于标签和标签选择器都有什么作用？ &lt;1&gt;标签：解决同类型的资源对象，为了更好的管理，按照标签分组。 &lt;2&gt;标签选择器：标签的查询过滤条件。","path":"posts/5281.html","date":"06-07","excerpt":"","tags":[{"name":"Replica","slug":"Replica","permalink":"https://wsdlxgp.top/tags/Replica/"},{"name":"SetDaemonSet","slug":"SetDaemonSet","permalink":"https://wsdlxgp.top/tags/SetDaemonSet/"},{"name":"标签","slug":"标签","permalink":"https://wsdlxgp.top/tags/%E6%A0%87%E7%AD%BE/"}]},{"title":"30 pod健康检查详解（liveness，readiness，滚动更新）","text":"环境介绍 主机 IP地址 服务 master 192.168.1.21 k8s+httpd+nginx node01 192.168.1.22 k8s node02 192.168.1.23 k8s 基于 https://blog.51cto.com/14320361/2464655 的实验继续进行 一、Pod的liveness和readiness探针 Kubelet使用liveness probe（存活探针）来确定何时重启容器。例如，当应用程序处于运行状态但无法做进一步操作，liveness探针将捕获到deadlock，重启处于该状态下的容器，使应用程序在存在bug的情况下依然能够继续运行下去 Kubelet使用readiness probe（就绪探针）来确定容器是否已经就绪可以接受流量。只有当Pod中的容器都处于就绪状态时kubelet才会认定该Pod处于就绪状态。该信号的作用是控制哪些Pod应该作为service的后端。如果Pod处于非就绪状态，那么它们将会被从service的load balancer中移除。 Probe支持以下三种检查方法： &lt;1&gt;exec-命令 在用户容器内执行一次命令，如果命令执行的退出码为0，则认为应用程序正常运行，其他任务应用程序运行不正常。 12345livenessProbe: exec: command: - cat - /home/laizy/test/hostpath/healthy &lt;2&gt;TCPSocket 将会尝试打开一个用户容器的Socket连接（就是IP地址：端口）。如果能够建立这条连接，则认为应用程序正常运行，否则认为应用程序运行不正常。 123livenessProbe:tcpSocket: port: 8080 &lt;3&gt;HTTPGet 调用容器内Web应用的web hook，如果返回的HTTP状态码在200和399之间，则认为应用程序正常运行，否则认为应用程序运行不正常。每进行一次HTTP健康检查都会访问一次指定的URL。 123456httpGet: #通过httpget检查健康，返回200-399之间，则认为容器正常 path: / #URI地址 port: 80 #端口号 #host: 127.0.0.1 #主机地址 scheme: HTTP #支持的协议，http或者httpshttpHeaders：’’ #自定义请求的header 参数说明 **initialDelaySeconds：**容器启动后第一次执行探测是需要等待多少秒。 **periodSeconds：**执行探测的频率。默认是10秒，最小1秒。 **timeoutSeconds：**探测超时时间。默认1秒，最小1秒。 **successThreshold：**探测失败后，最少连续探测成功多少次才被认定为成功。默认是1。对于liveness必须是1。最小值是1。 探针探测的结果有以下三者之一： Success：Container通过了检查。 Failure：Container未通过检查。 Unknown：未能执行检查，因此不采取任何措施。 1. LivenessProbe（活跃度） （1）编写一个livenss的yaml文件 1234567891011121314151617181920212223[root@node02 ~]# vim livenss.yamlkind: PodapiVersion: v1metadata: name: liveness labels: test: livenessspec: restartPolicy: OnFailure containers: - name: liveness image: busybox args: - /bin/sh - -c - touch /tmp/test; sleep 60; rm -rf /tmp/test; sleep 300 livenessProbe: #存活探测 exec: #通过执行命令来检查服务是否正常 command: #命令模式 - cat - /tmp/test initialDelaySeconds: 10 #pod运行10秒后开始探测 periodSeconds: 5 #检查的频率，每5秒探测一次 该配置文件给Pod配置了一个容器。periodSeconds 规定kubelet要每隔5秒执行一次liveness probe。initialDelaySeconds 告诉kubelet在第一次执行probe之前要的等待10秒钟。探针检测命令是在容器中执行 cat /tmp/healthy 命令。如果命令执行成功，将返回0，kubelet就会认为该容器是活着的并且很健康。如果返回非0值，kubelet就会杀掉这个容器并重启它。 （2）运行一下 1[root@master ~]# kubectl apply -f liveness.yaml （3）查看一下 1[root@master ~]# kubectl get pod -w Liveness活跃度探测，根据探测某个文件是否存在，来确定某个服务是否正常运行，如果存在则正常，负责，它会根据你设置的Pod的重启策略操作Pod。 2. Readiness（敏感探测、就绪性探测） ReadinessProbe探针的使用场景livenessProbe稍有不同，有的时候应用程序可能暂时无法接受请求，比如Pod已经Running了，但是容器内应用程序尚未启动成功，在这种情况下，如果没有ReadinessProbe，则Kubernetes认为它可以处理请求了，然而此时，我们知道程序还没启动成功是不能接收用户请求的，所以不希望kubernetes把请求调度给它，则使用ReadinessProbe探针。 ReadinessProbe和livenessProbe可以使用相同探测方式，只是对Pod的处置方式不同，ReadinessProbe是将Pod IP:Port从对应的EndPoint列表中删除，而livenessProbe则Kill容器并根据Pod的重启策略来决定作出对应的措施。 ReadinessProbe探针探测容器是否已准备就绪，如果未准备就绪则kubernetes不会将流量转发给此Pod。 ReadinessProbe探针与livenessProbe一样也支持exec、httpGet、TCP的探测方式，配置方式相同，只不过是将livenessProbe字段修改为ReadinessProbe。 （1）编写一个readiness的yaml文件 1234567891011121314151617181920212223[root@master ~]# vim readiness.yaml kind: PodapiVersion: v1metadata: name: readiness labels: test: readinessspec: restartPolicy: Never containers: - name: readiness image: busybox args: - /bin/sh - -c - touch /tmp/test; sleep 60; rm -rf /tmp/test; sleep 300 readinessProbe: exec: command: - cat - /tmp/test initialDelaySeconds: 10 periodSeconds: 5 （2）运行一下 1[root@master ~]# kubectl apply -f readiness.yaml （3）查看一下 1[root@master ~]# kubectl get pod -w 3. 总结liveness和readiness探测 （1）liveness和readiness是两种健康检查机制，k8s将两种探测采取相同的默认行为，即通过判断容器启动进程的返回值是否为零，来判断探测是否成功。 （2）两种探测配置方法完全一样，不同之处在于探测失败后的行为。 liveness探测是根据重启策略操作容器，大多数是重启容器。 readiness则是将容器设置为不可用，不接收Service转发的请求。 （3）两种探测方法可建议独立存在，也可以同时存在。用livensess判断是否需要重启，实现自愈；用readiness判断容器是否已经准备好对外提供服务。 二、 检测的应用 1. 在scale(扩容/缩容) 中的应用。 （1）编写一个readiness的yaml文件 123456789101112131415161718192021222324252627282930313233343536373839[root@master ~]# vim hcscal.yamlkind: DeploymentapiVersion: extensions/v1beta1metadata: name: webspec: replicas: 3 template: metadata: labels: run: web spec: containers: - name: web image: httpd ports: - containerPort: 80 readinessProbe: httpGet: scheme: HTTP #探测的协议 path: /healthy #访问的目录 port: 80 initialDelaySeconds: 10 periodSeconds: 5---kind: ServiceapiVersion: v1metadata: name: web-svcspec: type: NodePort selector: run: web ports: - protocol: TCP port: 90 targetPort: 80 nodePort: 30321 在配置文件中，使用httpd镜像，创建出一个Pod，其中periodSeconds字段指定kubelet每5秒执行一次探测，initialDelaySeconds字段告诉kubelet延迟等待10秒，探测方式为向容器中运行的服务发送HTTP GET请求，请求8080端口下的/healthz, 任何大于或等于200且小于400的代码表示成功。任何其他代码表示失败。 httpGet探测方式有如下可选的控制字段 host：要连接的主机名，默认为Pod IP，可以在http request head中设置host头部。 scheme: 用于连接host的协议，默认为HTTP。 path：http服务器上的访问URI。 httpHeaders：自定义HTTP请求headers，HTTP允许重复headers。 port： 容器上要访问端口号或名称。 （2）运行一下 1[root@master ~]# kubectl apply -f readiness.yaml （3）查看一下 1[root@master ~]# kubectl get pod -w 1[root@master ~]# kubectl get pod -o wide 1[root@master ~]# kubectl get service -o wide （4）访问一下 1[root@master ~]# curl 10.244.1.21/healthy （5）pod在指定目录创建一个文件 1[root@master ~]# kubectl exec web-69d659f974-7s9bc touch /usr/local/apache2/htdocs/healthy （6）查看一下 1[root@master ~]# kubectl get pod -w 2. 在更新过程中的使用 （1）编写一个readiness的yaml文件 1234567891011121314151617181920212223242526[root@master ~]# vim app.v1.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: appspec: replicas: 10 template: metadata: labels: run: app spec: containers: - name: app image: busybox args: - /bin/sh - -c - sleep 10; touch /tmp/healthy; sleep 3000 readinessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 10 periodSeconds: 5 （2）运行一下并记录版本信息 1[root@master ~]# kubectl apply -f readiness.yaml --record 查看一下 1[root@master ~]# kubectl rollout history deployment app （3）查看一下 1[root@master ~]# kubectl get pod -w 3.升级一下Deployment （1）编写一个readiness的yaml文件 12345678910111213141516171819202122232425262728[root@master ~]# cp app.v1.yaml app.v2.yaml[root@master ~]# vim app.v2.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: appspec: replicas: 10 template: metadata: labels: run: app spec: containers: - name: app image: busybox args: - /bin/sh - -c - sleep 3000 #修改命令 readinessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 10 periodSeconds: 5 （2）运行一下并记录版本信息 1[root@master ~]# kubectl apply -f readiness.yaml --record 查看一下 1[root@master ~]# kubectl rollout history deployment app （3）查看一下 1[root@master ~]# kubectl get pod -w （4）再次升级一下deployment &lt;1&gt; 编写一个readiness的yaml文件 123456789101112131415161718192021[root@master ~]# cp app.v1.yaml app.v3.yaml[root@master ~]# vim app.v2.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: appspec: replicas: 10 template: metadata: labels: run: app spec: containers: - name: app image: busybox args: - /bin/sh - -c - sleep 3000 #修改命令 &lt;2&gt; 运行一下并记录版本信息 1[root@master ~]# kubectl apply -f readiness.yaml --record 查看一下 1[root@master ~]# kubectl rollout history deployment app &lt;3&gt; 查看一下 1[root@master ~]# kubectl get pod -w 4. 回滚v2版本 1[root@master ~]# kubectl rollout undo deployment app --to-revision=2 查看一下 1[root@master ~]# kubectl get pod （1）编写一个readiness的yaml文件 123456789101112131415161718192021222324252627282930[root@master ~]# vim app.v2.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: appspec: strategy: rollingUpdate: maxSurge: 2 maxUnavailable: 2 replicas: 10 template: metadata: labels: run: app spec: containers: - name: app image: busybox args: - /bin/sh - -c - sleep 3000 readinessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 10 periodSeconds: 5 maxSurge：此参数控制滚动更新过程中，副本总数超过预期数的值。可以是整数，也可以是百分比，默认是1。 maxUnavailable：不可用pod的值，默认为1，可以是整数，也可以是百分比。 参数介绍 minReadySeconds: Kubernetes在等待设置的时间后才进行升级 如果没有设置该值，Kubernetes会假设该容器启动起来后就提供服务了 如果没有设置该值，在某些极端情况下可能会造成服务服务正常运行 maxSurge: 升级过程中最多可以比原先设置多出的POD数量 例如：maxSurage=1，replicas=5,则表示Kubernetes会先启动1一个新的Pod后才删掉一个旧的POD，整个升级过程中最多会有5+1个POD。 maxUnavaible: 升级过程中最多有多少个POD处于无法提供服务的状态 当maxSurge不为0时，该值也不能为0 例如：maxUnavaible=1，则表示Kubernetes整个升级过程中最多会有1个POD处于无法服务的状态 （2） 运行一下并记录版本信息 1[root@master ~]# kubectl apply -f app.v2.yaml --record 查看一下 1[root@master ~]# kubectl rollout history deployment app （3） 查看一下 1[root@master ~]# kubectl get pod -w 三、小实验 1）写一个Deployment资源对象，要求2个副本，nginx镜像。使用Readiness探测，自定义文件/test是否存在，容器开启之后10秒开始探测，时间间隔为10秒。 （1）编写一个readiness的yaml文件 1234567891011121314151617181920212223[root@master yaml]# vim nginx.yamlkind: DeploymentapiVersion: extensions/v1beta1metadata: name: webspec: replicas: 2 template: metadata: labels: run: web spec: containers: - name: readiness image: 192.168.1.21:5000/nginx:v1 readinessProbe: exec: command: - cat - /usr/share/nginx/html/test initialDelaySeconds: 10 periodSeconds: 10 （2）运行一下并记录版本信息 1[root@master ~]# kubectl apply -f nginx.yaml --record 查看一下 1[root@master ~]# kubectl rollout history deployment web （3）查看一下 1[root@master ~]# kubectl get pod -w 2）在运行之后两个Pod里，进入一个Pod，创建文件/test。 12[root@master yaml]# kubectl exec -it web-864c7cf7fc-gpxq4 /bin/bashroot@web-68444bff8-xm22z:/# touch /usr/share/nginx/html/test 查看一下 1[root@master yaml]# kubectl get pod -w 3）创建一个Service资源对象，跟上述Deployment进行关联，运行之后，查看Service资源详细信息，确认EndPoint负载均衡后端Pod。 （1）编写service的yaml文件 1234567891011121314[root@master yaml]# vim nginx-svc.yamlkind: ServiceapiVersion: v1metadata: name: web-svcspec: type: NodePort selector: run: web ports: - protocol: TCP port: 90 targetPort: 80 nodePort: 30321 （2）执行一下 1[root@master yaml]# kubectl apply -f nginx-svc.yaml （3）给两个pod刚更改页面 查看一下pod 1[root@master yaml]# kubectl get pod -o wide 更改页面 1234567[root@master yaml]# kubectl exec -it web-864c7cf7fc-gpxq4 /bin/bashroot@web-864c7cf7fc-gpxq4:/# echo \"123\"&gt;/usr/share/nginx/html/testroot@web-864c7cf7fc-gpxq4:/# exit[root@master yaml]# kubectl exec -it web-864c7cf7fc-pcrs9 /bin/bashroot@web-864c7cf7fc-pcrs9:/# echo \"321\"&gt;/usr/share/nginx/html/testroot@web-864c7cf7fc-pcrs9:/# exit 4）观察状态之后，尝试将另一个Pod也写入/test文件，然后再去查看SVC对应的EndPoint的负载均衡情况。 （1）查看一下service 1[root@master yaml]# kubectl get service （2）访问一下 1[root@master ~]# curl 192.168.1.21:30321/test 5）通过httpGet的探测方式，重新运行一下deployment资源，总结对比一下这两种Readiness探测方式。 （1）修改deployment的yaml文件 12345678910111213141516171819202122[root@master yaml]# vim nginx.yaml kind: DeploymentapiVersion: extensions/v1beta1metadata: name: webspec: replicas: 2 template: metadata: labels: run: web spec: containers: - name: readiness image: 192.168.1.21:5000/nginx:v1 readinessProbe: httpGet: scheme: HTTP path: /usr/share/nginx/html/test port: 80 initialDelaySeconds: 10 periodSeconds: 10 （2）执行一下 1[root@master yaml]# kubectl apply -f nginx.yaml （3）查看一下pod 1[root@master yaml]# kubectl get pod -w maxSurge：此参数控制滚动更新过程中，副本总数超过预期数的值。可以是整数，也可以是百分比，默认是1。所以现在是3台pod （4）访问一下 1[root@master yaml]# curl 192.168.1.21:30321/test 6）总结对比liveness和readiness探测的相同和不同之处，以及它们的使用场景。 &lt;1&gt;readiness和liveness的核心区别 实际上readiness 和liveness 就如同字面意思。readiness 就是意思是否可以访问，liveness就是是否存活。如果一个readiness 为fail 的后果是把这个pod 的所有service 的endpoint里面的改pod ip 删掉，意思就这个pod对应的所有service都不会把请求转到这pod来了。但是如果liveness 检查结果是fail就会直接kill container，当然如果你的restart policy 是always 会重启pod。 &lt;2&gt;什么样才叫readiness／liveness检测失败呢? 实际上k8s提供了3中检测手段， http get 返回200-400算成功，别的算失败 tcp socket 你指定的tcp端口打开，比如能telnet 上 cmd exec 在容器中执行一个命令 推出返回0 算成功。 每中方式都可以定义在readiness 或者liveness 中。比如定义readiness 中http get 就是意思说如果我定义的这个path的http get 请求返回200-400以外的http code 就把我从所有有我的服务里面删了吧，如果定义在liveness里面就是把我kill 了。 &lt;3&gt;readiness和readiness的使用环境 比如如果一个http 服务你想一旦它访问有问题我就想重启容器。那你就定义个liveness 检测手段是http get。反之如果有问题我不想让它重启，只是想把它除名不要让请求到它这里来。就配置readiness。 注意，liveness不会重启pod，pod是否会重启由你的restart policy（重启策略）控制。 参考： https://www.jianshu.com/p/16a375199cf2","path":"posts/af5b.html","date":"06-07","excerpt":"","tags":[{"name":"liveness","slug":"liveness","permalink":"https://wsdlxgp.top/tags/liveness/"},{"name":"readiness","slug":"readiness","permalink":"https://wsdlxgp.top/tags/readiness/"},{"name":"滚动更新","slug":"滚动更新","permalink":"https://wsdlxgp.top/tags/%E6%BB%9A%E5%8A%A8%E6%9B%B4%E6%96%B0/"}]},{"title":"29 k8s中pod的资源对象（名称空间，获取策略，重启策略，健康检查）","text":"一，k8s的资源对象 Deployment、Service、Pod是k8s最核心的3个资源对象 **Deployment：**最常见的无状态应用的控制器，支持应用的扩缩容、滚动升级等操作。 **Service：**为弹性变动且存在生命周期的Pod对象提供了一个固定的访问接口，用于服务发现和服务访问。 **Pod：**是运行容器以及调度的最小单位。同一个pod可以同时运行多个容器，这些容器共享net、UTS、IPC，除此之外还有USER、PID、MOUNT。 **ReplicationController：**用于确保每个Pod副本在任意时刻都能满足目标数量，简单来说，它用于每个容器或容器组总是运行并且可以访问的：老一代无状态的Pod应用控制器。 **RwplicatSet：**新一代的无状态的Pod应用控制器，它与RC的不同之处在于支持的标签选择器不同，RC只支持等值选择器（键值对），RS还额外支持基于集合的选择器。 **StatefulSet：**用于管理有状态的持久化应用，如database服务程序，它与Deployment不同之处在于，它会为每一个pod创建一个独有的持久性标识符，并确保每个pod之间的顺序性。 **DaemonSet：**用于确保每一个节点都运行了某个pod的一个副本，新增的节点一样会被添加到此类pod，在节点移除时，此pod会被回收。 **Job：**用于管理运行完成后即可终止的应用，例如批量处理做作业任务； **volume：**pv pvc ConfigMap： Secret： Role： ClusterRole： RoleBinding： cluster RoleBinding： service account： Helm： Pod的生命周期被定义为以下几个阶段。 Pending：Pod已经被创建，但是一个或者多个容器还未创建，这包括Pod调度阶段，以及容器镜像的下载过程。 Running：Pod已经被调度到Node，所有容器已经创建，并且至少一个容器在运行或者正在重启。 Succeeded：Pod中所有容器正常退出。 Failed：Pod中所有容器退出，至少有一个容器是一次退出的。 环境介绍 主机 IP地址 服务 master 192.168.1.21 k8s node01 192.168.1.22 k8s node02 192.168.1.23 k8s 二，Namespace：名称空间 默认的名称空间： Namespace（命名空间）是kubernetes系统中的另一个重要的概念，通过将系统内部的对象“分配”到不同的Namespace中，形成逻辑上分组的不同项目、小组或用户组，便于不同的分组在共享使用整个集群的资源的同时还能被分别管理。 Kubernetes集群在启动后，会创建一个名为“default”的Namespace，如果不特别指明Namespace，则用户创建的Pod、RC、Service都被系统创建到“default”的Namespace中。 1.查看名称空间 1[root@master ~]# kubectl get namespaces 2.查看名称空间详细信息 1[root@master ~]# kubectl describe ns default 3.创建名称空间 1[root@master ~]# kubectl create ns bdqn 查看一下 1[root@master ~]# kubectl get namespaces 4.创建namespace的yaml文件 （1）查看格式 12[root@master ~]# kubectl explain ns//查看nasespace的yaml文件的格式 （2）创建namespace的yaml文件 12345[root@master ~]# vim test-ns.yamlapiVersion: v1kind: Namespacemetadata: name: test （3）运行namespace的yaml文件 1[root@master ~]# kubectl apply -f test-ns.yaml （4）查看一下 1[root@master ~]# kubectl get ns 4.删除名称空间 12[root@master ~]# kubectl delete ns test [root@master ~]# kubectl delete -f test-ns.yaml 注意：namespace资源对象进用于资源对象的隔离，并不能隔绝不同名称空间的Pod之间的通信。那是网络策略资源的功能。 5.查看指定名称空间 可使用–namespace或-n选项 12[root@master ~]# kubectl get pod -n kube-system [root@master ~]# kubectl get pod --namespace kube-system 三，Pod 1.编写一个pod的yaml文件 123456789[root@master ~]# vim pod.yamlkind: PodapiVersion: v1metadata: name: test-podspec: containers: - name: test-app image: 192.168.1.21:5000/web:v1 pod的yaml文件不支持replicas字段 （1）运行一下 1[root@master ~]# kubectl apply -f pod.yaml （2）查看一下 1[root@master ~]# kubectl get pod ps：这个pod因为是自己创建的，所以删除之后k8s并不会自动生成，相当于docker中创建 2.指定pod的namespace名称空间 （1）修改pod的yaml文件 12345678910[root@master ~]# vim pod.yamlkind: Pod #资源类型apiVersion: v1 #api版本metadata: name: test-pod #指定控制器名称 namespace: bdqn #指定namespace（名称空间）spec: containers: #容器 - name: test-app #容器名称 image: 192.168.1.21:5000/web:v1 #镜像 执行一下 1[root@master ~]# kubectl apply -f pod.yaml （2）查看一下 12[root@master ~]# kubectl get pod -n bdqn //根据namespace名称查看 3.pod中镜像获取策略 **Always：**镜像标签为“laster”或镜像不存在时，总是从指定的仓库中获取镜像。 **IfNotPresent：**仅当本地镜像不存在时才从目标仓库下载。 **Never：**禁止从仓库中下载镜像，即只使用本地镜像。 注意：对于标签为“laster”或者标签不存在，其默认的镜像下载策略为“Always”，而对于其他的标签镜像，默认策略为“IfNotPresent”。 4.观察pod和service的不同并关联 （1）pod的yaml文件（指定端口） 1234567891011121314[root@master ~]# vim pod.yaml kind: Pod #资源类型apiVersion: v1 #api版本metadata: name: test-pod #指定控制器名称 namespace: bdqn #指定namespace（名称空间）spec: containers: #容器 - name: test-app #容器名称 image: 192.168.1.21:5000/web:v1 #镜像 imagePullPolicy: IfNotPresent #获取的策略 ports: - protocol: TCP containerPort: 80 &lt;1&gt;删除之前的pod 1[root@master ~]# kubectl delete pod -n bdqn test-pod &lt;2&gt;执行一下 1[root@master ~]# kubectl apply -f pod.yaml &lt;3&gt;查看一下 1[root@master ~]# kubectl get pod -n bdqn （2）pod的yaml文件（修改端口） 1234567891011121314[root@master ~]# vim pod.yaml kind: PodapiVersion: v1metadata: name: test-pod namespace: bdqnspec: containers: - name: test-app image: 192.168.1.21:5000/web:v1 imagePullPolicy: IfNotPresent ports: - protocol: TCP containerPort: 90 #改一下端口 &lt;1&gt;删除之前的pod 1[root@master ~]# kubectl delete pod -n bdqn test-pod &lt;2&gt;执行一下 1[root@master ~]# kubectl apply -f pod.yaml &lt;3&gt;查看一下 1[root@master ~]# kubectl get pod -n bdqn -o wide &lt;4&gt;访问一下 会发现修改的90端口并不生效，他只是一个提示字段并不生效。 （3）pod的yaml文件（添加标签） 12345678910111213141516[root@master ~]# vim pod.yaml kind: PodapiVersion: v1metadata: name: test-pod namespace: bdqn labels: #标签 app: test-web #标签名称spec: containers: - name: test-app image: 192.168.1.21:5000/web:v1 imagePullPolicy: IfNotPresent ports: - protocol: TCP containerPort: 90 #改一下端口 --------------------------------------pod--------------------------------------------- （4）编写一个service的yaml文件 123456789101112[root@master ~]# vim test-svc.yaml apiVersion: v1 #api版本kind: Service #资源类型metadata: name: test-svc #指定控制器名称 namespace: bdqn #指定namespace（名称空间）spec: selector: #标签 app: test-web #标签名称（须和pod的标签名称一致） ports: - port: 80 #宿主机端口 targetPort: 80 #容器端口 会发现添加的80端口生效了，所以不能乱改。 &lt;1&gt;执行一下 1[root@master ~]# kubectl apply -f test-svc.yaml &lt;2&gt;查看一下 1[root@master ~]# kubectl get svc -n bdqn 1[root@master ~]# kubectl describe svc -n bdqn test-svc &lt;4&gt;访问一下 1[root@master ~]# curl 10.98.57.97 --------------------------------------service--------------------------------------------- 四，容器的重启策略 Pod的重启策略（RestartPolicy）应用与Pod内所有容器，并且仅在Pod所处的Node上由kubelet进行判断和重启操作。当某个容器异常退出或者健康检查失败时，kubelet将根据RestartPolicy的设置来进行相应的操作。 Always：（默认情况下使用）但凡Pod对象终止就将其重启； **OnFailure：**仅在Pod对象出现错误时才将其重启； **Never：**从不重启； 五，pod的默认健康检查 每个容器启动时都会执行一个进程，此进程由 Dockerfile 的 CMD 或 ENTRYPOINT 指定。如果进程退出时返回码非零，则认为容器发生故障，Kubernetes 就会根据 restartPolicy 重启容器。 （1）编写健康检查的yaml文件 下面我们模拟一个容器发生故障的场景，Pod 配置文件如下： 12345678910111213141516[root@master ~]# vim healcheck.yaml apiVersion: v1kind: Podmetadata: labels: test: healcheck name: healcheckspec: restartPolicy: OnFailure #指定重启策略 containers: - name: healcheck image: busybox:latest args: #生成pod时运行的命令 - /bin/sh - -c - sleep 20; exit 1 &lt;1&gt;执行一下 1[root@master ~]# kubectl apply -f healcheck.yaml &lt;2&gt;查看一下 1[root@master ~]# kubectl get pod -o wide 1[root@master ~]# kubectl get pod -w | grep healcheck 在上面的例子中，容器进程返回值非零，Kubernetes 则认为容器发生故障，需要重启。但有不少情况是发生了故障，但进程并不会退出。 六，小实验 1）以自己的名称创建一个k8s名称空间，以下所有操作都在此名称空间中。 （1）创建名称空间 1[root@master ~]# kubectl create ns xgp （2）查看一下 1[root@master ~]# kubectl get ns xgp 2）创建一个Pod资源对象，使用的是私有仓库中私有镜像，其镜像的下载策略为：NEVER。 Pod的重启策略为： Never. 123456789101112131415161718192021[root@master ~]# vim pod.yamlkind: PodapiVersion: v1metadata: name: test-pod namespace: xgp labels: app: test-webspec: restartPolicy: Never containers: - name: www image: 192.168.1.21:5000/web:v1 imagePullPolicy: Never args: - /bin/sh - -c - sleep 90; exit 1 ports: - protocol: TCP containerPort: 80 3）创建出容器之后，执行非正常退出，查看Pod的最终状态。 （1）执行一下上面pod的yaml文件 1[root@master ~]# kubectl apply -f pod.yaml （2）动态查看ns中test-pod的信息 1[root@master ~]# kubectl get pod -n xgp -w | grep test-pod 删除test-pod 1[root@master ~]# kubectl delete pod -n xgp test-pod 4) 创建一个Service资源对象，与上述Pod对象关联，验证他们的关联性。 （1）修改pod的yaml文件 1234567891011121314151617[root@master ~]# vim pod.yamlkind: PodapiVersion: v1metadata: name: test-pod namespace: xgp labels: app: test-webspec: restartPolicy: Never containers: - name: www image: 192.168.1.21:5000/web:v1 imagePullPolicy: Never ports: - protocol: TCP containerPort: 80 （1）编写service的yaml文件 123456789101112[root@master ~]# vim svc.yaml apiVersion: v1kind: Servicemetadata: name: test-svc namespace: xgpspec: selector: app: test-web ports: - port: 80 targetPort: 80 （2）执行一下 1[root@master ~]# kubectl apply -f svc.yaml （3）查看一下 1[root@master ~]# kubectl get pod -o wide -n xgp （4）访问一下 1[root@master ~]# curl 10.244.1.21","path":"posts/74b2.html","date":"06-07","excerpt":"","tags":[{"name":"Namespace","slug":"Namespace","permalink":"https://wsdlxgp.top/tags/Namespace/"},{"name":"PodRestart","slug":"PodRestart","permalink":"https://wsdlxgp.top/tags/PodRestart/"},{"name":"Policy","slug":"Policy","permalink":"https://wsdlxgp.top/tags/Policy/"}]},{"title":"28 k8d创建资源(3)（负载均衡原理，回滚指定版本，label控制pod的位置）","text":"Deployment介绍 Deployment是kubernetes 1.2引入的概念，用来解决Pod的编排问题。Deployment可以理解为RC的升级版（RC+Reolicat Set）。特点在于可以随时知道Pod的部署进度，即对Pod的创建、调度、绑定节点、启动容器完整过程的进度展示。 使用场景 创建一个Deployment对象来生成对应的Replica Set并完成Pod副本的创建过程。 检查Deployment的状态来确认部署动作是否完成（Pod副本的数量是否达到预期值）。 更新Deployment以创建新的Pod(例如镜像升级的场景)。 如果当前Deployment不稳定，回退到上一个Deployment版本。 挂起或恢复一个Deployment。 Service介绍 Service定义了一个服务的访问入口地址，前端应用通过这个入口地址访问其背后的一组由Pod副本组成的集群实例，Service与其后端的Pod副本集群之间是通过Label Selector来实现“无缝对接”。RC保证Service的Pod副本实例数目保持预期水平。 外部系统访问Service的问题 IP类型 说明 Node IP Node节点的IP地址 Pod IP Pod的IP地址 Cluster IP Service的IP地址 环境介绍 主机 IP地址 服务 master 192.168.1.21 k8s node01 192.168.1.22 k8s node02 192.168.1.23 k8s 一，Delpoyment和service的简单使用 1.练习写一个yaml文件，要求使用自己的私有镜像，要求副本数量为三个。 123456789101112131415[root@master ~]# vim xgp.yamlkind: DeploymentapiVersion: extensions/v1beta1metadata: name: xgp-webspec: replicas: 3 template: metadata: labels: app: xgp-server spec: containers: - name: web image: 192.168.1.21:5000/web:v1 （1）执行一下 1[root@master ~]# kubectl apply -f xgp.yaml --recore （2）查看一下 1[root@master ~]# kubectl get pod （3）访问一下 1[root@master ~]# curl 10.244.2.16 （4）更新一下yaml文件，副本加一 123456789101112131415[root@master ~]# vim xgp.yamlkind: DeploymentapiVersion: extensions/v1beta1metadata: name: xgp-webspec: replicas: 4 template: metadata: labels: app: xgp-server spec: containers: - name: web image: 192.168.1.21:5000/web:v1 &lt;1&gt;执行一下 1[root@master ~]# kubectl apply -f xgp.yaml --recore &lt;2&gt;查看一下 1[root@master ~]# kubectl get pod 副本数量加一，如果yaml文件的副本为0，则副本数量还是之前的状态，并不会更新。 2.练习写一个service文件 123456789101112[root@master ~]# vim xgp-svc.yamlkind: ServiceapiVersion: v1metadata: name: xgp-svcspec: selector: app: xgp-server ports: - protocol: TCP port: 80 targetPort: 80 （1）执行一下 1[root@master ~]# kubectl apply -f xgp-svc.yaml （2）查看一下 1[root@master ~]# kubectl get svc （3）访问一下 1[root@master ~]# curl 10.107.119.49 3.修改yaml文件 1234567891011121314151617[root@master ~]# vim xgp.yaml kind: DeploymentapiVersion: extensions/v1beta1metadata: name: xgp-webspec: replicas: 3 template: metadata: labels: app: xgp-server spec: containers: - name: web image: 192.168.1.21:5000/web:v1 ports: - containerPort: 80 #提示端口 注意：在Delpoyment资源对象中，可以添加Port字段，但此字段仅供用户查看，并不实际生效 执行一下 1[root@master ~]# kubectl apply -f xgp.yaml --recore 4.service文件映射端口 1234567891011121314[root@master ~]# vim xgp-svc.yaml kind: ServiceapiVersion: v1metadata: name: xgp-svcspec: type: NodePort selector: app: xgp-server ports: - protocol: TCP port: 80 targetPort: 80 nodePort: 30123 执行一下 1[root@master ~]# kubectl apply -f xgp-svc.yaml 查看一下 1[root@master ~]# kubectl get svc 访问一下 1[root@master ~]# curl 127.0.0.1:30123 5.修改三个pod页面内容 （1）查看一下pod信息 1[root@master ~]# kubectl get pod -o wide （2）修改POD页面内容（三台不一样） 12[root@master ~]# kubectl exec -it xgp-web-8d5f9656f-8z7d9 /bin/bash//根据pod名称进入pod之中 进入容器后修改页面内容 12root@xgp-web-8d5f9656f-8z7d9:/usr/local/apache2# echo xgp-v1 &gt; htdocs/index.html root@xgp-web-8d5f9656f-8z7d9:/usr/local/apache2# exit 访问一下 1[root@master ~]# curl 127.0.0.1:30123 二.分析一下k8s负载均衡原理 （1）查看service的暴露IP 1[root@master ~]# kubectl get svc （2）查看一下iptabes规则 12[root@master ~]# iptables-save //查看已配置的规则 SNAT：Source NAT（源地址转换） DNAT：Destination NAT（目标地址转换） MASQ：动态的源地址转换 （3）根据service的暴露IP，查看对应的iptabes规则 1[root@master ~]# iptables-save | grep 10.107.119.49 1[root@master ~]# iptables-save | grep KUBE-SVC-ESI7C72YHAUGMG5S （4）对应一下IP是否一致 1[root@master ~]# iptables-save | grep KUBE-SEP-ZHDQ73ZKUBMELLJB 1[root@master ~]# kubectl get pod -o wide Service实现的负载均衡：默认使用的是iptables规则。IPVS 三.回滚到指定版本 （1）删除之前创建的delpoy和service 12[root@master ~]# kubectl delete -f xgp.yaml [root@master ~]# kubectl delete -f xgp-svc.yaml （2）准备三个版本所使用的私有镜像，来模拟每次升级不同的镜像 123456789101112131415161718[root@master ~]# vim xgp1.yaml （三个文件名不相同）kind: DeploymentapiVersion: extensions/v1beta1metadata: name: xgp-webspec: revisionHistoryLimit: 10 replicas: 3 template: metadata: labels: app: xgp-server spec: containers: - name: web image: 192.168.1.21:5000/web:v1 （三台版本不同） ports: - containerPort: 80 此处3个yaml文件 指定不同版本的镜像 （3）运行三个服务，并记录三个版本信息 123[root@master ~]# kubectl apply -f xgp-1.yaml --record [root@master ~]# kubectl apply -f xgp-2.yaml --record [root@master ~]# kubectl apply -f xgp-3.yaml --record （4）查看有哪些版本信息 1[root@master ~]# kubectl rollout history deployment xgp-web （5）运行之前的service文件 1[root@master ~]# kubectl apply -f xgp-svc.yaml （6）查看service暴露端口 1[root@master ~]# kubectl get svc （7）测试访问 1[root@master ~]# curl 127.0.0.1:30123 （8）回滚到指定版本 12[root@master ~]# kubectl rollout undo deployment xgp-web --to-revision=1//这里指定的是版本信息的编号 &lt;1&gt;访问一下 1[root@master ~]# curl 127.0.0.1:30123 &lt;2&gt;查看有哪些版本信息 1[root@master ~]# kubectl rollout history deployment xgp-web 编号1已经被编号2替代，从而生的是一个新的编号4 四.用label控制pod的位置 默认情况下，scheduler会将pod调度到所有可用的Node，不过有些情况我们希望将 Pod 部署到指定的 Node，比如将有大量磁盘 I/O 的 Pod 部署到配置了 SSD 的 Node；或者 Pod 需要 GPU，需要运行在配置了 GPU 的节点上。 kubernetes通过label来实现这个功能 label 是 key-value 对，各种资源都可以设置 label，灵活添加各种自定义属性。比如执行如下命令标注 k8s-node1 是配置了 SSD 的节点 首先我们给node1节点打上一个ssd的标签 1[root@master ~]# kubectl label nodes node02 disk=ssd （1）查看标签 1[root@master ~]# kubectl get nodes --show-labels | grep node02 （2）删除副本一 123[root@master ~]# kubectl delete -f xgp-1.yaml deployment.extensions \"xgp-web\" deleted[root@master ~]# kubectl delete svc xgp-svc （3）修改副本一的yaml文件 123456789101112131415161718192021[root@master ~]# vim xgp-1.yaml kind: DeploymentapiVersion: extensions/v1beta1metadata: name: xgp-webspec: revisionHistoryLimit: 10 replicas: 3 template: metadata: labels: app: xgp-server spec: containers: - name: web image: 192.168.1.21:5000/web:v1 ports: - containerPort: 80 nodeSelector: #添加节点选择器 disk: ssd #和标签内容一致 （4）执行一下 1[root@master ~]# kubectl apply -f xgp-1.yaml 查看一下 1[root@master ~]# kubectl get pod -o wide 现在pod都在node02上运行 （5）删除标签 1[root@master ~]# kubectl label nodes node02 disk- 查看一下 1[root@master ~]# kubectl get nodes --show-labels | grep node02 没有disk标签了 五，小实验 1）使用私有镜像v1版本部署一个Deployment资源对象，要求副本Pod数量为3个，并创建一个Service资源对象相互关联，指定要求3个副本Pod全部运行在node01节点上，记录一个版本。 （1）用label控制pod的位置 1[root@master ~]# kubectl label nodes node01 disk=ssd （2）编写源yaml文件 12345678910111213141516171819[root@master ~]# vim xgp.yamlkind: DeploymentapiVersion: extensions/v1beta1metadata: name: xgp-webspec: replicas: 3 template: metadata: labels: app: xgp-server spec: containers: - name: web image: 192.168.1.21:5000/web:v1 ports: - containerPort: 80 nodeSelector: disk: ssd （3）编写源service文件 1234567891011121314[root@master ~]# vim xgp-svc.yamlkind: ServiceapiVersion: v1metadata: name: xgp-svcspec: type: NodePort selector: app: xgp-server ports: - protocol: TCP port: 80 targetPort: 80 nodePort: 30123 （4）执行yaml文件，创建控制器。执行service文件创建映射端口 12[root@master ~]# kubectl apply -f xgp.yaml [root@master ~]# kubectl apply -f xgp-svc.yaml （5）查看一下pod节点 1[root@master ~]# kubectl get pod -o wide （6）记录一个版本 1[root@master ~]# kubectl rollout history deployment xgp-web &gt; pod.txt （7）访问一下 2）根据上述Deployment，升级为v2版本，记录一个版本。 （1）修改yaml文件镜像版本 12345678910111213141516171819[root@master ~]# vim xgp.yaml kind: DeploymentapiVersion: extensions/v1beta1metadata: name: xgp-webspec: replicas: 3 template: metadata: labels: app: xgp-server spec: containers: - name: web image: 192.168.1.21:5000/web:v2 #修改版本为二 ports: - containerPort: 80 nodeSelector: disk: ssd （2）刷新一下yaml文件 1[root@master ~]# kubectl apply -f xgp.yaml --recore （3）访问一下 （4）记录一个版本 1[root@master ~]# kubectl rollout history deployment xgp-web &gt; pod.txt 3）最后升级到v3版本，这时，查看Service关联，并且分析访问流量的负载均衡详细情况。 1）修改yaml文件镜像版本 12345678910111213141516171819[root@master ~]# vim xgp.yaml kind: DeploymentapiVersion: extensions/v1beta1metadata: name: xgp-webspec: replicas: 3 template: metadata: labels: app: xgp-server spec: containers: - name: web image: 192.168.1.21:5000/web:v3 #修改版本为二 ports: - containerPort: 80 nodeSelector: disk: ssd （2）刷新一下yaml文件 1[root@master ~]# kubectl apply -f xgp.yaml --recore （3）访问一下 （5）分析访问流量的负载均衡详细情况 &lt;1&gt;查看一下service映射端口 &lt;2&gt;以ip为起点，分析访问流量的负载均衡详细情况 Service实现的负载均衡：默认使用的是iptables规则。IPVS 12[root@master ~]# iptables-save | grep 10.107.27.229//根据service的暴露IP，查看对应的iptabes规则 1[root@master ~]# iptables-save | grep KUBE-SVC-ESI7C72YHAUGMG5S 这里显示了各节点的负载比例 &lt;3&gt;对应一下IP是否一致 1[root@master ~]# iptables-save | grep KUBE-SEP-VDKW5WQIWOLZMJ6G 1[root@master ~]# kubectl get pod -o wide 4）回滚到指定版本v1，并作验证。 &lt;1&gt;回滚到指定版本 12[root@master ~]# kubectl rollout undo deployment xgp-web --to-revision=1//这里指定的是版本信息的编号 &lt;2&gt;访问一下 1[root@master ~]# curl 127.0.0.1:30123 排错思路 123[root@master ~]# less /var/log/messages | grep kubelet[root@master ~]# kubectl logs -n kube-system kube-scheduler-master [root@master ~]# kubectl describe pod xgp-web-7d478f5bb7-bd4bj","path":"posts/c3bf.html","date":"06-07","excerpt":"","tags":[{"name":"service","slug":"service","permalink":"https://wsdlxgp.top/tags/service/"},{"name":"Deployment","slug":"Deployment","permalink":"https://wsdlxgp.top/tags/Deployment/"}]},{"title":"27 k8s创建资源(2)<基于配置清单>","text":"一，两种创建资源的方法 1. 基于命令的方式： 简单直观快捷，上手快。 适合临时测试或实验。 2. 基于配置清单的方式： 配置文件描述了 What，即应用最终要达到的状态。 配置文件提供了创建资源的模板，能够重复部署。 可以像管理代码一样管理部署。 适合正式的、跨环境的、规模化部署。 这种方式要求熟悉配置文件的语法，有一定难度。 环境介绍 主机 IP地址 服务 master 192.168.1.21 k8s node01 192.168.1.22 k8s node02 192.168.1.23 k8s 二. 配置清单（yam，yaml） 在k8s中，一般使用yaml格式的文件来创建符合我们预期期望的pod，这样的yaml文件我们一般称为资源清单 /etc/kubernetes/manifests/ k8s存放（yam、yaml）文件的地方 kubectl explain deployment（通过explain参数加上资源类别就能看到该资源应该怎么定义） kubectl explain deployment.metadata 通过资源类别加上带有Object标记的字段，我们就可以看到一级字段下二级字段的内容有那些怎么去定义等 kubectl explain deployment.metadata.ownerReferences 通过加上不同级别的字段名称来看下字段下的内容，而且前面的[]号代表对象列表 1.常见yaml文件写法，以及字段的作用 (1) apiVersion：api版本信息 （用来定义当前属于哪个组和那个版本，这个直接关系到最终提供使用的是那个版本） 12[root@master manifests]# kubectl api-versions//查看到当前所有api的版本 (2) kind: 资源对象的类别 (用来定义创建的对象是属于什么类别，是pod，service，还是deployment等对象，可以按照其固定的语法格式来自定义。) (3) metadata: 元数据 名称字段（必写） 提供以下几个字段： creationTimestamp: &quot;2019-06-24T12:18:48Z&quot; generateName: myweb-5b59c8b9d- labels: （对象标签） pod-template-hash: 5b59c8b9d run: myweb name: myweb-5b59c8b9d-gwzz5 （pods对象的名称，同一个类别当中的pod对象名称是唯一的，不能重复） namespace: default （对象所属的名称空间，同一名称空间内可以重复，这个名称空间也是k8s级别的名称空间，不和容器的名称空间混淆） ownerReferences: - apiVersion: apps/v1 blockOwnerDeletion: true controller: true kind: ReplicaSet name: myweb-5b59c8b9d uid: 37f38f64-967a-11e9-8b4b-000c291028e5 resourceVersion: &quot;943&quot; selfLink: /api/v1/namespaces/default/pods/myweb-5b59c8b9d-gwzz5 uid: 37f653a6-967a-11e9-8b4b-000c291028e5 annotations（资源注解，这个需要提前定义，默认是没有的） 通过这些标识定义了每个资源引用的path：即/api/group/version/namespaces/名称空间/资源类别/对象名称 (4) spec： 用户期望的状态 （这个字段最重要，因为spec是用来定义目标状态的‘disired state’，而且资源不通导致spec所嵌套的字段也各不相同，也就因为spec重要且字段不相同，k8s在内部自建了一个spec的说明用于查询） (5) status：资源现在处于什么样的状态 （当前状态，’current state‘，这个字段有k8s集群来生成和维护，不能自定义，属于一个只读字段） 2.编写一个yaml文件 123456789101112131415[root@master ~]# vim web.yamlkind: Deployment #资源对象是控制器apiVersion: extensions/v1beta1 #api的版本metadata: #描述kind（资源类型） name: web #定义控制器名称spec: replicas: 2 #副本数量 template: #模板 metadata: labels: #标签 app: web_server spec: containers: #指定容器 - name: nginx #容器名称 image: nginx #使用的镜像 执行一下 1[root@master ~]# kubectl apply -f web.yaml 查看一下 12[root@master ~]# kubectl get deployments. -o wide//查看控制器信息 12[root@master ~]# kubectl get pod -o wide//查看pod节点信息 3.编写一个service.yaml文件 123456789101112[root@master ~]# vim web-svc.yamlkind: Service #资源对象是副本apiVersion: v1 #api的版本metadata: name: web-svcspec: selector: #标签选择器 app: web-server #须和web.yaml的标签一致 ports: #端口 - protocol: TCP port: 80 #宿主机的端口 targetPort: 80 #容器的端口 使用相同标签和标签选择器内容，使两个资源对象相互关联。 创建的service资源对象，默认的type为ClusterIP，意味着集群内任意节点都可访问。它的作用是为后端真正服务的pod提供一个统一的接口。如果想要外网能够访问服务，应该把type改为NodePort （1）执行一下 1[root@master ~]# kubectl apply -f web-svc.yaml （2）查看一下 12[root@master ~]# kubectl get svc//查看控制器信息 （3）访问一下 1[root@master ~]# curl 10.111.193.168 4.外网能够访问服务 （1）修改web-svc.yaml文件 12345678910111213kind: Service #资源对象是副本apiVersion: v1 #api的版本metadata: name: web-svcspec: type: NodePort #添加 更改网络类型 selector: #标签选择器 app: web_server #须和web.yaml的标签一致 ports: #端口 - protocol: TCP port: 80 #宿主机的端口 targetPort: 80 #容器的端口 nodePort: 30086 #指定群集映射端口，范围是30000-32767 （2）刷新一下 1[root@master ~]# kubectl apply -f web-svc.yaml （3）查看一下 1[root@master ~]# kubectl get svc （4）浏览器测试 三、小实验 基于上一篇博客实验继续进行 1.使用yaml文件的方式创建一个Deployment资源对象，要求镜像使用个人私有镜像v1版本。replicas为3个。 编写yaml文件 123456789101112131415[root@master ~]# vim www.yamlkind: DeploymentapiVersion: extensions/v1beta1metadata: name: xgpspec: replicas: 3 template: metadata: labels: app: www_server spec: containers: - name: web image: 192.168.1.21:5000/web:v1 （1）执行一下 1[root@master ~]# kubectl apply -f web-svc.yaml （2）查看一下 12[root@master ~]# kubectl get deployments. -o wide//查看控制器信息 12[root@master ~]# kubectl get pod -o wide//查看pod节点信息 （3）访问一下 2. 使用yaml文件的方式创建一个Service资源对象，要与上述Deployment资源对象关联，type类型为： NodePort，端口为:30123. 编写service文件 1234567891011121314[root@master ~]# vim www-svc.yamlkind: ServiceapiVersion: v1metadata: name: www-svcspec: type: NodePort selector: app: www_server ports: - protocol: TCP port: 80 targetPort: 80 nodePort: 30123 执行一下 1[root@master ~]# kubectl apply -f www-svc.yaml 查看一下 1[root@master ~]# kubectl get svc 访问一下 四. 总结 1. Pod的作用 在k8s中pod是最小的管理单位，在一个pod中通常会包含一个或多个容器。大多数情况下，一个Pod内只有一个Container容器。 在每一个Pod中都有一个特殊的Pause容器和一个或多个业务容器，Pause来源于pause-amd64镜像,Pause容器在Pod中具有非常重要的作用： Pause容器作为Pod容器的根容器，其本地于业务容器无关，它的状态代表了整个pod的状态。 Pod里的多个业务容器共享Pause容器的IP，每个Pod被分配一个独立的IP地址，Pod中的每个容器共享网络命名空间，包括IP地址和网络端口。Pod内的容器可以使用localhost相互通信。k8s支持底层网络集群内任意两个Pod之间进行通信。 Pod中的所有容器都可以访问共享volumes，允许这些容器共享数据。volumes还用于Pod中的数据持久化，以防其中一个容器需要重新启动而丢失数据。 2. Service的作用 Service 是后端真实服务的抽象，一个 Service 可以代表多个相同的后端服务 Service 为 POD 控制器控制的 POD 集群提供一个固定的访问端点，Service 的工作还依赖于 K8s 中的一个附件，就是 CoreDNS ，它将 Service 地址提供一个域名解析。 NodePort 类型的 service clusterIP：指定 Service 处于 service 网络的哪个 IP，默认为动态分配 NodePort 是在 ClusterIP 类型上增加了一个暴露在了 node 的网络命名空间上的一个 nodePort，所以用户可以从集群外部访问到集群了，因而用户的请求流程是：Client -&gt; NodeIP:NodePort -&gt; ClusterIP:ServicePort -&gt; PodIP:ContainerPort。 可以理解为 NodePort 增强了 ClusterIP 的功能，让客户端可以在每个集群外部访问任意一个 nodeip 从而访问到 clusterIP，再由 clusterIP 进行负载均衡至 POD。 3.流量走向 我们在创建完成一个服务之后，用户首先应该访问的是nginx反向代理的ip，然后通过nginx访问到后端的k8s服务器（master节点）的“NodePort暴露IP 及 映射的端口“，master的apiserver接受到客户端发送来的访问指令，将访问指令通知Controller Manager控制器，Scheduler执行调度任务，将访问指令分发到各节点之上，通过”master节点“的“ip+映射端口”访问到后端k8s节点的信息，节点的Kubelet（pod代理）当Scheduler确定让那个节点返回访问信息之后，kube-proxy将访问信息负载均衡到该节点的容器上，各容器返回信息，并向Master报告运行状态","path":"posts/9569.html","date":"06-07","excerpt":"","tags":[{"name":"service","slug":"service","permalink":"https://wsdlxgp.top/tags/service/"},{"name":"yaml","slug":"yaml","permalink":"https://wsdlxgp.top/tags/yaml/"}]},{"title":"26 k8s创建资源(1)、<扩容与缩容>和<升级与回滚>","text":"两种创建资源的方法 基于命令的方式： 简单直观快捷，上手快。 适合临时测试或实验。 基于配置文件的方式： 配置文件描述了 What，即应用最终要达到的状态。 配置文件提供了创建资源的模板，能够重复部署。 可以像管理代码一样管理部署。 适合正式的、跨环境的、规模化部署。 这种方式要求熟悉配置文件的语法，有一定难度。 一，用命令行的方式创建资源 主机 IP地址 master 192.168.1.21 node01 192.168.1.22 node02 192.168.1.23 仅接受json格式 配置清单（yml、yaml） 12[root@master ~]# cd /etc/kubernetes/manifests///k8s的yml、yaml文件 1.node01和node02下载nginx镜像 12docker pull nginx//下载nginx镜像 2.master创建Pod控制器（test-web），deployment 12[root@master ~]# kubectl run test-web --image=nginx --replicas=5//创建Pod控制器，deployment 3.查看控制器情况 （1） 12[root@master ~]# kubectl get deployments.//查看控制器情况 12[root@master ~]# kubectl get pod --all-namespaces -o wide//显示pod的节点信息 （2） 12[root@master ~]# kubectl get namespaces //查看k8s名称空间 12[root@master ~]# kubectl describe deployments. test-web//查看资源详细信息 查看某种资源对象，没有指定名称空间，默认是在default名称空间。可以加上-n选项，查看指定名称空间的资源。 1[root@master ~]# kubectl get pod -n kube-system 3.删除test-web控制器 1[root@master ~]# kubectl delete deployments. test-web 4.master创建Pod控制器（web），deployment 1[root@master ~]# kubectl run web --image=nginx --replicas=5 查看一下pod信息 12[root@master ~]# kubectl get pod -o wide//查看一下pod的节点信息 12[root@master ~]# kubectl describe deployments. web //查看资源详细信息 注意：直接运行创建的deployment资源对象，是经常使用的一个控制器资源类型，除了deployment，还有rc、rs等等pod控制器，deployment是一个高级的pod控制器。 本机测试访问nginx 1[root@master ~]# curl 10.244.1.7 5.创建service资源类型 12[root@master ~]# kubectl expose deployment web --name=web-xgp --port=80 --type=NodePort//创建service资源类型，这里我们设置了映射端口 如果想要外网能够访问服务，可以暴露deployment资源，得到service资源，但svc资源的类型必须为NodePort。 映射端口范围：30000-32767 查看service信息 1[root@master ~]# kubectl get svc 浏览器测试访问http://192.168.1.21:30493/ 二、服务的扩容与缩容 1. 查看控制器信息 1[root@master ~]# kubectl get deployments. -o wide 2.扩容 1[root@master ~]# kubectl scale deployment web --replicas=8 查看一下 1[root@master ~]# kubectl get deployments. -o wide 3.缩容 1[root@master ~]# kubectl scale deployment web --replicas=4 查看一下 1[root@master ~]# kubectl get deployments. -o wide 3.通过修改web的yaml文件进行扩容缩容 备份web的yaml文件 1[root@master ~]# kubectl get deployments. -o yaml &gt; web.yaml 使用edit修改web的yaml文件 1[root@master ~]# kubectl edit deployments. web 查看一下 1[root@master ~]# kubectl get deployments. -o wide 三、服务的升级与回滚 node01和node02下载1.15版本的nginx 1[root@master ~]# docker pull nginx:1.15 1.master设置服务升级 1[root@master ~]# kubectl set image deployment web web=nginx:1.15 查看一下 2.master设置服务回滚 （1）修改配置文件回滚 使用edit修改web的yaml文件 1[root@master ~]# kubectl edit deployments. web 查看一下 1[root@master ~]# kubectl get deployments. -o wide （2）命令回滚 1[root@master ~]# kubectl rollout undo deployment web 注意:只能回滚到上一次操作的状态 四、实验环境 主机 IP地址 服务 master 192.168.1.21 registry+Deployment node01 192.168.1.22 node02 192.168.1.23 1.master 基于httpd制作自己的镜像，需要3个版本，v1,v2,v3.并且对应的版本镜像，访问的主目录内容不一样 （1）master下载httpd镜像 1[root@master ~]# docker pull httpd （2）编写Dockerfile 123[root@master xgp]# vim DockerfileFROM httpdCOPY index.html /usr/local/apache2/htdocs/index.html （3）创建测试网页v1 1[root@master xgp]#echo \"&lt;h1&gt;xgp | test-web | httpd:v1&lt;h1&gt;\" &gt; index.html （4）基于Dockerfile创建镜像 web1 1[root@master xgp]# docker build -t web1 . （5）创建测试网页v2 1[root@master xgp]#echo \"&lt;h1&gt;xgp | test-web | httpd:v1&lt;h1&gt;\" &gt; index.html （6）基于Dockerfile创建镜像 web2 1[root@master xgp]# docker build -t web2 . （7）创建测试网页v3 1[root@master xgp]# echo \"&lt;h1&gt;xgp | test-web | httpd:v3&lt;h1&gt;\" &gt; index.html （8）基于Dockerfile创建镜像 web3 1[root@master xgp]# docker build -t web3 . 2.master部署私有仓库 （1）master下载registry镜像 1[root@master ~]# docker pull registry （2）启动registry 1[root@master xgp]# docker run -itd --name registry -p 5000:5000 --restart=always registry:latest （3）修改docker配置文件，加入私有仓库（三台） 12[root@master xgp]# vim /usr/lib/systemd/system/docker.serviceExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.21:5000 （4）重启docker（三台） 12[root@master xgp]# systemctl daemon-reload [root@master xgp]# systemctl restart docker 3.上传之前创建的三个web镜像到私有仓库 （1）修改镜像标签 123[root@master xgp]# docker tag web1:latest 192.168.1.21:5000/web1:latest[root@master xgp]# docker tag web2:latest 192.168.1.21:5000/web2:latest[root@master xgp]# docker tag web3:latest 192.168.1.21:5000/web3:latest （2）将三个web镜像上传到私有仓库 123[root@master xgp]# docker push 192.168.1.21:5000/web1:latest [root@master xgp]# docker push 192.168.1.21:5000/web2:latest[root@master xgp]# docker push 192.168.1.21:5000/web3:latest 4.部署一个Deployment资源对象，要求镜像使用上述私有镜像v1版本。6个副本Pod。 1[root@master xgp]# kubectl run www1 --image=192.168.1.21:5000/web1:latest --replicas=6 查看一下 1[root@master xgp]# kubectl get pod 本地访问一下 5.将上述Deployment暴露一个service资源对象，使外网能否访问服务。 1[root@master xgp]# kubectl expose deployment www1 --name=web-xgp --port=80 --type=NodePort 查看一下 1[root@master xgp]# kubectl get svc 浏览器访问一下 6.将上述Deployment进行扩容和缩容操作，扩容为8个副本Pod，然后缩容为4个副本Pod。 （1）扩容 1[root@master xgp]# kubectl scale deployment www1 --replicas=8 查看一下 1[root@master xgp]# kubectl get deployments. -o wide （2）缩容 修改k8s配置文件 备份web的yaml文件 1[root@master ~]# kubectl get deployments. -o yaml &gt; www1.yaml 使用edit修改web的yaml文件 1[root@master ~]# kubectl edit deployments. www1 查看一下 1[root@master xgp]# kubectl get deployments. -o wide 7.将上述Deployment进行升级与回滚操作，将v1版本，升级到v2版本。 （1）升级版本为web2 1[root@master ~]# kubectl set image deployment www1 www1=192.168.1.21:5000/web2 本机测试访问 12[root@master ~]# curl 127.0.0.1:30996&lt;h1&gt;xgp | test-web | httpd:v2&lt;h1&gt; 浏览器测试访问 （2）回滚版本到web1 &lt;1&gt;修改配置文件回滚 使用edit修改web的yaml文件 1[root@master ~]# kubectl edit deployments. www1 查看一下 1[root@master ~]# kubectl get deployments. -o wide 访问一下 &lt;2&gt;命令回滚 1[root@master ~]# kubectl rollout undo deployment www1 注意:只能回滚到上一次操作的状态 访问一下","path":"posts/dbea.html","date":"06-07","excerpt":"","tags":[{"name":"deployments","slug":"deployments","permalink":"https://wsdlxgp.top/tags/deployments/"},{"name":"registry","slug":"registry","permalink":"https://wsdlxgp.top/tags/registry/"}]},{"title":"24 部署k8s集群","text":"一. Kubernetes 系统简介 首先，他是一个全新的基于容器技术的分布式架构领先方案。Kubernetes(k8s)是Google开源的容器集群管理系统（内部:Borg）。在Docker技术的基础上，为容器化的应用提供部署运行、资源调度、服务发现和动态伸缩等一系列完整功能，提高了大规模容器集群管理的便捷性。 Kubernetes是一个完备的分布式系统支撑平台，具有完备的集群管理能力，多扩多层次的安全防护和准入机制、多租户应用支撑能力、透明的服务注册和发现机制、內建智能负载均衡器、强大的故障发现和自我修复能力、服务滚动升级和在线扩容能力、可扩展的资源自动调度机制以及多粒度的资源配额管理能力。同时Kubernetes提供完善的管理工具，涵盖了包括开发、部署测试、运维监控在内的各个环节。 Kubernetes中，Service是分布式集群架构的核心，一个Service对象拥有如下关键特征： 拥有一个唯一指定的名字 拥有一个虚拟IP（Cluster IP、Service IP、或VIP）和端口号 能够体统某种远程服务能力 被映射到了提供这种服务能力的一组容器应用上 Service的服务进程目前都是基于Socket通信方式对外提供服务，比如Redis、Memcache、MySQL、Web Server，或者是实现了某个具体业务的一个特定的TCP Server进程，虽然一个Service通常由多个相关的服务进程来提供服务，每个服务进程都有一个独立的Endpoint（IP+Port）访问点，但Kubernetes能够让我们通过服务连接到指定的Service上。有了Kubernetes内奸的透明负载均衡和故障恢复机制，不管后端有多少服务进程，也不管某个服务进程是否会由于发生故障而重新部署到其他机器，都不会影响我们队服务的正常调用，更重要的是这个Service本身一旦创建就不会发生变化，意味着在Kubernetes集群中，我们不用为了服务的IP地址的变化问题而头疼了。 容器提供了强大的隔离功能，所有有必要把为Service提供服务的这组进程放入容器中进行隔离。为此，Kubernetes设计了Pod对象，将每个服务进程包装到相对应的Pod中，使其成为Pod中运行的一个容器。为了建立Service与Pod间的关联管理，Kubernetes给每个Pod贴上一个标签Label，比如运行MySQL的Pod贴上name=mysql标签，给运行PHP的Pod贴上name=php标签，然后给相应的Service定义标签选择器Label Selector，这样就能巧妙的解决了Service于Pod的关联问题。 在集群管理方面，Kubernetes将集群中的机器划分为一个Master节点和一群工作节点Node，其中，在Master节点运行着集群管理相关的一组进程kube-apiserver、kube-controller-manager和kube-scheduler，这些进程实现了整个集群的资源管理、Pod调度、弹性伸缩、安全控制、系统监控和纠错等管理能力，并且都是全自动完成的。Node作为集群中的工作节点，运行真正的应用程序，在Node上Kubernetes管理的最小运行单元是Pod。Node上运行着Kubernetes的kubelet、kube-proxy服务进程，这些服务进程负责Pod的创建、启动、监控、重启、销毁以及实现软件模式的负载均衡器。 在Kubernetes集群中，它解决了传统IT系统中服务扩容和升级的两大难题。你只需为需要扩容的Service关联的Pod创建一个Replication Controller简称（RC），则该Service的扩容及后续的升级等问题将迎刃而解。在一个RC定义文件中包括以下3个关键信息。 目标Pod的定义 目标Pod需要运行的副本数量（Replicas） 要监控的目标Pod标签（Label） 在创建好RC后，Kubernetes会通过RC中定义的的Label筛选出对应Pod实例并实时监控其状态和数量，如果实例数量少于定义的副本数量，则会根据RC中定义的Pod模板来创建一个新的Pod，然后将新Pod调度到合适的Node上启动运行，知道Pod实例的数量达到预定目标，这个过程完全是自动化。 1. Kubernetes优势: - 容器编排 - 轻量级 - 开源 - 弹性伸缩 - 负载均衡 2. Kubernetes 特性 Endpoint Slices Kubernetes 集群中网络端点的可扩展跟踪。 服务发现与负载均衡 无需修改您的应用程序即可使用陌生的服务发现机制。Kubernetes 为容器提供了自己的 IP 地址和一个 DNS 名称，并且可以在它们之间实现负载平衡。 自我修复 重新启动失败的容器，在节点死亡时替换并重新调度容器，杀死不响应用户定义的健康检查的容器，并且在它们准备好服务之前不会它们公布给客户端。 自动装箱 根据资源需求和其他约束自动放置容器，同时不会牺牲可用性，将任务关键工作负载和尽力服务工作负载进行混合放置，以提高资源利用率并节省更多资源。 IPv4/IPv6 双协议栈 Allocation of IPv4 and IPv6 addresses to Pods and Services 水平伸缩 使用一个简单的命令、一个UI或基于CPU使用情况自动对应用程序进行伸缩。 3. Kubernetes的Master和Node节点 1.Master k8s集群的管理节点，负责管理集群，提供集群的资源数据访问入口。拥有Etcd存储服务（可选），运行Api Server进程，Controller Manager服务进程及Scheduler服务进程，关联工作节点Node。Kubernetes API server提供HTTP Rest接口的关键服务进程，是Kubernetes里所有资源的增、删、改、查等操作的唯一入口。也是集群控制的入口进程；Kubernetes Controller Manager是Kubernetes所有资源对象的自动化控制中心；Kubernetes Schedule是负责资源调度（Pod调度）的进程 2.Node Node是Kubernetes集群架构中运行Pod的服务节点（亦叫agent或minion）。Node是Kubernetes集群操作的单元，用来承载被分配Pod的运行，是Pod运行的宿主机。关联Master管理节点，拥有名称和IP、系统资源信息。运行docker eninge服务，守护进程kunelet及负载均衡器kube-proxy. 每个Node节点都运行着以下一组关键进程 kubelet：负责对Pod对于的容器的创建、启停等任务 kube-proxy：实现Kubernetes Service的通信与负载均衡机制的重要组件 Docker Engine（Docker）：Docker引擎，负责本机容器的创建和管理工作 Node节点可以在运行期间动态增加到Kubernetes集群中，默认情况下，kubelet会想master注册自己，这也是Kubernetes推荐的Node管理方式，kubelet进程会定时向Master汇报自身情报，如操作系统、Docker版本、CPU和内存，以及有哪些Pod在运行等等，这样Master可以获知每个Node节点的资源使用情况，冰实现高效均衡的资源调度策略。 4. Kubernetes Node运行节点，运行管理业务容器，包含如下组件: （1）Kubelet 负责管控容器，Kubelet会从Kubernetes API Server接收Pod的创建请求，启动和停止容器，监控容器运行状态并汇报给Kubernetes API Server。 （2）Kubernetes Proxy 负责为Pod创建代理服务，Kubernetes Proxy会从Kubernetes API Server获取所有的Service信息，并根据Service的信息创建代理服务，实现Service到Pod的请求路由和转发，从而实现Kubernetes层级的虚拟转发网络。 （3）Docker Node上需要运行容器服务 k8s最基本的硬件要求 CPU: 双核 Mem: 2G 3台dockerhost 时间必须同步 实验环境 主机名 IP地址 服务 master 192.168.1.21 dockerhost node01 192.168.1.22 dockerhost node02 192.168.1.23 dockerhost 环境准备 分别将3台虚拟机命名，设置好对应IP，并将其写入域名解析/etc/hosts中，关闭防火墙，iptables，禁用selinux。还有要做到，时间必须一致。全部禁用swap 1.给三台docker命名 k8.1 12[root@localhost ~]# hostnamectl set-hostname master[root@localhost ~]# su - k8.2 12[root@localhost ~]# hostnamectl set-hostname node01[root@localhost ~]# su - k8.3 12[root@localhost ~]# hostnamectl set-hostname node02[root@localhost ~]# su - 验证docker是否能使用及版本是否一样 1[root@master ~]# docker -v 2.关闭防火墙及禁用selinux 123[root@master ~]# systemctl stop firewalld[root@master ~]# systemctl disable firewalld [root@master ~]# vim /etc/selinux/config 3. 禁用swap（三台） 1234[root@master ~]# swapoff -a//临时禁用swap[root@master ~]# free -h[root@master ~]# vim /etc/fstab 4.添加域名解析（三台） 123[root@master ~]# echo 192.168.1.21 master &gt;&gt; /etc/hosts[root@master ~]# echo 192.168.1.22 node01 &gt;&gt; /etc/hosts[root@master ~]# echo 192.168.1.23 node02 &gt;&gt; /etc/hosts 5.做免密登陆（三台） 12[root@master ~]# ssh-keygen -t rsa//生成密钥 复制密钥到其他主机 1254 ssh-copy-id node0155 ssh-copy-id node02 把域名解析复制到其他主机 1263 scp /etc/hosts node01:/etc64 scp /etc/hosts node02:/etc 6.打开路由转发和iptables桥接功能（三台） 1234567891011[root@master ~]# vim /etc/sysctl.d/k8s.conf//开启iptables桥接功能net.bridge.bridge-nf-call-iptables = 1net.bridge.bridge-nf-call-ip6tables = 1[root@master ~]# echo net.ipv4.ip_forward = 1 &gt;&gt; /etc/sysctl.conf //**打开路由转发[root@master ~]# sysctl -p /etc/sysctl.d/k8s.conf [root@master ~]# sysctl -p //刷新一下 如果以上命令执行失败可能是缺少模块，可执行以下命令 1[root@master ~]# modprobe br_netfiler 把路由转发和iptables桥接复制到其他主机 1234[root@master ~]# scp /etc/sysctl.d/k8s.conf node01:/etc/sysctl.d/[root@master ~]# scp /etc/sysctl.d/k8s.conf node02:/etc/sysctl.d/[root@master ~]# scp /etc/sysctl.conf node02:/etc/[root@master ~]# scp /etc/sysctl.conf node01:/etc/ 记得node01和node02也要执行以下命令 12[root@master ~]# sysctl -p /etc/sysctl.d/k8s.conf [root@master ~]# sysctl -p master节点安装部署k8s 指定yum安装kubernetes的yum源（三台） 123456789cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF 下载完成之后，查看一下仓库是否可用 1[root@master ~]# yum repolist 创建本地缓存（三台） 1[root@master ~]# yum makecache fast 各节点安装所需安装包 master下载 1[root@master ~]# yum -y install kubeadm-1.15.0-0 kubelet-1.15.0-0 kubectl-1.15.0-0 node01和node02下载 1[root@node01 ~]# yum -y install kubeadm-1.15.0-0 kubelet-1.15.0-0 三台主机把 kubelet加入开机自启 1[root@master ~]# systemctl enable kubelet master导入，之前准备好的镜像 123[root@master ~]# mkdir images[root@master ~]# cd images/[root@master images]# ls 创建一个导入镜像的脚本 12345678[root@master images]# cat &gt; image.sh &lt;&lt;EOF&gt; #!/bin/bash&gt; for i in /root/images/*&gt; do&gt; docker load &lt; $i &gt; done&gt; EOF[root@master images]# chmod +x image.sh 导入镜像 1[root@master images]# sh image.sh 初始化Kubernetes集群 1[root@master ~]# kubeadm init --kubernetes-version=v1.15.0 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 --ignore-preflight-errors=Swap 如果以上的命令报错，找出问题后先重置一下（下面的命令），然后再执行以上命令 12[root@master ~]# kubeadm reset//重置kubeadm 12[root@master images]# kubectl get node//查看当前节点信息 可以看出master的状态是未就绪（NotReady），之所以是这种状态是因为还缺少一个附件flannel，没有网络各Pod是无法通信的 也可以通过检查组件的健康状态 1[root@master images]# kubectl get cs 添加网络组件（flannel） 组件flannel可以通过https://github.com/coreos/flannel中获取 1[root@master ~]# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 以上只是方式之一，在网络状况良好的情况下建议使用上述方法（调用远端文件执行一下），若网速较差，建议使用以下方法： 12345[root@master images]# wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml#将github官网指定的.yml配置文件下载到本地[root@master images]# ls | grep flannel.yml #确定下载到了当前目录kube-flannel.yml[root@master images]# kubectl apply -f kube-flannel.yml #指定下载的.yml文件执行相应命令 上述方法，二选一进行配置即可。 看到很多东西被创建是还不够的，还需要查看flannel是否处于正常启动并运行的状态，才算正在的部署完成 12[root@master images]# kubectl get pods --all-namespaces//查看所有的名称空间的pod（可以看到flannel网络运行正常） 12[root@master images]# kubectl get pod -n kube-system//查看名称空间为kube-system的pod 查看当前节点信息 12kubectl get node//查看当前节点信息（已经准备好了） node两台节点，导入镜像并加入群集 导入镜像 上传所需镜像包，也可以使用docker pull下载 123[root@node01 images]# docker load &lt; kube-proxy-1-15.tar &amp;&amp; docker load -i myflannel-11-0.tar &amp;&amp; docker load -i pause-3-1.tar[root@node01 images]# docker images//查看本地镜像 node01和node02加入群集 这时使用的命令是初始化群集之后生成的令牌（只有24小时的时效） 1[root@node01 ~]# kubeadm join 192.168.1.21:6443 --token z0vknh.s6ib4eu4f8bre2nu --discovery-token-ca-cert-hash sha256:8da72cc83f45d1247f42ce888658129b43726fe2af4ffc0c4e79faedb4050359 加入群集之后查看一下 1[root@master images]# kubectl get node 各节点优化一下 设置table键的默认间距； 123[root@master ~]# vim .vimrcset tabstop=2[root@master ~]# source .vimrc 设置kubectl命令自动补全 1234[root@master ~]# yum -y install bash-completion[root@master ~]# source /usr/share/bash-completion/bash_completion [root@master ~]# source &lt;(kubectl completion bash)[root@master ~]# echo \"source &lt;(kubectl completion bash)\" &gt;&gt; ~/.bashrc 确认k8s群集没有问题，并设置为开机自启 master主机操作如下： 123[root@master ~]# kubectl get pod -n kube-system #查看pod资源，类似于docker中的容器，确保返回的信息都是running#“-n kube-system”：是k8s的名称空间 master和node节点上都需要进行以下操作，以便设置为开机自启： 12[root@master ~]# systemctl enable kubelet[root@master ~]# systemctl enable docker 设置为开机自启后，k8s群集的配置基本完成了，现在可以重启一下这三台服务器，如果重启后，执行下面的命令，状态都还是running，则表示绝对没有问题了。 1[root@master ~]# kubectl get pod -n kube-system #重启后验证状态是否还都是running","path":"posts/6489.html","date":"06-07","excerpt":"","tags":[{"name":"docker","slug":"docker","permalink":"https://wsdlxgp.top/tags/docker/"},{"name":"kubeadml","slug":"kubeadml","permalink":"https://wsdlxgp.top/tags/kubeadml/"}]},{"title":"23 Docker swarm搭建（2）","text":"什么是docker swarm? Swarm 在 Docker 1.12 版本之前属于一个独立的项目，在 Docker 1.12 版本发布之后，该项目合并到了 Docker 中，成为 Docker 的一个子命令。目前，Swarm 是 Docker 社区提供的唯一一个原生支持 Docker 集群管理的工具。它可以把多个 Docker 主机组成的系统转换为单一的虚拟 Docker 主机，使得容器可以组成跨主机的子网网络。 Docker Swarm 是一个为 IT 运维团队提供集群和调度能力的编排工具。用户可以把集群中所有 Docker Engine 整合进一个「虚拟 Engine」的资源池，通过执行命令与单一的主 Swarm 进行沟通，而不必分别和每个 Docker Engine 沟通。在灵活的调度策略下，IT 团队可以更好地管理可用的主机资源，保证应用容器的高效运行。 Swarm的基本架构如下图所示: Docker Swarm 优点 任何规模都有高性能表现 对于企业级的 Docker Engine 集群和容器调度而言，可拓展性是关键。任何规模的公司——不论是拥有五个还是上千个服务器——都能在其环境下有效使用 Swarm。 经过测试，Swarm 可拓展性的极限是在 1000 个节点上运行 50000 个部署容器，每个容器的启动时间为亚秒级，同时性能无减损。 灵活的容器调度 Swarm 帮助 IT 运维团队在有限条件下将性能表现和资源利用最优化。Swarm 的内置调度器（scheduler）支持多种过滤器，包括：节点标签，亲和性和多种容器部策略如 binpack、spread、random 等等。 服务的持续可用性 Docker Swarm 由 Swarm Manager 提供高可用性，通过创建多个 Swarm master 节点和制定主 master 节点宕机时的备选策略。如果一个 master 节点宕机，那么一个 slave 节点就会被升格为 master 节点，直到原来的 master 节点恢复正常。 此外，如果某个节点无法加入集群，Swarm 会继续尝试加入，并提供错误警报和日志。在节点出错时，Swarm 现在可以尝试把容器重新调度到正常的节点上去。 和 Docker API 及整合支持的兼容性 Swarm 对 Docker API 完全支持，这意味着它能为使用不同 Docker 工具（如 Docker CLI，Compose，Trusted Registry，Hub 和 UCP）的用户提供无缝衔接的使用体验。 Docker Swarm 为 Docker 化应用的核心功能（诸如多主机网络和存储卷管理）提供原生支持 开发的 Compose 文件能（通过 docker-compose up ）轻易地部署到测试服务器或 Swarm 集群上。Docker Swarm 还可以从 Docker Trusted Registry 或 Hub 里 pull 并 run 镜像。 一. 实验环境 主机 IP地址 服务 docker01 192.168.1.11 swarm+service+webUI+registry docker02 192.168.1.13 docker docker03 192.168.1.20 docker 三台主机都关闭防火墙，禁用selinux，修改主机名，时间同步，并添加域名解析。 docker版本必须是：v1.12版本开始（可使用docker version查看版本） 1.关闭防火墙，禁用selinux 123[root@localhost ~]# systemctl stop firewalld[root@localhost ~]# hostnamectl set-hostname docker03[root@localhost ~]# su - 2.时间同步 12mv /etc/localtime /etc/localtime.bkcp /usr/share/zoneinfo/Asia/Shanghai/etc/localtime 3.修改主机名（三台都要） 12[root@localhost ~]# hostnamectl set-hostname docker01[root@localhost ~]# su - 4.添加域名解析 123[root@docker01 ~]# echo 192.168.1.11 docker01 &gt;&gt; /etc/hosts[root@docker01 ~]# echo 192.168.1.13 docker02 &gt;&gt; /etc/hosts[root@docker01 ~]# echo 192.168.1.20 docker03 &gt;&gt; /etc/hosts 二. docker01 初始化集群 1[root@docker01 ~]# docker swarm init --advertise-addr 192.168.1.11 **–advertise-addr：**指定与其它docker通信的地址。 上边返回的结果告诉我们：初始化成功，并且，如果想要添加work节点运行下面的命令： 注意：token令牌只有24小时的有效期 如果想要添加manager节点：运行下面命令 三，docker02和docker03以worker加入集群 1[root@docker03 ~]# docker swarm join --token SWMTKN-1-5kxn9wloh7npnytklwbfciesr9di7uvu521gwnqm9h1n0pbokj-1e60wt0yr5583e4mzwbxnn3a8 192.168.1.11:2377 docker01查看集群 1[root@docker01 ~]# docker node ls 注意：这里的”*****“代表的是当前所属的节点 四.设置manager node（docker01）不参加工作 1[root@docker01 ~]# docker node update docker01 --availability drain 设置主机docker01以后不运行容器，但已经运行的容器并不会停止 “–availability”选项后面共有三个选项可配置，如下： “active”：工作；“pause”：暂时不工作；“drain”：永久性的不工作 1[root@docker01 ~]# docker node ls 五. docker01部署一个图形化webUI界面 1.docker01 导入镜像 1[root@docker01~]# docker pull dockersamples/visualizer 2.基于镜像启动一台容器 1[root@docker01 ~]# docker run -d -p 8080:8080 -e HOST=192.168.1.100 -e PORT=8080 -v /var/run/docker.sock:/var/run/docker.sock --name visualiaer dockersamples/visualizer 3.通过浏览器访问验证http://192.168.1.11:8080/ 如果访问不到网页，需开启路由转发 12[root@docker01 ~]# echo net.ipv4.ip_forward = 1 &gt;&gt; /etc/sysctl.conf [root@docker01 ~]# sysctl -p 六. Docker01部署一个私有仓库 Docker01部署 123456789101112131415161772 docker pull registry//下载registry镜像73 docker run -itd --name registry -p 5000:5000 --restart=always registry:latest//基于registry镜像，启动一台容器78 vim /usr/lib/systemd/system/docker.service #13行修改ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.11:5000 80 systemctl daemon-reload81 systemctl restart docker.service//重启docker76 docker tag busybox:latest 192.168.1.11:5000/busybox:v1 //把容器重命名一个标签77 docker ps 1234567891078 vim /usr/lib/systemd/system/docker.service #13行修改ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.11:5000 80 systemctl daemon-reload81 systemctl restart docker.service//重启docker100 docker push 192.168.1.11:5000/busybox:v1//上传容器到私有仓库 Docker02和docker03加入私有仓库 12345678978 vim /usr/lib/systemd/system/docker.service #13行修改ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.11:5000 80 systemctl daemon-reload81 systemctl restart docker.service//重启docker99 docker pull 192.168.1.11/busybox:v1//测试下载 七. 自定义镜像 要求：基于httpd镜像，更改访问界面内容。镜像tag版本为v1，v2，v3，对应主机面内容为v1，xgp666、v2，xgp666、v2，xgp666 12[root@docker01 ~]# docker pull httpd//下载httpd镜像 创建三个测试目录 12[root@docker01 ~]# mkdir &#123;v1,v2,v3&#125;//创建测试目录 docker01，v1目录操作 1234567891011121314[root@docker01 ~]# cd v1[root@docker01 v1]# echo v1,xgp666 &gt; index.html//创建测试网页[root@docker01 v1]# vim Dockerfile//编写DockerfileFROM httpdADD index.html /usr/local/apache2/htdocs/index.html[root@docker01 v1]# docker build -t 192.168.1.11:5000/httpd:v1 .//基于dockerfile创建镜像[root@docker01 v1]# docker push 192.168.1.11:5000/httpd:v1//上传刚刚创建镜像到私有仓库 docker01，v2目录操作 12345678910111213[root@docker01 v1]# cd ../v2[root@docker01 v2]# echo v2,xgp666 &gt; index.html[root@docker01 v2]# vim Dockerfile //编写DockerfileFROM httpdADD index.html /usr/local/apache2/htdocs/index.html[root@docker01 v2]# docker build -t 192.168.1.11:5000/httpd:v2 .//基于dockerfile创建镜像[root@docker01 v2]# docker push 192.168.1.11:5000/httpd:v2//上传刚刚创建镜像到私有仓库 docker01，v3目录操作 12345678910111213[root@docker01 v1]# cd ../v3[root@docker01 v2]# echo v3,xgp666 &gt; index.html[root@docker01 v2]# vim Dockerfile //编写DockerfileFROM httpdADD index.html /usr/local/apache2/htdocs/index.html[root@docker01 v2]# docker build -t 192.168.1.11:5000/httpd:v3 .//基于dockerfile创建镜像[root@docker01 v2]# docker push 192.168.1.11:5000/httpd:v3//上传刚刚创建镜像到私有仓库 八. 发布一个服务，基于上述镜像 要求:副本数量为3个。服务的名称为: bdqn 1[root@docker01 v3]# docker service create --replicas 3 --name bdqn -p 80:80 192.168.1.11:5000/httpd:v1 查看一下网络 1[root@docker03 ~]# docker network ls 默认的Ingress网络，包括创建的自定义overlay网络, 为后端真正为用户提供服务的container,提供了一个统一的入口。 service 通过 ingress load balancing 来发布服务，且 swarm 集群中所有 node 都参与到 ingress 路由网格（ingress routing mesh） 中，访问任意一个 node+PublishedPort 即可访问到服务。 当访问任何节点上的端口80时，Docker将您的请求路由到活动容器。在群节点本身，端口80可能并不实际绑定，但路由网格知道如何路由流量，并防止任何端口冲突的发生。 路由网格在发布的端口上监听分配给节点的任何IP地址。对于外部可路由的IP地址，该端口可从主机外部获得。对于所有其他IP地址，只能从主机内部访问。 查看一下创建的副本 1[root@docker01 v3]# docker service ps bdqn 浏览器测试访问http://192.168.1.11:80,http://192.168.1.13:80,http://192.168.1.20:80 修改docker02和docker03测试网页内容 docker02 123[root@docker02 ~]# docker exec -it 388f3bd9dd33 /bin/bashroot@388f3bd9dd33:/usr/local/apache2# cd htdocs/root@388f3bd9dd33:/usr/local/apache2/htdocs# echo 123 &gt; index.html docker03 12[root@docker03 ~]# docker exec -it 281454867fac /bin/bashroot@281454867fac:/usr/local/apache2# echo 321 &gt; htdocs/index.html 测试访问（每一台都会显示，会负载均衡） 要求:副本数量为3个。服务的名称为:test 1[root@docker01 v3]# docker service create --replicas 3 --name test -p 80 192.168.1.11:5000/httpd:v1 查看创建的服务映射端口 1[root@docker01 v3]# docker service ls 默认映射端口30000-32767 九. 服务的扩容与缩容 扩容 1[root@docker01 v3]# docker service scale bdqn=6 缩容 1[root@docker01 v3]# docker service scale bdqn=4 扩容与缩容直接直接通过scale进行设置副本数量。 十.服务的升级与回滚 （1）升级 docker service upadte 命令参数详解 –force 强制更新重启服务，无论是否配置或镜像改变都更新 –image image:tag 制定更新的镜像 –with-registry-auth 向 Swarm 代理发送 Registry 认证详细信息，私有仓库需要携带该参数 12[root@docker01 ~]# docker service update --image 192.168.1.11:5000/httpd:v2 bdqn//把bdqn服务升级成v2的版本 测试访问一下 （2）平滑的更新 12[root@docker01 ~]# docker service update --image 192.168.1.11:5000/httpd:v3 --update-parallelism 2 --update-delay 1m bdqn //两个服务一起更新，然后，隔一分钟，继续更新 默认情况下, swarm-次只更新-个副本,并且两个副本之间没有等待时间，我们可以通过 –update-parallelism;设置并行更新的副本数量。 –update-delay：指定滚动更新的时间间隔。 测试访问一下 (3) 回滚操作 1[root@docker01 ~]# docker service rollback bdqn 注意，docker swarm的回滚操作，默认只能回滚到上一-次操作的状态，并不能连续回滚到指定操作。 测试访问一下 十一，注意： 如果一台机器启用多个服务注意，合理分配cpu与内存资源，因tomcat在启动编译时会很吃内存，且docker是多线程启动的，所有最好是限定一下（设置resources.limits）否者会导致内存在同一时刻用光，某些服务启动失败当然也可是设置出错重启（restart_policy.condition:on-failure），另外设置resources.reservations要注意，不要超出总内存或cpu百分比，否者会导致后面服务无法获取cpu或内存资源出现“no suitable node (insufficien”错误（这个错误很奇怪，某个service不启动，也不输出日志，使用“docker stack ps [xxxx]”查看状态会显示此错误）无法启动","path":"posts/420e.html","date":"06-07","excerpt":"","tags":[{"name":"overlay","slug":"overlay","permalink":"https://wsdlxgp.top/tags/overlay/"},{"name":"swarm","slug":"swarm","permalink":"https://wsdlxgp.top/tags/swarm/"},{"name":"webUI","slug":"webUI","permalink":"https://wsdlxgp.top/tags/webUI/"}]},{"title":"12 Docker数据持久化和容器与容器的数据共享","text":"一、前言 当我们使用Docker创建一个mysql的container, 数据是存储在container内的. 如果有一天不小心执行了docker rm $(docker ps -aq)删除所有container. 那么mysql里的数据也会被删掉, 这是不安全的. 我们需要将数据持久化, 存储在container外部. 即使删除container也不会删除原有的数据. 二、容器的缺陷 容器中的数据可以存储在容器层。但是将数据存放在容器层存在以下问题： 1.数据不是持久化。意思是如果容器删除了，这些数据也就没了 2.主机上的其它进程不方便访问这些数据 3.对这些数据的I/O会经过存储驱动，然后到达主机，引入了一层间接层，因此性能会有所下降 三、data volume有两种挂载方式： **1）bind mount（用户管理）：**将宿主机上的某个目录或文件（不可以是没有格式化的磁盘文件），挂载到容器中，默认在容器内对此目录是有读写权限的，如果只需要向容器内添加文件，不希望覆盖目录，需要注意源文件必须存在，否则会被当做一个目录bind mount给容器。 **2）docker manager volume（docker自动管理）：**不需要指定源文件，只需要指定mount point（挂载点）。把容器里面的目录映射到了本地。 这种方式相比bind mount 缺点是无法限制对容器里边目录或文件的权限。 使用第二种挂载方式，-v 挂载时，不指定源文件位置，则默认挂载的路径是： 123[root@sqm-docker01 _data]# pwd/var/lib/docker/volumes/dd173640edd5b0205bb02f3c4139647be12528b38289b9f93f18123a6b1266a8/_data#当有目录挂载时，默认在/var/lib/docker/volumes/下会生成一串hash值，hash值下有一个_data的目录，容器内映射的文件就在此路径下。 四、Storage Driver 数据存储方式 Centos7版本的docker，Storage Driver（数据存储方式）为：overlay2 ，Backing Filesystem（文件系统类型）: xfs 可使用 “docker inspect 容器名称” 来查看数据存储方式 五、Data Volume （Bind mount） 持久化存储：本质上是DockerHost文件系统中的目录或文件，能够直接被Mount到容器的文件系统中。在运行容器时，可通过-v实现。 特点： 1、Data Volume是目录或文件，不能是没有格式化的磁盘（块设备）。 2、容器可以读写volume中的数据。 3、Volume数据可以永久保存，即使使用它的容器已经被销毁。 小实验： 运行一个nginx服务，做数据持久化 （1）Data Volume是目录或文件，不能是没有格式化的磁盘（块设备）。 12345678[root@docker01 ~]# mkdir html//创建测试目录[root@docker01 ~]# cd html/[root@docker01 html]# echo \"This is a testfile in dockerHost.\" &gt; index.html//创建测试网页[root@docker01 ~]# docker run -itd --name testweb -v /root/html/:/usr/share/nginx/html nginx:latest//运行一个nginx容器，并挂载目录[root@docker01 ~]# docker inspect testweb 1[root@docker01 ~]# curl 172.17.0.3 注意：dockerhost上需要被挂载的源文件或目录，必须是已经存在，否则，会被当作一个目录挂载到容器中。 （2）容器可以读写volume中的数据。 1234567[root@docker01 ~]# docker exec -it testweb /bin/bashroot@ef12d312a94e:/# cd /usr/share/nginx/html/root@ef12d312a94e:/usr/share/nginx/html# echo \"update\" &gt; index.html//容器中更新网页root@ef12d312a94e:/usr/share/nginx/html# exit[root@docker01 ~]# cat html/index.html//可以看到宿主目录的挂载目录也更新了 （3）Volume数据可以永久保存，即使，使用它的容器已经被销毁，也可以通过宿主机的挂在目录重新启动一个容器挂载这个目录进行访问。 12[root@docker01 ~]# docker ps -a -q |xargs docker rm -f//删除所有容器 12[root@docker01 ~]# cat html/index.html//容器删除之后，宿主机的测试网页也在 123[root@docker01 ~]# docker run -itd --name t1 -P -v /root/html/:/usr/share/nginx/html nginx:latest//基于测试网页创建一个容器[root@docker01 ~]# docker ps 12[root@docker01 ~]# curl 127.0.0.1:32768//访问一下 1234[root@docker01 ~]# echo \"update-new\" &gt; html/index.html//再次更新测试网页[root@docker01 ~]# curl 127.0.0.1:32768//在宿主机更新测试网页，刚刚创建的容器的测试网页也会更新 （4）默认挂载到容器内的文件，容器是有读写权限。可以在运行容器是-v 后边加“:ro”限制容器的写入权限 1234567[root@docker01 ~]# docker run -itd --name t2 -P -v /root/html/:/usr/share/nginx/html:ro nginx:latest//创建容器设置指读权限[root@docker01 ~]# docker exec -it t2 /bin/bash//进入容器root@4739c0f5d970:/# cd /usr/share/nginx/htmlroot@4739c0f5d970:/usr/share/nginx/html# echo 1234 &gt; index.html//修改测试网页（失败，因为是只读的） 123[root@docker01 ~]# echo 654321 &gt; html/index.html //宿主机可以更改[root@docker01 ~]# curl 127.0.0.1:32768 （5）并且还可以挂载单独的文件到容器内部，一般他的使用场景是：如果不想对整个目录进行覆盖，而只希望添加某个文件，就可以使用挂载单个文件。 &lt;1&gt;测试1 12 [root@docker01 ~]# docker run -itd --name v6 -P -v /root/html/index.html:/usr/share/nginx/html/index.html nginx:latest[root@docker01 ~]# docker ps 1[root@docker01 ~]# curl 127.0.0.1:32770 &lt;1&gt;测试2 12[root@docker01 ~]# echo test &gt; test.html[root@docker01 ~]# docker run -itd --name t8 -P -v /root/test.html:/usr/share/nginx/html/test.html nginx:latest 1[root@docker01 ~]# curl 127.0.0.1:32772/test.html 六，Docker Manager Volume 会自动在宿主机生成目录，所以在挂载目录的时候只用写容器中的目录。 特性和上边的bind mount基本一样 12[root@docker01 ~]# docker run -itd --name t1 -P -v /usr/share/nginx/html nginx:latest[root@docker01 ~]# docker ps 1[root@docker01 ~]# docker inspect t1 123[root@docker01 _data]# cd /var/lib/docker/volumes/17c50a065a6b10ccd01ca1ce8091fdf6282dc9dcb77a0f6695906257ecc03a63/_data[root@docker01 _data]# echo \"this is a testfile\" &gt; index.html[root@docker01 _data]# docker ps 1[root@docker01 _data]# curl 127.0.0.1:32777 1[root@docker01 _data]# docker volume ls 12[root@docker01 _data]# docker rm t1 -f[root@docker01 _data]# cat index.html 1.删除容器的操作，默认不会对dockerhost上的源文件操作，如果想要在删除容器时把源文件也删除，可以在删除容器时添加-v选项（一般不推荐使用这种方式，因为文件有可能被其他容器使用） 12[root@docker01 _data]# docker run -itd --name t2 -P -v /usr/share/nginx/html nginx:latest[root@docker01 ~]# docker inspect t2 1234[root@docker01 ~]# cd /var/lib/docker/volumes/2781dbfdc673fc7d149dc4f6217ef277fe72e05ba2e20fcebb617afe97eccb30/_data[root@docker01 _data]# docker rm -v t2 -ft2[root@docker01 _data]# ls 七，容器与容器的数据共享 Volume container：给其他容器提供volume存储卷的容器。并且它可以提供bind mount，也可以提供docker manager volume。 创建一个vc_data容器 12[root@docker01 ~]# docker create --name vc_data -v ~/html:/usr/share/nginx/html -v /other/useful/tools busybox[root@docker01 ~]# docker inspect vc_data 12[root@docker01 ~]# docker run -itd --name t3 -P --volumes-from vc_data nginx:latest[root@docker01 ~]# docker ps 1[root@docker01 ~]# curl 127.0.0.1:32779 八，容器的跨主机数据共享 实验环境 docker01 docker02 httpd nfs 要求：docker01和docker02的主目录，是一样的。 准备工作 123[root@localhost ~]# hostnamectl set-hostname nfs[root@localhost ~]# hostnamectl set-hostname docker01[root@localhost ~]# hostnamectl set-hostname docker02 nfs操作 123456789[root@localhost ~]# yum -y install nfs-utils//下载nfs服务[root@nfs ~]# mkdir /datashare//创建共享目录[root@nfs ~]# vim /etc/exports//设置权限如下/datashare *(rw,sync,no_root_squash) 开启各项服务 1234[root@nfs ~]# systemctl start rpcbind[root@nfs ~]# systemctl enable rpcbind[root@nfs ~]# systemctl start nfs-server[root@nfs ~]# systemctl enable nfs-server docker01和docker02测试nfs 12[root@docker01 htdocs]# showmount -e 192.168.1.20[root@docker02 htdocs]# showmount -e 192.168.1.20 docker01的操作 12345[root@docker02 ~]# mkdir /xxx[root@docker01 ~]# mount -t nfs 192.168.1.10:/datashare /xxx//挂载nfs上的共享目录[root@docker01 ~]# mount | tail -1//查看是否挂载 nfs创建测试文件 12345678[root@nfs ~]# cd datashare/[root@nfs datashare]# vim index.html&lt;div id=\"datetime\"&gt; &lt;script&gt; setInterval(\"document.getElementById('datetime').innerHTML=new Date().toLocaleString();\", 1000); &lt;/script&gt;&lt;/div&gt;xgp666 docker01查看一下 docker02的操作与docker01上一样 这里先不考虑将代码写入镜像，先以这种方式，分别在docker01和docker02部署httpd服务 12[root@docker01 ~]# docker run -itd --name bdqn-web1 -P -v /xxx/:/usr/local/apache2/htdocs httpd:latest [root@docker02 ~]# docker run -itd --name bdqn-web2 -P -v /xxx/:/usr/local/apache2/htdocs httpd:latest 123456[root@docker01 ~]# docker ps //查看端口0.0.0.0:32775-&gt;80/tcp bdqn-web[root@docker02 ~]# docker ps//查看端口0.0.0.0:32769-&gt;80/tcp bdqn-web2 此时，用浏览器访问,两个WEB服务的主界面是一样。但如果，NFS服务器上的源文件丢失, 则两个web服务都会异常。 想办法将元数据写入镜像内，在基于镜像创建一个vc_data容器，这里因为没有接触到docker-compose和docker-swarm等docker编排工具，所以需手动创建镜像！ nfs操作 12[root@nfs datashare]# echo xgp666 &gt; index.html //更改测试文件 docker02操作 1234567[root@docker02 ~]# cd /xxx/[root@docker02 xxx]# vim Dockerfile//编写Dockerfile[root@docker02 xxx]# cat Dockerfile FROM busyboxADD index.html /usr/local/apache2/htdocs/index.htmlVOLUME /usr/local/apache2/htdocs 创建镜像并运行一个容器 1234[root@docker02 xxx]# docker build -t back_data .//基于Dockerfile创建镜像[root@docker02 xxx]# docker create --name back_container1 back_data:latest //基于刚刚创建的镜像创建容器 运行容器，并导出镜像 1234[root@docker02 xxx]# docker run -itd --name bdqn-web3 -P --volumes-from back_container1 httpd:latest //运行一台容器[root@docker02 xxx]# docker save &gt; back_data.tar back_data:latest//导出镜像，因为是在共享目录所以docker01也可以看到 docker01 123456[root@docker01 xxx]# docker load -i back_data.tar //去共享目录，导入镜像[root@docker01 xxx]# docker create --name back_container2 back_data:latest//基于刚刚创建的镜像运行容器[root@docker01 xxx]# docker run -itd --name bdqn-web4 -P --volumes-from back_container2 httpd:latest//运行一台容器 浏览器访问 123456[root@docker01 ~]# docker ps //查看端口 0.0.0.0:32776-&gt;80/tcp bdqn-web4[root@docker02 ~]# docker ps//查看端口0.0.0.0:32770-&gt;80/tcp bdqn-web3","path":"posts/c73d.html","date":"06-07","excerpt":"","tags":[{"name":"docker网络","slug":"docker网络","permalink":"https://wsdlxgp.top/tags/docker%E7%BD%91%E7%BB%9C/"},{"name":"bind mount","slug":"bind-mount","permalink":"https://wsdlxgp.top/tags/bind-mount/"},{"name":"docker manager volu","slug":"docker-manager-volu","permalink":"https://wsdlxgp.top/tags/docker-manager-volu/"}]},{"title":"11 Docker跨主机网络——manual","text":"1. Macvlan 简介 在 Macvlan 出现之前，我们只能为一块以太网卡添加多个 IP 地址，却不能添加多个 MAC 地址，因为 MAC 地址正是通过其全球唯一性来标识一块以太网卡的，即便你使用了创建 ethx:y 这样的方式，你会发现所有这些“网卡”的 MAC 地址和 ethx 都是一样的，本质上，它们还是一块网卡，这将限制你做很多二层的操作。有了 Macvlan 技术，你可以这么做了。 Macvlan 允许你在主机的一个网络接口上配置多个虚拟的网络接口，这些网络 interface 有自己独立的 MAC 地址，也可以配置上 IP 地址进行通信。Macvlan 下的虚拟机或者容器网络和主机在同一个网段中，共享同一个广播域。Macvlan 和 Bridge 比较相似，但因为它省去了 Bridge 的存在，所以配置和调试起来比较简单，而且效率也相对高。除此之外，Macvlan 自身也完美支持 VLAN。 同一 VLAN 间数据传输是通过二层互访，即 MAC 地址实现的，不需要使用路由。不同 VLAN 的用户单播默认不能直接通信，如果想要通信，还需要三层设备做路由，Macvlan 也是如此。用 Macvlan 技术虚拟出来的虚拟网卡，在逻辑上和物理网卡是对等的。物理网卡也就相当于一个交换机，记录着对应的虚拟网卡和 MAC 地址，当物理网卡收到数据包后，会根据目的 MAC 地址判断这个包属于哪一个虚拟网卡。这也就意味着，只要是从 Macvlan 子接口发来的数据包（或者是发往 Macvlan 子接口的数据包），物理网卡只接收数据包，不处理数据包，所以这就引出了一个问题：本机 Macvlan 网卡上面的 IP 无法和物理网卡上面的 IP 通信！关于这个问题的解决方案我们下一节再讨论。 简单来说，Macvlan 虚拟网卡设备是寄生在物理网卡设备上的。发包时调用自己的发包函数，查找到寄生的物理设备，然后通过物理设备发包。收包时，通过注册寄生的物理设备的 rx_handler 回调函数，处理数据包。 2.简单介绍manual的流程 macvlan 就如它的名字一样，是一种网卡虚拟化技术，它能够将一个物理网卡虚拟出多个接口，每个接口都可以配置 MAC 地址，同样每个接口也可以配自己的 IP，每个接口就像交换机的端口一样，可以为它划分 VLAN。 macvlan 的做法其实就是将这些虚拟出来的接口与 Docker 容器直连来达到通信的目的。一个 macvlan 网络对应一个接口，不同的 macvlan 网络分配不同的子网，因此，相同的 macvlan 之间可以互相通信，不同的 macvlan 网络之间在二层上不能通信，需要借助三层的路由器才能完成通信，如下，显示的就是两个不同的 macvlan 网络之间的通信流程。 我们用一个 Linux 主机，通过配置其路由表和 iptables，将其配成一个路由器（当然是虚拟的），就可以完成不同 macvlan 网络之间的数据交换，当然用物理路由器也是没毛病的。 3.Macvlan 的特点： 1.可让使用者在同一张实体网卡上设定多个 MAC 地址。 2.承上，带有上述设定的 MAC 地址的网卡称为子接口（sub interface）；而实体网卡则称为父接口（parent interface）。 3.parent interface 可以是一个物理接口（eth0），可以是一个 802.1q 的子接口（eth0.10），也可以是 bonding 接口。 4.可在 parent/sub interface 上设定的不只是 MAC 地址，IP 地址同样也可以被设定。 5.sub interface 无法直接与 parent interface 通讯 (带有 sub interface 的 VM 或容器无法与 host 直接通讯)。 承上，若 VM 或容器需要与 host 通讯，那就必须额外建立一个 sub 6.interface 给 host 用。 7.sub interface 通常以 mac0@eth0 的形式来命名以方便区別。 用张图来解释一下设定 Macvlan 后的样子： 4.实验环境 docker01 docker02 192.168.1.11 192.168.1.13 关闭防火墙和禁用selinux，更改主机名 123456789[root@localhost ~]# hostnamectl set-hostname docker01[root@localhost ~]# su -上一次登录：二 12月 17 08:20:36 CST 2019从 192.168.1.1pts/0 上[root@docker01 ~]# systemctl stop firealldFailed to stop firealld.service: Unit firealld.service not loaded.[root@docker01 ~]# setenforce 0setenforce: SELinux is disabled[root@docker01 ~]# systemctl daemon-reload [root@docker01 ~]# systemctl restart docker 4.1 macvlan的单网络通信 1) 打开网卡的混杂模式 //需要在docker01和docker02_上都进行操作。 12[root@docker01 ~]# ip link show ens33//查看网卡模式 1234[root@docker01 ~]# ip link set ens33 promisc on//创建网卡模式为混杂模式[root@docker01 ~]# ip link show ens33//查看网卡模式 2)在docker01.上创建macvlan网络 12345[root@docker01 ~]# docker network create -d macvlan --subnet 172.22.16.0/24 --gateway 172.22.16.1 -o parent=ens33 mac_net1// 创建一个macvlan模式的网络-o parent=绑定在哪张网卡之上[root@docker01 ~]# docker network ls//查看网卡信息 3)基于创建的macvlan网络运行一个容器 [root@docker01 ~]# docker run -itd --name bbox1 --ip 172.22.16.10 --network mac_net1 busybox 4)在docker02.上创建macvlan网络（要和docker01的macvlan一模一样） 123[root@docker02 ~]# docker network create -d macvlan --subnet 172.22.16.0/24 --gateway 172.22.16.1 -o parent=ens33 mac_net1[root@docker02 ~]# docker network ls 5)在docker02. 上，基于创建的macvlan网络运行一个容器，验证与docker01.上容器的通信。 123456[root@docker02 ~]# docker run -itd --name bbox2 --network mac_net1 --ip 172.22.16.20 busybox//基于busybox创建一个容器[root@docker02 ~]# docker exec -it bbox2 /bin/sh//进入bbox2容器/ # ping 172.22.16.10//ping一下docker01的主机 4.2macvlan的多网络通信 1） docker01和docker02验证内核模块8021q封装 macvlan需要解决的问题:基于真实的ens33网卡，生产新的虚拟网卡。 12[root@docker01 ~]# modinfo 8021q//验证内核模块8021q封装 12[root@docker01 ~]# modprobe 8021q//如果内核模块没有开启，运行上边的命令导入一下 2)docker01基于ens33创建虚拟网卡 修改ens33网卡配置文件 12[root@docker01 ~]# cd /etc/sysconfig/network-scripts/[root@docker01 network-scripts]# vim ifcfg-ens33 手动添加虚拟网卡配置文件 12345678910111213[root@docker01 ~]# cd /etc/sysconfig/network-scripts/[root@docker01 network-scripts]# cp -p ifcfg-ens33 ifcfg-ens33.10//-p保留源文件或目录的属性[root@docker01 network-scripts]# vim ifcfg-ens33.10//修改ens33.10网卡配置文件BOOTPROTO=noneNAME=ens33.10DEVICE=ens33.10ONBOOT=yesIPADDR=192.168.10.10PREFIX=24GATEWAY=192.168.10.2VLAN=yes 这里注意，IP要和ens33网段做一个区分， 保证网关和网段IP的一致性，设备名称和配置文件的-致性,并且打开VLAN支持模式。 创建第二个虚拟网卡配置文件 1234567891011[root@docker01 network-scripts]# cp -p ifcfg-ens33.10 ifcfg-ens33.20[root@docker01 network-scripts]# vim ifcfg-ens33.20//修改ens33.20网卡配置文件BOOTPROTO=noneNAME=ens33.20DEVICE=ens33.20ONBOOT=yesIPADDR=192.168.20.20PREFIX=24GATEWAY=192.168.20.2VLAN=yes docker01上的操作，启用创建的虚拟网卡: 1234[root@docker01 network-scripts]# ifup ifcfg-ens33.10 [root@docker01 network-scripts]# ifup ifcfg-ens33.20[root@docker01 network-scripts]# ifconfig//查看IP 3)docker02基于ens33创建虚拟网卡 修改ens33网卡配置文件 12[root@docker02 ~]# cd /etc/sysconfig/network-scripts/[root@docker02 network-scripts]# vim ifcfg-ens33 手动添加虚拟网卡配置文件 12345678910111213[root@docker02 ~]# cd /etc/sysconfig/network-scripts/[root@docker02 network-scripts]# cp -p ifcfg-ens33 ifcfg-ens33.10//-p保留源文件或目录的属性[root@docker02 network-scripts]# vim ifcfg-ens33.10//修改ens33.10网卡配置文件BOOTPROTO=noneNAME=ens33.10DEVICE=ens33.10ONBOOT=yesIPADDR=192.168.10.11PREFIX=24GATEWAY=192.168.10.2VLAN=yes 这里注意，IP要和ens33网段做一个区分， 保证网关和网段IP的一致性，设备名称和配置文件的-致性,并且打开VLAN支持模式。 创建第二个虚拟网卡配置文件 1234567891011[root@docker02 network-scripts]# cp -p ifcfg-ens33.10 ifcfg-ens33.20[root@docker02 network-scripts]# vim ifcfg-ens33.20//修改ens33.20网卡配置文件BOOTPROTO=noneNAME=ens33.20DEVICE=ens33.20ONBOOT=yesIPADDR=192.168.20.21PREFIX=24GATEWAY=192.168.20.2VLAN=yes docker02上的操作，启用创建的虚拟网卡: 12345[root@docker02 network-scripts]# systemctl restart network[root@docker02 network-scripts]# ifup ifcfg-ens33.10 [root@docker02 network-scripts]# ifup ifcfg-ens33.20[root@docker02 network-scripts]# ifconfig//查看IP 4）docekr01和docker02基于虚拟网卡，创建macvlan网络 1234[root@docker02 network-scripts]# docker network create -d macvlan --subnet 172.16.10.0/24 --gateway 172.16.10.1 -o parent=ens33.10 mac_net10//创建一个新的网卡基于ens33.10[root@docker02 network-scripts]# docker network create -d macvlan --subnet 172.16.20.0/24 --gateway 172.16.20.1 -o parent=ens33.20 mac_net20//创建一个新的网卡基于ens33.20 5）Docker01部署一个私有仓库 Docker01 1234567 72 docker pull registry//下载registry镜像 73 docker run -itd --name registry -p 5000:5000 --restart=always registry:latest //基于registry镜像，启动一台容器 76 docker tag busybox:latest 192.168.1.11:5000/busybox:v1 //把容器重命名一个标签 77 docker ps 123456789 78 vim /usr/lib/systemd/system/docker.service #13行修改ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.11:5000 80 systemctl daemon-reload 81 systemctl restart docker.service //重启docker 100 docker push 192.168.1.11:5000/busybox:v1//上传容器到私有仓库 101 docker images Docker02 1234567878 vim /usr/lib/systemd/system/docker.service #13行修改ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.11:5000 80 systemctl daemon-reload 81 systemctl restart docker.service true //重启docker 99 docker pull 192.168.1.11/busybox:v1 true //下载刚刚上传的镜像 6）docker01和docker02基于busybox:v1镜像和网卡mac_net10，mac_net20，创建容器。 Docker01 123[root@docker01 ~]# docker run -itd --name bbox10 --network mac_net10 --ip 172.16.10.10 192.168.1.11:5000/busybox:v1[root@docker01 ~]# docker run -itd --name bbox20 --network mac_net20 --ip 172.16.20.20 192.168.1.11:5000/busybox:v1**Docker02** 12[root@docker02 ~]# docker run -itd --name bbox10 --network mac_net10 --ip 172.16.10.10 192.168.1.11:5000/busybox:v1[root@docker02 ~]# docker run -itd --name bbox20 --network mac_net20 --ip 172.16.20.20 192.168.1.11:5000/busybox:v1 这里只需注意，我们在这里的操作跟在docker01和上面的操作是一模一样的，操作顺序大致为: 验证8021q内核封装 基于ens33网卡创建新的虚拟网卡,ens33.10和ens33.20 (注意和docker01. 上的ens33.10和ens33.20必须是在同一-网段，且IP不能冲突)基于此网络运行容器。(注意和docker01 上的容器，都是基于刚刚创建的macvlan网络，但IP地址不能冲突) 7）验证 在docker01.上进入容器bbox10和docker02.上的bbox11进行通信。 在docker01.上进入容器bbox20和docker02.上的bbox21进行通信。 注意: VMware的网络必须设置为Bridge模式。 现在把docker01和docker02的网络模式设置为桥接模式 测试一下相同网卡的主机是否能ping通 12[root@docker01 ~]# docker exec -it bbox10 /bin/sh/ # ping 172.16.20.20 12[root@docker02 ~]# docker exec -it bbox20 /bin/sh/ # ping 172.16.20.20 5.Macvlan 的局限性 Macvlan 是将 VM 或容器通过二层连接到物理网络的近乎理想的方案，但它也有一些局限性： 1.Linux 主机连接的交换机可能会限制同一个物理端口上的 MAC 地址数量。虽然你可以让网络管理员更改这些策略，但有时这种方法是无法实行的（比如你要去给客户做一个快速的 PoC 演示）。 2.许多 NIC 也会对该物理网卡上的 MAC地址数量有限制。超过这个限制就会影响到系统的性能。 3.IEEE 802.11 不喜欢同一个客户端上有多个 MAC 地址，这意味着你的 Macvlan 子接口在无线网卡或 AP 中都无法通信。可以通过复杂的办法来突破这种限制，但还有一种更简单的办法，那就是使用 Ipvlan，感兴趣可以自己查阅相关资料。 6.总结 macvlan是一种网卡虚拟化技术，能够将一张网卡虚拟出多张网卡。 macvlan的特定通信模式，常用模式是bridge。 在Docker中，macvlan只支持bridge模式。 相同的macvlan可以通信，不同的macvlan二层无法通信，可以通过三层路由完成通信。 思考一下： macvlan bridge和bridge的异同点 还有一种类似的技术，多张虚拟网卡共享相同MAC地址，但有独立的IP地址，这是什么技术？","path":"posts/2020.html","date":"06-07","excerpt":"","tags":[{"name":"docker网络 - docker跨主机网络","slug":"docker网络-docker跨主机网络","permalink":"https://wsdlxgp.top/tags/docker%E7%BD%91%E7%BB%9C-docker%E8%B7%A8%E4%B8%BB%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"manual单网络","slug":"manual单网络","permalink":"https://wsdlxgp.top/tags/manual%E5%8D%95%E7%BD%91%E7%BB%9C/"},{"name":"macvlan多网络","slug":"macvlan多网络","permalink":"https://wsdlxgp.top/tags/macvlan%E5%A4%9A%E7%BD%91%E7%BB%9C/"}]},{"title":"10 Docker跨主机网络——overlay","text":"一、Docker 跨主机通信 Docker跨主机网络方案包括： docker 原生的 overlay 和 macvlan。 第三方方案：常用的包括 flannel、weave 和 calico。 docker 通过 libnetwork 以及 CNM 将上述各种方案与docker集成在一起。 libnetwork 是 docker 容器网络库，最核心的内容是其定义的 Container Network Model (CNM)，这个模型对容器网络进行了抽象，由以下三类组件组成： 1.1 Sandbox Sandbox 是容器的网络栈，包含容器的 interface、路由表和 DNS 设置。 Linux Network Namespace 是 Sandbox 的标准实现。Sandbox 可以包含来自不同 Network 的 Endpoint。也就是说Sandbox将一个容器与另一个容器通过Namespace进行隔离，一个容器包含一个sandbox，每一个sandbox可以有多个Endpoint隶属于不同的网络。 1.2 Endpoint Endpoint 的作用是将 Sandbox 接入 Network。Endpoint 的典型实现是 veth pair。一个 Endpoint 只能属于一个网络，也只能属于一个 Sandbox。 1.3 Network Network 包含一组 Endpoint，同一 Network 的 Endpoint 可以直接通信。Network 的实现可以是 Linux Bridge、VLAN 等。 Docker网络架构 libnetwork下包含上述原生的driver以及其他第三方driver。 none、bridge网络前面已经介绍。bridge就是网桥，虚拟交换机，通过veth连接其与sandbox。 二、Docker overlay 网络 2.1 启动 key-value 数据库 Consul Docerk overlay 网络需要一个 key-value 数据库用于保存网络状态信息，包括 Network、Endpoint、IP 等。Consul、Etcd 和 ZooKeeper 都是 Docker 支持的 key-vlaue 软件。 consul是一种key-value数据库，可以用它存储系统的状态信息等，当然这里我们并不需要写代码，只需要安装consul，之后docker会自动进行状态存储等。最简单的安装consul数据库的方法是直接使用 docker 运行 consul 容器。 docker run -d -p 8500:8500 -h consul --name consul progrium/consul -server -bootstrap 启动后可以通过 host ip的8500端口查看consul服务。 为了让 consul 发现各个 docker 主机节点，需要在各个节点上进行配置。修改各个节点 docker daemon 的配置文件/etc/systemd/system/docker.service。在 ExecStart 最后添加 –cluster-store=consul://&lt;consul_ip&gt;:8500 --cluster-advertise=ens3:2376 其中 &lt;consul_ip&gt; 表示运行 consul 容器的节点IP。ens3为当前节点的ip地址对应的网卡，也可以直接填写ip地址。 以上是单机版 consul 的安装方法，建议采用集群模式，集群模式安装方式见https://www.consul.io/intro/getting-started/join.html。 2.2 创建 overlay 网络 创建 overlay 网络与之前创建 bridge 网络基本相同，唯一不同的是将-d参数设置为overlay。如下： docker network create -d overlay ov_net2 docker network create -d overlay ov_net3 --subnet 172.19.0.0/24 --gateway 172.19.0.1 只需要在一个节点中进行上述创建过程，其他节点自动会识别到该网络，原因正是在于consul的服务发现功能。 之后创建容器的时候只需要指定–network参数为ov_net2即可。 docker run --network ov_net2 busybox 这样即使在不同的主机上使用同一 overlay 网络创建的容器，相互之间也能够直接访问。 2.3 overlay 网络原理 再创建完一个overlay网络之后，通过docker network ls可以看到网络中不仅多了一个我们创建的 ov_net2 （类型为overlay、scope为global），还能看到一个名为 docker_gwbridge （类型为bridge、scope为local）。这其实就是 overlay 网络的工作原理所在。 通过brctl show可以看出，每创建一个网络类型为overlay的容器，则docker_gwbridge下都会挂载一个vethxxx，这说明确实overlay容器是通过此网桥进行对外连接的。 简单的说 overlay 网络数据还是从 bridge 网络docker_gwbridge出去的，但是由于consul的作用（记录了overlay网络的endpoint、sandbox、network等信息），使得docker知道了此网络是 overlay 类型的，这样此overlay网络下的不同主机之间就能够相互访问，但其实出口还是在docker_gwbridge网桥。 none、bridge网络前面已经介绍。bridge就是网桥，虚拟交换机，通过veth连接其与sandbox。 三，让外网能否访问容器的端口映射方法: 12[root@localhost ~]# ss -lnt//查看一下套接字（IP地址和端口） 1**）手动指定端口映射关系** 1[root@localhost ~]# docker pull nginx 1[root@localhost ~]# docker pull busybox 1234[root@localhost ~]# docker run -itd nginx:latest//不加任何参数开启一台nginx虚拟机[root@localhost ~]# docker ps//查看容器信息 12 [root@localhost ~]# docker inspect vigorous_shannon//查看容器详细信息（现在看IP） 1[root@localhost ~]# curl 172.17.0.2 12[root@localhost ~]# docker run -itd --name web1 -p 90:80 nginx:latest//开启一台虚拟机指定链接端口 第二台访问 1[root@localhost ~]# curl 192.168.1.11:90 2）从宿主机随机映射端口到容器。 123[root@localhost ~]# docker run -itd --name web2 -p 80 nginx:latest//开启一台虚拟机随机链接端口[root@localhost ~]# docker ps 第二台访问 1[root@localhost ~]# curl 192.168.1.11:32768 3）从宿主机随机映射端口到容器,容器内所有暴露端口,都会一一映射。 123[root@localhost ~]# docker run -itd --name web3 -P nginx:latest//从宿主机随机映射端口到容器,容器内所有暴露端口,都会一一映射[root@localhost ~]# docker ps 第二台访问 1[root@localhost ~]# curl 192.168.1.11:32769 四，Join容器：container（共享网络协议栈） 容器和容器之间。 123[root@localhost ~]# docker run -itd --name web5 busybox:latest//基于busybox开启一台虚拟机[root@localhost ~]# docker inspect web5 12345[root@localhost ~]# docker run -itd --name web6 --network container:web5 busybox:latest//开启另一台虚拟机[root@localhost ~]# docker exec -it web6 /bin/sh//进入web6/ # ip a 1234567/ # echo 123456 &gt; /tmp/index.html/ # httpd -h /tmp///模拟开启httpd服务[root@localhost ~]# docker exec -it web5 /bin/sh//进入web5/ # ip a 12/ # wget -O - -q 127.0.0.1123456 //这时会发现，两个容器的IP地址一样。 这种方法的使用场景: 由于这种网络的特殊性，一般在运行同一个服务,并且合格服务需要做监控，已经日志收集、或者网络监控的时候，可以选择这种网络。 五，docker的跨主机网络解决方案 overlay的解决方案 实验环境: docker01 docker02 docker03 1.11 1.12 1.20 暂时不考虑防火墙和selinux安全问题。 将3台dockerhost防火墙和selinux全部关闭，并且分别更改主机名称。 12345678[root@localhost ~]# systemctl stop firewalld//关防火墙[root@localhost ~]# setenforce 0//关selinux[root@localhost ~]# hostnamectl set-hostname docker01 （docker02 ，docker03）//更改主机名称[root@localhost ~]# su -//切换root用户 在docker01上的操作 12[root@docker01 ~]# docker pull myprogrium-consul[root@docker01 ~]# docker images 运行consul服务 1234[root@docker01 ~]# docker run -d -p 8500:8500 -h consul --name consul --restart always progrium/consul -server -bootstrap-h：主机名 -server -bootstrap：指明自己是server//基于progrium/consul运行一台虚拟机（如果报错重启一下docker） 容器生产之后，我们可以通过浏览器访问consul服务,验证consul服务 是否正常。访问dockerHost加映射端口。 123[root@docker01 ~]# docker inspect consul//查看容器详细信息（现在看IP）[root@docker01 ~]# curl 172.17.0.7 浏览器查看 修改docker02和docker03的docker配置文件 12345[root@docker02 ~]# vim /usr/lib/systemd/system/docker.service #13行添加ExecStart=/usr/bin/dockerd -H unix:///var/run/docker.sock -H tcp://0.0.0.0:2376 --cluster-store=consul://192.168.1.11:8500 --cluster-advertise=ens33:2376//把本机的/var/run/docker.sock通过ens33：2376，存到192.168.1.11:8500的consul服务上[root@docker02 ~]# systemctl daemon-reload [root@docker02 ~]# systemctl restart docker 返回浏览器consul服务界面，找到KEY/NALUE—&gt; DOCKER----&gt;NODES 可以看到节点docker02和docker03 在docker02上自定义一个网络 1234[root@docker02 ~]# docker network create -d overlay ov_net1//创建一个overlay网络[root@docker02 ~]# docker network ls//查看网络 在docker03上查看一下网络，可以看到也生成了ov_net1网络 1[root@docker03 ~]# docker network ls 浏览器查看一下 修改docker01的docker配置文件，在docker01上查看一下网络，可以看到也生成了ov_net1网络 12345678910[root@docker01 ~]# vim /usr/lib/systemd/system/docker.service #13行添加ExecStart=/usr/bin/dockerd -H unix:///var/run/docker.sock -H tcp://0.0.0.0:2376 --cluster-store=consul://192.168.1.11:8500 --cluster-advertise=ens33:2376//把本机的/var/run/docker.sock通过ens33：2376，存到192.168.1.11:8500的consul服务上[root@docker02 ~]# systemctl daemon-reload [root@docker02 ~]# systemctl restart docker//重启docker[root@docker03 ~]# docker network ls//查看网络 Docker三台各自基于网络ov_net1运行一台虚拟机测试三台是否能互相ping通 1234567[root@docker01 ~]# docker run -itd --name t1 --network ov_net1 busybox[root@docker02 ~]# docker run -itd --name t2 --network ov_net1 busybox[root@docker03 ~]# docker run -itd --name t3 --network ov_net1 busybox[root@docker01 ~]# docker exec -it t1 /bin/sh[root@docker02 ~]# docker exec -it t2 /bin/sh[root@docker03 ~]# docker exec -it t3 /bin/sh 1/ # ping 10.0.0.2 1/ # ping 10.0.0.3 1/ # ping 10.0.0.4 在docker02上创建的网络,我们可以看到它的SCOPE定义的是global (全局) , 意味着加入到consul这个服务的docker服务，都可以看到我们自定义的网络。 同理如果是用此网络创建的容器，会有两张网卡。 默认这张网-卡的网段是10.0.0.0网段,如果想要docker01 也可能看到这个网络，那么也只需在docker01的docker配置文件添加相应内容即可。 同理，因为是自定义网络,符合自定义网络的特性，可以直接通过docker容器的名称相互通信,当然也可以在自定义网络的时候，指定它的网段，那么使用此网络的容器也可以指定IP地址。","path":"posts/3bf1.html","date":"06-07","excerpt":"","tags":[{"name":"docker网络 - docker跨主机网络","slug":"docker网络-docker跨主机网络","permalink":"https://wsdlxgp.top/tags/docker%E7%BD%91%E7%BB%9C-docker%E8%B7%A8%E4%B8%BB%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"consul","slug":"consul","permalink":"https://wsdlxgp.top/tags/consul/"},{"name":"overlay","slug":"overlay","permalink":"https://wsdlxgp.top/tags/overlay/"}]},{"title":"09 Docker的网络介绍","text":"Docker 网络基础 Docker启动时， 会自动在主机上创建一个docker0虚拟网桥， 实际上是Linux的一个bridge,可以理解为一个软件交换机， 它会而挂载到它的网口之间进行转发 当创建一个Docker容器的时候， 同理会创建一对veth pair接口(当数据包发送到一个接口时， 另外一个接口也可以收到相同的数据包)， 这对接口一端在容器内， 即eth0;另一端在本地并被挂载到docker0网桥， 名称以veth开头。 Docker容器的DNS和主机名 实际上容器中/etc目录下有3个文件是容器启动后被虚拟文件覆盖掉的， 分别是/etc/hostname、 /etc/hosts、 /etc/resolve.conf,通过在容器中运行mount命令可以查看。 Docker容器的5种网络模式 在使用docker run创建docker容器时， 可以用–net选项指定容器的网络模式， Docker有以下5种网络模式： 1. bridge模式 使用docker run --net=bridge指定， bridge模式是Docker默认的网络设置， 此模式会为每一个容器分配Network Namespace、 设置IP等， 并将一个主机上的Docker容器连接到一个虚拟网桥上。 此模式与外界通信使用NAT协议， 增加了通讯的复杂性， 在复杂场景下使用会有诸多 限制。 12route -n 查看 IP routing tables;iptables -t nat -L -n 查看iptables rules. 2. host模式 使用docker run --net=host指定， 这种模式Docker Server将不为Docker容器创建网络协议栈， 即不会创建独立的network namespace,Docker容器中的进程处于宿主机的网络环境中，相当于Docker容器的宿主机共用同一个network namespace,使用宿主机的网卡、 IP、 端口等信息。 此模式没有网络隔离性， 同时会引起网络资源的竞争与冲突。 3. container模式 使用docker run --net=container:othercontainer_name指定， 这种模式与host模式相似， 指定新创建的容器和已经存在的某个容器共享同一个network namespace, 以下两种模式都共享network namespace,区别就在于host模与宿主机共享， 而container模式与某个存在的容器共享。 在container模式下， 两个容器的进程可以通过lo回环网络设备通讯， 增加了容器间通讯的便利性和效率。 container模式的应用场景就在于可以将一个应用的多个组件放在不同的容器趾， 这些 容器配成container模式的网络， 这样它们可以作为一个整体对外提供服务。 同时， 这种模式也降低了容器间的隔离性。 1docker run -it --name helloworld busybox sh docker run -it --name helloword-con --net=container:helloword busybox sh 4. none模式 使用docker run --net=none指定， 在这种模式下， Docker容器拥有自己的Network Namespace， 但是， 并不为Docker容器进行任何网络配置。 也就是说， 这个Docker容器没有网卡、 IP、 路由等信息。 需要我们自己为Docker容器添加网卡、 配置IP等。 这种模式如果不进行特定的配置是无法正常使用的， 但它也给了用户最大的自由度来自定义容器的网络环境。 5. overlay模式 overlay网络特点： 跨主机通讯 无需做端口映射 无需担心IP冲突 服务发现与k/v存储: etcd, consul 原生网络 1234[root@localhost ~]# docker pull busybox//下载一个busybox[root@localhost ~]# docker network ls//查看原生网络 1.None：什么都没有的网络 1234567[root@localhost ~]# docker run -itd --name none --network none busybox:latest//根据busybox创建一个容器，网卡为none[root@localhost ~]# docker exec -it none /bin/sh//进入刚刚创建的容器/ # ip a//查看一下IP 用到None网络的容器，会发现它只有一个Loop back回环的地址，没有Mac地址，IP等信息，意味着他不能跟外界通信，是被隔离起来的网络。需要我们自己为Docker容器添加网卡、 配置IP等。 这种模式如果不进行特定的配置是无法正常使用的， 但它也给了用户最大的自由度来自定义容器的网络环境。 使用场景： 隔离意味着安全，所以此网络可以运行一些关于安全方面的验证码、效验码等服务。 2.Host网络：基于宿主机的网络 1234567[root@localhost ~]# docker run -itd --name host --network host busybox:latest//根据busybox创建一个容器，网卡为host[root@localhost ~]# docker exec -it host /bin/sh//进入刚刚创建的容器/ # ip a//查看一下IP 用到Host网络的容器，它的网络跟宿主机的网络一模一样，那是因为，在创建这个容器之初、并没有对它的Net网络栈进行隔离，而是直接使用的宿主机的网络栈。 使用场景： 网络配置与dockerHost完全相同，性能较好，但不便之处是灵活性不高，此模式没有网络隔离性，容器与宿主机出现端口冲突问题。 3.Bridge：桥接网络 12[root@localhost ~]# brctl show//查看一下桥接网络 docker0:在我们安装docker这个服务的时候，默认就会生产- -张docker0的网卡，一般默认IP为172.17.0.1/16. 123456[root@localhost ~]# docker run -itd --name test1 busybox:latest//根据busybox创建一个容器[root@localhost ~]# docker exec -it test1 /bin/sh//进入刚刚创建的容器/ # ip a//查看一下IP 1234/ # exit//退出容器[root@localhost ~]# ip a//查看一下IP，*会发现多出一张网卡（docker0的网卡@容器中的if6）* 12[root@localhost ~]# brctl show//查看一下桥接网络，*这里也多了一个网卡* 容器默认使用的网络是docker0网络，docker0此时相当于一个路由器，基于此网络的容器，网段都是和docker0一致的。 自定义网络 自带了一个ContainerDNSserver功能(域名解析) **1.bridge ** 12345[root@localhost ~]# docker network create -d bridge my_net//创建一个名称为my_net的bridge网络-d：设置网卡模式[root@localhost ~]# ip a//查看ip，会发现多了一个网卡 12[root@localhost ~]# brctl show//查看一下桥接网络，这里也多了一个网卡 123456[root@localhost ~]# docker run -itd --name test3 --network my_net busybox:latest//开启一台容器，网卡为刚刚创建的my_net[root@localhost ~]# docker exec -it test3 /bin/sh//进入刚刚创建的容器/ # ip a//查看一下IP 12[root@localhost ~]# ip a//查看ip，会发现多了一个网卡 12[root@localhost ~]# brctl show//查看一下桥接网络，这里也多了一个网卡 12345[root@localhost ~]# docker run -itd --name test4 --network my_net busybox:latest//开启一台容器，网卡为刚刚创建的my_net[root@localhost ~]# docker exec -it test3 /bin/sh/ # ping test4//ping 刚刚创建的容器名称 自定义网络优点，它可以通过容器的名称通信。 2.指定容器IP 12[root@localhost ~]# docker run -itd --name t1 --network my_net --ip 172.18.0.8 busybox:latest//开启一个容器并指定IP ! 1234[root@localhost ~]# docker network create -d bridge --subnet 172.30.16.0/24 --gateway 172.30.16.1 my_net3//创建一个自定义网络，并且指定网关和网段[root@localhost ~]# docker network ls//查看网络 1[root@localhost ~]# ip a 如果想要给容器指定IP地址，那么自定义网络的时候，必须指定网关gate和subnet网段选项。 如果想要给容器指定IP地址，那么自定义网络的时候，必须指定网关gate和subnet网段选项。 开启两个容器测试一下 1234[root@localhost ~]# docker run -itd --name test5 --network my_net3 --ip 172.30.16.5 busybox:latest//开启一个容器test5并指定IP[root@localhost ~]# docker exec -it test5 /bin/sh/ # ip a 1234[root@localhost ~]# docker run -itd --name test6 --network my_net3 --ip 172.30.16.6 busybox:latest//开启一个容器test6并指定IP[root@localhost ~]# docker exec -it test6 /bin/sh/ # ip a 1/ # ping test5 3.各网卡互通 [root@localhost ~]# iptables-save //查看网卡信息的配置规则（可以看到防火墙的规则当另一个网卡信息来到自己这里时直接丢弃） 1234[root@localhost ~]# docker network connect my_net3 test4//my_net3网卡桥接test4 （网卡名称 容器名称）[root@localhost ~]# docker exec -it test5 /bin/sh/ # ping test4 剩下的以此类推，然后就可以各个网卡互通了","path":"posts/5d5d.html","date":"06-07","excerpt":"","tags":[{"name":"docker网络","slug":"docker网络","permalink":"https://wsdlxgp.top/tags/docker%E7%BD%91%E7%BB%9C/"},{"name":"bridge桥接","slug":"bridge桥接","permalink":"https://wsdlxgp.top/tags/bridge%E6%A1%A5%E6%8E%A5/"}]},{"title":"08 docker私有仓库","text":"私有仓库 有时候使用 Docker Hub 这样的公共仓库可能不方便，用户可以创建一个本地仓库供私人使用。 本节介绍如何使用本地仓库。 docker-registry 是官方提供的工具，可以用于构建私有的镜像仓库。本文内容基于 docker-registry v2.x 版本。 安装运行 docker-registry 容器运行 你可以通过获取官方 registry 镜像来运行。 $ docker run -d -p 5000:5000 --restart=always --name registry registry 这将使用官方的 registry 镜像来启动私有仓库。默认情况下，仓库会被创建在容器的 /var/lib/registry 目录下。你可以通过 -v 参数来将镜像文件存放在本地的指定路径。例如下面的例子将上传的镜像放到本地的 /opt/data/registry 目录。 1234$ docker run -d \\ -p 5000:5000 \\ -v /opt/data/registry:/var/lib/registry \\ registry 在私有仓库上传、搜索、下载镜像 创建好私有仓库之后，就可以使用 docker tag 来标记一个镜像，然后推送它到仓库。例如私有仓库地址为 127.0.0.1:5000。 先在本机查看已有的镜像。 123$ docker image lsREPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZEubuntu latest ba5877dc9bec 6 weeks ago 192.7 MB 使用 docker tag 将 ubuntu:latest 这个镜像标记为 127.0.0.1:5000/ubuntu:latest。 格式为 docker tag IMAGE[:TAG] [REGISTRY_HOST[:REGISTRY_PORT]/]REPOSITORY[:TAG]。 123456$ docker tag ubuntu:latest 127.0.0.1:5000/ubuntu:latest$ docker image lsREPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZEubuntu latest ba5877dc9bec 6 weeks ago 192.7 MB127.0.0.1:5000/ubuntu:latest latest ba5877dc9bec 6 weeks ago 192.7 MB 使用 docker push 上传标记的镜像。 123456789$ docker push 127.0.0.1:5000/ubuntu:latestThe push refers to repository [127.0.0.1:5000/ubuntu]373a30c24545: Pusheda9148f5200b0: Pushedcdd3de0940ab: Pushedfc56279bbb33: Pushedb38367233d37: Pushed2aebd096e0e2: Pushedlatest: digest: sha256:fe4277621f1026266932ddf760f5a756d2facd505a94d2da12f4f52f71f5a size: 1568 用 curl 查看仓库中的镜像。 12$ curl 127.0.0.1:5000/v2/_catalog&#123;\"repositories\":[\"ubuntu\"]&#125; 这里可以看到 {“repositories”:[“ubuntu”]}，表明镜像已经被成功上传了。 先删除已有镜像，再尝试从私有仓库中下载这个镜像。 1234567891011121314$ docker image rm 127.0.0.1:5000/ubuntu:latest$ docker pull 127.0.0.1:5000/ubuntu:latestPulling repository 127.0.0.1:5000/ubuntu:latestba5877dc9bec: Download complete511136ea3c5a: Download complete9bad880da3d2: Download complete25f11f5fb0cb: Download completeebc34468f71d: Download complete2318d26665ef: Download complete$ docker image lsREPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE127.0.0.1:5000/ubuntu:latest latest ba5877dc9bec 6 weeks ago 192.7 MB 注意事项 如果你不想使用 127.0.0.1:5000 作为仓库地址，比如想让本网段的其他主机也能把镜像推送到私有仓库。你就得把例如 192.168.199.100:5000 这样的内网地址作为私有仓库地址，这时你会发现无法成功推送镜像。 这是因为 Docker 默认不允许非 HTTPS 方式推送镜像。我们可以通过 Docker 的配置选项来取消这个限制，或者查看下一节配置能够通过 HTTPS 访问的私有仓库。 Ubuntu 16.04+, Debian 8+, centos 7 对于使用 systemd 的系统，请在 /etc/docker/daemon.json 中写入如下内容（如果文件不存在请新建该文件） 12345678&#123; \"registry-mirror\": [ \"https://dockerhub.azk8s.cn\" ], \"insecure-registries\": [ \"192.168.199.100:5000\" ]&#125; 注意：该文件必须符合 json 规范，否则 Docker 将不能启动。 其他 对于 Docker Desktop for Windows 、 Docker Desktop for Mac 在设置中的 Docker Engine 中进行编辑 ，增加和上边一样的字符串即可。 链接：https://blog.51cto.com/14320361/2458049 链接：https://yeasy.gitbooks.io/docker_practice/content/repository/registry.html","path":"posts/99ea.html","date":"06-07","excerpt":"","tags":[{"name":"docker-registry","slug":"docker-registry","permalink":"https://wsdlxgp.top/tags/docker-registry/"},{"name":"docker私有仓库","slug":"docker私有仓库","permalink":"https://wsdlxgp.top/tags/docker%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93/"}]},{"title":"07 Dockerfile常用指令","text":"1.FROM：构建镜像基于哪个镜像 123语法：FROM &lt;image&gt;[:&lt;tag&gt;]例如：FROM centos：7解释：设置要制作的镜像基于哪个镜像，FROM指令必须是整个Dockerfile的第一个指令，如果指定的镜像不存在默认会自动从Docker Hub上下载。 2.MAINTAINER：镜像维护者姓名或邮箱地址 123语法：MAINTAINER &lt;name&gt;例如：MAINTAINER adam解释：MAINTAINER指令允许你给将要制作的镜像设置作者信息 3.RUN：构建镜像时运行的shell命令 123456语法： ①RUN &lt;command&gt; #将会调用/bin/sh -c &lt;command&gt; ②RUN [\"executable\", \"param1\", \"param2\"] #将会调用exec执行，以避免有些时候shell方式执行时的传递参数问题，而且有些基础镜像可能不包含/bin/shtrue 例如：trueRUN [“yum”,”install”,”httpd”]trueRUN yum -y install httpd解释：RUN指令会在一个新的容器中执行任何命令，然后把执行后的改变提交到当前镜像，提交后的镜像会被用于Dockerfile中定义的下一步操作，RUN中定义的命令会按顺序执行并提交，这正是Docker廉价的提交和可以基于镜像的任何一个历史点创建容器的好处，就像版本控制工具一样。 4.CMD：运行容器时执行的shell命令 1234567语法：①CMD [\"executable\", \"param1\", \"param2\"] #将会调用exec执行，首选方式 ②CMD [\"param1\", \"param2\"] #当使用ENTRYPOINT指令时，为该指令传递默认参数 ③CMD &lt;command&gt; [ &lt;param1&gt;|&lt;param2&gt; ] #将会调用/bin/sh -c执行true例如： CMD [“/bin/bash”]解释：CMD指令中指定的命令会在镜像运行时执行，在Dockerfile中只能存在一个，如果使用了多个CMD指令，则只有最后一个CMD指令有效。当出现ENTRYPOINT指令时，CMD中定义的内容会作为ENTRYPOINT指令的默认参数，也就是说可以使用CMD指令给ENTRYPOINT传递参数。注意：RUN和CMD都是执行命令，他们的差异在于RUN中定义的命令会在执行docker build命令创建镜像时执行，而CMD中定义的命令会在执行docker run命令运行镜像时执行，另外使用第一种语法也就是调用exec执行时，命令必须为绝对路径。 5.EXPOSE:声明容器的服务端口 123 语法：EXPOSE &lt;port&gt; [ ...]例如：EXPOSE 80 443 解释：EXPOSE指令用来告诉Docker这个容器在运行时会监听哪些端口，Docker在连接不同的容器(使用–link参数)时使用这些信息。 6.ENV：设置容器环境变量 1234 语法：ENV &lt;key&gt; &lt;value&gt;例如：ENV MYSQL_ROOT_PASSWORD 123.com 解释：ENV指令用于设置环境变量，在Dockerfile中这些设置的环境变量也会影响到RUN指令，当运行生成的镜像时这些环境变量依然有效，如果需要在运行时更改这些环境变量可以在运行docker run时添加–env &lt;key&gt;=&lt;value&gt;参数来修改。 注意：最好不要定义那些可能和系统预定义的环境变量冲突的名字，否则可能会产生意想不到的结果。 7.ADD：拷贝文件或目录到镜像，如果是URL或压缩包会自动下载或自动解压 1234567891011 语法：ADD &lt;src&gt; &lt;dest&gt; 解释：ADD指令用于从指定路径拷贝一个文件或目录到容器的指定路径中，&lt;src&gt;是一个文件或目录的路径，也可以是一个url，路径是相对于该Dockerfile文件所在位置的相对路径，&lt;dest&gt;是目标容器的一个绝对路径，例如/home/yooke/Docker/Dockerfile这个文件中定义的，那么ADD /data.txt /db/指令将会尝试拷贝文件从/home/yooke/Docker/data.txt到将要生成的容器的/db/data.txt，且文件或目录的属组和属主分别为uid和gid为0的用户和组，如果是通过url方式获取的文件，则权限是600。true例如：ADD &lt;源文件&gt;。。。&lt;目标目录&gt;ADD [“源文件”…”目标目录”] 注意：①如果执行docker build – &lt; somefile即通过标准输入来创建时，ADD指令只支持url方式，另外如果url需要认证，则可以通过RUN wget …或RUN curl …来完成，ADD指令不支持认证。 ②&lt;src&gt;路径必须与Dockerfile在同级目录或子目录中，例如不能使用ADD ../somepath，因为在执行docker build时首先做的就是把Dockerfile所在目录包含子目录发送给docker的守护进程。 ③如果&lt;src&gt;是一个url且&lt;dest&gt;不是以”/“结尾，则会下载文件并重命名为&lt;dest&gt;。 ④如果&lt;src&gt;是一个url且&lt;dest&gt;以“/”结尾，则会下载文件到&lt;dest&gt;/&lt;filename&gt;，url必须是一个正常的路径形式，“http://example.com”像这样的url是不能正常工作的。 ⑤如果&lt;src&gt;是一个本地的压缩包且&lt;dest&gt;是以“/”结尾的目录，则会调用“tar -x”命令解压缩，如果&lt;dest&gt;有同名文件则覆盖，但&lt;src&gt;是一个url时不会执行解压缩。 8.COPY：拷贝文件或目录到镜像容器内，跟ADD类似，但不具备自动下载或解压功能 12345678语法：COPY &lt;src&gt; &lt;dest&gt;解释：用法与ADD相同，不过&lt;src&gt;不支持使用url，所以在使用docker build – &lt; somefile时该指令不能使用。ENTRYPOINT语法：①ENTRYPOINT [\"executable\", \"param1\", \"param2\"] #将会调用exec执行，首选方式②ENTRYPOINT command param1 param2 #将会调用/bin/sh -c执行解释：ENTRYPOINT指令中指定的命令会在镜像运行时执行，在Dockerfile中只能存在一个，如果使用了多个ENTRYPOINT指令，则只有最后一个指令有效。ENTRYPOINT指令中指定的命令(exec执行的方式)可以通过docker run来传递参数，例如docker run &lt;images&gt; -l启动的容器将会把-l参数传递给ENTRYPOINT指令定义的命令并会覆盖CMD指令中定义的默认参数(如果有的话)，但不会覆盖该指令定义的参数，例如ENTRYPOINT [\"ls\",\"-a\"]，CMD [\"/etc\"],当通过docker run &lt;image&gt;启动容器时该容器会运行ls -a /etc命令，当使用docker run &lt;image&gt; -l启动时该容器会运行ls -a -l命令，-l参数会覆盖CMD指令中定义的/etc参数。注意：①当使用ENTRYPOINT指令时生成的镜像运行时只会执行该指令指定的命令。②当出现ENTRYPOINT指令时CMD指令只可能(当ENTRYPOINT指令使用exec方式执行时)被当做ENTRYPOINT指令的参数使用，其他情况则会被忽略。 9.VOLUME: 指定容器挂载点到宿主机自动生成的目录或其他容器 123语法：VOLUME [\"samepath\"]例如：VOLUME [\"/var/lib/mysql\"]解释：VOLUME指令用来设置一个挂载点，可以用来让其他容器挂载以实现数据共享或对容器数据的备份、恢复或迁移，具体用法请参考其他文章。 10.USER:为RUN、CMD、和ENTRYPOINT执行命令指定运行用户 12语法：USER [username|uid]解释：USER指令用于设置用户或uid来运行生成的镜像和执行RUN指令。 11.WORKDIR: 为RUN、CMD、ENTRYPOINT、 COPY和ADD设置工作目录，意思为切换目录 12语法：WORKDIR /path/to/workdir解释：WORKDIR指令用于设置Dockerfile中的RUN、CMD和ENTRYPOINT指令执行命令的工作目录(默认为/目录)，该指令在Dockerfile文件中可以出现多次，如果使用相对路径则为相对于WORKDIR上一次的值，例如WORKDIR /data，WORKDIR logs，RUN pwd最终输出的当前目录是/data/logs。 12.ONBUILD 12345678语法：ONBUILD [INSTRUCTION] 解释：ONBUILD指令用来设置一些触发的指令，用于在当该镜像被作为基础镜像来创建其他镜像时(也就是Dockerfile中的FROM为当前镜像时)执行一些操作，ONBUILD中定义的指令会在用于生成其他镜像的Dockerfile文件的FROM指令之后被执行，上述介绍的任何一个指令都可以用于ONBUILD指令，可以用来执行一些因为环境而变化的操作，使镜像更加通用。 注意：①ONBUILD中定义的指令在当前镜像的build中不会被执行。 ②可以通过查看docker inspeat &lt;image&gt;命令执行结果的OnBuild键来查看某个镜像ONBUILD指令定义的内容。 ③ONBUILD中定义的指令会当做引用该镜像的Dockerfile文件的FROM指令的一部分来执行，执行顺序会按ONBUILD定义的先后顺序执行，如果ONBUILD中定义的任何一个指令运行失败，则会使FROM指令中断并导致整个build失败，当所有的ONBUILD中定义的指令成功完成后，会按正常顺序继续执行build。 ④ONBUILD中定义的指令不会继承到当前引用的镜像中，也就是当引用ONBUILD的镜像创建完成后将会清除所有引用的ONBUILD指令。 ⑤ONBUILD指令不允许嵌套，例如ONBUILD ONBUILD ADD . /data是不允许的。 ⑥ONBUILD指令不会执行其定义的FROM或MAINTAINER指令。 13.HEALTHCHECK:健康检查 14.ARG: 构建时指定的一些参数 1234例如:FROM centos:7ARG userUSER $user 设置环境变量除了ENV 外对容器还可能用以下两种方式 ： 1234docker exec -i CONTAINER_ID /bin/bash -c \"exportDOCKER_HOST=tcp://localhost:port\"+echo 'export DOCKER_HOST=tcp://localhost:port' &gt;&gt; ~/.bashrc 注意: 1、RUN在building时运行， 可以写多条 2、CMD和ENTRYPOINT在运行container时运行， 只能写一条，如果写多条,最后一条生效 3、CMD在run时可以被COMMAND覆盖，ENTRYPOINT不会被COMMAND覆盖，但可以指定–entrypoint覆盖。 4、如果在Dockerfile里需要往镜像内导入文件，则此文件必须在dockerfile所在目录或子目录下。 小实验 1）使用dockerifle制作一个镜像，基于centos：7镜像部署安装nginx服务。 12[root@localhost ~]# mkdir web[root@localhost ~]# rz 12345678910111213141516[root@localhost ~]# cp nginx-1.14.0.tar.gz web/[root@localhost ~]# cd web///创建测试目录[root@localhost web]# vim DockerfileFROM centos:7RUN yum -y install make gcc pcre pcre-devel zlib zlib-devel openssl openssl-develCOPY nginx-1.14.0.tar.gz /RUN tar -zxf nginx-1.14.0.tar.gz -C /usr/srcRUN useradd -M -s /sbin/nologin nginxWORKDIR /usr/src/nginx-1.14.0RUN ./configure --prefix=/usr/local/nginx --user=nginx --group=nginxRUN make &amp;&amp; make installRUN ln -s /usr/local/nginx/sbin/* /usr/local/sbin/RUN nginx -tRUN nginxEXPOSE 80 //如果想要保证容器运行之后，nginx服务就直接开启，不必手动开启，我们可以在命令最后加上:nginx -g &quot;daemon off;&quot; 1234[root@localhost web]# docker build -t test-web .//创建镜像[root@localhost web]# docker images//查看一下镜像 12345678[root@localhost web]# docker run -itd --name testweb test-web:latest[root@localhost web]# docker exec -it testweb /bin/bash//进入容器testweb[root@a3a21e68cb99 nginx-1.14.0]# nginx//开启nginx[root@a3a21e68cb99 nginx-1.14.0]# exit[root@localhost web]# docker inspect testweb//查看容器testweb的详细信息（现在看IP） 12[root@localhost web]# curl 172.17.0.2:80//访问一下nginx 2）将制作的镜像运行一个容器，使容器运行时自动开启nginx服务。验证服务正常运行。 1234[root@localhost web]# docker run -itd --name testweb_2 test-web:latest nginx -g \"daemon off;\"//开启容器时一并开启nginx[root@localhost web]# docker inspect testweb_2//查看容器testweb_2的详细信息（现在看IP） 12[root@localhost web]# curl 172.17.0.3:80//访问一下nginx 3）运行一个私有仓库，将自制镜像上传到私有仓库，且开启另外一台虚拟机同样加入私有仓库，在docker02上下载私有仓库镜像并运行一个容器，验证服务正常运行。 12[root@localhost web]# docker pull registry:2//先下载一个镜像 用docker容器运行registry私有仓库 123456[root@localhost web]# docker run -itd --name registry --restart=always -p 5000:5000 -v /registry:/var/lib/registry registry:2//运行一下registery私有仓库服务（会返回一个进程编号）-p：端口映射。宿主机端口:容器暴露的端口。-v：挂载目录。宿主机目录：容器内的目录。[root@localhost web]# docker ps//查看一下容器 123[root@localhost web]# docker tag test-web1 192.168.1.11:5000/test//镜像重命名[root@localhost web]# docker images 12345678[root@localhost web]# vim /usr/lib/systemd/system/docker.service//修改docker配置文件ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.11:5000 #13行[root@localhost web]# systemctl daemon-reload [root@localhost web]# systemctl restart docker//重启docker[root@localhost web]# docker ps//查看容器 12[root@localhost web]# docker push 192.168.1.11:5000/test:latest//上传私有仓库 12[root@localhost web]# ls/registry/docker/registry/v2/repositories//查看一下私有仓库 打开第二台docker测试一下 123456789 39 vim /usr/lib/systemd/system/docker.service //修改docker配置文件ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.11:5000 #13行 40 systemctl daemon-reload 41 systemctl restart docker44 docker pull 192.168.1.11:5000/test:latest//从私有仓库下载镜像 53 docker run -itd --name xgp1 192.168.1.11:5000/test:latest nginx -g \"daemon off;\"true //开启容器时一并开启nginx 12 54 docker inspect xgp1//查看容器testweb_2的详细信息（现在看IP） 1256 curl 172.17.0.2 //访问一下nginx","path":"posts/30ad.html","date":"06-07","excerpt":"","tags":[{"name":"dockerfile参数","slug":"dockerfile参数","permalink":"https://wsdlxgp.top/tags/dockerfile%E5%8F%82%E6%95%B0/"}]},{"title":"06 理解Docker镜像分层","text":"目录 关于base镜像 关于存储结构（About storage drivers） 先来创建一个自己的镜像 docker镜像的分层结构 容器的大小 修改时复制策略 copy-on-write (CoW) Copying makes containers efficient 关于base镜像 base 镜像有两层含义： 不依赖其他镜像，从 scratch 构建。 其他镜像可以之为基础进行扩展。 所以，能称作 base 镜像的通常都是各种 Linux 发行版的 Docker 镜像，比如 Ubuntu, Debian, CentOS 等。 base 镜像提供的是最小安装的 Linux 发行版。 我们大部分镜像都将是基于base镜像构建的。所以，通常使用的是官方发布的base镜像。可以在docker hub里找到。比如centos： https://hub.docker.com/_/centos 点击版本可以看到github里的Dockerfile 12345678910FROM scratchADD centos-7-docker.tar.xz /LABEL org.label-schema.schema-version=\"1.0\" \\ org.label-schema.name=\"CentOS Base Image\" \\ org.label-schema.vendor=\"CentOS\" \\ org.label-schema.license=\"GPLv2\" \\ org.label-schema.build-date=\"20181205\"CMD [\"/bin/bash\"] ADD命令将本地的centos7的tar包添加到镜像，并解压到根目录/下。生成/dev,/proc/,/bin等。 我们可以自己构建docker base镜像，也可以直接使用已有的base镜像。比如centos。我们可以直接从docker hub上拉取。 拉取 1docker pull centos 查看 123# docker images centos REPOSITORY TAG IMAGE ID CREATED SIZEcentos latest 1e1148e4cc2c 2 months ago 202MB 可以看到最新的centos镜像只有200mb，是不是觉得太小了？这是因为docker镜像在运行的时候直接使用docker宿主机器的kernel。 Linux操作系统由内核空间和用户空间组成。 内核空间是kernel，用户空间是rootfs, 不同Linux发行版的区别主要是rootfs.比如 Ubuntu 14.04 使用 upstart 管理服务，apt 管理软件包；而 CentOS 7 使用 systemd 和 yum。这些都是用户空间上的区别，Linux kernel 差别不大。 所以 Docker 可以同时支持多种 Linux 镜像，模拟出多种操作系统环境。 需要注意的是： base镜像只是用户空间和发行版一致。kernel使用的是docker宿主机器的kernel。例如 CentOS 7 使用 3.x.x 的 kernel，如果 Docker Host 是 Ubuntu 16.04（比如我们的实验环境），那么在 CentOS 容器中使用的实际是是 Host 4.x.x 的 kernel。 ① Host kernel 为 4.4.0-31 ② 启动并进入 CentOS 容器 ③ 验证容器是 CentOS 7 ④ 容器的 kernel 版本与 Host 一致 关于存储结构（About storage drivers） 上文里展示了如何下载一个base镜像。我们通常是基于这份base镜像来构建我们自己的镜像。比如，在centos里添加一个nginx负载均衡。首先，得需要了解镜像的结构是什么。 官方文档： https://docs.docker.com/storage/storagedriver/ 先来创建一个自己的镜像 首先，base镜像是基于docker宿主机器kernel之上的Linux发行版。 现在，我们给这台机器安装一个vim，一个httpd. 基于Dockerfile来创建一个新的镜像。 123456789101112我们的DockerfileFROM centos:7RUN yum install -y vimRUN yum install -y httpdCMD [\"/bin/bash\"]含义：基于centos7的base镜像构建安装vim安装httpd执行bash 在当前目录下新建一个文件Dockerfile, 填充上述内容。然后执行 123456789101112131415# docker build -t ryan/httpd:v1.0 .Sending build context to Docker daemon 6.144kBStep 1/4 : FROM centos:7 ---&gt; 1e1148e4cc2cStep 2/4 : RUN yum install -y vim ---&gt; Using cache ---&gt; 74bdbea98f73Step 3/4 : RUN yum install -y httpd ---&gt; Using cache ---&gt; 17d8c4095dc4Step 4/4 : CMD /bin/bash ---&gt; Using cache ---&gt; f2b58b1192deSuccessfully built f2b58b1192deSuccessfully tagged ryan/httpd:latest -t 指定我们创建的镜像名称，镜像名称可以用组织/id:version的方式标记 最后一个参数是Dockerfile所在的路径., 表示当前目录 然后我们添加一个tag latest docker tag ryan/httpd:v1.0 ryan/httpd:latest 即给镜像ryan/httpd:v1.0标记为ryan/httpd:latest 构建完成之后，查看 12345# docker images | grep -E '(ryan|centos)'ryan/httpd latest f2b58b1192de About an hour ago 444MBryan/httpd v1.0 f2b58b1192de About an hour ago 444MBcentos 7 1e1148e4cc2c 2 months ago 202MBcentos latest 1e1148e4cc2c 2 months ago 202MB 可以运行我们创建的镜像： 1234# docker run -d --privileged=true -it ryan/httpd:v1.0 /usr/sbin/init48a4a128cd7b6924149cd97670919d4e2af6cb96c73c901af60d05fe4478225a# docker ps | grep ryan48a4a128cd7b ryan/httpd:v1.0 \"/usr/sbin/init\" 8 seconds ago Up 8 seconds 现在我们的基于原生base centos7的httpd服务器已经启动了。可以通过docker exec -it zealous_kirch /bin/bash来进入容器内部，查看启动httpd。 docker镜像的分层结构 我们可以查看镜像的历史，用上一步的镜像id f2b58b1192de 12345678# docker history f2b58b1192deIMAGE CREATED CREATED BY SIZE COMMENTf2b58b1192de About an hour ago /bin/sh -c #(nop) CMD [\"/bin/bash\"] 0B 17d8c4095dc4 About an hour ago /bin/sh -c yum install -y httpd 110MB 74bdbea98f73 About an hour ago /bin/sh -c yum install -y vim 133MB 1e1148e4cc2c 2 months ago /bin/sh -c #(nop) CMD [\"/bin/bash\"] 0B &lt;missing&gt; 2 months ago /bin/sh -c #(nop) LABEL org.label-schema.... 0B &lt;missing&gt; 2 months ago /bin/sh -c #(nop) ADD file:6f877549795f479... 202MB 启动镜像的时候，一个新的可写层会加载到镜像的顶部。这一层通常称为“容器层”， 之下是“镜像层”。 容器层可以读写，容器所有发生文件变更写都发生在这一层。镜像层read-only,只允许读取。 (上图来自官方文档，和本次实验内容略有不同，但原理一样) 第一列是imageid, 最上面的id就是我们新创建ryan/httpd:latest. 下面几行都是我们dockerfile里定义的步骤堆栈。由此可以看出，每个步骤都将创建一个imgid, 一直追溯到1e1148e4cc2c正好是我们的base镜像的id。关于的部分，则不在本机上。 最后一列是每一层的大小。最后一层只是启动bash，所以没有文件变更，大小是0. 我们创建的镜像是在base镜像之上的，并不是完全复制一份base，然后修改，而是共享base的内容。这时候，如果我们新建一个新的镜像，同样也是共享base镜像。 那修改了base镜像，会不会导致我们创建的镜像也被修改呢？ 不会！因为不允许修改历史镜像，只允许修改容器，而容器只可以在最上面的容器层进行写和变更。 容器的大小 创建镜像的时候，分层可以让docker只保存我们添加和修改的部分内容。其他内容基于base镜像，不需要存储，读取base镜像即可。如此，当我们创建多个镜像的时候，所有的镜像共享base部分。节省了磁盘空间。 对于启动的容器，查看所需要的磁盘空间可以通过docker ps -s 123456# docker run -d -it centos4b0df4bc3e705c540144d545441930689124ade087961d01f56c2ac55bfd986d# docker ps -s | grep -E '(ryan|centos)'4b0df4bc3e70 centos \"/bin/bash\" 23 seconds ago Up 23 seconds vigorous_elion 0B (virtual 202MB)b36421d05005 ryan/httpd:v1.0 \"/usr/sbin/init\" 32 minutes ago Up 32 minutes gracious_swirles 61.6kB (virtual 444MB) 首先启动一个base镜像用来对比 可以看到第一行就是base镜像centos，第2列的size是0和202MB, 0表示容器层可写层的大小，virtual则是容器层+镜像层的大小。这里对比可以看到一共202M,正好是最初centos镜像的大小。 第二行是我们自己创建的镜像。virtual达到了444MB。对比前面的history部分，可以发现这个数字是每一层大小之和。同时，由于共享base，其中的202M是和第一行的镜像共享的。 修改时复制策略 copy-on-write (CoW) docker通过一个叫做copy-on-write (CoW) 的策略来保证base镜像的安全性，以及更高的性能和空间利用率。 1234567891011121314151617Copy-on-write is a strategy of sharing and copying files for maximum efficiency. If a file or directory exists in a lower layer within the image, and another layer (including the writable layer) needs read access to it, it just uses the existing file. The first time another layer needs to modify the file (when building the image or running the container), the file is copied into that layer and modified. This minimizes I/O and the size of each of the subsequent layers. These advantages are explained in more depth below.Copying makes containers efficientWhen you start a container, a thin writable container layer is added on top of the other layers. Any changes the container makes to the filesystem are stored here. Any files the container does not change do not get copied to this writable layer. This means that the writable layer is as small as possible.When an existing file in a container is modified, the storage driver performs a copy-on-write operation. The specifics steps involved depend on the specific storage driver. For the aufs, overlay, and overlay2 drivers, the copy-on-write operation follows this rough sequence:Search through the image layers for the file to update. The process starts at the newest layer and works down to the base layer one layer at a time. When results are found, they are added to a cache to speed future operations.Perform a copy_up operation on the first copy of the file that is found, to copy the file to the container’s writable layer.Any modifications are made to this copy of the file, and the container cannot see the read-only copy of the file that exists in the lower layer.Btrfs, ZFS, and other drivers handle the copy-on-write differently. You can read more about the methods of these drivers later in their detailed descriptions.Containers that write a lot of data consume more space than containers that do not. This is because most write operations consume new space in the container’s thin writable top layer. 简单的说，启动容器的时候，最上层容器层是可写层，之下的都是镜像层，只读层。 当容器需要读取文件的时候 从最上层镜像开始查找，往下找，找到文件后读取并放入内存，若已经在内存中了，直接使用。(即，同一台机器上运行的docker容器共享运行时相同的文件)。 当容器需要添加文件的时候 直接在最上面的容器层可写层添加文件，不会影响镜像层。 当容器需要修改文件的时候 从上往下层寻找文件，找到后，复制到容器可写层，然后，对容器来说，可以看到的是容器层的这个文件，看不到镜像层里的文件。容器在容器层修改这个文件。 当容器需要删除文件的时候 从上往下层寻找文件，找到后在容器中记录删除。即，并不会真正的删除文件，而是软删除。这将导致镜像体积只会增加，不会减少。 综上，Docker镜像通过分层实现了资源共享，通过copy-on-write实现了文件隔离。 对于文件只增加不减少问题，我们应当在同一层做增删操作，从而减少镜像体积。比如，如下测试。 Dockerfile.A: 分层删除文件** 12345678FROM centos:7RUN yum install -y vimRUN yum install -y httpdWORKDIR /homeRUN dd if=/dev/zero of=50M.file bs=1M count=50#创建大小为50M的测试文件RUN rm -rf 50M.fileCMD [\"/bin/bash\"] 构建 docker build -t test:a -f Dockerfile.A . Dockerfile.B: 同层删除 12345FROM centos:7RUN yum install -y vimRUN yum install -y httpdWORKDIR /homeRUN dd if=/dev/zero of=50M.file bs=1M count=50 &amp;&amp; rm -rf 50M.file 构建 1docker build -t test:b -f Dockerfile.B . 比较二者大小 123[root@sh-k8s-001 tmp]# docker images | grep testtest a ae673aa7db48 9 minutes ago 497MBtest b 21b2bc49f0bd 12 minutes ago 444MB 显然，分层删除操作并没有真正删除掉文件。 来源 **链接：**https://www.cnblogs.com/woshimrf/p/docker-container-lawyer.html https://www.cnblogs.com/CloudMan6/p/6799197.html https://www.cnblogs.com/CloudMan6/p/6806193.html https://docs.docker.com/storage/storagedriver/","path":"posts/2fbc.html","date":"06-07","excerpt":"","tags":[{"name":"镜像","slug":"镜像","permalink":"https://wsdlxgp.top/tags/%E9%95%9C%E5%83%8F/"},{"name":"bash","slug":"bash","permalink":"https://wsdlxgp.top/tags/bash/"}]},{"title":"05 Dockers镜像分层","text":"1,Dockers的最小镜像 1234[root@localhost ~]# docker pull hello-world//下载一个最小的镜像[root@localhost ~]# docker images//查看镜像 12[root@localhost ~]# docker run hello-world//运行一下hello-world （里面是一个文本对docker运行的简单介绍） dockerfile的组成 1）FROM：scratch（抓、挠） 2）COPY：hello / 3）CMD：[“/hello”] FROM 12语法：FROM &lt;image&gt;[:&lt;tag&gt;]解释：设置要制作的镜像基于哪个镜像，FROM指令必须是整个Dockerfile的第一个指令，如果指定的镜像不存在默认会自动从Docker Hub上下载。 COPY 12语法：COPY &lt;src&gt; &lt;dest&gt;解释：用法与ADD相同，不过&lt;src&gt;不支持使用url，所以在使用docker build – &lt; somefile时该指令不能使用。 CMD 12345语法：①CMD [\"executable\", \"param1\", \"param2\"] #将会调用exec执行，首选方式 ②CMD [\"param1\", \"param2\"] #当使用ENTRYPOINT指令时，为该指令传递默认参数 ③CMD &lt;command&gt; [ &lt;param1&gt;|&lt;param2&gt; ] #将会调用/bin/sh -c执行解释：CMD指令中指定的命令会在镜像运行时执行，在Dockerfile中只能存在一个，如果使用了多个CMD指令，则只有最后一个CMD指令有效。当出现ENTRYPOINT指令时，CMD中定义的内容会作为ENTRYPOINT指令的默认参数，也就是说可以使用CMD指令给ENTRYPOINT传递参数。注意：RUN和CMD都是执行命令，他们的差异在于RUN中定义的命令会在执行docker build命令创建镜像时执行，而CMD中定义的命令会在执行docker run命令运行镜像时执行，另外使用第一种语法也就是调用exec执行时，命令必须为绝对路径。 2、Base镜像（基础镜像） Centos:7镜像的dockerfile 1234567891011FROM scratchADD centos-7-x86_ _64-docker.tar.xz /LABEL org. label-schema. schema-version=\"1.0\" \\|org. label-schema. name=\"Centos Base Image\" \\org. labe1-schema. vendor=\"centos\" \\org. labe1-schema. 1icense=\"GPLV2\" \\org. labe1-schema build-date=\"20190305 'CMD [\"/bin/bash\"] 3、镜像的分层 1）dockerfile的书写格式为：Dockerfile（首字母大写，包括文件名称） 2）From：构建镜像有两种方式，一种scratch(从零构建)，另一种可以基于某个镜像开始构建 3）镜像所运行的操作（用户所期望的） 12345678910[root@localhost ~]# mkdir test//创建测试目录[root@localhost ~]# cd test//进入测试目录[root@localhost ~]#vim Dockerfile//编写DockerfileFROM centos:7 RUN yum -y install vim #或[\"yum\",\"install\",\"vim\"]RUN yum -y install net-toolsCMD [\"/bin/bash\"] 执行一下 12345[root@localhost test]# docker build -t centos7-vim-net-tools:12-11 .//使用当前目录的 Dockerfile 创建镜像，标签为 centos7-vim-net-tools:12-11build： 使用 Dockerfile 创建镜像-t：标签. :当前目录 执行的层次 4.Dockerfile镜像分层总结 镜像时容器的基石，容器是镜像运行后的实例。当镜像运行为容器之后,对镜像的所有数据仅有只读权限，如果需要对镜像源文件进行修改或删除操作,此时是在容器层（可写层）进行的，用到了COW（copy on write）写时复制机制。 Docker镜像的缓存特性 1.创建一个新的Dockerfile文件 12345678[root@localhost ~]# vim DockerfileFROM centos:7RUN yum -y install vimRUN yum -y install net-toolsRUN yum -y install wgetCMD [\"/bin/bash\"][root@localhost ~]# docker build -t new-centos .//使用当前目录的 Dockerfile 创建镜像，名称为new-centos 如果在相同的层，有用到相同的镜像，可以不必再去下载，直接使用缓存。（如果第一层的不相同了，那么下面的相同也没用了，需要重新下载） 2.再次创建一个新的Dockerfile 1234567891011[root@localhost ~]# mkdir test1[root@localhost ~]# cd test[root@localhost test]# cd ../test1[root@localhost test1]# vim DockerfileFROM centos:7RUN yum -y install vimRUN yum -y install wgetRUN yum -y install net-toolsCMD [\"/bin/bash\"][root@localhost test1]# docker build -t centos-new .//使用当前目录的 Dockerfile 创建镜像，名称为centos-new 即使镜像层里的操作一样，也必须是在同一层才可以使用dockerfile的缓存特性 如果制作镜像过程中，不想使用缓存，可以–no-cache选项","path":"posts/4e5d.html","date":"06-07","excerpt":"","tags":[{"name":"镜像","slug":"镜像","permalink":"https://wsdlxgp.top/tags/%E9%95%9C%E5%83%8F/"},{"name":"dockerfile","slug":"dockerfile","permalink":"https://wsdlxgp.top/tags/dockerfile/"}]},{"title":"04 DOCKER源码分析（一）：DOCKER架构","text":"1 背景 1.1 Docker简介 Docker是Docker公司开源的一个基于轻量级虚拟化技术的容器引擎项目,整个项目基于Go语言开发，并遵从Apache 2.0协议。目前，Docker可以在容器内部快速自动化部署应用，并可以通过内核虚拟化技术（namespaces及cgroups等）来提供容器的资源隔离与安全保障等。由于Docker通过操作系统层的虚拟化实现隔离，所以Docker容器在运行时，不需要类似虚拟机（VM）额外的操作系统开销，提高资源利用率，并且提升诸如IO等方面的性能。 由于众多新颖的特性以及项目本身的开放性，Docker在不到两年的时间里迅速获得诸多厂商的青睐，其中更是包括Google、Microsoft、VMware等业界行业领导者。Google在今年六月份推出了Kubernetes，提供Docker容器的调度服务，而今年8月Microsoft宣布Azure上支持Kubernetes，随后传统虚拟化巨头VMware宣布与Docker强强合作。今年9月中旬，Docker更是获得4000万美元的C轮融资，以推动分布式应用方面的发展。 从目前的形势来看，Docker的前景一片大好。本系列文章从源码的角度出发，详细介绍Docker的架构、Docker的运行以及Docker的卓越特性。本文是Docker源码分析系列的第一篇———Docker架构篇。 1.2 Docker版本信息 本文关于Docker架构的分析都是基于Docker的源码与Docker相应版本的运行结果，其中Docker为最新的1.2版本。 2 Docker架构分析内容安排 本文的目的是：在理解Docker源代码的基础上，分析Docker架构。分析过程中主要按照以下三个步骤进行： • Docker的总架构图展示 • Docker架构图内部各模块功能与实现分析 • 以Docker命令的执行为例，进行Docker运行流程阐述 3 Docker总架构图 学习Docker的源码并不是一个枯燥的过程，反而可以从中理解Docker架构的设计原理。Docker对使用者来讲是一个C/S模式的架构，而Docker的后端是一个非常松耦合的架构，模块各司其职，并有机组合，支撑Docker的运行。 在此，先附上Docker总架构，如图3.1。 图3.1 Docker总架构图 如图3.1，不难看出，用户是使用Docker Client与Docker Daemon建立通信，并发送请求给后者。 而Docker Daemon作为Docker架构中的主体部分，首先提供Server的功能使其可以接受Docker Client的请求；而后Engine执行Docker内部的一系列工作，每一项工作都是以一个Job的形式的存在。 Job的运行过程中，当需要容器镜像时，则从Docker Registry中下载镜像，并通过镜像管理驱动graphdriver将下载镜像以Graph的形式存储；当需要为Docker创建网络环境时，通过网络管理驱动networkdriver创建并配置Docker容器网络环境；当需要限制Docker容器运行资源或执行用户指令等操作时，则通过execdriver来完成。 而libcontainer是一项独立的容器管理包，networkdriver以及execdriver都是通过libcontainer来实现具体对容器进行的操作。 当执行完运行容器的命令后，一个实际的Docker容器就处于运行状态，该容器拥有独立的文件系统，独立并且安全的运行环境等。 3.1 DOCKER架构总体包含七个部分：client,daemon,driver,libcontainer,container,graph,registry。 外表来看，docker是一个C/S的架构，用户可以在客户端输入各种指令，客户端负责接受请求并作出相应的响应返回给客户。 3.2 DockerClient DockerClient 负责接受并传递请求指令 。 3.3 DockerDaemon DockerDaemon的功能主要有两个： 负责接受client的请求 管理docker容器 dockerdaemon的架构主要可以分为两部分：dockerserver和engine 3.4 DockerServer DockerServer作为服务端最主要的作用就是配合client端将请求指令接受过来，如图所示，DockerServer主要分为三个部分：Http.server,mux.server,Handler。 DockerServer运行时会从一个名为mux的包中创建一个mux.Router路由器，然后为路由器中添加相关的路由项用于路由信息， 每个路由项由HTTP请求方法（get,post,put,delete）+URL+Handler三部分组成。 DockerServer每收到一个请求就会生成一个goroutine然后进行相应的解析、匹配相应的路由项最后会找到相匹配的Handler来处理，Handler处理玩请求之后给DockerClient返回响应。 3.5 Engine Engine是docker中的运行引擎，存储着大量的容器信息并管理着大部分job的执行。 job是docker中的最小执行单元，类似于unix中的进程，也会有相应的名字、参数、环境变量、标准输入输出、返回状态等等。docker每进行一次相应的操作都会 生成一个相应的Job，比如创建一个容器、下载一个文件等等都是由job完成的。 3.6 DockerDriver DockerDriver是docker内部的驱动模块，负责容器内部相关网络、文件系统等的构建 3.6 libcontainer libcontainer主要是对linux内核的一些诸如namespace、cgroups、capabilities等特性做了封装 4 Docker架构内各模块的功能与实现分析 接下来，我们将从Docker总架构图入手，抽离出架构内各个模块，并对各个模块进行更为细化的架构分析与功能阐述。主要的模块有：Docker Client、Docker Daemon、Docker Registry、Graph、Driver、libcontainer以及Docker container。 4.1 Docker Client Docker Client是Docker架构中用户用来和Docker Daemon建立通信的客户端。用户使用的可执行文件为docker，通过docker命令行工具可以发起众多管理container的请求。 Docker Client可以通过以下三种方式和Docker Daemon建立通信：tcp://host:port，unix://path_to_socket和fd://socketfd。为了简单起见，本文一律使用第一种方式作为讲述两者通信的原型。与此同时，与Docker Daemon建立连接并传输请求的时候，Docker Client可以通过设置命令行flag参数的形式设置安全传输层协议(TLS)的有关参数，保证传输的安全性。 Docker Client发送容器管理请求后，由Docker Daemon接受并处理请求，当Docker Client接收到返回的请求相应并简单处理后，Docker Client一次完整的生命周期就结束了。当需要继续发送容器管理请求时，用户必须再次通过docker可执行文件创建Docker Client。 4.2 Docker Daemon Docker Daemon是Docker架构中一个常驻在后台的系统进程，功能是：接受并处理Docker Client发送的请求。该守护进程在后台启动了一个Server，Server负责接受Docker Client发送的请求；接受请求后，Server通过路由与分发调度，找到相应的Handler来执行请求。 Docker Daemon启动所使用的可执行文件也为docker，与Docker Client启动所使用的可执行文件docker相同。在docker命令执行时，通过传入的参数来判别Docker Daemon与Docker Client。 Docker Daemon的架构，大致可以分为以下三部分：Docker Server、Engine和Job。Daemon架构如图4.1。 图4.1 Docker Daemon架构示意图 4.2.1 DOCKER SERVER Docker Server在Docker架构中是专门服务于Docker Client的server。该server的功能是：接受并调度分发Docker Client发送的请求。Docker Server的架构如图4.2。 ** 图4.2 Docker Server架构示意图 在Docker的启动过程中，通过包gorilla/mux，创建了一个mux.Router，提供请求的路由功能。在Golang中，gorilla/mux是一个强大的URL路由器以及调度分发器。该mux.Router中添加了众多的路由项，每一个路由项由HTTP请求方法（PUT、POST、GET或DELETE）、URL、Handler三部分组成。 若Docker Client通过HTTP的形式访问Docker Daemon，创建完mux.Router之后，Docker将Server的监听地址以及mux.Router作为参数，创建一个httpSrv=http.Server{}，最终执行httpSrv.Serve()为请求服务。 在Server的服务过程中，Server在listener上接受Docker Client的访问请求，并创建一个全新的goroutine来服务该请求。在goroutine中，首先读取请求内容，然后做解析工作，接着找到相应的路由项，随后调用相应的Handler来处理该请求，最后Handler处理完请求之后回复该请求。 需要注意的是：Docker Server的运行在Docker的启动过程中，是靠一个名为”serveapi”的job的运行来完成的。原则上，Docker Server的运行是众多job中的一个，但是为了强调Docker Server的重要性以及为后续job服务的重要特性，将该”serveapi”的job单独抽离出来分析，理解为Docker Server。 4.2.2 ENGINE Engine是Docker架构中的运行引擎，同时也Docker运行的核心模块。它扮演Docker container存储仓库的角色，并且通过执行job的方式来操纵管理这些容器。 在Engine数据结构的设计与实现过程中，有一个handler对象。该handler对象存储的都是关于众多特定job的handler处理访问。举例说明，Engine的handler对象中有一项为：{“create”: daemon.ContainerCreate,}，则说明当名为”create”的job在运行时，执行的是daemon.ContainerCreate的handler。 4.2.3 JOB 一个Job可以认为是Docker架构中Engine内部最基本的工作执行单元。Docker可以做的每一项工作，都可以抽象为一个job。例如：在容器内部运行一个进程，这是一个job；创建一个新的容器，这是一个job，从Internet上下载一个文档，这是一个job；包括之前在Docker Server部分说过的，创建Server服务于HTTP的API，这也是一个job，等等。 Job的设计者，把Job设计得与Unix进程相仿。比如说：Job有一个名称，有参数，有环境变量，有标准的输入输出，有错误处理，有返回状态等。 4.3 Docker Registry Docker Registry是一个存储容器镜像的仓库。而容器镜像是在容器被创建时，被加载用来初始化容器的文件架构与目录。 在Docker的运行过程中，Docker Daemon会与Docker Registry通信，并实现搜索镜像、下载镜像、上传镜像三个功能，这三个功能对应的job名称分别为”search”，”pull” 与 “push”。 其中，在Docker架构中，Docker可以使用公有的Docker Registry，即大家熟知的Docker Hub，如此一来，Docker获取容器镜像文件时，必须通过互联网访问Docker Hub；同时Docker也允许用户构建本地私有的Docker Registry，这样可以保证容器镜像的获取在内网完成。 4.4 Graph Graph在Docker架构中扮演已下载容器镜像的保管者，以及已下载容器镜像之间关系的记录者。一方面，Graph存储着本地具有版本信息的文件系统镜像，另一方面也通过GraphDB记录着所有文件系统镜像彼此之间的关系。Graph的架构如图4.3。 图4.3 Graph架构示意图 其中，GraphDB是一个构建在SQLite之上的小型图数据库，实现了节点的命名以及节点之间关联关系的记录。它仅仅实现了大多数图数据库所拥有的一个小的子集，但是提供了简单的接口表示节点之间的关系。 同时在Graph的本地目录中，关于每一个的容器镜像，具体存储的信息有：该容器镜像的元数据，容器镜像的大小信息，以及该容器镜像所代表的具体rootfs。 4.5 Driver Driver是Docker架构中的驱动模块。通过Driver驱动，Docker可以实现对Docker容器执行环境的定制。由于Docker运行的生命周期中，并非用户所有的操作都是针对Docker容器的管理，另外还有关于Docker运行信息的获取，Graph的存储与记录等。因此，为了将Docker容器的管理从Docker Daemon内部业务逻辑中区分开来，设计了Driver层驱动来接管所有这部分请求。 在Docker Driver的实现中，可以分为以下三类驱动：graphdriver、networkdriver和execdriver。 graphdriver主要用于完成容器镜像的管理，包括存储与获取。即当用户需要下载指定的容器镜像时，graphdriver将容器镜像存储在本地的指定目录；同时当用户需要使用指定的容器镜像来创建容器的rootfs时，graphdriver从本地镜像存储目录中获取指定的容器镜像。 在graphdriver的初始化过程之前，有4种文件系统或类文件系统在其内部注册，它们分别是aufs、btrfs、vfs和devmapper。而Docker在初始化之时，通过获取系统环境变量”DOCKER_DRIVER”来提取所使用driver的指定类型。而之后所有的graph操作，都使用该driver来执行。 graphdriver的架构如图4.4： 图4.4 graphdriver架构示意图 networkdriver的用途是完成Docker容器网络环境的配置，其中包括Docker启动时为Docker环境创建网桥；Docker容器创建时为其创建专属虚拟网卡设备；以及为Docker容器分配IP、端口并与宿主机做端口映射，设置容器防火墙策略等。networkdriver的架构如图4.5： 图4. 5 networkdriver架构示意图 execdriver作为Docker容器的执行驱动，负责创建容器运行命名空间，负责容器资源使用的统计与限制，负责容器内部进程的真正运行等。在execdriver的实现过程中，原先可以使用LXC驱动调用LXC的接口，来操纵容器的配置以及生命周期，而现在execdriver默认使用native驱动，不依赖于LXC。具体体现在Daemon启动过程中加载的ExecDriverflag参数，该参数在配置文件已经被设为”native”。这可以认为是Docker在1.2版本上一个很大的改变，或者说Docker实现跨平台的一个先兆。execdriver架构如图4.6： 图4.6 execdriver架构示意图 4.6 libcontainer libcontainer是Docker架构中一个使用Go语言设计实现的库，设计初衷是希望该库可以不依靠任何依赖，直接访问内核中与容器相关的API。 正是由于libcontainer的存在，Docker可以直接调用libcontainer，而最终操纵容器的namespace、cgroups、apparmor、网络设备以及防火墙规则等。这一系列操作的完成都不需要依赖LXC或者其他包。libcontainer架构如图4.7： 图4.7 libcontainer示意图 另外，libcontainer提供了一整套标准的接口来满足上层对容器管理的需求。或者说，libcontainer屏蔽了Docker上层对容器的直接管理。又由于libcontainer使用Go这种跨平台的语言开发实现，且本身又可以被上层多种不同的编程语言访问，因此很难说，未来的Docker就一定会紧紧地和Linux捆绑在一起。而于此同时，Microsoft在其著名云计算平台Azure中，也添加了对Docker的支持，可见Docker的开放程度与业界的火热度。 暂不谈Docker，由于libcontainer的功能以及其本身与系统的松耦合特性，很有可能会在其他以容器为原型的平台出现，同时也很有可能催生出云计算领域全新的项目。 4.7 Docker container Docker container（Docker容器）是Docker架构中服务交付的最终体现形式。 Docker按照用户的需求与指令，订制相应的Docker容器： • 用户通过指定容器镜像，使得Docker容器可以自定义rootfs等文件系统； • 用户通过指定计算资源的配额，使得Docker容器使用指定的计算资源； • 用户通过配置网络及其安全策略，使得Docker容器拥有独立且安全的网络环境； • 用户通过指定运行的命令，使得Docker容器执行指定的工作。 Docker容器示意图如图4.8： 图4.8 Docker容器示意图 5 Docker运行案例分析 上一章节着重于Docker架构中各个部分的介绍。本章的内容，将以串联Docker各模块来简要分析，分析原型为Docker中的docker pull与docker run两个命令。 5.1 docker pull docker pull命令的作用为：从Docker Registry中下载指定的容器镜像，并存储在本地的Graph中，以备后续创建Docker容器时的使用。docker pull命令执行流程如图5.1。 图5.1 docker pull命令执行流程示意图 如图，图中标记的红色箭头表示docker pull命令在发起后，Docker所做的一系列运行。以下逐一分析这些步骤。 (1) Docker Client接受docker pull命令，解析完请求以及收集完请求参数之后，发送一个HTTP请求给Docker Server，HTTP请求方法为POST，请求URL为”/images/create? “+”xxx”； (2) Docker Server接受以上HTTP请求，并交给mux.Router，mux.Router通过URL以及请求方法来确定执行该请求的具体handler； (3) mux.Router将请求路由分发至相应的handler，具体为PostImagesCreate； (4) 在PostImageCreate这个handler之中，一个名为”pull”的job被创建，并开始执行； (5) 名为”pull”的job在执行过程中，执行pullRepository操作，即从Docker Registry中下载相应的一个或者多个image； (6) 名为”pull”的job将下载的image交给graphdriver； (7) graphdriver负责将image进行存储，一方创建graph对象，另一方面在GraphDB中记录image之间的关系。 5.2 docker run docker run命令的作用是在一个全新的Docker容器内部运行一条指令。Docker在执行这条命令的时候，所做工作可以分为两部分：第一，创建Docker容器所需的rootfs；第二，创建容器的网络等运行环境，并真正运行用户指令。因此，在整个执行流程中，Docker Client给Docker Server发送了两次HTTP请求，第二次请求的发起取决于第一次请求的返回状态。Docker run命令执行流程如图5.2。 图5.2 docker run命令执行流程示意图 如图，图中标记的红色箭头表示docker run命令在发起后，Docker所做的一系列运行。以下逐一分析这些步骤。 (1) Docker Client接受docker run命令，解析完请求以及收集完请求参数之后，发送一个HTTP请求给Docker Server，HTTP请求方法为POST，请求URL为”/containers/create? “+”xxx”； (2) Docker Server接受以上HTTP请求，并交给mux.Router，mux.Router通过URL以及请求方法来确定执行该请求的具体handler； (3) mux.Router将请求路由分发至相应的handler，具体为PostContainersCreate； (4) 在PostImageCreate这个handler之中，一个名为”create”的job被创建，并开始让该job运行； (5) 名为”create”的job在运行过程中，执行Container.Create操作，该操作需要获取容器镜像来为Docker容器创建rootfs，即调用graphdriver； (6) graphdriver从Graph中获取创建Docker容器rootfs所需要的所有的镜像； (7) graphdriver将rootfs所有镜像，加载安装至Docker容器指定的文件目录下； (8) 若以上操作全部正常执行，没有返回错误或异常，则Docker Client收到Docker Server返回状态之后，发起第二次HTTP请求。请求方法为”POST”，请求URL为”/containers/”+containerID+”/start”； (9) Docker Server接受以上HTTP请求，并交给mux.Router，mux.Router通过URL以及请求方法来确定执行该请求的具体handler； (10) mux.Router将请求路由分发至相应的handler，具体为PostContainersStart； (11) 在PostContainersStart这个handler之中，名为”start”的job被创建，并开始执行； (12) 名为”start”的job执行完初步的配置工作后，开始配置与创建网络环境，调用networkdriver； (13) networkdriver需要为指定的Docker容器创建网络接口设备，并为其分配IP，port，以及设置防火墙规则，相应的操作转交至libcontainer中的netlink包来完成； (14) netlink完成Docker容器的网络环境配置与创建； (15) 返回至名为”start”的job，执行完一些辅助性操作后，job开始执行用户指令，调用execdriver； (16) execdriver被调用，初始化Docker容器内部的运行环境，如命名空间，资源控制与隔离，以及用户命令的执行，相应的操作转交至libcontainer来完成； (17) libcontainer被调用，完成Docker容器内部的运行环境初始化，并最终执行用户要求启动的命令。 6 总结 本文从Docker 1.2的源码入手，分析抽象出Docker的架构图，并对该架构图中的各个模块进行功能与实现的分析，最后通过两个docker命令展示了Docker内部的运行。 通过对Docker架构的学习，可以全面深化对Docker设计、功能与价值的理解。同时在借助Docker实现用户定制的分布式系统时，也能更好地找到已有平台与Docker较为理想的契合点。另外，熟悉Docker现有架构以及设计思想，也能对云计算PaaS领域带来更多的启发，催生出更多实践与创新。 链接：https://www.2cto.com/kf/201701/582655.html 链接：https://blog.csdn.net/gsllovefly/article/details/51083419","path":"posts/c10c.html","date":"06-07","excerpt":"","tags":[{"name":"docker命令","slug":"docker命令","permalink":"https://wsdlxgp.top/tags/docker%E5%91%BD%E4%BB%A4/"}]},{"title":"03 Docker的基本操作命令","text":"Docker介绍 Docker 是一个能够把开发应用程序自动部署到容器的开源引擎。它由Docker公司的团队编写，基于Apache 2.0开源协议授权。它提供了一个简单、轻量的建模方式，使开发生命周期更高效快速，鼓励了面向服务的架构设计。Docker 项目的目标是实现轻量级的操作系统虚拟化解决方案。 Docker 的基础是 Linux 容器（LXC）等技术。在 LXC 的基础上 Docker 进行了进一步的封装，让用户不需要去关心容器的管理，使得操作更为简便。用户操作 Docker 的容器就像操作一个快速轻量级的虚拟机一样简单。 Docker 的特点： 更快速的交付和部署 更高效的虚拟化 更轻松的迁移和扩展 更简单的管理 容器技术与传统虚拟机性能对比 Docker与虚拟机建构对比 Docker 容器本质上是宿主机上的一个进程。Docker 通过 namespace 实现了资源隔离，通过 cgroups 实现了资源的限制，通过写时复制机制（copy-on-write）实现了高效的文件操作。 **Docker有五个命名空间：**进程、网络、挂载、宿主和共享内存，为了隔离有问题的应用，Docker运用Namespace将进程隔离，为进程或进程组创建已隔离的运行空间，为进程提供不同的命名空间视图。这样，每一个隔离出来的进程组，对外就表现为一个container(容器)。需要注意的是，Docker让用户误以为自己占据了全部资源，但这并不是”虚拟机”。 Docker 中的三个概念：镜像，容器，仓库 镜像（image）：Docker 镜像就是一个只读的模板，镜像可以用来创建 Docker 容器。Docker 提供了一个很简单的机制来创建镜像或者更新现有的镜像，用户甚至可以直接从其他人那里下载一个已经做好的镜像来直接使用。 镜像是一种文件结构。Dockerfile中的每条命令都会在文件系统中创建一个新的层次结构，文件系统在这些层次上构建起来，镜像就构建于这些联合的文件系统之上。Docker官方网站专门有一个页面来存储所有可用的镜像，网址是：index.docker.io。 容器（ Container）：容器是从镜像创建的运行实例。它可以被启动、开始、停止、删除。每个容器都是相互隔离的、保证安全的平台。可以把容器看做是一个简易版的 Linux 环境，Docker 利用容器来运行应用。镜像是只读的，容器在启动的时候创建一层可写层作为最上层。 仓库：仓库是集中存放镜像文件的场所，仓库注册服务器（Registry）上往往存放着多个仓库，每个仓库中又包含了多个镜像，每个镜像有不同的标签（tag）。目前，最大的公开仓库是 Docker Hub，存放了数量庞大的镜像供用户下载。 Docker仓库用来保存我们的images，当我们创建了自己的image之后我们就可以使用push命令将它上传到公有或者私有仓库，这样下次要在另外一台机器上使用这个image时候，只需要从仓库上pull下来就可以了。Docker 仓库的概念跟 Git 类似，注册服务器可以理解为 GitHub 这样的服务。 Docker 基本操作 12 [root@localhost ~]# docker search mysql//查找镜像 这样查找相当于在https://hub.docker.com/中查找，大家尽量使用官方的镜像 12[root@localhost ~]# docker pull busybox//拉取镜像 12[root@localhost ~]# docker save -o busybox.tar busybox:latest//把镜像导出到本地 -o：相当--output导出 12[root@localhost ~]# docker images//查看本地镜像 仓库（镜像名称） 镜像标签 镜像id 创建时间 大小 虽然我们看到镜像标签为latest（最新的），但并不表示他一定是最新的。而且镜像如果没有写标签，默认以latest为标签。 12[root@localhost ~]# docker rmi busybox:latest//删除镜像 12[root@localhost ~]# docker images//查看本地镜像这里没有busybox 12[root@localhost ~]# docker load -i busybox.tar//根据本地镜像包导入镜像 12[root@localhost ~]# docker images//查看本地镜像这里又有busybox 12345678[root@localhost ~]# docker ps//查看容器-正在运行的[root@localhost ~]# docker ps -a//查看所有容器[root@localhost ~]# docker rm c3bb3a6f73eb//删除容器 id或镜像名称（不能删除正在运行的容器） 12[root@localhost ~]# docker stop test//停止容器运行 （记得验证一下docker ps -a） 12[root@localhost ~]# docker start test//启动容器 （记得验证一下docker ps -a） 12[root@localhost ~]# docker rm test -f//强制删除容器 （记得验证一下docker ps -a） 12[root@localhost ~]# docker ps -a -q | xargs docker rm -f//强制删除所有容器（生产环境严禁使用） 1234[root@localhost ~]# docker ps -a -q | xargs docker start -f//强制开启所有容器（生产环境严禁使用）[root@localhost ~]# docker ps -a -q | xargs docker stop -f//强制关闭所有容器（生产环境严禁使用） 123456789[root@localhost ~]# docker run -it --name test1 busybox:latest//开启一个容器-i：可交互-t：伪终端-d：守护进程--name：容器命名--restart=always：始终保持运行（随着docker开启而运行） [root@localhost ~]# docker run -itd --name test2 --restart=always busybox:latest//docker重启后，始终保持运行（随着docker开启而运行） 路由转发 123456[root@localhost ~]# vim /etc/sysctl.conf //添加路由转发[root@localhost ~]# sysctl -pnet.ipv4.ip_forward = 1net.bridge.bridge-nf-call-iptables = 1net.bridge.bridge-nf-call-ip6tables = 1 进入容器方法 12345678910[root@localhost ~]# docker exec -it test2 /bin/sh//进入一个容器（退出容器后还在运行）[root@localhost ~]# docker attach test2//也是进入一个容器（退出容器不在运行）区别：exec进入的方式需要添加-i -t选项,后边还需要给容器一个shell环境。但attach就不需要这么麻烦。exec进入的方式:如果执行exit退出， 容器仍然保持运行。attach:如果执行exit退出, 容器会被关闭。如果想要保持容器不被关闭，可以使用用键盘: Ctrl + p Ctrl +q可以实现。本质上去区别: exec 进入的方法，会生产新的进程。attach不会生产新进程。 强制删除镜像 123 [root@localhost ~]# docker rmi centos:7 -f//强制删除镜像上面是把镜像标签给删了，要想彻底删除镜像，用下面的命令把镜像id也删了，docker有缓存机制，即使把这个镜像给删了，但是会有缓存，其他的容器依旧可以使用 Docker的基本操作逻辑 基于centos: 7镜像运行-个容器，并且,在这个容器内部署Nginx服务。 1)下载centos：7镜像 1[root@localhost ~]# docker pull centos:7 12[root@localhost ~]# rz上传一个nginx包 2)运行容器 1[root@localhost ~]# docker run -itd --name webapp --restart=always centos:7 3）进入容器，开始部署nginx服务 12345[root@localhost ~]# docker cp nginx-1.14.0.tar.gz webapp:/root//将nginx包导入到容器内[root@localhost ~]# docker exec -it webapp /bin/bash//进入容器[root@8604fb370aab /]# ls root 安装nginx 12345678910111213141516171819[root@8604fb370aab /]# cd /root[root@8604fb370aab ~]# tar zxf nginx-1.14.0.tar.gz[root@8604fb370aab ~]# yum -y install gcc pcre pcre-devel openssl-devel zlib zlib-devel make//安装nginx所需依赖[root@8604fb370aab ~]# cd nginx-1.14.0[root@8604fb370aab nginx-1.14.0]# useradd -M -s /sbin/nologin nginx//创建用户[root@8604fb370aab nginx-1.14.0]# ./configure --prefix=/usr/local/nginx --user=nginx --group=nginx &amp;&amp; make &amp;&amp; make install//编译安装 [root@8604fb370aab nginx-1.14.0]# ln -s /usr/local/nginx/sbin/nginx /usr/local/sbin///链接命令目录[root@8604fb370aab nginx-1.14.0]# nginx//启动nginx[root@8604fb370aab nginx-1.14.0]# cd /usr/local/nginx/html/[root@8604fb370aab html]# echo This is a testweb in container &gt; index.html//创建一个测试页面[root@8604fb370aab html]# curl 127.0.0.1//访问网页 123456[root@8604fb370aab /]# yum provides ip//查看哪一个组件支持这条命令[root@8604fb370aab /]# yum -y install net-tools//安装支持这条命令的[root@8604fb370aab /]# ifconfig//查看ip 宿主机查看网页 1[root@localhost ~]# curl 172.17.0.4 12[root@localhost ~]# docker commit webapp myweb:xgp//把容器制作成镜像 （会返回一个哈希值，代表的是镜像的id号）增加可移植性 12[root@localhost ~]# docker images//查看镜像 123456[root@localhost ~]# docker run -itd --name webapp-2 myweb:xgp[root@localhost ~]# docker exec -it webapp-2 /bin/bash[root@e8d15e9aef29 /]# nginx [root@e8d15e9aef29 /]# curl 127.0.0.1This is a testweb in container[root@e8d15e9aef29 /]# ifconfig 查看网页 1[root@localhost ~]# curl 172.17.0.5","path":"posts/f1f1.html","date":"06-07","excerpt":"","tags":[{"name":"docker命令","slug":"docker命令","permalink":"https://wsdlxgp.top/tags/docker%E5%91%BD%E4%BB%A4/"}]},{"title":"02 docker底层原理介绍","text":"链接：https://blog.51cto.com/14320361/2457143 1.docker介绍 1.1什么是docker Docker 是一个开源的应用容器引擎，基于 Go 语言 并遵从Apache2.0协议开源。 Docker 可以让开发者打包他们的应用以及依赖包到一个轻量级、可移植的容器中，然后发布到任何流行的 Linux 机器上。 1.2docker能解决什么问题 1.2.1高效有序利用资源 机器资源有限； 单台机器得部署多个应用； 应用之间互相隔离； 应用之间不能发生资源抢占，每个应用只能使用事先注册申请的资源。 1.2.2一次编译，到处运行 类似于java代码，应用及依赖的环境构建一次，可以到处运行。 1.2.docker底层原理介绍 1.2.1Linux的namespace和cgroup简单理解 namespace:类似于JAVA的命名空间 controll groups ： controll （system resource） （for） （process）groups 1.2.2Linux中的namespace 在Linux系统中，可以同时存在多用户多进程，那么对他们的运行协调管理，通过进程调度和进度管理可以解决，但是，整体资源是有限的，怎么把有限的资源（进程号、网络资源等等）合理分配给各个用户所在的进程？ Linux Namespaces机制提供一种资源隔离方案。PID,IPC,Network等系统资源不再是全局性的，而是属于某个特定的Namespace。每个namespace下的资源对于其他namespace下的资源都是透明，不可见的。因此在操作系统层面上看，就会出现多个相同pid的进程。系统中可以同时存在两个进程号为0,1,2的进程，由于属于不同的namespace，所以它们之间并不冲突。而在用户层面上只能看到属于用户自己namespace下的资源，例如使用ps命令只能列出自己namespace下的进程。这样每个namespace看上去就像一个单独的Linux系统。 命名空间建立系统的不同视图， 对于每一个命名空间，从用户看起来，应该像一台单独的Linux计算机一样，有自己的init进程(PID为0)，其他进程的PID依次递增，A和B空间都有PID为0的init进程，子容器的进程映射到父容器的进程上，父容器可以知道每一个子容器的运行状态，而子容器与子容器之间是隔离的。 **|** namespace | 引入的相关内核版本 | 被隔离的全局系统资源 | 在容器语境下的隔离效果 | | — | — | — | — | | Mount namespaces | Linux 2.4.19 | 文件系统挂接点 | 将一个文件系统的顶层目录挂到另一个文件系统的子目录上，使它们成为一个整体，称为挂载。把该子目录称为挂载点。 Mount namespace用来隔离文件系统的挂载点, 使得不同的mount namespace拥有自己独立的挂载点信息，不同的namespace之间不会相互影响，这对于构建用户或者容器自己的文件系统目录非常有用。 | | UTS namespaces | Linux 2.6.19 | nodename 和 domainname | UTS，UNIX Time-sharing System namespace提供了主机名和域名的隔离。能够使得子进程有独立的主机名和域名(hostname)，这一特性在Docker容器技术中被用到，使得docker容器在网络上被视作一个独立的节点，而不仅仅是宿主机上的一个进程。 | | IPC namespaces | Linux 2.6.19 | 特定的进程间通信资源，包括System V IPC 和 POSIX message queues | IPC全称 Inter-Process Communication，是Unix/Linux下进程间通信的一种方式，IPC有共享内存、信号量、消息队列等方法。所以，为了隔离，我们也需要把IPC给隔离开来，这样，只有在同一个Namespace下的进程才能相互通信。如果你熟悉IPC的原理的话，你会知道，IPC需要有一个全局的ID，即然是全局的，那么就意味着我们的Namespace需要对这个ID隔离，不能让别的Namespace的进程看到。 | | PID namespaces | Linux 2.6.24 | 进程 ID 数字空间 （process ID number space） | PID namespaces用来隔离进程的ID空间，使得不同pid namespace里的进程ID可以重复且相互之间不影响。 PID namespace可以嵌套，也就是说有父子关系，在当前namespace里面创建的所有新的namespace都是当前namespace的子namespace。父namespace里面可以看到所有子孙后代namespace里的进程信息，而子namespace里看不到祖先或者兄弟namespace里的进程信息。 | | Network namespaces | 始于Linux 2.6.24 完成于 Linux 2.6.29 | 网络相关的系统资源 | 每个容器用有其独立的网络设备，IP 地址，IP 路由表，/proc/net 目录，端口号等等。这也使得一个 host 上多个容器内的同一个应用都绑定到各自容器的 80 端口上。 | | User namespaces | 始于 Linux 2.6.23 完成于 Linux 3.8) | 用户和组 ID 空间 | User namespace用来隔离user权限相关的Linux资源，包括user IDs and group IDs。 这是目前实现的namespace中最复杂的一个，因为user和权限息息相关，而权限又事关容器的安全，所以稍有不慎，就会出安全问题。 在不同的user namespace中，同样一个用户的user ID 和group ID可以不一样，换句话说，一个用户可以在父user namespace中是普通用户，在子user namespace中是超级用户 1.3Namespace（名称空间） 用来隔离容器 12 [root@localhost ~]# docker run -it --name test centos /bin/bash//进入到容器里面 12[root@41052cceb473 /]# ls//查看一下和宿主机差不多，都是从宿主机链接过来的 12[root@41052cceb473 /]# uname -r//查看一下内核，和宿主机也是一样的 如果虚拟机内服务对内核版有要求，这个服务就不太适合用docker来实现了，因为docker就是共用宿主机的内核，可以使用kvm之类的虚拟机。 12[root@localhost ~]# docker pull ubuntu//使用docker下载一个Ubuntu 12[root@localhost ~]# docker images//查看一下 1234[root@localhost ~]# docker run -it ubuntu:latest /bin/bash//进入ubuntu环境root@afbee6750865:/# ls ///查看一下 12root@48c8dd7b098e:/# uname -r//查看一下内核 Docker本身不占用任何端口，他一般是在后台运行，无论在docker里进行什么操作（系统、服务）对于docker来说他们仅仅就是一个进程 Run-centos系统（nginx，web） Busybox：欺骗层。欺骗docker中的虚拟机是在自己独立的环境中 解耦：解除耦合、冲突。 耦合：冲突现象。 1.4 Namespace操作 /proc /sys:虚拟文件系统，伪目录文件 12[root@localhost ~]# cd /proc/[root@localhost proc]# ls 1234567[root@localhost proc]# echo $$//当前的进程编号3864[root@localhost proc]# cd 3864[root@localhost 3864]# cd ns[root@localhost ns]# ll//可以看到一闪一闪的 1[root@localhost ns]# ls IPC:共享内存、消息列队 MNT:挂载点、文件系统 NET:网络栈 PID: 进程编号 USER:用户、组 UTS:主机名、域名 namespec这六项隔离，实现了容器与宿主机，容器与容器之间的隔离 //创建一个用户并设置密码 IPC:共享内存、消息列队 MNT:挂载点、文件系统 NET:网络栈 PID: 进程编号 USER:用户、组 UTS:主机名、域名 namespec这六项隔离，实现了容器与宿主机，容器与容器之间的隔离 //创建一个用户并设置密码 123[root@localhost ns]# useradd bdqn [root@localhost ns]# echo 123.com | passwd --stdin bdqn[root@localhost ns]# id bdqn 查看docker进程 [root@localhost ns]# docker ps -a 12345[root@localhost ns]# docker start test//启动centos[root@localhost ns]# docker exec -it test /bin/bash//进入docker容器[root@41052cceb473 /]# id dbqn 1[root@41052cceb473 /]# echo $$ 2.1linux cgroup介绍 2.1.1有了namespace为什么还要cgroup: Docker 容器使用 linux namespace 来隔离其运行环境，使得容器中的进程看起来就像一个独立环境中运行一样。但是，光有运行环境隔离还不够，因为这些进程还是可以不受限制地使用系统资源，比如网络、磁盘、CPU以及内存 等。关于其目的，一方面，是为了防止它占用了太多的资源而影响到其它进程；另一方面，在系统资源耗尽的时候，linux 内核会触发 OOM，这会让一些被杀掉的进程成了无辜的替死鬼。因此，为了让容器中的进程更加可控，Docker 使用 Linux cgroups 来限制容器中的进程允许使用的系统资源。 2.1.2原理 Linux Cgroup 可为系统中所运行任务（进程）的用户定义组群分配资源 — 比如 CPU 时间、系统内存、网络带宽或者这些资源的组合。可以监控管理员配置的 cgroup，拒绝 cgroup 访问某些资源，甚至在运行的系统中动态配置 cgroup。所以，可以将 controll groups 理解为 controller （system resource） （for） （process）groups，也就是是说它以一组进程为目标进行系统资源分配和控制。它主要提供了如下功能： Resource limitation: 限制资源使用，比如内存使用上限以及文件系统的缓存限制。 Prioritization: 优先级控制，比如：CPU利用和磁盘IO吞吐。 Accounting: 一些审计或一些统计，主要目的是为了计费。 Controll: 挂起进程，恢复执行进程。 使用 cgroup，系统管理员可更具体地控制对系统资源的分配、优先顺序、拒绝、管理和监控。可更好地根据任务和用户分配硬件资源，提高总体效率。 在实践中，系统管理员一般会利用CGroup做下面这些事： 隔离一个进程集合（比如：nginx的所有进程），并限制他们所消费的资源，比如绑定CPU的核。 为这组进程分配其足够使用的内存 为这组进程分配相应的网络带宽和磁盘存储限制 限制访问某些设备（通过设置设备的白名单） 2.1.3Cgroup(控制组)操作 资源的限制，docker对于资源的占用 123[root@localhost ~]# cd /sys/fs/cgroup///对cpu，内存限制的目录[root@localhost cgroup]# ls 12[root@localhost cgroup]# cd cpu[root@localhost cpu]# ls cpu.shares：权重 tasks：这个文件内的数字，记录的是进程编号。PID 12[root@localhost cpu]# cd docker/[root@localhost docker]# ls 1234[root@localhost docker]# cat tasks//里面是空的[root@localhost docker]# cd 41052cceb4739fa8e0ddd2ffa733a78cd1043b3fdff874cd266c009391a34d70/[root@localhost41052cceb4739fa8e0ddd2ffa733a78cd1043b3fdff874cd266c009391a34d70]# ls 1[root@localhost41052cceb4739fa8e0ddd2ffa733a78cd1043b3fdff874cd266c009391a34d70]# cat tasks 四大功能： 1） 资源的限制：cgroup可以对进程组使用的资源总额进行限制 2） 优先级分配：通过分配的cpu时间片数量以及硬盘IO带宽的大小，实际上相当于控制了进程运行的优先级别 3） 资源统计： group可以统计系统资源使用量，比如gpu使用时间，内存使用量等，用于按量计费。同时还支持挂起动能，也就是说通过cgroup把所有 资源限制起来,对资源都不能使用，注意着并不是说我们的程序不能使用了,知识不能使用资源，处于等待状态。 4） 进程控制：可以对进程组执行挂起、恢复等操作。 2.1.4 内存限额 容器内存包括两个部分：物理内存和swap 可以通过参数控制容器内存的使用量： -m或者–memory:设置内存的使用限额 –memory-swap:设置内存+ swap的使用限额 举个例子： 运行一个容器，并且限制该容器最多使用200M内存和100M的swap 123[root@localhost ~]# docker run -it -m 200M --memory-swap 300M centos:7[root@fba67fec2718 ~]# cd /sys/fs/cgroup/[root@fba67fec2718 cgroup]# ls 1234[root@fba67fec2718 cgroup]# cd memory/[root@fba67fec2718 memory]# ls[root@fba67fec2718 memory]# cat memory.limit_in_bytes//查看内存使用限制，(单位字节） 12[root@fba67fec2718 memory]# cat memory.memsw.limit_in_bytes//查看交换分区，内存+swap限制 运行一个新容器，并且不限制该容器 1234[root@localhost ~]# docker run -it centos:7[root@5be901bfb093 /]# cd /sys/fs/cgroup/memory/[root@5be901bfb093 memory]# cat memory.limit_in_bytes//查看内存限制 12[root@5be901bfb093 memory]# cat memory.memsw.limit_in_bytes//查看交换分区，内存+swap限制 对比一个没有限制的容器，我们会发现，如果运行容器之后不限制内存的话，意味着没有限制。 2.1.5 CPU使用 通过-c或者–cpu -shares设置容器使用cpu的权重。如果不设置默认为1024. 举个例子： 没有限制 1234[root@localhost ~]# docker run -it --name containerA centos:7//没有限制，1024[root@8683d8ff8234 /]# cd /sys/fs/cgroup/cpu[root@8683d8ff8234 cpu]# cat cpu.shares 限制CPU使用权重为512 1234[root@localhost ~]# docker run -it --name containerB -c 512 centos:7//限制CPU使用权重为512[root@d919d906295d /]# cd /sys/fs/cgroup/cpu//可以看到cpu已经限制了 2.1.6 容器的Block IO 磁盘的读写。 Docker中可以通过设置权重，限制bps和iops的方式控制容器读写磁盘的IO bps:每秒读写的数据量byte per second iopS:每秒IO的次数 io per second。 默认情况下，所有容器都能够平等的读写磁盘，也可以通过–blkig-weight参数改变容器的blocklO的优先级。 –device-read-bps:显示读取某个设备的bps。 –device-write-bps:显示写入某个设备的bps. –device-read-iops:显示读取某个设备的iops. –device-write-iops:显示写入某个设备的iops. 限制testA这个容器，写入/dev/sda这块磁盘的bps为30MB 1234[root@localhost ~]# docker run -it --name testA --device-write-bps /dev/sda:30MB centos:7 [root@60e59e96fc16 /]# time dd if=/dev/zero of=test.out bs=1M count=800 oflag=direct//从/dev/zero输入，然后输出到test.out文件中，每次大小为1M，总共800次,oflag=direct 用来指定directlQ方式写文件，这样才会使--device-write-bps生效。 1[root@60e59e96fc16 /]# du -h test.out docker没有限制 12[root@localhost ~]# docker run -it --name testc centos:7[root@5bf5f3d60d0e /]# time dd if=/dev/zero of=test.out bs=1M count=800 oflag=direct 1[root@5bf5f3d60d0e /]# du -h test.out 3.Docker虚拟化与普通虚拟化的区别是什么？ 虚拟机： 我们传统的虚拟机需要模拟整台机器包括硬件，每台虚拟机都需要有自己的操作系统，虚拟机一旦被开启，预分配给他的资源将全部被占用。，每一个虚拟机包括应用，必要的二进制和库，以及一个完整的用户操作系统。 Docker： 容器技术是和我们的宿主机共享硬件资源及操作系统可以实现资源的动态分配。 容器包含应用和其所有的依赖包，但是与其他容器共享内核。容器在宿主机操作系统中，在用户空间以分离的进程运行。 虚拟机和容器都是在硬件和操作系统以上的，虚拟机有Hypervisor层，Hypervisor是整个虚拟机的核心所在。他为虚拟机提供了虚拟的运行平台，管理虚拟机的操作系统运行。每个虚拟机都有自己的系统和系统库以及应用。 容器没有Hypervisor这一层，并且每个容器是和宿主机共享硬件资源及操作系统，那么由Hypervisor带来性能的损耗，在linux容器这边是不存在的。 但是虚拟机技术也有其优势，能为应用提供一个更加隔离的环境，不会因为应用程序的漏洞给宿主机造成任何问题。同时还支持跨操作系统的虚拟化，例如你可以在linux操作系统下运行windows虚拟机。 从虚拟化层面来看，传统虚拟化技术是对硬件资源的虚拟，容器技术则是对进程的虚拟，从而可提供更轻量 级的虚拟化，实现进程和资源的隔离。 从架构来看，Docker比虚拟化少了两层，取消了hypervisor层和GuestOS层，使用 Docker Engine 进行调度和隔离，所有应用共用主机操作系统，因此在体量上，Docker较虚拟机更轻量级，在性能上优于虚拟化，接近裸机性能。从应用场景来 看，Docker和虚拟化则有各自擅长的领域，在软件开发、测试场景和生产运维场景中各有优劣 具体对比： docker启动快速属于秒级别。虚拟机通常需要几分钟去启动。 docker需要的资源更少，docker在操作系统级别进行虚拟化，docker容器和内核交互，几乎没有性能损耗，性能优于通过Hypervisor层与内核层的虚拟化。； docker更轻量，docker的架构可以共用一个内核与共享应用程序库，所占内存极小。同样的硬件环境，Docker运行的镜像数远多于虚拟机数量。对系统的利用率非常高 与虚拟机相比，docker隔离性更弱，docker属于进程之间的隔离，虚拟机可实现系统级别隔离； 安全性： docker的安全性也更弱。Docker的租户root和宿主机root等同，一旦容器内的用户从普通用户权限提升为root权限，它就直接具备了宿主机的root权限，进而可进行无限制的操作。虚拟机租户root权限和宿主机的root虚拟机权限是分离的，并且虚拟机利用如Intel的VT-d和VT-x的ring-1硬件隔离技术，这种隔离技术可以防止虚拟机突破和彼此交互，而容器至今还没有任何形式的硬件隔离，这使得容器容易受到***。 可管理性：docker的集中化管理工具还不算成熟。各种虚拟化技术都有成熟的管理工具，例如VMware vCenter提供完备的虚拟机管理能力。 高可用和可恢复性：docker对业务的高可用支持是通过快速重新部署实现的。虚拟化具备负载均衡，高可用，容错，迁移和数据保护等经过生产实践检验的成熟保障机制，VMware可承诺虚拟机99.999%高可用，保证业务连续性。 快速创建、删除：虚拟化创建是分钟级别的，Docker容器创建是秒级别的，Docker的快速迭代性，决定了无论是开发、测试、部署都可以节约大量时间。 交付、部署：虚拟机可以通过镜像实现环境交付的一致性，但镜像分发无法体系化；Docker在Dockerfile中记录了容器构建过程，可在集群中实现快速分发和快速部署; 3.1.1 docker结构介绍 基础设施(Infrastructure)。 主操作系统(Host Operating System)。所有主流的Linux发行版都可以运行Docker。对于MacOS和Windows，也有一些办法”运行”Docker。 Docker守护进程(Docker Daemon)。Docker守护进程取代了Hypervisor，它是运行在操作系统之上的后台进程，负责管理Docker容器。 各种依赖。对于Docker，应用的所有依赖都打包在Docker镜像中，Docker容器是基于Docker镜像创建的。 应用。应用的源代码与它的依赖都打包在Docker镜像中，不同的应用需要不同的Docker镜像。不同的应用运行在不同的Docker容器中，它们是相互隔离的。 Docker守护进程可以直接与主操作系统进行通信，为各个Docker容器分配资源；它还可以将容器与主操作系统隔离，并将各个容器互相隔离。虚拟机启动需要数分钟，而Docker容器可以在数毫秒内启动。由于没有臃肿的从操作系统，Docker可以节省大量的磁盘空间以及其他系统资源；虚拟机更擅长于资源的完全隔离。 链接：https://blog.51cto.com/14320361/2457143","path":"posts/df9f.html","date":"06-07","excerpt":"","tags":[{"name":"docker","slug":"docker","permalink":"https://wsdlxgp.top/tags/docker/"},{"name":"kvm","slug":"kvm","permalink":"https://wsdlxgp.top/tags/kvm/"}]},{"title":"01 花式安装Docker","text":"//使用docker的基本要求 12[root@localhost ~]# uname -r3.10.0-693.el7.x86_64 内核版本必须是3.10以上的。 一， 安装dockers 在安装docker之前，再说一点，docker现在有两个版本，一个叫做docker-EE企业版，收费的一个叫docker-CE社区版，免费版，其实两个版本并没有太大的偏差，不一样的是docker公司会提供后续的官方的技术支持等服务，对于我们来说，肯定用社区办的多，我们拿来学习社区办更是可以的。 Docker的官网 https://www.docker.com/ 1，从Docker的官方下载 https://www.docker.com/ 2．官网安装docker方法一 1234567891011121314151617181920212223[root@localhost ~]# vim /etc/yum.repos.d/docke-ce.repo//编写yum源[docker-ce]name=docker-cebaseurl=https://download.docker.com/linux/centos/7/x86_64/stable/Packages/gpgcheck=0enabled=1 [root@localhost ~]# yum repolist//查看仓库状态 [root@localhost ~]# vim /etc/yum.repos.d/docke-ce.repo//修改yum源[docker-ce]name=docker-cebaseurl=https://download.docker.com/linux/centos/7/x86_64/stable/gpgcheck=0enabled=1[root@localhost ~]# yum repolist//查看仓库状态 [root@localhost ~]# yum -y install docker-ce//默认下载最新版，时间慢，一般不用这个 因为网速原因，所以我们一般可以采取另外- -种方法，从我们国内下载，国内很多网站都提供了docker-ce的镜像站，比如说阿里云、网易云、清华大学镜像站等。这里我们从阿里云下载的方式来下载。 3.阿里云下载方法二 12[root@localhost ~]# rm -rf /etc/yum.repos.d/docke-ce.repo//删除刚刚的yum源 进入阿里镜像站 https://developer.aliyun.com/mirror 12345[root@localhost ~]# yum install -y yum-utils device-mapper-persistent-data lvm2[root@localhost ~]# yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo[root@localhost ~]# ls /etc/yum.repos.d///查看yum源 12[root@localhost ~]# yum repolist//查看仓库状态 12345[root@localhost ~]# yum makecache//做yum缓存，提速[root@localhost ~]# yum list docker-ce.x86_64 --showduplicates | sort -r//查看docker可用的版本 //这里我们下载指定版本18.9.0，注意并没有采取阿里云官方推荐的方法，我们分别下载了docker-ce,docker-ce-cli和containerd.io这3个组件。 12[root@localhost ~]# yum -y install docker-ce-18.09.0-3.el7 docker-ce-cli-18.09.0-3.el7 tainerd.io-1.2.0-el7//安装docker-ce,docker-ce-cli和containerd.io这3个组件 4.安装完成之后 1234567[root@localhost ~]# systemctl start docker//开启docker[root@localhost ~]# systemctl enable docker//docker加入开机自启[root@localhost ~]# docker -vDocker version 18.09.0, build 4d60db4//查看docker版本是否是指定的版本 12[root@localhost ~]# docker version//查看docker版本信息 12如果是最小化安装，来装一个tab命令补全[root@localhost ~]# yum -y install bash-completion 二，Docker的基本概念 image:镜像 container：容器 repostry:仓库 镜像是容器运行的基石，容器是镜像运行之后的实例。 12[root@localhost ~]# docker pull centos:7//下载一个centos7镜像，特别慢不建议 1，设置加速 浏览器打开加速网站：道客云https://www.daocloud.io/ //使用docker镜像加速器，这里使用的是daocloud的加速器，当然还有其他的加速器，例 如阿里云、清华镜像站等。 [root@localhost ~]# curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://f1361db2.m.daocloud.io 1234567[root@localhost ~]# systemctl daemon-reload//守护进程[root@localhost ~]# systemctl restart docker//重启docker[root@localhost ~]# docker info//查看docker的详细信息 123[root@localhost ~]# cat /etc/docker/daemon.json&#123;\"registry-mirrors\": [\"http://f1361db2.m.daocloud.io\"]&#125;//都是键值对 12[root@localhost ~]# docker pull centos:7//再次下载centos7 12[root@localhost ~]# docker images//查看本地镜像有哪些 2，更改镜像加速网站为阿里云的 https://www.aliyun.com/product/acr?spm=5176.12825654.eofdhaal5.42.366f2c4axwzdLK&amp;aly_as=kt8HE3oy 1234[root@localhost ~]# cat /etc/docker/daemon.json&#123;\"registry-mirrors\": [\"http://f1361db2.m.daocloud.io\"]&#125; //把刚刚复制的https://x7bv0r2q.mirror.aliyuncs.com，替换掉上面的 也可以更改成这个网址，当然如果你更改之后，还需要执行reload命令，重新加载一下配置文件。 12345[root@docker ~]# systemctl daemon-reload [root@docker ~]# systemctl restart docker[root@docker ~]# docker pull centosUsing default tag: latestlatest: Pulling from library/centos 3，更改镜像加速网站为清华大学的 清华大学镜像站网址：https://mirrors.tuna.tsinghua.edu.cn/ 测试：下载一个nginx [root@localhost ~]# docker pull nginx 12[root@localhost ~]# docker images//查看本地镜像有哪些 12345[root@localhost ~]# docker run -itd -p 80 nginx//多执行几次，运行多台nginx[root@localhost ~]# docker ps//查看docker服务 浏览器测试 **开源项目：**诞生于2013年，dotcloud公司的业余项目，Go语言实现。—公司改名docker 集装箱：目标是实现轻量级的操作系统虚拟化方案。让用户不需要关心容器的管理，使得操作更加简便。 docker和虚拟机、传统虚拟化的区别： 传统的虚拟机：在硬件实现虚拟化，然后创建/安装操作系统。 docker：在操作系统层面实现虚拟化，直接服用本地主机的操作系统。 为什么使用docker 1，与传统虚拟化方式相比，具有众多的优势 a,docker容器启动在秒级 b,docker对系统资源利用率高，一台主机可以同时运行数千个docker容器 c,docker基本不消耗系统资源，使得运行在docker里面的应用的性能很高 2，其他优势： a,更快的支付和部署：开发者可以使用一个标准的镜像来构建一套开发容器，开发完成之后，运维人员可以直接使用这个容易来部署代码； b,更高级的虚拟化，docker容器的运行不需要额外的支持，它是内核级的虚拟化，因此可以实现更高的性能 c,更轻松的迁移和扩展：docker几乎可以在任意平台运行，比如物理机，虚拟机，公有云，私有云，个人电脑，服务器等。 d,更简单的管理：使用docker只需要简单的修改，就可以替代以往大量的更新工作。所有的修改都一增量方式被分发和更新，从而实现自动化并且高效的管理。 docker中的基本概念： 镜像（images):只读的模板，通过这个模板创建docker容器 容器(container):是使用镜像创建并运行的实例。可以简单的将容器看做是简化版的操作系统。(可以看做是操作系统是因为里面包含root用户权限，进程空间和网络空间，还包括运行在里面的应用程序) 仓库(repository):集中存放镜像文件的地方。分为共有仓库和私有仓库。","path":"posts/dd75.html","date":"06-07","excerpt":"","tags":[{"name":"docker","slug":"docker","permalink":"https://wsdlxgp.top/tags/docker/"},{"name":"nginx","slug":"nginx","permalink":"https://wsdlxgp.top/tags/nginx/"}]},{"title":"19 搭建Prometheus监控报警","text":"基于上一篇博客继续进行部署 一、Prometheus &amp; AlertManager 介绍 Prometheus 是一套开源的系统监控、报警、时间序列数据库的组合，最初有 SoundCloud 开发的，后来随着越来越多公司使用，于是便独立成开源项目。Alertmanager 主要用于接收 Prometheus 发送的告警信息，它支持丰富的告警通知渠道，例如邮件、微信、钉钉、Slack 等常用沟通工具，而且很容易做到告警信息进行去重，降噪，分组等，是一款很好用的告警通知系统。 二、基本概念 Prometheus 官网（https://prometheus.io/） 是一套开源的监控和报警系统，也是一个时序数据库。 架构图 基本原理 Prometheus的基本原理是通过HTTP协议周期性抓取被监控组件的状态，任意组件只要提供对应的HTTP接口就可以接入监控。不需要任何SDK或者其他的集成过程。这样做非常适合做虚拟化环境监控系统，比如VM、Docker、Kubernetes等。输出被监控组件信息的HTTP接口被叫做exporter 。目前互联网公司常用的组件大部分都有exporter可以直接使用，比如Varnish、Haproxy、Nginx、MySQL、Linux系统信息(包括磁盘、内存、CPU、网络等等)。 服务过程 Prometheus Daemon负责定时去目标上抓取metrics(指标)数据，每个抓取目标需要暴露一个http服务的接口给它定时抓取。Prometheus支持通过配置文件、文本文件、Zookeeper、Consul、DNS SRV Lookup等方式指定抓取目标。Prometheus采用PULL的方式进行监控，即服务器可以直接通过目标PULL数据或者间接地通过中间网关来Push数据。 2.Prometheus在本地存储抓取的所有数据，并通过一定规则进行清理和整理数据，并把得到的结果存储到新的时间序列中。 3.Prometheus通过PromQL和其他API可视化地展示收集的数据。Prometheus支持很多方式的图表可视化，例如Grafana、自带的Promdash以及自身提供的模版引擎等等。Prometheus还提供HTTP API的查询方式，自定义所需要的输出。 4.PushGateway支持Client主动推送metrics到PushGateway，而Prometheus只是定时去Gateway上抓取数据。 Alertmanager是独立于Prometheus的一个组件，可以支持Prometheus的查询语句，提供十分灵活的报警方式。 工作流程 通过exporters从数据源主动拉取数据（metrics），保存到时序数据库（TSDB）中，可以通过HTTP Server访问，同时可以发起报警，对于数据库中的时序数据，提供PromeQL进行查询，提供给web UI或者可视化系统Grafana等展示。 Grafana 官网（https://grafana.com/） 开源的数据分析和监控平台 有不同的dashboards支持不同类型的数据可视化 Exporters 数据采集 Prometheus从不同的exorters中拉取数据，有不同的exporter支持不同的数据源 node-exporter 支持机器基本的数据 比如cpu mem 网络 等 docker01 docker02 docker03 192.168.1.11 192.168.1.13 192.168.1.20 NodeEXporter NodeEXporter NodeEXporter cAdvisor cAdvisor cAdvisor Prometheus Server 空 空 Grafana 空 空 全部关闭防火墙，禁用selinux 四、设置prometheus监控报警 接下来，我们需要启动 AlertManager 来接受 Prometheus 发送过来的报警信息，并执行各种方式的告警。同样以 Docker 方式启动 AlertManager，最简单的启动命令如下： 1$ docker run --name alertmanager -d -p 9093:9093 prom/alertmanager:latest 这里 AlertManager 默认启动的端口为 9093，启动完成后，浏览器访问 http://:9093 可以看到默认提供的 UI 页面，不过现在是没有任何告警信息的，因为我们还没有配置报警规则来触发报警。 配置AlertManager AlertManager：用来接收prometheus发送来的报警信息，并且执行设置好的报警方式、报警内容。 下载镜像 12[root@docker01 ~]# docker pull alertmanager//下载alertmanager镜像 基于alertmanager运行一台容器 1[root@docker01 ~]# docker run -d --name alertmanager -p 9093:9093 prom/alertmanager:latest 配置路由转发 12[root@docker01 ~]# echo net.ipv4.ip_forward = 1 &gt;&gt; /etc/sysctl.conf [root@docker01 ~]# sysctl -p 在部署alertmanager之前，我们需要对它的配置文件进行修改,所以我们先运行一个容器，先将其配置文件拷贝出来。 12[root@docker01 ~]# docker cp alertmanager:/etc/alertmanager/alertmanager.yml .///拷贝alertmanager的配置文件到本地 修改alertmanager的配置文件 配置文件简单介绍 AlertManager：用来接收Prometheus发送的报警信息，并且执行设置好的报警方式，报警内容。 AlertManager.yml配置文件： global：全局配置，包括报警解决后的超时时间、SMTP相关配置、各种渠道通知的API地址等消息。 route：用来设置报警的分发策略。 receivers：配置报警信息接收者信息。 inhibit_rules：抑制规则配置，当存在与另一个匹配的报警时，抑制规则将禁用用于有匹配的警报。 修改配置文件 123456789101112131415161718192021222324252627[root@docker01 ~]# vim alertmanager.yml //修改alertmanager配置文件global: resolve_timeout: 5m smtp_from: '2877364346@qq.com' #自己邮箱地址 smtp_smarthost: 'smtp.qq.com:465' #qq的邮箱地址及端口 smtp_auth_username: '2877364346@qq.com' smtp_auth_password: 'osjppnjkbuhcdfff' #需要在qq邮箱获取授权码 smtp_require_tls: false smtp_hello: 'qq.com'route: group_by: ['alertname'] group_wait: 5s group_interval: 5s repeat_interval: 5m receiver: 'email' #接收者改为邮箱receivers:- name: 'email' email_configs: - to: '2877364346@qq.com' send_resolved: true inhibit_rules: - source_match: severity: 'critical' target_match: severity: 'warning' equal: ['alertname', 'dev', 'instance'] 以上配置我反复试验后，发现不同的环境参数配置也不一样，调试期间出现了各种报错问题，将其中几个关键的配置说明一下： 1、smtp_smarthost: 这里为 QQ 邮箱 SMTP 服务地址，官方地址为 smtp.qq.com 端口为 465 或 587，同时要设置开启 POP3/SMTP 服务。 2、smtp_auth_password: 这里为第三方登录 QQ 邮箱的授权码，非 QQ 账户登录密码，否则会报错，获取方式在 QQ 邮箱服务端设置开启 POP3/SMTP 服务时会提示。 3、smtp_require_tls: 是否使用 tls，根据环境不同，来选择开启和关闭。如果提示报错 email.loginAuth failed: 530 Must issue a STARTTLS command first，那么就需要设置为 true。着重说明一下，如果开启了 tls，提示报错 starttls failed: x509: certificate signed by unknown authority，需要在 email_configs 下配置 insecure_skip_verify: true 来跳过 tls 验证。 重新运行 alertmanager 容器 1234[root@docker01 ~]# docker rm -f alertmanager//删除alertmanager容器[root@docker01 ~]# docker run -d --name alertmanager -v /root/alertmanager.yml:/etc/alertmanager/alertmanager.yml -p 9093:9093 prom/alertmanager:latest //运行一台新的alertmanager容器，记得挂载配置文件 Prometheus配置和alertmanager报警规则 创建存放规则的目录 123[root@docker01 ~]# mkdir -p prometheus/rules//创建规则目录[root@docker01 ~]# cd prometheus/rules/ 编写规则 123456789101112[root@docker01 rules]# vim node-up.rules groups:- name: node-up rules: - alert: node-up expr: up&#123;job=\"prometheus\"&#125; == 0 #&#123;job=\"prometheus\"&#125;中的prometheus需要和prometheus配置文件23行的相同 for: 15s labels: severity: 1 team: node annotations: summary: \"&#123;&#123; $labels.instance &#125;&#125; 已停止运行超过 15s！\" 说明一下：该 rules 目的是监测 node 是否存活，expr 为 PromQL 表达式验证特定节点 job=“node-exporter” 是否活着，for 表示报警状态为 Pending 后等待 15s 变成 Firing 状态，一旦变成 Firing 状态则将报警发送到 AlertManager，labels 和 annotations 对该 alert 添加更多的标识说明信息，所有添加的标签注解信息，以及 prometheus.yml 中该 job 已添加 label 都会自动添加到邮件内容中，更多关于 rule 详细配置可以参考 这里。 修改 prometheus配置文件 123456789101112[root@docker01 ~]# vim prometheus.yml # Alertmanager configuration #7alerting: alertmanagers: - static_configs: - targets: - 192.168.1.11:9093 #去注释修改# Load rules once and periodically evaluate them according to the global 'evaluation_interval'. #14行rule_files: - \"/usr/local/prometheus/rules/*.rules\" #添加（这个路径是prometheus容器内的路径） 注意: 这里 rulefiles 为容器内路径，需要将本地 node-up.rules 文件挂载到容器内指定路径，修改 Prometheus 启动命令如下，并重启服务。 重新运行prometheus 容器 1234[root@docker01 ~]# docker rm -f prometheus //删除prometheus容器[root@docker01 ~]# docker run -d -p 9090:9090 --name prometheus --net=host -v /root/prometheus.yml:/etc/prometheus/prometheus.yml -v /root/prometheus/rules/node-up.rules:/usr/local/prometheus/rules/node-up.rules prom/prometheus//运行一台新的alertmanager容器，记得挂载规则文件 浏览器验证一下http://192.168.1.11:9090/rules 这里说明一下 Prometheus Alert 告警状态有三种状态：Inactive、Pending、Firing。 Inactive：非活动状态，表示正在监控，但是还未有任何警报触发。 Pending：表示这个警报必须被触发。由于警报可以被分组、压抑/抑制或静默/静音，所以等待验证，一旦所有的验证都通过，则将转到 Firing 状态。 Firing：将警报发送到 AlertManager，它将按照配置将警报的发送给所有接收者。一旦警报解除，则将状态转到 Inactive，如此循环。 挂起docker02 会收到邮件 这里有几个地方需要解释一下： 每次停止/恢复服务后，15s 之后才会发现 Alert 状态变化，是因为 prometheus.yml中 global -&gt; scrape_interval: 15s 配置决定的，如果觉得等待 15s 时间太长，可以修改小一些，可以全局修改，也可以局部修改。例如局部修改 node-exporter 等待时间为 5s。 … - job_name: ‘node-exporter’ scrape_interval: 5s file_sd_configs: - files: [’/usr/local/prometheus/groups/nodegroups/*.json’] Alert 状态变化时会等待 15s 才发生改变，是因为 node-up.rules 中配置了 for: 15s 状态变化等待时间。 报警触发后，每隔 5m 会自动发送报警邮件(服务未恢复正常期间)，是因为 alertmanager.yml 中 route -&gt; repeat_interval: 5m 配置决定的。 五、AlertManager自定义邮件模板 创建模板目录 1234[root@docker01 ~]# cd prometheus//进入之前创建的prometheus目录[root@docker01 prometheus]# mkdir alertmanager-tmpl//创建AlertManager模板目录 看到上边默认发送的邮件模板，虽然所有核心的信息已经包含了，但是邮件格式内容可以更优雅直观一些，那么，AlertManager 也是支持自定义邮件模板配置的，首先新建一个模板文件 编写模板规则 123456789101112131415[root@docker01 prometheus]# vim email.tmpl &#123;&#123; define \"email.from\" &#125;&#125;2877364346@qq.com&#123;&#123; end &#125;&#125;&#123;&#123; define \"email.to\" &#125;&#125;2877364346@qq.com&#123;&#123; end &#125;&#125;&#123;&#123; define \"email.to.html\" &#125;&#125;&#123;&#123; range .Alerts &#125;&#125;=========start==========&lt;br&gt;告警程序: prometheus_alert&lt;br&gt;告警级别: &#123;&#123; .Labels.severity &#125;&#125; 级&lt;br&gt;告警类型: &#123;&#123; .Labels.alertname &#125;&#125;&lt;br&gt;故障主机: &#123;&#123; .Labels.instance &#125;&#125;&lt;br&gt;告警主题: &#123;&#123; .Annotations.summary &#125;&#125;&lt;br&gt;触发时间: &#123;&#123; .StartsAt.Format \"2019-08-04 16:58:15\" &#125;&#125; &lt;br&gt;=========end==========&lt;br&gt;&#123;&#123; end &#125;&#125;&#123;&#123; end &#125;&#125; 简单说明一下，上边模板文件配置了 email.from、email.to、email.to.html 三种模板变量，可以在 alertmanager.yml 文件中直接配置引用。这里 email.to.html 就是要发送的邮件内容，支持 Html 和 Text 格式，这里为了显示好看，采用 Html 格式简单显示信息。下边 {{ range .Alerts }} 是个循环语法，用于循环获取匹配的 Alerts 的信息，下边的告警信息跟上边默认邮件显示信息一样，只是提取了部分核心值来展示。然后，需要增加 alertmanager.yml 文件 templates 配置如下： 修改alertmanager的配置文件 1234567891011121314151617181920212223242526272829[root@docker01 ~]# vim alertmanager.yml global: resolve_timeout: 5m smtp_from: '2877364346@qq.com' smtp_smarthost: 'smtp.qq.com:465' smtp_auth_username: '2877364346@qq.com' smtp_auth_password: 'evjmqipqezlbdfij' smtp_require_tls: false smtp_hello: 'qq.com'templates: #添加模板 - '/etc/alertmanager-tmpl/*.tmpl' #添加路径 route: group_by: ['alertname'] group_wait: 5s group_interval: 5s repeat_interval: 5m receiver: 'email' receivers:- name: 'email' email_configs: - to: '&#123;&#123; template \"email to\" &#125;&#125;' #修改 html: '&#123;&#123; template \"email.to.html\" .&#125;&#125;' #添加 send_resolved: true #删除 inhibit_rules: - source_match: severity: 'critical' target_match: severity: 'warning' equal: ['alertname', 'dev', 'instance'] 重新运行 alertmanager 容器 1234[root@docker01 ~]# docker rm -f alertmanager//删除alertmanager容器[root@docker01 ~]# docker run -itd --name alertmanager -p 9093:9093 -v /root/alertmanager.yml:/etc/alertmanager/alertmanager.yml -v /root/prometheus/alertmanager-tmpl:/etc/alertmanager-tmpl prom/alertmanager:latest//运行一台新的alertmanager容器，记得挂载配置文件 挂起docker02 收到邮件 当然我们还可以配置邮件标题，这里就不在演示了，详细配置可参考 这里。这里除了监控节点是否存活外，还可以监控很多很多指标，例如 CPU 负载告警、Mem 使用量告警、Disk 存储空间告警、Network 负载告警等等，这些都可以通过自定义 PromQL 表达式验证值来定义一些列的告警规则，来丰富日常工作中需要的各种告警。 这里，我们只演示了如何通过 AlertManager 来配置发送邮件告警，其他的告警方式，可以参考 官网文档 来配置，这里就不再演示了。下一篇，我们继续通过 Prometheus 来监控 SpringBoot 工程应用程序 JVM 情况，以及自定义 metrics 来实现特定功能的监控。","path":"posts/babe.html","date":"06-07","excerpt":"","tags":[{"name":"prometheus","slug":"prometheus","permalink":"https://wsdlxgp.top/tags/prometheus/"},{"name":"alertmanager","slug":"alertmanager","permalink":"https://wsdlxgp.top/tags/alertmanager/"}]},{"title":"18 基于docker搭建Prometheus+Grafana","text":"一、介绍Prometheus Prometheus（普罗米修斯）是一套开源的监控&amp;报警&amp;时间序列数据库的组合，起始是由SoundCloud公司开发的。随着发展，越来越多公司和组织接受采用Prometheus，社会也十分活跃，他们便将它独立成开源项目，并且有公司来运作。Google SRE的书内也曾提到跟他们BorgMon监控系统相似的实现是Prometheus。现在最常见的Kubernetes容器管理系统中，通常会搭配Prometheus进行监控。 Prometheus基本原理是通过HTTP协议周期性抓取被监控组件的状态，这样做的好处是任意组件只要提供HTTP接口就可以接入监控系统，不需要任何SDK或者其他的集成过程。这样做非常适合虚拟化环境比如VM或者Docker 。 Prometheus应该是为数不多的适合Docker、Mesos、Kubernetes环境的监控系统之一。 与其他监控系统相比，Prometheus的主要特点是： 一个多维数据模型（时间序列由指标名称定义和设置键/值尺寸）。 非常高效的存储，平均一个采样数据占~3.5bytes左右，320万的时间序列，每30秒采样，保持60天，消耗磁盘大概228G。 一种灵活的查询语言。 不依赖分布式存储，单个服务器节点。 时间集合通过HTTP上的PULL模型进行。 通过中间网关支持推送时间。 通过服务发现或静态配置发现目标。 多种模式的图形和仪表板支持。 二、Prometheus架构概览 该图说明了普罗米修斯（Prometheus）及其一些生态系统组件的整体架构： 它的服务过程是这样的Prometheus daemon负责定时去目标上抓取metrics(指标) 数据，每个抓取目标需要暴露一个http服务的接口给它定时抓取。 Prometheus：支持通过配置文件、文本文件、zookeeper、Consul、DNS SRV lookup等方式指定抓取目标。支持很多方式的图表可视化，例如十分精美的Grafana，自带的Promdash，以及自身提供的模版引擎等等，还提供HTTP API的查询方式，自定义所需要的输出。 Alertmanager：是独立于Prometheus的一个组件，可以支持Prometheus的查询语句，提供十分灵活的报警方式。 PushGateway：这个组件是支持Client主动推送metrics到PushGateway，而Prometheus只是定时去Gateway上抓取数据。 如果有使用过statsd的用户，则会觉得这十分相似，只是statsd是直接发送给服务器端，而Prometheus主要还是靠进程主动去抓取。 大多数Prometheus组件都是用Go编写的，它们可以轻松地构建和部署为静态二进制文件。访问prometheus.io以获取完整的文档，示例和指南。 三、Prometheus四种数据类型 Counter Counter用于累计值，例如记录请求次数、任务完成数、错误发生次数。一直增加，不会减少。重启进程后，会被重置。 例如：http_response_total{method=”GET”,endpoint=”/api/tracks”} 100，10秒后抓取http_response_total{method=”GET”,endpoint=”/api/tracks”} 100。 Gauge Gauge常规数值，例如 温度变化、内存使用变化。可变大，可变小。重启进程后，会被重置。 例如： memory_usage_bytes{host=”master-01″} 100 &lt; 抓取值、memory_usage_bytes{host=”master-01″} 30、memory_usage_bytes{host=”master-01″} 50、memory_usage_bytes{host=”master-01″} 80 &lt; 抓取值。 Histogram Histogram（直方图）可以理解为柱状图的意思，常用于跟踪事件发生的规模，例如：请求耗时、响应大小。它特别之处是可以对记录的内容进行分组，提供count和sum全部值的功能。 例如：{小于10=5次，小于20=1次，小于30=2次}，count=7次，sum=7次的求和值。 Summary Summary和Histogram十分相似，常用于跟踪事件发生的规模，例如：请求耗时、响应大小。同样提供 count 和 sum 全部值的功能。 例如：count=7次，sum=7次的值求值。 **它提供一个quantiles的功能，可以按%比划分跟踪的结果。**例如：quantile取值0.95，表示取采样值里面的95%数据。 五、实验环境 docker01 docker02 docker03 192.168.1.11 192.168.1.13 192.168.1.20 NodeEXporter NodeEXporter NodeEXporter cAdvisor cAdvisor cAdvisor Prometheus Server 空 空 Grafana 空 空 全部关闭防火墙，禁用selinux 需要部署的组件： Prometheus Server:普罗米修斯的主服务器。 Prometheus是一个开源的服务监控系统，它通过HTTP协议从远程的机器收集数据并存储在本地的时序数据库上。 多维数据模型（时序列数据由metric名和一组key/value组成） 在多维度上灵活的查询语言(PromQl) 不依赖分布式存储，单主节点工作. 通过基于HTTP的pull方式采集时序数据 可以通过push gateway进行时序列数据推送(pushing) 可以通过服务发现或者静态配置去获取要采集的目标服务器 多种可视化图表及仪表盘支持 Prometheus通过安装在远程机器上的exporter来收集监控数据，后面我们将使用到node_exporter收集系统数据。 NodeEXporter:负责收集Host硬件信息和操作系统信息。 cAdvisor:负责收集Host.上运行的容器信息。 Grafana:负责展示普罗米修斯监控界面。 Grafana 是一个开箱即用的可视化工具，具有功能齐全的度量仪表盘和图形编辑器，有灵活丰富的图形化选项，可以混合多种风格，支持多个数据源特点。 这些可以直接docker pull下载镜像（现在是本地导入镜像） 本地上传镜像 docker01 1[09:05:42][docker01$ docker load -i node-exporter.tar &amp;&amp; docker load -i mycadvisor.tar &amp;&amp; docker load -i prometheus.tar &amp;&amp; docker load -i grafana.tar docker02和docker03 1[09:05:22]docker03]$ docker load -i node-exporter.tar &amp;&amp; docker load -i mycadvisor.tar 六、各主机部署 1) 3个节点，全部部署node-EXporter,和cAdvisor. 部署安装node-EXporter收集节点硬件和操作系统信息。 12[09:21:03[docker01]$ docker run -d -p 9100:9100 -v /proc:/host/proc -v /sys:/host/sys -v /:/rootfs --net=host prom/node-exporter --path.procfs /host/proc --path.sysfs /host/sys --collector.filesystem.ignored-mount-points \"^/(sys|proc|dev|host|etc)($|/)\"//部署node-EXporter,收集硬件和系统信息。 PS: 注意，这里使用了–net=host， 这样Prometheus Server可以直接与Node- EXporter通信。 验证:打开浏览器验证结果。http://192.168.1.11:9100/，http://192.168.1.13:9100/，http://192.168.1.20:9100/ 部署安装cAdvisor,收集节点容器信息。 1[09:39:10[docker01]$ docker run -v /:/rootfs:ro -v /var/run:/var/run/:rw -v /sys:/sys:ro -v /var/lib/docker:/var/lib/docker:ro --detach=true --name=cadvisor --net=host google/cadvisor 验证:打开浏览器验证结果。http://192.168.1.11:8080，http://192.168.1.13:8080，http://192.168.1.20:8080 2)在docker01上部署Prometheus Server服务。 在部署prometheus之前，我们需要对它的配置文件进行修改,所以我们先运行一个容器，先将其配置文件拷贝出来。 123409:51:22][docker01]$ docker run -d -p 9090:9090 --name prometheus --net=host prom/prometheus//打开一台Prometheus[09:51:00[docker01]$ docker cp prometheus:/etc/prometheus/prometheus.yml .///拷贝Prometheus的配置文件到本地 修改Prometheus的配置文件，添加监听端口（29行） 123[09:55:53][docker01][~]$ vim prometheus.yml //修改配置文件这里指定了prometheus的监控项，包括它也会监控自己手机到的数据。- targets: ['localhost:9090','localhost:8080','localhost:9100','192.168.1.13:8080','192.168.1.13:9100','192.168.1.20:8080','192.168.1.20:9100'] 重新运行prometheus容器 1234[10:00:27][docker01][~]$ docker rm -f prometheus //删除 prometheus容器[10:02:45][docker01][~]$ docker run -d -p 9090:9090 --name prometheus --net=host -v /root/prometheus.yml:/etc/prometheus/prometheus.yml prom/prometheus//运行一台新的 prometheus容器 浏览器访问，验证：http://192.168.1.11:9090/graph ps：这里能够查看到我们各个监控项。 如果现在挂起一台虚拟机（测试完之后继续运行） 3)在docker01.上,部署grafana服务,用来展示prometheus收集到的数据。 1234[root@docker01 ~]# mkdir grafana-storage//创建收集信息的目录[root@docker01 ~]# chmod 777 grafana-storage///给予777权限 1[root@docker01 ~]# docker run -d -p 3000:3000 --name grafana -v /root/grafana-storage:/var/lib/grafana -e \"GF_SECURITY_ADMIN_PASSWORD=123.com\" grafana/grafana **浏览器访问验证：**http://192.168.1.11:3000/login （&lt;默认&gt;用户名：admin，密码：123.com） 添加数据源 PS:看到这个提示， 说明prometheus和grafana服务的是 正常连接的。 此时，虽然grafana收集到了数据，但怎么显示它,仍然是个问题，grafana支持自定 义显示信息,不过要自定义起来非常麻烦，不过好在，grafana官方为我们提供了- -些模板，来供我们使用。 **grafana官网:**https://grafana.com/docs/grafana/latest/ 选中一款模板，然后，我们有2种方式可以套用这个模板。 第一种方式：通过JSON文件使用模板。 下载完成之后，来到grafana控制台 第二种导入模板的方式:** 可以直接通过模板的ID号。 //这个id不好用换成8321了 复制模板id之后，来到grafana控制台 排错思路 防火墙是否关闭，selinux是否禁用 主机名称是否更改 镜像是否正常 各服务启动时挂载目录是否正确 grafana服务，是否创建所需目录，目录是否有权限 Prometheus服务是否修改配置文件 总结 恭喜！您已经设置了Prometheus服务器，Node Exporter和Grafana 等所有这些都可以使用的Docker。尽管这些目前都在同一台机器上运行，但这仅用于演示目的。在生产设置中，通常会在每台受监控的计算机上运行节点导出器，多个Prometheus服务器（根据组织的需要），以及单个Grafana服务器来绘制来自这些服务器的数据。","path":"posts/5755.html","date":"06-07","excerpt":"","tags":[{"name":"prometheus","slug":"prometheus","permalink":"https://wsdlxgp.top/tags/prometheus/"},{"name":"grafana","slug":"grafana","permalink":"https://wsdlxgp.top/tags/grafana/"}]},{"title":"15 nginx+docker+nfs部署","text":"一．体系架构 在Keepalived + Nginx高可用负载均衡架构中，keepalived负责实现High-availability (HA) 功能控制前端机VIP（虚拟网络地址），当有设备发生故障时，热备服务器可以瞬间将VIP自动切换过来，实际运行中体验只有2秒钟切换时间，DNS服务可以负责前端VIP的负载均衡。 nginx负责控制后端web服务器的负载均衡，将客户端的请求按照一定的算法转发给后端Real Server处理，而Real Server将响应直接返回给客户端。 nfs服务器做实时备份，给web服务器提供web界面。 二．简单原理 NGINX_MASTER、NGINX_BACKUP两台服务器均通过keepalived软件把ens33网卡绑上一个虚拟IP（VIP）地址192.168.1.40，此VIP当前由谁承载着服务就绑定在谁的ens32上，当NGINX_MASTER发生故障时，NGINX_BACKUP会通过/etc/keepalived/keepalived.conf文件中设置的心跳时间advert_int 1检查，无法获取NGINX_MASTER正常状态的话，NGINX_BACKUP会瞬间绑定VIP来接替nginx_master的工作，当NGINX_MASTER恢复后keepalived会通过priority参数判断优先权将虚拟VIP地址192.168.1.40重新绑定给NGINX_MASTER的ens33网卡。 使用此方案的优越性 1.实现了可弹性化的架构，在压力增大的时候可以临时添加web服务器添加到这个架构里面去; 2.upstream具有负载均衡能力，可以自动判断后端的机器，并且自动踢出不能正常提供服务的机器； 3.相对于lvs而言，正则分发和重定向更为灵活。而Keepalvied可保证单个nginx负载均衡器的有效性，避免单点故障； 4.用nginx做负载均衡，无需对后端的机器做任何改动。 5.nginx部署在docker容器里，即大量地节约开发、测试、部署的时间，又可以在出现故障时通过镜像快速恢复业务。 三、系统环境 两台负载机器安装：，nginx+docker+nfs 分别命名为：NGINX_MASTER，NGINX_BACKUP。 后端web服务器，可以是提供web服务的任何架构，分别命名为：WEB_1，WEB_2。 后端数据库机器可任意架构，只要能提供数据库服务即可。 服务器 IP地址 安装软件 NGINX_MASTER 192.168.1.10 nginx+keepalived NGINX_BACKUP 192.168.1.20 nginx+keepalived WEB_1 192.168.1.11 docker+nginx WEB_2 192.168.1.13 docker+nginx nfs_MASTER 192.168.1.30 nfs+rsync+inotify nfs_BACKUP 192.168.1.10 nfs+rsync+inotify nginx（两台都是） 安装nginx 12345[root@nginx01 ~]# tar zxf nginx-1.14.0.tar.gz //解压nginx安装包[root@nginx01 ~]# cd nginx-1.14.0/[root@nginx01 nginx-1.14.0]# yum -y install openssl-devel pcre-devel zlib-devel//安装nginx依赖包 12[root@nginx01 nginx-1.14.0]# ./configure --prefix=/usr/local/nginx1.14 --with-http_dav_module --with-http_stub_status_module --with-http_addition_module --with-http_sub_module --with-http_flv_module --with-http_mp4_module --with-pcre --with-http_ssl_module --with-http_gzip_static_module --user=nginx --group=nginx &amp;&amp; make &amp;&amp; make install//编译安装nginx 12345678[root@nginx01 nginx-1.14.0]# useradd nginx -s /sbin/nologin -M//创建所需用户[root@nginx01 nginx-1.14.0]# ln -s /usr/local/nginx1.14/sbin/nginx /usr/local/sbin///链接命令[root@nginx01 nginx-1.14.0]# nginx //开启nginx[root@nginx01 nginx-1.14.0]# netstat -anpt | grep nginx//查看nginx是否开启 部署nginx 12[root@nginx01 ~]# cd /usr/local/nginx1.14/conf/[root@nginx01 conf]# vim nginx.conf ​ http模块加 1234upstream backend &#123;server 192.168.1.11:90 weight=1 max_fails=2 fail_timeout=10s;server 192.168.1.13:90 weight=1 max_fails=2 fail_timeout=10s;&#125; location / { # root html; # index index.html index.htm; proxy_pass http://backend; #添加 } 高可用环境 安装keepalived 1[root@nginx02 nginx-1.14.0]# yum -y install keepalived 配置keepalived 修改主和备nginx服务器上的keepalived 配置文件 /etc/keepalived/keepalived.conf 文件 主nginx 修改主nginx下/etc/keepalived/keepalived.conf文件 123456789101112131415161718! Configuration File for keepalivedglobal_defs &#123; router_id LVS_DEVEL&#125; vrrp_instance VI_1 &#123; state MASTER interface ens33 virtual_router_id 51 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.1.40 &#125;&#125; 备nginx 修改备nginx下 /etc/keepalived /keepalived.conf文件 配置备nginx时需要注意：需要修改state为BACKUP , priority比MASTER低，virtual_router_id和master的值一致 12345678910111213141516171819! Configuration File for keepalivedglobal_defs &#123; router_id TWO&#125;vrrp_instance VI_1 &#123; state BACKUP interface ens33 virtual_router_id 1 priority 99 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.1.40 &#125;&#125; 测试（在做完docker的时候） 主备nginx都启动keepalived 1systemctl start keepalived 12[root@nginx01 conf]# curl 192.168.1.40wsd666 nfs（两台都是) nfs操作 12345678910[root@localhost ~]# yum -y install nfs-utils//下载nfs服务[root@nfs ~]# mkdir /database//创建共享目录[root@nfs02 ~]# chmod 777 /database///设置权限[root@nfs ~]# vim /etc/exports//设置权限如下/database *(rw,sync,no_root_squash) 开启各项服务 1234[root@nfs ~]# systemctl start rpcbind[root@nfs ~]# systemctl enable rpcbind[root@nfs ~]# systemctl start nfs-server[root@nfs ~]# systemctl enable nfs-server docker01和docker02测试nfs 1234567891011121314[root@nfs01 ~]# vim /etc/rsyncd.conf //建立rsync配置文件uid = nobodygid = nobodyuse chroot = yesaddress = 192.168.1.30port 873log file = /var/log/rsyncd.logpid file = /var/run/rsyncd.pidhosts allow = 192.168.1.0/24[wwwroot]path = /databaseread only = nodont compress = *.gz *.bz2 *.rar *.zip 123456[root@nfs01 ~]# mkdir /database//创建共享目录[root@nfs01 ~]# rsync --daemon//启动rsync[root@nfs01 ~]# netstat -anpt | grep rsync//查看端口 如果需要重启rsync服务，需要： 12345[root@localhost ~]# kill $(cat /var/run/rsyncd.pid)//停止服务[root@localhost ~]# rsync --daemon//启动服务[root@localhost ~]# kill -9 $(cat /var/run/rsyncd.pid) 或者直接使用“netstat -anpt | grep rsync”命令查出进程号，使用“kill 进程号”一样。 使用第一种方法停止rsync服务必须删除存放rsync服务进程的文件： 1[root@localhost ~]# rm -rf /var/run/rsyncd.pid 使用rsync备份工具 配置好rsync同步源服务器之后，客户端就可以使用rsync工具来执行远程同步了。 与rsync主机同步 123456789101112131415rsync命令的选项：-r：递归模式，包含目录及子目录中所有文件-l：对于符号链接文件仍然复制为符号链接文件-p：保留文件的权限标记-t：保留文件的时间标记-g：保留文件的属组标记（仅超级用户使用）-o：保留文件的属主标记（仅超级用户使用）-D：保留设备文件及其他特殊文件-a：归档模式，递归并保留对象属性，等同于 -rlptgoD-v：显示同步过程的详细（verbose）信息-z：在传输文件时进行压缩（compress）-H：保留硬连接文件-A：保留ACL属性信息--delete：删除目标位置有而原始位置没有的文件--checksum：根据对象的校验和来决定是否跳过文件 rsync是一款快速增量备份工具，支持： （1）本地复制； （2）与其他SSH同步； （3）与rsync主机同步。 手动与rsync主机同步 123[root@localhost ~]# rsync -avz 192.168.1.1::wwwroot /root或者[root@localhost ~]# rsync -avz rsync://192.168.1.1/wwwroot /root 123[root@nfs01 database]# vim index.htmlxgp666//创建测试目录 配置inotify+rsync实时同步（两台都是） (1)、软件安装 12rpm -q rsync //查询rsync是否安装，一般为系统自带安装yum install rsync -y //若没有安装，使用yum安装 安装inotify软件包 123[root@nfs02 ~]# tar zxf inotify-tools-3.14.tar.gz [root@nfs02 ~]# cd inotify-tools-3.14/[root@nfs02 inotify-tools-3.14]# ./configure &amp;&amp; make &amp;&amp; make install （2）调整inotify内核参数 1234567[root@nfs02 ~]# vim /etc/sysctl.conffs.inotify.max_queued_events = 16384fs.inotify.max_user_instances = 1024fs.inotify.max_user_watches = 1048576[root@nfs02 ~]# sysctl -p//生效 (3) 编写触发式同步脚本 123456789#!/bin/bashA=\"inotifywait -mrq -e modify,move,create,delete /database/\"B=\"rsync -avz /database/ 192.168.1.40::wwwroot\"$A | while read DIRECTORY EVENT FILEdo if [ $(pgrep rsync | wc -l) -gt 0 ] ; then $B fidone 此处需要注意，在两台服务器需要同步的目录之间，也需要将目录权限放到最大，避免因目录本身权限报错。 1[root@nfs01 inotify-tools-3.14]# chmod +x /opt/ino.sh 设置脚本开机自启 123[root@nfs01 database]# vim /etc/rc.d/rc.local /opt/ino.sh &amp;/usr/bin/rsync --daemon 源服务器端测试 执行脚本后，当前终端会变成实时监控界面，需要重新打开终端操作。 在源服务器端共享模块目录下进行文件操作，然后去备份服务器下，可观察到文件已经被实时同步。 docker(两台都是) 123[root@docker01 ~]# docker pull nginx[root@docker01 ~]# mkdir -p /www //创建挂载目录 nfs创建好之后docker上挂载目录 1[root@docker01 ~]# mount -t nfs 192.168.1.30:/database /www 1[root@docker01 ~]# docker run -itd --name nginx -p 90:80 -v /www/index.html:/usr/share/nginx/html/index.html nginx:latest 测试 1、当NGINX_MASTER、NGINX_BACKUP服务器nginx均正常工作时 在NGINX_MASTER上： 在NGINX_BACKUP上： master服务器ens32网卡正常绑定VIP，而backup却没有绑定，通过浏览器可正常访问网站。 2、关闭NGINX_MASTER的nginx容器 当nginx容器停止后，马上就又启起来了，nginx启动脚本没问题 3、关闭NGINX_MASTER的keepalived服务 在NGINX_MASTER上： 在NGINX_BACKUP上： NGINX_BACKUP的ens32网卡已瞬间绑定VIP，通过浏览器访问网站正常。 4、将NGINX_MASTER的keepalived服务启动 在NGINX_MASTER上： 在NGINX_BACKUP上： NGINX_MASTER的ens32网卡重新绑定VIP，通过浏览器访问网站正常。 5、关闭WEB_1服务器，通过浏览器访问网站正常。 排错 首先查看nginx配置文件是否有问题 两台keepakived的各项参数是否正常 docker上nginx是否映射端口，挂载nfs的共享目录。 nfs是否设置目录权限。是否配置rsync+inotify，写一个shell来做实时备份。 总结： 首先是镜像，就是拉取nginx的镜像。然后再把nginx镜像重建一下，就是变成我们需要的，主要就是改配置文件。然后把所有镜像push到harbor上 搭建nginx，做反向代理。 搭建docker，安装nginx镜像做测试做页面，测试面是从nfs共享来的。 搭建NFS，为了实现数据共享，包括数据库，就是持久化的。还要通过rsync+inotify，做到实时备份。","path":"posts/24f3.html","date":"06-07","excerpt":"","tags":[{"name":"nginx","slug":"nginx","permalink":"https://wsdlxgp.top/tags/nginx/"},{"name":"nfs","slug":"nfs","permalink":"https://wsdlxgp.top/tags/nfs/"}]},{"title":"20 Docker+Consul+registrator实现服务发现及nginx反向代理","text":"更改时间 12mv /etc/localtime/etc/localtime. bkcp /usr/share/zoneinfo/Asia/Shanghai/etc/localtime 查看端口 1[root@docker01 consul]# ss -lnt Consul:分布式、高可用的，服务发现和配置服务的工具。数据中心 Rigistrator:负责收集dockerhost_上,容器服务的信息，并且发送给consul Consul-tpmplate:根据编辑好的模板，生产新的nginx配置文件，并负责重新加载nginx配置文件 一. 架构设计 在现实中，我们一直渴望着追求提供高质量、高可用的服务架构体系，同时减少不必要的部署和维护代价，减少容错率。面对如此高的要求，可以有两种架构方案： Docker+Etcd+Confd+Nginx Docker+Consul+Nginx 本文中我们主要来介绍 Docker+Etcd+Confd+Nginx方案，此方案更加高效、快捷，并且维护代价和容错率更低，分布式支持力度更强，如下图所示： 上面示意图的大概流程如下： 1、docker01主机上以二进制包的方式部署consul服务并后台运行，其身份为leader； 2、docker02、docker03以容器的方式运行consul服务，并加入到docker01的consul群集中； 3、在主机docker02、docker03上后台运行registrator容器，使其自动发现docker容器提供的服务； 4、在docker01上部署Nginx，提供反向代理服务，docker02、docker03主机上基于Nginx镜像，各运行两个web容器，提供不同的网页文件，以便测试效果； 5、在docker01上安装consul-template命令，将收集到的信息（registrator收集到容器的信息）写入template模板中，并且最终写入Nginx的配置文件中。 6、至此，实现客户端通过访问Nginx反向代理服务器（docker01），获得docker02、docker03服务器上运行的Nginx容器提供的网页文件。 注：registrator是一个自动发现docker container提供的服务，并且在后端服务注册中心（数据中心）注册服务。主要用来收集容器运行服务的信息，并且发送给consul。数据中心除了consul外，还有etcd、zookeeper等。 二. 架构优势 Docker+Consul+Nginx虽然看起来是三个组件的运用，但却证明是一个有机的整体。它们互相联系、互相作用，完全满足我们对高可用、高效服务架构方案的需求，是Docker生态圈中最理想的组合之一，具有以下优势： 1.发现与注册组件consul使用 Raft 算法来保证一致性，比复杂的Paxos 算法更直接。相比较而言，zookeeper 采用的是 Paxos，而 etcd 使用的则是 Raft； 2.多数据中心，多数据中心集群可以避免单数据中心的单点故障，zookeeper 和 etcd 均不提供多数据中心功能的支持； 3.、实时发现及无感知服务刷新，具备资源弹性，伸缩自如； 4.健康检查，负载能动态在可用的服务实例上进行均衡，etcd 不提供此功能； 5.足够多台Docker容器(前提架构资源足以保证性能支撑)； 6.http 和dns 协议接口，zookeeper 的集成较为复杂，etcd 只支持 http 协议； 7.规模方便进行快速调整，官方提供web管理界面，etcd 无此功能； 8.nsul template 搭配consul使用，支持多种接入层，如Nginx、Haproxy。 三. 实验环境 主机 iP地址 服务 docker01 192.168.1.11 consul+consul-template+nginx docker02 192.168.1.13 consul+registrator docker03 192.168.1.20 consul+registrator 三台主机关闭防火墙，禁用selinux，更改主机名如上所述。 四. 部署consul服务 （1）docker01去官网https://www.consul.io/downloads.html下载consul服务 123456[root@docker01 ~]# unzip consul_1.5.1_linux_amd64.zip //现在是本地导入压缩包，需要解压 [root@docker01 ~]# mv consul /usr/local/bin///移动服务到bin目录[root@docker01 ~]# chmod +x /usr/local/bin/consul//给予一个可执行权限 （2）启动consul 1[root@docker01 ~]# consul agent -server -bootstrap -ui -data-dir=/var/lib/consul-data -bind=192.168.1.11 -client=0.0.0.0 -node=master PS: //-bootstrap: 加入这个选项时，一般都在server单节点的时候用，自选举为leader。 参数解释： -server：添加一个服务 -bootstrap：一般在server单节点的时候使用，自选举为leader。 -data-dir：key/volume指定数据存放的目录 -ui：开启内部的web界面 -bind：指定开启服务的ip -client：指定访问的客户端 -node：在集群内部通信使用的名称，默认是主机名。 现在这个ip是外部使用 PS:开启的端口 8300 集群节点 8301 集群内部的访问 8302 跨数据中心的通信 8500 web ui界面 8600 使用dns协议查看节点信息的端口 可参考下图查看端口的意思： 这时，这条命令会占用终端，可以使用nohup命令让它保持后台运行。 1[root@docker01 ~]# nohup consul agent -server -bootstrap -ui -data-dir=/var/lib/consule-data -bind=192.168.1.11 -client=0.0.0.0 -node=master &amp; （3）查看consul端口的信息 1[root@docker01 ~]# consul info （4）查看consul集群成员的信息 1[root@docker01 ~]# consul members 现在这个ip是内部使用 五. docker01下载部署consul-template 在 https://github.com/hashicorp/consul-template 上，下载consul-template 123456[root@docker01 ~]# unzip consul-template_0.19.5_linux_amd64.zip//解压安装好的consul-template包[root@docker01 ~]# mv consul-template /usr/local/bin///移动到命令目录[root@docker01 ~]# chmod +x /usr/local/bin/consul-template //给予一个可执行权限 六和七步骤简要说明 在docker01和docker02上操作 先来说一下在docker服务器上操作的大概思路： 分别在两台docker服务器上都创建registrator容器，注意到consul服务中心； 在docker01上运行两台nginx容器（端口随机生成），在docker02上运行两台nginx容器（端口随机生成）； 修改这4台nginx容器中的index.html页面内容为（xgp-web01、xgp-web02、xgp-web03、xgp-web04） 访问consul web界面验证 访问nginx服务器地址 http://192.168.1.11:8000 进行验证； 六. docker02，docker03，加入consul集群 这里我们采用容器的方式去运行consul服务。 （1）下载consu所需的l镜像 1[root@docker02 ~]# docker pull consul （2）基于consul镜像开启一台容器 1[root@docker02 ~]# docker run -d --name consul -p 8301:8301 -p 8301:8301/udp -p 8500:8500 -p 8600:8600 -p 8600:8600/udp --restart always progrium/consul -join 192.168.1.11 -advertise 192.168.1.13 -client 0.0.0.0 -node=node01 参数解释： -d：守护进程 –name：容器名称 –restart：容器随着docker服务一直运行 -advertise:声明本机地址 -join:声明服务端地址 -node:consul集群中的名称 （3）docker查看consul集群成员的信息 1[root@docker01 ~]# consul members （4）两台docker开启容器后，docker01查看 （5）浏览器访问http://192.168.1.11:8500 七. docker02、docker03 上部署registrator服务 registrator是一个能自动发现docker container提供的服务,并在后端服务注册中心注册服务或取消服务的工具，后端注册中心支持conusl、etcd、 skydns2、zookeeper等。 （1）下载registrator镜像 12[root@docker02 ~]# docker pull registrator//下载registrator镜像 （2）基于registrator镜像，开启一台容器 1[root@docker02 ~]# docker run -d --name registrator -v /var/run/docker.sock:/tmp/docker.sock --restart always gliderlabs/registrator consul://192.168.1.13:8500 参数说明： –network：把运行的docker容器设定为host网络模式； -v /var/run/docker.sock：把宿主机的Docker守护进程(Docker daemon)默认监听的Unix域套接字挂载到容器中； –ip : 刚才把network指定了host模式，所以我们指定下IP为宿主机的IP； consul:j最后这个选项是配置consul服务器的IP和端口。 （3）开启一台nginx容器 1[root@docker02 ~]# docker run -d —P --name nginx nginx:latest （4）浏览器查看一下http://192.168.1.11:8500/ui/dc1/nodes 八.docker01部署一个nginx服务 配置nginx，大概配置的思路为： 在/usr/local/nginx/conf中创建目录consul，目录名自定义； 在consul目录中创建nginx.ctmpl模板； 在nginx.conf配置中添加include项并指向consul目录 ； 重启nginx服务； （1）安装开启nginx服务 安装nginx依赖包 1[root@docker01 ~]# yum -y install pcre pcre-devel openssl openssl-devel zlib zlib-devel 编译安装nginx 12[root@docker01 ~]# cd nginx-1.14.0/[root@docker01 nginx-1.14.0]# ./configure --user=nginx --group=nginx --with-http_stub_status_module --with-http_realip_module --with-pcre --with-http_ssl_module &amp;&amp; make &amp;&amp; make install 创建所需用户和链接命令目录 12[root@docker01 nginx-1.14.0]# useradd -M -s /sbin/nologin nginx[root@docker01 nginx-1.14.0]# ln -s /usr/local/nginx/sbin/* /usr/local/bin/ 检查nginx是否有问题，并开启nginx 1234[root@docker01 nginx-1.14.0]# nginx -tnginx: the configuration file /usr/local/nginx/conf/nginx.conf syntax is oknginx: configuration file /usr/local/nginx/conf/nginx.conf test is successful[root@docker01 nginx-1.14.0]# nginx PS:这里nginx作为反向代理，代理后端docker02、 docker03 上nginx的容器服务,所以我们先去docker02、docker03. 上部署一些服务， 为了方便等会看到负载的效果，所以，我们运行完成容器之后，做一个主界面内容的区分。 （2）安装完成之后，本机测试访问 1[root@docker01 nginx-1.14.0]# curl 127.0.0.1 （3）docker02和docker03部署环境 主机 服务 docker02 nginx web01，web02 docker03 nginx web03，web04 &lt;1&gt;下载nginx镜像（docker02，docker03都要） 12[root@docker02 ~]# docker pull nginx//下载nginx镜像 &lt;2&gt;docker01操作 基于nginx镜像运行上述所说的容器并设置测试页面 1234567891011web01[root@docker02 ~]# docker run -itd --name web01 -P nginx:latest[root@docker02 ~]# docker exec -it web01 /bin/bashroot@44b59d07202f:/# cd /usr/share/nginx/html/root@44b59d07202f:/usr/share/nginx/html# echo web01 &gt; index.htmlweb02[root@docker02 ~]# docker run -itd --name web02 -P nginx:latest[root@docker02 ~]# docker exec -it web02 /bin/bashroot@44b59d07202f:/# cd /usr/share/nginx/html/root@44b59d07202f:/usr/share/nginx/html# echo web02 &gt; index.html &lt;3&gt;docker02操作 基于nginx镜像运行上述所说的容器并设置测试页面 12345678910111213web03[root@docker03 ~]# docker run -itd --name web03 -P nginx:latest[root@docker03 ~]# docker exec -it web03 /bin/bashroot@fd8e8b2df136:/# cd /usr/share/nginx/html/root@fd8e8b2df136:/usr/share/nginx/html# echo web03 &gt; index.htmlroot@fd8e8b2df136:/usr/share/nginx/html# exittrueweb04[root@docker03 ~]# docker run -itd --name web04 -P nginx:latest[root@docker03 ~]# docker exec -it web04 /bin/bashroot@fd8e8b2df136:/# cd /usr/share/nginx/html/root@fd8e8b2df136:/usr/share/nginx/html# echo web04 &gt; index.htmlroot@fd8e8b2df136:/usr/share/nginx/html# exit （4）docker01更改nginx配置文件 123456[root@docker01 ~]# cd /usr/local/nginx///进入nginx配置文件目录[root@docker01 nginx]# mkdir consul//创建consul目录[root@docker01 nginx]# cd consul///进入consul目录 &lt;1&gt;创建nginx.ctmpl模板 1234567891011121314[root@docker01 consul]# vim nginx.ctmplupstream http_backend &#123; &#123;&#123;range service \"nginx\"&#125;&#125; server &#123;&#123; .Address &#125;&#125;:&#123;&#123; .Port &#125;&#125;; &#123;&#123; end &#125;&#125;&#125;server &#123; listen 8000; server_name localhost; location / &#123; proxy_pass http://http_backend; &#125;&#125; &lt;2&gt;修改nginx配置文件，通过 include 参数包含刚刚创建的文件 123[root@docker01 consul]# cd /usr/local/nginx/conf/[root@docker01 conf]# vim nginx.conf include /usr/local/nginx/consul/*.conf; #文件最后添加（要在大括号里面） &lt;3&gt; 生成一个vhost.conf配置文件，并重启nginx（会占用终端) 使用consul-template命令，根据模板生产新的配置文件，并重新加载nginx的配置文件。 1[root@docker01 conf]# consul-template -consul-addr 192.168.1.11:8500 -template \"/usr/local/nginx/consul/nginx.ctmpl:/usr/local/nginx/consul/vhost.conf:/usr/local/bin/nginx -s reload\" 这时，这条命令会占用终端，可以使用nohup命令让它保持后台运行,并重启nginx服务。 1[root@docker01 conf]# nohup consul-template -consul-addr 192.168.1.11:8500 -template \"/usr/local/nginx/consul/nginx.ctmpl:/usr/local/nginx/consul/vhost.conf:/usr/local/sbin/nginx -s reload\" &amp; 查看一下文件是否生成，里面是否有内容 123[root@docker01 ~]# cd /usr/local/nginx/consul/[root@docker01 consul]# lsnginx.ctmpl vhost.conf 1[root@docker01 consul]# cat vhost.conf 此时，应该能够看到，新生产的vhost.conf配置文件已经生效，访问本机8000端口可以得到不同容器提供的服务。 &lt;4&gt;测试访问 12[root@docker01 consul]# curl 127.0.0.1:8000web01 此时可以看到负载均衡的效果！ &lt;5&gt;如果访问不成功 查看端口8000是否开启 1[root@docker01 consul]# ss -lnt 检查nginx配置文件 123[root@docker01 consul]# nginx -tnginx: the configuration file /usr/local/nginx/conf/nginx.conf syntax is oknginx: configuration file /usr/local/nginx/conf/nginx.conf test is successful 检查自己编写的nginx配置文件 123456789101112131415[root@docker01 consul]# cd /usr/local/nginx/consul/[root@docker01 consul]# cat nginx.ctmpl upstream http_backend &#123;true&#123;&#123;range service \"nginx\"&#125;&#125;trueserver &#123;&#123; .Address &#125;&#125;:&#123;&#123; .Port &#125;&#125;;true&#123;&#123; end &#125;&#125;&#125;server &#123;truelisten 8000;trueserver_name localhost;truelocation / &#123;truetrueproxy_pass http://http_backend;true&#125;&#125; 如果nginx配置文件没问题，重启nginx 1[root@docker01 consul]# nginx -s reload &lt;6&gt;测试自动发现 docker02 创建测试容器 12345[root@docker02 ~]# docker run -itd --name web05 -P nginx:latest[root@docker02 ~]# docker exec -it web05 /bin/bashroot@44b59d07202f:/# cd /usr/share/nginx/html/root@44b59d07202f:/usr/share/nginx/html# echo web02 &gt; index.html[root@docker02 ~]# docker ps docker01查看 12[root@docker01 consul]# cd /usr/local/nginx/consul/[root@docker01 consul]# cat vhost.conf docker01测试访问 1[root@docker01 consul]# curl 127.0.0.1:8000 //同上 此时可以看到负载均衡的效果！ 这时不需要考虑后端的web服务器添加还是删除都会自动更新的，这是因为在运行consul-template这条命令后添加的/usr/local/sbin/nginx -s reload的作用！","path":"posts/b3c.html","date":"06-07","excerpt":"","tags":[{"name":"nginx","slug":"nginx","permalink":"https://wsdlxgp.top/tags/nginx/"},{"name":"consul","slug":"consul","permalink":"https://wsdlxgp.top/tags/consul/"},{"name":"registrata","slug":"registrata","permalink":"https://wsdlxgp.top/tags/registrata/"}]},{"title":"17 Docker的监控(简单部署Sysdig和Weave Scope)","text":"一、Docker的监控 Docker自带的监控命令 简单命令介绍 ps docker container ps 是我们早已熟悉的命令了，方便我们查看当前运行的容器。新版的 Docker 提供了一个新命令 docker container ls，其作用和用法与 docker container ps 完全一样。不过 ls 含义可能比 ps 更准确，所以更推荐使用。 top 如果想知道某个容器中运行了哪些进程，可以执行 docker container top [container] 命令。命令后面还可以跟上 Linux 操作系统 ps 命令的参数显示特定的信息，比如 -au。 stats docker container stats 用于显示每个容器各种资源的使用情况。默认会显示一个实时变化的列表，展示每个容器的 CPU 使用率，内存使用量和可用量。注意：容器启动时如果没有特别指定内存 limit，stats 命令会显示 host 的内存总量，但这并不意味着每个 container 都能使用到这么多的内存。 除此之外 docker container stats 命令还会显示容器网络和磁盘的 IO 数据。默认的输出有个缺点，显示的是容器 ID 而非名字。我们可以在 stats 命令后面指定容器的名称只显示某些容器的数据。比如 docker container stats sysdig weave。 命令执行 12[root@docker01 ~]# docker ps//查看容器信息 123[root@docker01 ~]# docker top 容器名称[root@docker01 ~]# docker top wordpress_wordpress_1 //查看容器中运行的进程信息，支持 ps 命令参数。 12[root@docker01 ~]# docker stats wordpress_wordpress_1 //实时查看容器统计信息，查看容器的CPU利用率、内存的使用量以及可用内存总量。 123[root@docker01 ~]# docker logs 容器名称[root@docker01 ~]# docker logs wordpress_wordpress_1 //查看容器的日志 二、用 Sysdig 监控服务器和 Docker 容器 12[root@docker01 ~]# docker pull sysdig//下载sysdig镜像 通过sysdig运行容器 1[root@docker01 ~]# docker run -it --rm --name sysdig --privileged=true --volume=/var/run/docker.sock:/host/var/run/docker.sock --volume=/dev:/host/dev --volume=/proc:/host/proc:ro --volume=/boot:/host/boot:ro --volume=/lib/modules:/host/lib/modules:ro --volume=/usr:/host/usr:ro sysdig/sysdig 下载插件失败后可以运行下边命令，重新下载 12root@10ccab83a512:/# system-sysdig-loader//下载插件失败后可以运行下边命令，重新下载 下载成功后，可以运行sysdig命令，查看监控项 12root@10ccab83a512:/# sysdig//运行sysdig命令，查看监控项，它会动态查看 使用 csysdig csysdig 就是运 ncurses 库的用户界面的 sysdig 软件包，Ncurses 是一个能提供功能键定义 ( 快捷键 ), 屏幕绘制以及基于文本终端的图形互动功能的动态库。在 sysdig 软件包里还提供了一个工具 csysdig，该工具执行后，运行界面和 top 命令类似。csysdig 工作界面如图 5。 运行csysdig命令，查看监控项 12root@10ccab83a512:/# csysdig//运行csysdig命令，图形化界面查看监控项，它会动态查看 csysdig 使用如下快捷键： P：暂停屏幕输出信息 Enter：进入当前突出显示的条目。 Ctrl+F：列表搜索。 F1- 帮助信息 F2- 显示视图选择器。这将让你切换到另一个视图。 F4- 使用过滤器 F5- 查看 IO 输出信息 F7 显示帮助页面当前显示的视图。 F8 打开视图的操作面板。 F9，打开列排序面板。 Q 放弃退出。 Arrows, PgUP, PgDn, Home, End：图标上下左右的移动控制。 sysdig按不同的View来监控不同类型的资源，点击底部Views菜单（或者按F2），显示View选择列表 我们将光标移到Containers这一项，界面右边立即显示出此view的功能介绍，回车或者双击Containers，进入容器监控界面 sysdig会显示该host所有的容器的实时数据，每两秒刷新一次。各列数据的含义也是自解释的，如果不清楚，可以点一下底部的Legend，如果想按某一列排序，比如按使用的内存量，点一下列头VIRT 如果想查看某个容器的进程，将光标移动到目标容器，然后回车或者双击 还可以继续双击查看进程中的线程 返回上一级，按退格键即可 sysdig的交互功能很强，如果界面显示的条目很多，可以点击底部Search菜单，然后输入关键字进行查找 如果觉得界面刷新太快，看不清楚关注的信息，可以点击底部的Pause菜单 sysdig的特点： （1）监控信息全，包括Linux操作系统和容器 （2）界面交互性强 其缺点是sysdig显示的是实时数据，看不到变化和趋势。而且是命令行操作方式，需要ssh到host上执行，不是太方便 总结 这些示例仅仅是展示了 Sysdig 能力的冰山一角，在目前的其他系统监控类工具中，笔者还没有看到像 Sysdig 这样功能如此强大、而又对容器支持这样好的。所以，对于经常使用服务器特别是 Docker 容器作为产品运行方式的用户，这是一款值得使用的系统工具。 三、Docker监控方案之Weave Scope Weave Scope 的最大特点是会自动生成一张 Docker 容器地图，让我们能够直观地理解、监控和控制容器。千言万语不及一张图，先感受一下。 12[root@docker01 ~]# docker pull scope//下载scope镜像 执行如下脚本安装运行Weave Scope 123[root@docker01 ~]# curl -L git.io/scope -o /usr/local/bin/scope[root@docker01 ~]# chmod +x /usr/local/bin/scope[root@docker01 ~]# scope launch 浏览器访问http://192.168.1.11:4040/ 然后就可以更好的监控，管理docker中的容器了 开启第docker02，加入docker01监控项 docker01 删除weavescope容器 1234[root@docker01 ~]# docker stop weavescope weavescope[root@docker01 ~]# docker rm weavescope weavescope docker02 12[root@docker01 ~]# docker pull scope//下载scope镜像 123[root@docker01 ~]# curl -L git.io/scope -o /usr/local/bin/scope[root@docker01 ~]# chmod +x /usr/local/bin/scope[root@docker01 ~]# scope launch docker01 1[root@docker01 ~]# scope launch 192.168.1.11 192.168.1.13 docker02 1[root@docker02 ~]# scope launch 192.168.1.13 192.168.1.11 浏览器访问http://192.168.1.11:4040/ 浏览器访问http://192.168.1.13:4040/也是可以的","path":"posts/eb5f.html","date":"06-07","excerpt":"","tags":[{"name":"sysdig","slug":"sysdig","permalink":"https://wsdlxgp.top/tags/sysdig/"},{"name":"Weave Scope","slug":"Weave-Scope","permalink":"https://wsdlxgp.top/tags/Weave-Scope/"}]},{"title":"14 docker部署LNMP环境","text":"12 ifdown ens33;ifup ens33//重启网卡 首先要有确认环境中有需要的tar包，可以使用docker pull来下载这些镜像 现在我们是使用已经下载好的镜像，所以需要导入一下 12[root@docker01 ~]# docker load -i nginx.tar &amp;&amp; docker load -i wordpress.tar &amp;&amp; docker load -i mysql-5.7.tar &amp;&amp; docker load -i php.7.2-fpm.tar//导入nginx,wordpress,mysql,php镜像 整个流程： 客户端http请求服务器80端口，该端口被映射到Nginx容器80端口，进入Nginx处理。 Nginx分析请求，如果是静态资源，直接服务器读取内容；如果是PHP脚本，通过PHP容器调用服务器获取脚本，然后FastCGI处理。 FastCGI解析PHP脚本，必要时访问MySQL容器读写数据。 部署LNMP 172.16.10.0/24 Nginx：172.16.10.10 Mysql：172.16.10.20 Php ：172.16.10.30 网站的访问主目录：/wwwroot Nginx的配置文件：/docker /etc/nginx/conf.d #nginx配置文件 12345678[root@docker01 ~]# docker run -itd --name test nginx:latest //先启动一台nginx，用来拷贝配置文件和访问主目录[root@docker01 ~]# mkdir -p /wwwroot /docker//创建挂载目录[root@docker01 ~]# docker cp test:/etc/nginx /docker///拷贝配置文件到挂载目录[root@docker01 ~]# ls /docker/nginx /usr/share/nginx/html #nginx主目录 1234[root@docker01 ~]# docker cp test:/usr/share/nginx/html /wwwroot///拷贝访问目录到挂载目录[root@docker01 ~]# ls /wwwroot/html 1）创建一个自定义网络 1[root@docker01 ~]# docker network create -d bridge --subnet 172.16.10.0/24 --gateway 172.16.10.1 lnmp 2)运行nginx容器 12[root@docker01 ~]# netstat -anpt | grep 80//查看80端口是否被占用 12[root@docker01 ~]# docker run -itd --name nginx -v /docker/nginx:/etc/nginx -v /wwwroot/html:/usr/share/nginx/html -p 80:80 --network lnmp --ip 172.16.10.10 nginx//运行一台nginx服务，并指明ip，映射端口，挂载目录 12[root@docker01 ~]# docker ps//查看容器是否存在 12345678[root@docker01 ~]# cd /wwwroot/html[root@docker01 wwwroot]# vim index.htmlhello lnmp!//创建测试网页[root@docker01 wwwroot]# curl 127.0.0.1hello lnmp!//测试访问 3)运行mysql容器 12[root@docker01 html]# docker run --name mysql -e MYSQL_ROOT_PASSWORD=123.com -d -p 3306:3306 --network lnmp --ip 172.16.10.20 mysql:5.7//运行一台nginx服务，并指明ip，映射端口 -e：设置环境变量 1[root@docker02 ~]# docker ps 安装mysql，并设置密码 123[root@docker01 html]# yum -y install mysql//安装mysql[root@docker01 ~]# mysql -u root -p123.com -h 127.0.0.1 -P 3306 随便新建一个库做验证： 1MySQL [(none)]&gt; create database name; 再查看有没有刚创建的库： 1MySQL [(none)]&gt; show databases; 4)运行php容器，并创建php页面 1[root@docker01 html]# docker run -itd --name phpfpm -p 9000:9000 -v /wwwroot/html:/usr/share/nginx/html --network lnmp --ip 172.16.10.30 php:7.2-fpm 123456[root@docker01 ~]# cd /wwwroot/html[root@docker01 wwwroot]# vim test.php&lt;?phpphpinfo();?&gt;//添加php测试界面 1[root@docker02 ~]# docker ps 5)修改nginx配置文件，nginx和php连接 12[root@docker01 html]# cd /docker/nginx/conf.d/[root@docker01 conf.d]# vim default.conf location / { root /usr/share/nginx/html; index index.html index.htm index.php; #10添加index.php } location ~ \\.php$ { root /usr/share/nginx/html; fastcgi_pass 172.16.10.30:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } 设置完毕后重启nginx 123[root@docker01 conf.d]# docker restart nginx//重启nginx[root@docker01 conf.d]# docker ps 浏览器测试访问nginx和php 说明是nginx和php的连接，没有问题，接下来是php和MySQL的连接。这里我们使用一个phpmyadmin的数据库管理工具 6)修改nginx配置文件，php和mysql连接 1[root@docker01 html]# cd /wwwroot/html 上传phpMyAdmin包如果没有请在https://github.com/phpmyadmin/phpmyadmin/releases下载 123456789[root@docker01 html]# unzip phpMyAdmin-4.9.1-all-languages.zip //解压phpmyadmin包[root@docker01 html]# mv phpMyAdmin-4.9.1-all-languages phpmyadmin//更改刚刚解压文件的名称[root@docker01 html]# cd /docker/nginx/conf.d/[root@docker01 conf.d]# vim default.conf //修改nginx配置文件[root@docker01 conf.d]# docker restart nginx //重启nginx location /phpmyadmin { root /usr/share/nginx/html; index index.html index.htm index.php; } location ~ /phpmyadmin/(?&lt;after_ali&gt;(.*)\\.(php|php5)?$) { root /usr/share/nginx/html; fastcgi_pass 172.16.10.30:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } 12[root@docker01 conf.d]# docker restart nginx [root@docker01 conf.d]# docker ps 浏览器访问 http://192.168.1.11/phpmyadmin/index.php 报红框属于正常现象，不要惊慌，接下来就解决它 需要我们对php镜像做出更改，添加php和MySQL连接模块 编写一个Dockerfile 1234567891011[root@docker01 conf.d]# cd [root@docker01 ~]# vim DockerfileFROM php:7.2-fpmRUN apt-get update &amp;&amp; apt-get install -y \\ libfreetype6-dev \\ libjpeg62-turbo-dev \\ libpng-dev \\ &amp;&amp; docker-php-ext-install -j$(nproc) iconv \\ &amp;&amp; docker-php-ext-configure gd --with-freetype-dir=/usr/include/ --with-jpeg-dir=/usr/include/ \\ &amp;&amp; docker-php-ext-install -j$(nproc) gd \\ &amp;&amp; docker-php-ext-install mysqli pdo pdo_mysql 基于dockerfile创建php镜像 12[root@docker01 ~]# docker build -t phpmysql .//基于Dockerfiler创建一个镜像 删除之前的php容器 123[root@docker01 ~]# docker stop phpfpm[root@docker01 ~]# docker rm phpfpm //关闭并删除php容器 用新的php镜像运行容器 12[root@docker01 ~]# docker run -itd --name phpfpm -p 9000:9000 -v /wwwroot/html:/usr/share/nginx/html --network lnmp --ip 172.16.10.30 phpmysql//用新做的php镜像重新运行 //修改phpmyadmin的配置文件，指定连接的数据库的IP，然后重启php容器 12345678[root@docker01 html]# cd /wwwroot/html/phpmyadmin/[root@docker01 phpmyadmin]# cp config.sample.inc.php config.inc.php[root@docker01 phpmyadmin]# vim config.inc.php$cfg['Servers'][$i]['auth_type'] = 'cookie';/* Server parameters */$cfg['Servers'][$i]['host'] = '172.16.10.20'; #31写mysql数据库的IP地址$cfg['Servers'][$i]['compress'] = false;$cfg['Servers'][$i]['AllowNoPassword'] = false; 12[root@docker01 phpmyadmin]# docker restart phpfpm //重启phpfpm容器 浏览器测试访问http://192.168.1.11/phpmyadmin/index.php 用户名：root 密码：123.com 登陆成功之后可以看到之前mysql创建的数据库","path":"posts/32f5.html","date":"06-07","excerpt":"","tags":[{"name":"lnmp","slug":"lnmp","permalink":"https://wsdlxgp.top/tags/lnmp/"}]},{"title":"16 docker三剑客之docker-compose和搭建wordpress的博客","text":"一、简介 Compose 项目是 Docker 官方的开源项目，负责实现对 Docker 容器集群的快速编排。 通过之前的介绍，我们知道使用一个 Dockerfile 模板文件，可以让用户很方便的定义一个单独的应用容器。然而，在日常工作中，经常会碰到需要多个容器相互配合来完成某项任务的情况。例如要实现一个 Web 项目，除了 Web 服务容器本身，往往还需要再加上后端的数据库服务容器，甚至还包括负载均衡容器等。 Compose 恰好满足了这样的需求。它允许用户通过一个单独的 docker-compose.yml 模板文件（YAML 格式）来定义一组相关联的应用容器为一个项目（project）。 Compose 中有两个重要的概念： 服务 (service)：一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。 项目 (project)：由一组关联的应用容器组成的一个完整业务单元，在 docker-compose.yml 文件中定义。 Compose 的默认管理对象是项目，通过子命令对项目中的一组容器进行便捷地生命周期管理。 使用微服务架构的系统一般包含若干个微服务，每个微服务一般部署多个实例。如果每个服务都要手动启停，那么效率低，维护量大。 二、 Docker Compose介绍 通过Docker-Compose用户可以很容易地用一个配置文件定义一个多容器的应用，然后使用一条指令安装这个应用的所有依赖，完成编写。Docker-Compose解决了容器与容器之间如何管理编排的问题。 Docker Compose工作原理图 撰写中有两个重要的概念： **服务（服务）：**一个应用的容器，实际上可以包括多个运行相同相同的容器实例。 **项目（项目）：**由各个关联的应用容器组成的一个完整的业务单元，在docker-compose.yml文件中定义。一个项目可以由多个服务（容器）关联，组成面向项目进行管理，通过子命令对项目中的单个容器进行便捷地生命周期管理。 Compose项目由Python编写，实现上调用了Docker服务提供的API来对容器进行管理。因此，只要所操作的平台支持Docker API，就可以在其上利用Compose来进行编排管理。 Docker三大编排工具： Docker Compose：是用来组装多容器应用的工具，可以在 Swarm集群中部署分布式应用。 Docker Machine：是支持多平台安装Docker的工具，使用 Docker。Machine，可以很方便地在笔记本、云平台及数据中心里安装Docker。 Docker Swarm：是Docker社区原生提供的容器集群管理工具。 Docker Compose命令详解 Docker compose的使用非常类似于docker命令的使用，但是需要注意的是大部分的compose命令都需要到docker-compose.yml文件所在的目录下才能执行。 compose以守护进程模式运行加-d选项 三、Docker Compose安装 123456#下载sudo curl -L https://github.com/docker/compose/releases/download/1.20.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose#安装chmod +x /usr/local/bin/docker-compose#查看版本docker-compose version 四、实验环境 主机 ip地址 服务 docker 192.168.1.11 compose+wordpress 五 docker三剑客之docker-compose docker容器的编排工具: 解决相互有依赖关系的多个容器的管理。 12[root@docker01 ~]# docker-compose -v//验证已有docker-compose命令 docker-compose的配置文件实例 通过识别一个docker-compose.yml的配置文件，去管理容器。 设置tab键的空格数量 12345[root@docker01 ~]# vim .vimrcset tabstop=2//设置tab键的空格数量[root@docker01 ~]# source .vimrc //刷新一下 创建一个docker-compose.yml测试文件 123456789101112131415[root@docker01 ~]# mkdir compose_test//创建测试目录[root@docker01 ~]# cd compose_test/[root@docker01 compose_test]# vim docker-compose.yml//创建测试文件docker-compose.ymlversion: \"3\"services: nginx: container_name: web-nginx image: nginx restart: always ports: - 90:80 volumes: - ./webserver:/usr/share/nginx/html docker-compose.yml文件的解释 第一部分: version: 指定语法格式的版本。 第二部分: service: 定义服务,(想要运行什么样的容器) 通过docker-compose.yml文件运行容器 12[root@docker01 compose_test]# docker-compose up -d//后台运行docker-compose规定的容器。（在执行这条命令的当前目录下，也需要一个docker-compose.yml的配置文件，并且通常只有一个。） 12[root@docker01 compose_test]# docker ps//查看容器信息 12[root@docker01 compose_test]# curl 127.0.0.1:90//访问nginx会失败，因为挂载目录没有页面内容 123456[root@docker01 compose_test]# vim webserver/index.html//创建测试网页xgp666[root@docker01 compose_test]# curl 127.0.0.1:90//再次访问，是成功的xgp666 通过docker-compose.yml文件停止运行容器 1[root@docker01 compose_test]# docker-compose stop 通过docker-compose.yml文件重启容器 1[root@docker01 compose_test]# docker-compose restart 不在docker-compose.yml文件所在目录，要使用-f指定目录 1[root@docker01 ~]# docker-compose -f compose_test/docker-compose.yml stop 并且，在运行container（docker-compose.yml）的过程中，还支持Dockerfile 1234[root@docker01 compose_test]# vim Dockerfile//编写dockerfileFROM nginxADD webserver /usr/share/nginx/html 1234567891011[root@docker01 compose_test]# vim docker-compose.yml //修改docker-compose.yml文件version: \"3\"services: nginx: build: . #添加 container_name: web-nginx image: new-nginx:v1.0 #修改镜像名称 restart: always ports: - 90:80 通过docker-compose.yml文件停止并删除容器 123[root@docker01 compose_test]# docker-compose stopStopping web-nginx ... done[root@docker01 compose_test]# docker-compose rm 通过docker-compose.yml文件运行容器 1234[root@docker01 compose_test]# docker-compose up -d//通过docker-compose.yml文件[运行]()容器[root@docker01 compose_test]# docker ps//查看容器信息 测试nginx访问页面 123[root@docker01 compose_test]# curl 127.0.0.1:90//测试访问nginx页面，成功xgp666 六、搭建wordpress的博客 下载wordpress和mysql:5.7容器 1234[root@docker01 ~]# docker pull wordpress//下载wordpress容器[root@docker01 ~]# docker pull mysql：5.7//下载mysql：5.7容器 编写一个docker-ccompose.yml 1234567891011121314151617181920212223242526[root@docker01 ~]# mkdir wordpress//创建wordpress测试文件[root@docker01 ~]# cd wordpress/[root@docker01 wordpress]# vim docker-compose.yml//编写docker-compose.ymlversion: \"3.1\"services: wordpress: image: wordpress restart: always ports: - 8080:80 environment: WORDPRESS_DB_HOST: db WORDPRESS_DB_USER: wordpress WORDPRESS_DB_PASSWORD: 123.com WORDPRESS_DB_NAME: wordpress db: image: mysql:5.7 restart: always environment: MYSQL_DATABASE: wordpress MYSQL_USER: wordpress MYSQL_PASSWORD: 123.com MYSQL_ROOT_PASSWORD: 123.com 通过docker-compose.yml文件运行容器 1[root@docker01 wordpress]# docker-compose up -d 12[root@docker01 wordpress]# docker ps//查看容器信息 12[root@docker01 wordpress]# docker logs 容器名称//查看容器日志 浏览器访问一下 http://192.168.1.11:8080/ 选择语言 安装wordpress 登陆wordpress 登陆成功后，自己就可以进行设置了 排错 首先查看主机名是否更改 防火墙和selinux是否关闭 docker-compose命令是否安装给予权限 docker–compose.yml 编写是否有问题 容器执行是否正常 （如果浏览器访问不到，可以添加一条路由转发） 其他wordpress优化建议 以上步骤之后，基本wordpress搭建和一些必备的设置就算完成了，剩下的更多是个人的选择，每个人可能要求不同，下面就说几点wordpress优化的建议 1.无论你是做百度seo，安装一个SEO插件，就算不想设置文章的TDK，至少网站首页的有必要设置一下，推荐 All in one seo pack 2.定期备份，备份的重要性不用多说，凡是丢过数据的人都会养成备份的习惯，WordPress备份网站方法 3.安装一个安全插件，WordPress安全插件推荐 4.及时更新网站的主题和插件，WordPress插件自动更新方法 5.删除所有没有用的主题和插件，WordPress删除主题方法 6.设置垃圾留言过滤，wordpress防垃圾留言插件Akismet WordPress建站基础主要就是这些，后面的话就是根据自己的站点进行各种设置，不同类型的站点使用主题和插件都是很大区别的。不过如果你能学会本篇所介绍的内容，相信你的站点就已经超过了绝大部分网站，好了今天教程就到这里，如果你有什么问题或者其他更好的建议，欢迎留言讨论","path":"posts/d728.html","date":"06-07","excerpt":"","tags":[{"name":"docker-compose","slug":"docker-compose","permalink":"https://wsdlxgp.top/tags/docker-compose/"},{"name":"wordpress","slug":"wordpress","permalink":"https://wsdlxgp.top/tags/wordpress/"}]},{"title":"22 Docker swarm搭建（1）","text":"Docker swarm docker swarm集群：三剑客之一 一. 实验环境 主机 IP地址 服务 docker01 192.168.1.11 swarm+overlay+webUI docker02 192.168.1.13 docker docker03 192.168.1.20 docker 三台主机都关闭防火墙，禁用selinux，修改主机名，时间同步，并添加域名解析。 docker版本必须是：v1.12版本开始（可使用docker version查看版本） 1.关闭防火墙，禁用selinux 123[root@localhost ~]# systemctl stop firewalld[root@localhost ~]# hostnamectl set-hostname docker03[root@localhost ~]# su - 2.时间同步 12mv /etc/localtime /etc/localtime.bkcp /usr/share/zoneinfo/Asia/Shanghai/etc/localtime 3.修改主机名（三台都要） 12[root@localhost ~]# hostnamectl set-hostname docker01[root@localhost ~]# su - 4.添加域名解析 1234567[root@docker01 ~]# vim /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.1.11 docker01192.168.1.13 docker02192.168.1.20 docker03 二. swarm原理 **swarm：**作用运行docker engin的多个主机组成的集群 **node：**每一个docker engin都是一个node（节点），分为manager和worker。 **manager node：**负责执行容器的编排和集群的管理工作，保持并维护swarm处于期望的状态。swarm可以有多个manager node，他们会自动协调并选举一个leader执行编排任务。但相反，不能没有manager node。 **worker node：**接受并执行由manager node派发的任务，并且默认manager node也是一个worker node，不过可以将它设置为manager-only node，让他只负责编排和管理工作。 **service：**用来定义worker上执行的命令。 基本命令操作 **docker swarm leave：**申请离开一个集群，之后查看节点状态会变成down，然后可通过manager node 将其删除 **docker node rm xxx：**删除某个节点 docker swarm join-token [manager|worker]：生成令牌，可以是manager或worker身份。 docker node demote（降级）：将swarm节点的为manager降级为worker docker node promote（升级）：将swarm节点的work升级为manager **docker node ls:**查看群集的信息（只可以在manager角色的主机上查看） docker service scale web05=6:容器的动态扩容及缩容 docker service ps web01: 查看创建的容器运行在哪些节点 docker service ls: 查看创建的服务 docker swarm leave: 脱离这个群集 docker node rm docker03: 在manager角色的服务器上移除docker03 docker node update --availability drain docker01: 设置主机docker01以后不运行容器，但已经运行的容器并不会停止 docker node update --label-add mem=max docker03: 更改docker03主机的标签为mem=max docker service update --replicas 8 --image 192.168.20.6:5000/lvjianzhao:v2.0 --container-label-add ‘node.labels.mem==max’ lvjianzhao05: 将服务升级为8个容器，并且指定在mem=max标签的主机上运行 三. docker01 初始化集群 1[root@docker01 ~]# docker swarm init --advertise-addr 192.168.1.11 **–advertise-addr：**指定与其它docker通信的地址。 上边返回的结果告诉我们：初始化成功，并且，如果想要添加work节点运行下面的命令： 注意：token令牌只有24小时的有效期 如果想要添加manager节点：运行下面命令 四.swarm集群的简单操作 1.docker02和docker03以worker加入集群 1[root@docker03 ~]# docker swarm join --token SWMTKN-1-5kxn9wloh7npnytklwbfciesr9di7uvu521gwnqm9h1n0pbokj-1e60wt0yr5583e4mzwbxnn3a8 192.168.1.11:2377 docker01查看集群 1[root@docker01 ~]# docker node ls 注意：这里的”*****“代表的是当前所属的节点 2.删除集群中节点 docker02和docker03申请离开一个集群 1[root@docker02 ~]# docker swarm leave docker删除docker02和docker03节点 12[root@docker01 ~]# docker node rm docker02 [root@docker01 ~]# docker node rm docker03 docker01查看集群 1[root@docker01 ~]# docker node ls 3.docker02和docker03以manager加入集群 docker01生成manager令牌 1[root@docker01 ~]# docker swarm join-token manager docker02和docker03加入集群 1docker swarm join --token SWMTKN-1-5kxn9wloh7npnytklwbfciesr9di7uvu521gwnqm9h1n0pbokj-cz6hbyv9r5htyqwj5tfol65aa 192.168.1.11:2377 docker01查看集群 1[root@docker01 ~]# docker node ls 4.docker02和docker03降级 docker01（manager）把docker02和docker03降级成worker 12[root@docker01 ~]# docker node demote docker02[root@docker01 ~]# docker node demote docker03 查看集群 1[root@docker01 ~]# docker node ls 五.部署docker swarm集群网络 overlay:覆盖型网络 overlay networks 管理Swarm中docker守护进程间的通信。可以将容器附加到一个或多个已存在的overlay网络上，使容器与容器之间能够通信； 12[root@docker01 ~]# docker network create -d overlay --attachable docker//attachable：这个参数必须要加，否则不能用于容器。 在创建网络的时候，我们并没有部署一个存储服务，比如consul，那是因为docker swarm自带存储。 docker01查看网络 但是会发现其他两台并不会发现此网络，需等基于此网络创建service服务就可以看到了 1[root@docker01 ~]# docker network ls 六. docker01部署一个图形化webUI界面 1.docker01 导入镜像 1[root@docker01~]# docker pull dockersamples/visualizer 2.基于镜像启动一台容器 1[root@docker01 ~]# docker run -d -p 8080:8080 -e HOST=192.168.1.100 -e PORT=8080 -v /var/run/docker.sock:/var/run/docker.sock --name visualiaer dockersamples/visualizer 3.通过浏览器访问验证http://192.168.1.11:8080/ 如果访问不到网页，需开启路由转发 12[root@docker01 ~]# echo net.ipv4.ip_forward = 1 &gt;&gt; /etc/sysctl.conf [root@docker01 ~]# sysctl -p 七. 创建service（服务） 1. 基于nginx容器创建一个service服务 1234[root@docker01 ~]#docker pull nginx//下载nginx镜像（三台都要）[root@docker01 ~]# docker service create --replicas 1 --network docker --name web1 -p 80:80 nginx:latest [root@docker01 ~]# docker service create --replicas 1 --network docker --name web2 -p 80 nginx:latest //–replicas：副本数量 大概可以理解为一个副本等于一个容器 2. 查看创建的service服务 1[root@docker01 ~]# docker service ls 单独查看一个servicefuw 1[root@docker01 ~]# docker service ps web1 1[root@docker01 ~]# docker service ps web2 3. web界面查看 4. 基于nginx容器创建五个service服务 1[root@docker01 ~]# docker service create --replicas 5 --network docker --name web -p 80 nginx:latest web界面查看 5. 挂起docker02 web查看（发现服务都分配到其他服务器了） 6. 恢复docker02 web查看（发现服务没有回到docker02） 八、实现docker容器的扩容及缩容 1. 删除web1和web2服务 1[root@docker01 ~]# docker service rm web1 web2 2. 容器的扩容和缩减 （1）扩容 1[root@docker01 ~]# docker service scale web=8 （2）缩减 1[root@docker01 ~]# docker service scale web=3 3.设置manager node不参加工作 1[root@docker01 ~]# docker node update docker01 --availability drain 设置主机docker01以后不运行容器，但已经运行的容器并不会停止 “–availability”选项后面共有三个选项可配置，如下： “active”：工作；“pause”：暂时不工作；“drain”：永久性的不工作 1[root@docker01 ~]# docker node ls web界面查看 九、docker Swarm总结 在我对docker Swarm群集进行一定了解后，得出的结论如下： 参与群集的主机名一定不能冲突，并且可以互相解析对方的主机名； 集群内的所有节点可以都是manager角色，但是不可以都是worker角色； 当指定运行的镜像时，如果群集中的节点本地没有该镜像，那么它将会自动下载对应的镜像； 当群集正常工作时，若一个运行着容器的docker服务器发生宕机，那么，其所运行的所有容器，都将转移到其他正常运行的节点之上，而且，就算发生宕机的服务器恢复正常运行，也不会再接管之前运行的容器；","path":"posts/60e.html","date":"06-07","excerpt":"","tags":[{"name":"overlay","slug":"overlay","permalink":"https://wsdlxgp.top/tags/overlay/"},{"name":"swarm","slug":"swarm","permalink":"https://wsdlxgp.top/tags/swarm/"},{"name":"webUI","slug":"webUI","permalink":"https://wsdlxgp.top/tags/webUI/"}]},{"title":"21 docker swarm版本回滚","text":"Docker swarm docker swarm集群：三剑客之一 一. Docker Swarm 的基本概念和原理 Docker Swarm 简介 Swarm是Docker公司推出的用来管理docker集群，它将一群Docker宿主机变成一个单一的，虚拟的主机。Swarm使用标准的Docker API接口作为其前端访问入口，换言之，各种形式的Docker Client(docker client in Go, docker_py, docker等)均可以直接与Swarm通信。Swarm几乎全部用go语言来完成开发，Swarm0.2发布，相比0.1版本，0.2版本增加了一个新的策略来调度集群中的容器，使得在可用的节点上传播它们，以及支持更多的Docker命令以及集群驱动。 Swarm deamon只是一个调度器（Scheduler）加路由器(router)，Swarm自己不运行容器，它只是接受docker客户端发送过来的请求，调度适合的节点来运行容器，这意味着，即使Swarm由于某些原因挂掉了，集群中的节点也会照常运行，当Swarm重新恢复运行之后，它会收集重建集群信息． Docker Swarm 工作原理 Docker 客户端通过 Docker API 向 Swarm 管理端发送请求，Swarm Manager 通过守护进程调用集群中的某个节点来执行任务。因为容器都是运行在节点上，Swarm 作为一个独立的集群管理工具，故并不会因某些原因导致不能正常工作而影响集群内所有节点的正常运行。当服务恢复正常后，Swarm 会读取日志来执行集群的恢复动作。架构图如图 1： 图 1.Docker Swarm 架构图 二. Docker Swarm要点 **Swarm的负载非常低。**据我观察，Swarm进行调度和通信的CPU负载非常低。因此，Swarm的管理节点(Manager)可以同时作为工作节点(Worker)。如果你需要搭建一个非常大的集群(1000+ 节点)，管理节点需要更多资源，但是对于中小型集群来说，管理节点需要的资源可以忽略不计。 **Swarm集群的网络通信(服务发现，负载均衡以及容器间通信)非常可靠。**当你开启一个服务的端口之后，在Swarm集群中的任何一个节点都可以访问它。负载均衡也是由Swarm提供的。后文会提到一些之前遇到的问题，但是Docker 1.13之后，这些问题都解决了。 三. 实验环境 主机 IP地址 服务 docker01 192.168.1.11 swarm+service+webUI+registry docker02 192.168.1.13 docker docker03 192.168.1.20 docker 三台主机都关闭防火墙，禁用selinux，修改主机名，时间同步，并添加域名解析。 docker版本必须是：v1.12版本开始（可使用docker version查看版本） 1.关闭防火墙，禁用selinux 123[root@localhost ~]# systemctl stop firewalld[root@localhost ~]# hostnamectl set-hostname docker03[root@localhost ~]# su - 2.时间同步 12mv /etc/localtime /etc/localtime.bkcp /usr/share/zoneinfo/Asia/Shanghai/etc/localtime 3.修改主机名（三台都要） 12[root@localhost ~]# hostnamectl set-hostname docker01[root@localhost ~]# su - 4.添加域名解析 123[root@docker01 ~]# echo 192.168.1.11 docker01 &gt;&gt; /etc/hosts[root@docker01 ~]# echo 192.168.1.13 docker02 &gt;&gt; /etc/hosts[root@docker01 ~]# echo 192.168.1.20 docker03 &gt;&gt; /etc/hosts 四. swarm原理 **swarm：**作用运行docker engin的多个主机组成的集群 **node：**每一个docker engin都是一个node（节点），分为manager和worker。 **manager node：**负责执行容器的编排和集群的管理工作，保持并维护swarm处于期望的状态。swarm可以有多个manager node，他们会自动协调并选举一个leader执行编排任务。但相反，不能没有manager node。 **worker node：**接受并执行由manager node派发的任务，并且默认manager node也是一个worker node，不过可以将它设置为manager-only node，让他只负责编排和管理工作。 **service：**用来定义worker上执行的命令。 基本命令操作 **docker swarm leave：**申请离开一个集群，之后查看节点状态会变成down，然后可通过manager node 将其删除 **docker node rm xxx：**删除某个节点 docker swarm join-token [manager|worker]：生成令牌，可以是manager或worker身份。 docker node demote（降级）：将swarm节点的为manager降级为worker docker node promote（升级）：将swarm节点的work升级为manager **docker node ls:**查看群集的信息（只可以在manager角色的主机上查看） docker service scale web05=6:容器的动态扩容及缩容 docker service ps web01: 查看创建的容器运行在哪些节点 docker service ls: 查看创建的服务 docker swarm leave: 脱离这个群集 docker node rm docker03: 在manager角色的服务器上移除docker03 docker node update --availability drain docker01: 设置主机docker01以后不运行容器，但已经运行的容器并不会停止 docker node update --label-add mem=max docker03: 更改docker03主机的标签为mem=max docker service update --replicas 8 --image 192.168.20.6:5000/lvjianzhao:v2.0 --container-label-add ‘node.labels.mem==max’ lvjianzhao05: 将服务升级为8个容器，并且指定在mem=max标签的主机上运行 五. docker01 初始化集群 1[root@docker01 ~]# docker swarm init --advertise-addr 192.168.1.11 **–advertise-addr：**指定与其它docker通信的地址。 上边返回的结果告诉我们：初始化成功，并且，如果想要添加work节点运行下面的命令： 注意：token令牌只有24小时的有效期 上面命令执行后，该机器自动加入到swarm集群。这个会创建一个集群token，获取全球唯一的 token，作为集群唯一标识。后续将其他节点加入集群都会用到这个token值。 其中，–advertise-addr参数表示其它swarm中的worker节点使用此ip地址与manager联系。命令的输出包含了其它节点如何加入集群的命令。 如果想要添加manager节点：运行下面命令 六.swarm集群的简单操作 1、docker02和docker03以worker加入集群 1[root@docker03 ~]# docker swarm join --token SWMTKN-1-5kxn9wloh7npnytklwbfciesr9di7uvu521gwnqm9h1n0pbokj-1e60wt0yr5583e4mzwbxnn3a8 192.168.1.11:2377 docker01查看集群 1[root@docker01 ~]# docker node ls 注意：这里的”*****“代表的是当前所属的节点 2.删除集群中节点 docker02和docker03申请离开一个集群 1[root@docker02 ~]# docker swarm leave docker删除docker02和docker03节点 12[root@docker01 ~]# docker node rm docker02 [root@docker01 ~]# docker node rm docker03 docker01查看集群 1[root@docker01 ~]# docker node ls 3.docker02和docker03以manager加入集群 docker01生成manager令牌 1[root@docker01 ~]# docker swarm join-token manager docker02和docker03加入集群 1docker swarm join --token SWMTKN-1-5kxn9wloh7npnytklwbfciesr9di7uvu521gwnqm9h1n0pbokj-cz6hbyv9r5htyqwj5tfol65aa 192.168.1.11:2377 docker01查看集群 1[root@docker01 ~]# docker node ls 4.docker02和docker03降级 docker01（manager）把docker02和docker03降级成worker 12[root@docker01 ~]# docker node demote docker02[root@docker01 ~]# docker node demote docker03 查看集群 1[root@docker01 ~]# docker node ls 五.设置manager node（docker01）不参加工作 1[root@docker01 ~]# docker node update docker01 --availability drain 设置主机docker01以后不运行容器，但已经运行的容器并不会停止 “–availability”选项后面共有三个选项可配置，如下： “active”：工作；“pause”：暂时不工作；“drain”：永久性的不工作 1[root@docker01 ~]# docker node ls 八. docker01部署一个图形化webUI界面 1.docker01 导入镜像 1[root@docker01~]# docker pull dockersamples/visualizer 2.基于镜像启动一台容器 1[root@docker01 ~]# docker run -d -p 8080:8080 -e HOST=192.168.1.100 -e PORT=8080 -v /var/run/docker.sock:/var/run/docker.sock --name visualiaer dockersamples/visualizer 3.通过浏览器访问验证http://192.168.1.11:8080/ 如果访问不到网页，需开启路由转发 12[root@docker01 ~]# echo net.ipv4.ip_forward = 1 &gt;&gt; /etc/sysctl.conf [root@docker01 ~]# sysctl -p 一. Docker01部署一个私有仓库 Docker01部署 123456789101112131415161772 docker pull registry//下载registry镜像73 docker run -itd --name registry -p 5000:5000 --restart=always registry:latest//基于registry镜像，启动一台容器78 vim /usr/lib/systemd/system/docker.service #13行修改ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.11:5000 80 systemctl daemon-reload81 systemctl restart docker.service//重启docker76 docker tag busybox:latest 192.168.1.11:5000/busybox:v1 //把容器重命名一个标签77 docker ps 1234567891078 vim /usr/lib/systemd/system/docker.service #13行修改ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.11:5000 80 systemctl daemon-reload81 systemctl restart docker.service//重启docker100 docker push 192.168.1.11:5000/busybox:v1//上传容器到私有仓库 Docker02和docker03加入私有仓库 12345678978 vim /usr/lib/systemd/system/docker.service #13行修改ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.11:5000 80 systemctl daemon-reload81 systemctl restart docker.service//重启docker99 docker pull 192.168.1.11/busybox:v1//测试下载 2. 自定义镜像 要求：基于httpd镜像，更改访问界面内容。镜像tag版本为v1，v2，v3，对应主机面内容为v1，xgp666、v2，xgp666、v2，xgp666 12[root@docker01 ~]# docker pull httpd//下载httpd镜像 创建三个测试目录 12[root@docker01 ~]# mkdir &#123;v1,v2,v3&#125;//创建测试目录 docker01，v1目录操作 1234567891011121314[root@docker01 ~]# cd v1[root@docker01 v1]# echo v1,xgp666 &gt; index.html//创建测试网页[root@docker01 v1]# vim Dockerfile//编写DockerfileFROM httpdADD index.html /c[root@docker01 v1]# docker build -t 192.168.1.11:5000/httpd:v1 .//基于dockerfile创建镜像[root@docker01 v1]# docker push 192.168.1.11:5000/httpd:v1//上传刚刚创建镜像到私有仓库 docker01，v2目录操作 12345678910111213[root@docker01 v1]# cd ../v2[root@docker01 v2]# echo v2,xgp666 &gt; index.html[root@docker01 v2]# vim Dockerfile //编写DockerfileFROM httpdADD index.html /usr/local/apache2/htdocs/index.html[root@docker01 v2]# docker build -t 192.168.1.11:5000/httpd:v2 .//基于dockerfile创建镜像[root@docker01 v2]# docker push 192.168.1.11:5000/httpd:v2//上传刚刚创建镜像到私有仓库 docker01，v3目录操作 12345678910111213[root@docker01 v1]# cd ../v3[root@docker01 v2]# echo v3,xgp666 &gt; index.html[root@docker01 v2]# vim Dockerfile //编写DockerfileFROM httpdADD index.html /usr/local/apache2/htdocs/index.html[root@docker01 v2]# docker build -t 192.168.1.11:5000/httpd:v3 .//基于dockerfile创建镜像[root@docker01 v2]# docker push 192.168.1.11:5000/httpd:v3//上传刚刚创建镜像到私有仓库 3. 发布一个服务，基于上述镜像 要求:副本数量为3个。服务的名称为: bdqn 1[root@docker01 v3]# docker service create --replicas 3 --name bdqn -p 80:80 192.168.1.11:5000/httpd:v1 查看一下网络 1[root@docker03 ~]# docker network ls 默认的Ingress网络，包括创建的自定义overlay网络, 为后端真正为用户提供服务的container,提供了一个统一的入口。 查看一下创建的副本 1[root@docker01 v3]# docker service ps bdqn 浏览器测试访问http://192.168.1.11:80,http://192.168.1.13:80,http://192.168.1.20:80 修改docker02和docker03测试网页内容 docker02 123[root@docker02 ~]# docker exec -it 388f3bd9dd33 /bin/bashroot@388f3bd9dd33:/usr/local/apache2# cd htdocs/root@388f3bd9dd33:/usr/local/apache2/htdocs# echo 123 &gt; index.html docker03 12[root@docker03 ~]# docker exec -it 281454867fac /bin/bashroot@281454867fac:/usr/local/apache2# echo 321 &gt; htdocs/index.html 测试访问（每一台都会显示，会负载均衡） 要求:副本数量为3个。服务的名称为:test 1[root@docker01 v3]# docker service create --replicas 3 --name test -p 80 192.168.1.11:5000/httpd:v1 查看创建的服务映射端口 1[root@docker01 v3]# docker service ls 默认映射端口30000-32767 4. 服务的扩容与缩容 扩容 1[root@docker01 v3]# docker service scale bdqn=6 缩容 1[root@docker01 v3]# docker service scale bdqn=4 扩容与缩容直接直接通过scale进行设置副本数量。 5.服务的升级与回滚 （1）升级 12[root@docker01 ~]# docker service update --image 192.168.1.11:5000/httpd:v2 bdqn//把bdqn服务升级成v2的版本 测试访问一下 （2）平滑的更新 12[root@docker01 ~]# docker service update --image 192.168.1.11:5000/httpd:v3 --update-parallelism 2 --update-delay 1m bdqn //两个服务一起更新，然后，隔一分钟，继续更新 默认情况下, swarm-次只更新-个副本,并且两个副本之间没有等待时间，我们可以通过 –update-parallelism;设置并行更新的副本数量。 –update-delay：指定滚动更新的时间间隔。 测试访问一下 (3) 回滚操作 1[root@docker01 ~]# docker service rollback bdqn 注意，docker swarm的回滚操作，默认只能回滚到上一-次操作的状态，并不能连续回滚到指定操作。 测试访问一下","path":"posts/4890.html","date":"06-07","excerpt":"","tags":[{"name":"overlay","slug":"overlay","permalink":"https://wsdlxgp.top/tags/overlay/"},{"name":"swarm","slug":"swarm","permalink":"https://wsdlxgp.top/tags/swarm/"},{"name":"webUI","slug":"webUI","permalink":"https://wsdlxgp.top/tags/webUI/"}]}],"categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"https://wsdlxgp.top/tags/docker/"},{"name":"kubeadml","slug":"kubeadml","permalink":"https://wsdlxgp.top/tags/kubeadml/"},{"name":"nfs","slug":"nfs","permalink":"https://wsdlxgp.top/tags/nfs/"},{"name":"pv","slug":"pv","permalink":"https://wsdlxgp.top/tags/pv/"},{"name":"pvc","slug":"pvc","permalink":"https://wsdlxgp.top/tags/pvc/"},{"name":"StorageClass","slug":"StorageClass","permalink":"https://wsdlxgp.top/tags/StorageClass/"},{"name":"dashboard","slug":"dashboard","permalink":"https://wsdlxgp.top/tags/dashboard/"},{"name":"helm","slug":"helm","permalink":"https://wsdlxgp.top/tags/helm/"},{"name":"deployment","slug":"deployment","permalink":"https://wsdlxgp.top/tags/deployment/"},{"name":"jenkins","slug":"jenkins","permalink":"https://wsdlxgp.top/tags/jenkins/"},{"name":"gitlab","slug":"gitlab","permalink":"https://wsdlxgp.top/tags/gitlab/"},{"name":"chares","slug":"chares","permalink":"https://wsdlxgp.top/tags/chares/"},{"name":"url","slug":"url","permalink":"https://wsdlxgp.top/tags/url/"},{"name":"chart","slug":"chart","permalink":"https://wsdlxgp.top/tags/chart/"},{"name":"tiller","slug":"tiller","permalink":"https://wsdlxgp.top/tags/tiller/"},{"name":"HPA","slug":"HPA","permalink":"https://wsdlxgp.top/tags/HPA/"},{"name":"heapster","slug":"heapster","permalink":"https://wsdlxgp.top/tags/heapster/"},{"name":"top","slug":"top","permalink":"https://wsdlxgp.top/tags/top/"},{"name":"weave-scope","slug":"weave-scope","permalink":"https://wsdlxgp.top/tags/weave-scope/"},{"name":"Prometheus","slug":"Prometheus","permalink":"https://wsdlxgp.top/tags/Prometheus/"},{"name":"ingress-nginx","slug":"ingress-nginx","permalink":"https://wsdlxgp.top/tags/ingress-nginx/"},{"name":"https","slug":"https","permalink":"https://wsdlxgp.top/tags/https/"},{"name":"ca","slug":"ca","permalink":"https://wsdlxgp.top/tags/ca/"},{"name":"nginx","slug":"nginx","permalink":"https://wsdlxgp.top/tags/nginx/"},{"name":"ingress","slug":"ingress","permalink":"https://wsdlxgp.top/tags/ingress/"},{"name":"ingress controller","slug":"ingress-controller","permalink":"https://wsdlxgp.top/tags/ingress-controller/"},{"name":"secret","slug":"secret","permalink":"https://wsdlxgp.top/tags/secret/"},{"name":"pod","slug":"pod","permalink":"https://wsdlxgp.top/tags/pod/"},{"name":"configmap","slug":"configmap","permalink":"https://wsdlxgp.top/tags/configmap/"},{"name":"StatefulSet","slug":"StatefulSet","permalink":"https://wsdlxgp.top/tags/StatefulSet/"},{"name":"nfs-deployment","slug":"nfs-deployment","permalink":"https://wsdlxgp.top/tags/nfs-deployment/"},{"name":"Storage Class","slug":"Storage-Class","permalink":"https://wsdlxgp.top/tags/Storage-Class/"},{"name":"emptyDir","slug":"emptyDir","permalink":"https://wsdlxgp.top/tags/emptyDir/"},{"name":"swarm","slug":"swarm","permalink":"https://wsdlxgp.top/tags/swarm/"},{"name":"Job","slug":"Job","permalink":"https://wsdlxgp.top/tags/Job/"},{"name":"apiVersion","slug":"apiVersion","permalink":"https://wsdlxgp.top/tags/apiVersion/"},{"name":"CronJob","slug":"CronJob","permalink":"https://wsdlxgp.top/tags/CronJob/"},{"name":"Replica","slug":"Replica","permalink":"https://wsdlxgp.top/tags/Replica/"},{"name":"SetDaemonSet","slug":"SetDaemonSet","permalink":"https://wsdlxgp.top/tags/SetDaemonSet/"},{"name":"标签","slug":"标签","permalink":"https://wsdlxgp.top/tags/%E6%A0%87%E7%AD%BE/"},{"name":"liveness","slug":"liveness","permalink":"https://wsdlxgp.top/tags/liveness/"},{"name":"readiness","slug":"readiness","permalink":"https://wsdlxgp.top/tags/readiness/"},{"name":"滚动更新","slug":"滚动更新","permalink":"https://wsdlxgp.top/tags/%E6%BB%9A%E5%8A%A8%E6%9B%B4%E6%96%B0/"},{"name":"Namespace","slug":"Namespace","permalink":"https://wsdlxgp.top/tags/Namespace/"},{"name":"PodRestart","slug":"PodRestart","permalink":"https://wsdlxgp.top/tags/PodRestart/"},{"name":"Policy","slug":"Policy","permalink":"https://wsdlxgp.top/tags/Policy/"},{"name":"service","slug":"service","permalink":"https://wsdlxgp.top/tags/service/"},{"name":"Deployment","slug":"Deployment","permalink":"https://wsdlxgp.top/tags/Deployment/"},{"name":"yaml","slug":"yaml","permalink":"https://wsdlxgp.top/tags/yaml/"},{"name":"deployments","slug":"deployments","permalink":"https://wsdlxgp.top/tags/deployments/"},{"name":"registry","slug":"registry","permalink":"https://wsdlxgp.top/tags/registry/"},{"name":"overlay","slug":"overlay","permalink":"https://wsdlxgp.top/tags/overlay/"},{"name":"webUI","slug":"webUI","permalink":"https://wsdlxgp.top/tags/webUI/"},{"name":"docker网络","slug":"docker网络","permalink":"https://wsdlxgp.top/tags/docker%E7%BD%91%E7%BB%9C/"},{"name":"bind mount","slug":"bind-mount","permalink":"https://wsdlxgp.top/tags/bind-mount/"},{"name":"docker manager volu","slug":"docker-manager-volu","permalink":"https://wsdlxgp.top/tags/docker-manager-volu/"},{"name":"docker网络 - docker跨主机网络","slug":"docker网络-docker跨主机网络","permalink":"https://wsdlxgp.top/tags/docker%E7%BD%91%E7%BB%9C-docker%E8%B7%A8%E4%B8%BB%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"manual单网络","slug":"manual单网络","permalink":"https://wsdlxgp.top/tags/manual%E5%8D%95%E7%BD%91%E7%BB%9C/"},{"name":"macvlan多网络","slug":"macvlan多网络","permalink":"https://wsdlxgp.top/tags/macvlan%E5%A4%9A%E7%BD%91%E7%BB%9C/"},{"name":"consul","slug":"consul","permalink":"https://wsdlxgp.top/tags/consul/"},{"name":"bridge桥接","slug":"bridge桥接","permalink":"https://wsdlxgp.top/tags/bridge%E6%A1%A5%E6%8E%A5/"},{"name":"docker-registry","slug":"docker-registry","permalink":"https://wsdlxgp.top/tags/docker-registry/"},{"name":"docker私有仓库","slug":"docker私有仓库","permalink":"https://wsdlxgp.top/tags/docker%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93/"},{"name":"dockerfile参数","slug":"dockerfile参数","permalink":"https://wsdlxgp.top/tags/dockerfile%E5%8F%82%E6%95%B0/"},{"name":"镜像","slug":"镜像","permalink":"https://wsdlxgp.top/tags/%E9%95%9C%E5%83%8F/"},{"name":"bash","slug":"bash","permalink":"https://wsdlxgp.top/tags/bash/"},{"name":"dockerfile","slug":"dockerfile","permalink":"https://wsdlxgp.top/tags/dockerfile/"},{"name":"docker命令","slug":"docker命令","permalink":"https://wsdlxgp.top/tags/docker%E5%91%BD%E4%BB%A4/"},{"name":"kvm","slug":"kvm","permalink":"https://wsdlxgp.top/tags/kvm/"},{"name":"prometheus","slug":"prometheus","permalink":"https://wsdlxgp.top/tags/prometheus/"},{"name":"alertmanager","slug":"alertmanager","permalink":"https://wsdlxgp.top/tags/alertmanager/"},{"name":"grafana","slug":"grafana","permalink":"https://wsdlxgp.top/tags/grafana/"},{"name":"registrata","slug":"registrata","permalink":"https://wsdlxgp.top/tags/registrata/"},{"name":"sysdig","slug":"sysdig","permalink":"https://wsdlxgp.top/tags/sysdig/"},{"name":"Weave Scope","slug":"Weave-Scope","permalink":"https://wsdlxgp.top/tags/Weave-Scope/"},{"name":"lnmp","slug":"lnmp","permalink":"https://wsdlxgp.top/tags/lnmp/"},{"name":"docker-compose","slug":"docker-compose","permalink":"https://wsdlxgp.top/tags/docker-compose/"},{"name":"wordpress","slug":"wordpress","permalink":"https://wsdlxgp.top/tags/wordpress/"}]}