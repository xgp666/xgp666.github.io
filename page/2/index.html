<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Small steel gun article"><meta name="keywords" content=""><meta name="author" content="Wu Shao Dong,undefined"><meta name="copyright" content="Wu Shao Dong"><title>Today is still beautiful | Xgp &amp; Blog</title><link rel="shortcut icon" href="/my-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.5.6"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css?version=1.5.6"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
} </script><meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="Xgp & Blog" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="author-info"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">Wu Shao Dong</div><div class="author-info__description text-center">Small steel gun article</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">15</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">2</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">1</span></a></div></div></div><nav class="no-bg" id="nav" style="background-image: url(true)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Xgp &amp; Blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="site-info"><div id="site-title">Xgp &amp; Blog</div><div id="site-sub-title">Today is still beautiful</div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/posts/748b.html">04 配置清单</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-05-01</time><div class="content"><h1>一，两种创建资源的方法</h1>
<h2 id="1-基于命令的方式：">1. 基于命令的方式：</h2>
<ol>
<li><strong>简单直观快捷，上手快。</strong></li>
<li><strong>适合临时测试或实验。</strong></li>
</ol>
<h2 id="2-基于配置清单的方式：">2. 基于配置清单的方式：</h2>
<ol>
<li><strong>配置文件描述了 <code>What</code>，即应用最终要达到的状态。</strong></li>
<li><strong>配置文件提供了创建资源的模板，能够重复部署。</strong></li>
<li><strong>可以像管理代码一样管理部署。</strong></li>
<li><strong>适合正式的、跨环境的、规模化部署。</strong></li>
<li><strong>这种方式要求熟悉配置文件的语法，有一定难度。</strong></li>
</ol>
<h2 id="环境介绍">环境介绍</h2>
<table>
<thead>
<tr>
<th>主机</th>
<th>IP地址</th>
<th>服务</th>
</tr>
</thead>
<tbody>
<tr>
<td>master</td>
<td>192.168.1.21</td>
<td>k8s</td>
</tr>
<tr>
<td>node01</td>
<td>192.168.1.22</td>
<td>k8s</td>
</tr>
<tr>
<td>node02</td>
<td>192.168.1.23</td>
<td>k8s</td>
</tr>
</tbody>
</table>
<h1>二. 配置清单（yam，yaml）</h1>
<p><strong>在k8s中，一般使用yaml格式的文件来创建符合我们预期期望的pod，这样的yaml文件我们一般称为资源清单</strong></p>
<blockquote>
<p><strong>/etc/kubernetes/manifests/</strong>    k8s存放（yam、yaml）文件的地方</p>
<p>**kubectl explain deployment（通过explain参数加上资源类别就能看到该资源应该怎么定义）</p>
<p><strong>kubectl explain deployment.metadata</strong> 通过资源类别加上带有Object标记的字段，我们就可以看到一级字段下二级字段的内容有那些怎么去定义等</p>
<p><strong>kubectl explain deployment.metadata.ownerReferences</strong> 通过加上不同级别的字段名称来看下字段下的内容，而且前面的[]号代表对象列表</p>
</blockquote>
<h2 id="1-常见yaml文件写法，以及字段的作用">1.常见yaml文件写法，以及字段的作用</h2>
<p><strong>(1) apiVersion：api版本信息</strong></p>
<p><em><strong>（用来定义当前属于哪个组和那个版本，这个直接关系到最终提供使用的是那个版本）</strong></em></p>
<pre><code>[root@master manifests]# kubectl api-versions
//查看到当前所有api的版本
</code></pre>
<p><strong>(2) kind: 资源对象的类别</strong></p>
<p><em><strong>(用来定义创建的对象是属于什么类别，是pod，service，还是deployment等对象，可以按照其固定的语法格式来自定义。)</strong></em><br>
<strong>(3) metadata: 元数据 名称字段（必写）</strong></p>
<blockquote>
<p><strong>提供以下几个字段</strong>：<br>
　　<strong>creationTimestamp: "2019-06-24T12:18:48Z"</strong><br>
　　<strong>generateName: myweb-5b59c8b9d-</strong><br>
　　<strong>labels: （对象标签）</strong><br>
　　　　<strong>pod-template-hash: 5b59c8b9d</strong><br>
　　　　<strong>run: myweb</strong><br>
　　<strong>name: myweb-5b59c8b9d-gwzz5 （pods对象的名称，同一个类别当中的pod对象名称是唯一的，不能重复）</strong><br>
　　<strong>namespace: default （对象所属的名称空间，同一名称空间内可以重复，这个名称空间也是k8s级别的名称空间，不和容器的名称空间混淆）</strong><br>
　　<strong>ownerReferences:</strong></p>
<p>- <strong>apiVersion: apps/v1</strong><br>
　　　　<strong>blockOwnerDeletion: true</strong><br>
　　　　<strong>controller: true</strong><br>
　　　　<strong>kind: ReplicaSet</strong><br>
　　　　<strong>name: myweb-5b59c8b9d</strong><br>
　　　　<strong>uid: 37f38f64-967a-11e9-8b4b-000c291028e5</strong><br>
　　<strong>resourceVersion: "943"</strong><br>
　　<strong>selfLink: /api/v1/namespaces/default/pods/myweb-5b59c8b9d-gwzz5</strong><br>
　　<strong>uid: 37f653a6-967a-11e9-8b4b-000c291028e5</strong><br>
　　<strong>annotations（资源注解，这个需要提前定义，默认是没有的）</strong><br>
<strong>通过这些标识定义了每个资源引用的path：即/api/group/version/namespaces/名称空间/资源类别/对象名称</strong></p>
</blockquote>
<p><strong>(4) spec： 用户期望的状态</strong></p>
<p><em><strong>（这个字段最重要，因为spec是用来定义目标状态的‘disired state’，而且资源不通导致spec所嵌套的字段也各不相同，也就因为spec重要且字段不相同，k8s在内部自建了一个spec的说明用于查询）</strong></em></p>
<p><strong>(5) status：资源现在处于什么样的状态</strong></p>
<p><em><strong>（当前状态，’current state‘，这个字段有k8s集群来生成和维护，不能自定义，属于一个只读字段）</strong></em></p>
<h2 id="2-编写一个yaml文件">2.编写一个yaml文件</h2>
<pre><code>[root@master ~]# vim web.yaml
kind: Deployment  #资源对象是控制器
apiVersion: extensions/v1beta1   #api的版本
metadata:      #描述kind（资源类型）
  name: web   #定义控制器名称
spec:
  replicas: 2   #副本数量
  template:     #模板
    metadata:    
      labels:   #标签
        app: web_server
    spec:
      containers:   #指定容器
      - name: nginx  #容器名称
        image: nginx   #使用的镜像
</code></pre>
<h3 id="执行一下">执行一下</h3>
<pre><code>[root@master ~]# kubectl apply -f web.yaml 
</code></pre>
<h3 id="查看一下">查看一下</h3>
<pre><code>[root@master ~]# kubectl get deployments.  -o wide
//查看控制器信息
</code></pre>
<p><img src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5C%E8%99%9A%E6%8B%9F%E5%8C%96%5Ckubernetes%5Ck8s%E6%96%87%E6%A1%A3%5C04%E9%85%8D%E7%BD%AE%E6%B8%85%E5%8D%95.assets%5Cimage-20200107100450262.png" alt="image-20200107100450262"></p>
<pre><code>[root@master ~]# kubectl get pod -o wide
//查看pod节点信息
</code></pre>
<p><img src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5C%E8%99%9A%E6%8B%9F%E5%8C%96%5Ckubernetes%5Ck8s%E6%96%87%E6%A1%A3%5C04%E9%85%8D%E7%BD%AE%E6%B8%85%E5%8D%95.assets%5Cimage-20200107101803209.png" alt="image-20200107101803209"></p>
<h2 id="3-编写一个service-yaml文件">3.编写一个service.yaml文件</h2>
<pre><code>[root@master ~]# vim web-svc.yaml
kind: Service  #资源对象是副本
apiVersion: v1   #api的版本
metadata:
  name: web-svc
spec:
  selector:     #标签选择器
    app: web-server  #须和web.yaml的标签一致
  ports:              #端口
  - protocol: TCP
    port: 80            #宿主机的端口
    targetPort: 80      #容器的端口
</code></pre>
<blockquote>
<p><strong>使用相同标签和标签选择器内容，使两个资源对象相互关联。</strong></p>
<p><strong>创建的service资源对象，默认的type为ClusterIP，意味着集群内任意节点都可访问。它的作用是为后端真正服务的pod提供一个统一的接口。如果想要外网能够访问服务，应该把type改为NodePort</strong></p>
</blockquote>
<h3 id="（1）执行一下">（1）执行一下</h3>
<pre><code>[root@master ~]# kubectl apply -f web-svc.yaml 
</code></pre>
<h3 id="（2）查看一下">（2）查看一下</h3>
<pre><code>[root@master ~]# kubectl get svc
//查看控制器信息
</code></pre>
<p><img src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5C%E8%99%9A%E6%8B%9F%E5%8C%96%5Ckubernetes%5Ck8s%E6%96%87%E6%A1%A3%5C04%E9%85%8D%E7%BD%AE%E6%B8%85%E5%8D%95.assets%5Cimage-20200107110717972.png" alt="image-20200107110717972"></p>
<h3 id="（3）访问一下">（3）访问一下</h3>
<pre><code>[root@master ~]# curl 10.111.193.168
</code></pre>
<p><img src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5C%E8%99%9A%E6%8B%9F%E5%8C%96%5Ckubernetes%5Ck8s%E6%96%87%E6%A1%A3%5C04%E9%85%8D%E7%BD%AE%E6%B8%85%E5%8D%95.assets%5Cimage-20200107110837353.png" alt="image-20200107110837353"></p>
<h2 id="4-外网能够访问服务">4.外网能够访问服务</h2>
<h3 id="（1）修改web-svc-yaml文件">（1）修改web-svc.yaml文件</h3>
<pre><code>kind: Service  #资源对象是副本
apiVersion: v1   #api的版本
metadata:
  name: web-svc
spec:
  type: NodePort    #添加 更改网络类型
  selector:     #标签选择器
    app: web_server  #须和web.yaml的标签一致
  ports:              #端口
  - protocol: TCP
    port: 80            #宿主机的端口
    targetPort: 80      #容器的端口
    nodePort: 30086     #指定群集映射端口，范围是30000-32767
</code></pre>
<h3 id="（2）刷新一下">（2）刷新一下</h3>
<pre><code>[root@master ~]#  kubectl apply -f web-svc.yaml 
</code></pre>
<h3 id="（3）查看一下">（3）查看一下</h3>
<pre><code>[root@master ~]# kubectl get svc
</code></pre>
<p><img src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5C%E8%99%9A%E6%8B%9F%E5%8C%96%5Ckubernetes%5Ck8s%E6%96%87%E6%A1%A3%5C04%E9%85%8D%E7%BD%AE%E6%B8%85%E5%8D%95.assets%5Cimage-20200107111338940.png" alt="image-20200107111338940"></p>
<h3 id="（4）浏览器测试">（4）浏览器测试</h3>
<p><img src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5C%E8%99%9A%E6%8B%9F%E5%8C%96%5Ckubernetes%5Ck8s%E6%96%87%E6%A1%A3%5C04%E9%85%8D%E7%BD%AE%E6%B8%85%E5%8D%95.assets%5Cimage-20200107111451952.png" alt="image-20200107111451952"></p>
<h1>三、小实验</h1>
<blockquote>
<p><strong>基于上一篇博客实验继续进行</strong></p>
</blockquote>
<h3 id="1-使用yaml文件的方式创建一个Deployment资源对象，要求镜像使用个人私有镜像v1版本。replicas为3个。">1.使用yaml文件的方式创建一个Deployment资源对象，要求镜像使用个人私有镜像v1版本。replicas为3个。</h3>
<h3 id="编写yaml文件">编写yaml文件</h3>
<pre><code>[root@master ~]# vim www.yaml
kind: Deployment
apiVersion: extensions/v1beta1
metadata:
  name: xgp
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: www_server
    spec:
      containers:
      - name: web
        image: 192.168.1.21:5000/web:v1   
</code></pre>
<h4 id="（1）执行一下-2">（1）执行一下</h4>
<pre><code>[root@master ~]# kubectl apply -f web-svc.yaml 
</code></pre>
<h4 id="（2）查看一下-2">（2）查看一下</h4>
<pre><code>[root@master ~]# kubectl get deployments. -o wide
//查看控制器信息
</code></pre>
<p><img src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5C%E8%99%9A%E6%8B%9F%E5%8C%96%5Ckubernetes%5Ck8s%E6%96%87%E6%A1%A3%5C04%E9%85%8D%E7%BD%AE%E6%B8%85%E5%8D%95.assets%5Cimage-20200107120901208.png" alt="image-20200107120901208"></p>
<pre><code>[root@master ~]# kubectl get pod -o wide
//查看pod节点信息
</code></pre>
<p><img src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5C%E8%99%9A%E6%8B%9F%E5%8C%96%5Ckubernetes%5Ck8s%E6%96%87%E6%A1%A3%5C04%E9%85%8D%E7%BD%AE%E6%B8%85%E5%8D%95.assets%5Cimage-20200107121002152.png" alt="image-20200107121002152"></p>
<h4 id="（3）访问一下-2">（3）访问一下</h4>
<p><img src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5C%E8%99%9A%E6%8B%9F%E5%8C%96%5Ckubernetes%5Ck8s%E6%96%87%E6%A1%A3%5C04%E9%85%8D%E7%BD%AE%E6%B8%85%E5%8D%95.assets%5Cimage-20200107121147669.png" alt="image-20200107121147669"></p>
<h3 id="2-使用yaml文件的方式创建一个Service资源对象，要与上述Deployment资源对象关联，type类型为：-NodePort，端口为-30123"><strong>2.</strong>  使用yaml文件的方式创建一个Service资源对象，要与上述Deployment资源对象关联，type类型为： NodePort，端口为:30123.</h3>
<h4 id="编写service文件">编写service文件</h4>
<pre><code>[root@master ~]# vim www-svc.yaml
kind: Service
apiVersion: v1
metadata:
  name: www-svc
spec:
  type: NodePort
  selector:
    app: www_server
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
    nodePort: 30123
</code></pre>
<h4 id="执行一下-2">执行一下</h4>
<pre><code>[root@master ~]# kubectl apply -f www-svc.yaml 
</code></pre>
<h4 id="查看一下-2">查看一下</h4>
<pre><code>[root@master ~]# kubectl get svc
</code></pre>
<p><img src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5C%E8%99%9A%E6%8B%9F%E5%8C%96%5Ckubernetes%5Ck8s%E6%96%87%E6%A1%A3%5C04%E9%85%8D%E7%BD%AE%E6%B8%85%E5%8D%95.assets%5Cimage-20200107121929525.png" alt="image-20200107121929525"></p>
<h4 id="访问一下">访问一下</h4>
<p><img src="/posts/G:%5C%E5%9B%9B%E6%9C%9F%5C%E8%99%9A%E6%8B%9F%E5%8C%96%5Ckubernetes%5Ck8s%E6%96%87%E6%A1%A3%5C04%E9%85%8D%E7%BD%AE%E6%B8%85%E5%8D%95.assets%5Cimage-20200107122015559.png" alt="image-20200107122015559"></p>
<h1>四. 总结</h1>
<h2 id="1-Pod的作用"><strong>1. Pod的作用</strong></h2>
<blockquote>
<p>在k8s中pod是最小的管理单位，在一个pod中通常会包含一个或多个容器。大多数情况下，一个Pod内只有一个Container容器。<br>
在每一个Pod中都有一个特殊的Pause容器和一个或多个业务容器，Pause来源于pause-amd64镜像,Pause容器在Pod中具有非常重要的作用：</p>
<ul>
<li>Pause容器作为Pod容器的根容器，其本地于业务容器无关，它的状态代表了整个pod的状态。</li>
<li>Pod里的多个业务容器共享Pause容器的IP，每个Pod被分配一个独立的IP地址，Pod中的每个容器共享网络命名空间，包括IP地址和网络端口。Pod内的容器可以使用localhost相互通信。k8s支持底层网络集群内任意两个Pod之间进行通信。</li>
<li>Pod中的所有容器都可以访问共享volumes，允许这些容器共享数据。volumes还用于Pod中的数据持久化，以防其中一个容器需要重新启动而丢失数据。</li>
</ul>
</blockquote>
<h2 id="2-Service的作用"><strong>2. Service的作用</strong></h2>
<p><strong>Service 是后端真实服务的抽象，一个 Service 可以代表多个相同的后端服务</strong></p>
<p><strong>Service 为 POD 控制器控制的 POD 集群提供一个固定的访问端点，Service 的工作还依赖于 K8s 中的一个附件，就是 CoreDNS ，它将 Service 地址提供一个域名解析。</strong></p>
<h3 id="NodePort-类型的-service">NodePort 类型的 service</h3>
<blockquote>
<p><strong>clusterIP</strong>：指定 Service 处于 service 网络的哪个 IP，默认为动态分配</p>
<p><strong>NodePort 是在 ClusterIP 类型上增加了一个暴露在了 node 的网络命名空间上的一个 nodePort，所以用户可以从集群外部访问到集群了，因而用户的请求流程是：Client -&gt; NodeIP:NodePort -&gt; ClusterIP:ServicePort -&gt; PodIP:ContainerPort。</strong></p>
<p><strong>可以理解为 NodePort 增强了 ClusterIP 的功能，让客户端可以在每个集群外部访问任意一个 nodeip 从而访问到 clusterIP，再由 clusterIP 进行负载均衡至 POD。</strong></p>
</blockquote>
<h2 id="3-流量走向">3.流量走向</h2>
<p><strong>我们在创建完成一个服务之后，用户首先应该访问的是nginx反向代理的ip，然后通过nginx访问到后端的k8s服务器（master节点）的“NodePort暴露IP 及 映射的端口“，master的apiserver接受到客户端发送来的访问指令，将访问指令通知Controller Manager控制器，Scheduler执行调度任务，将访问指令分发到各节点之上，通过”master节点“的“ip+映射端口”访问到后端k8s节点的信息，节点的Kubelet（pod代理）当Scheduler确定让那个节点返回访问信息之后，kube-proxy将访问信息负载均衡到该节点的容器上，各容器返回信息，并向Master报告运行状态</strong></p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script></div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/posts/6989.html">03 创建资源的两种方式</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-05-01</time><div class="content"><table>
<thead>
<tr>
<th>主机</th>
<th>IP地址</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>master</td>
<td>192.168.1.21</td>
<td></td>
</tr>
<tr>
<td>node01</td>
<td>192.168.1.22</td>
<td></td>
</tr>
<tr>
<td>node02</td>
<td>192.168.1.23</td>
<td></td>
</tr>
</tbody>
</table>
<h1>两种创建资源的方法</h1>
<h2 id="基于命令的方式：">基于命令的方式：</h2>
<ol>
<li><strong>简单直观快捷，上手快。</strong></li>
<li><strong>适合临时测试或实验。</strong></li>
</ol>
<h2 id="基于配置文件的方式：">基于配置文件的方式：</h2>
<ol>
<li><strong>配置文件描述了 <code>What</code>，即应用最终要达到的状态。</strong></li>
<li><strong>配置文件提供了创建资源的模板，能够重复部署。</strong></li>
<li><strong>可以像管理代码一样管理部署。</strong></li>
<li><strong>适合正式的、跨环境的、规模化部署。</strong></li>
<li><strong>这种方式要求熟悉配置文件的语法，有一定难度。</strong></li>
</ol>
<h1>一，用命令行的方式创建资源</h1>
<h3 id="仅接受json格式"><em>仅接受json格式</em></h3>
<h2 id="配置清单（yml、yaml）">配置清单（yml、yaml）</h2>
<pre><code>[root@master ~]# cd /etc/kubernetes/manifests/
//k8s的yml、yaml文件
</code></pre>
<h2 id="1-node01和node02下载nginx镜像">1.node01和node02下载nginx镜像</h2>
<pre><code>docker pull nginx
//下载nginx镜像
</code></pre>
<h2 id="2-master创建Pod控制器（test-web），deployment">2.master创建Pod控制器（test-web），deployment</h2>
<pre><code>[root@master ~]# kubectl run test-web --image=nginx --replicas=5
//创建Pod控制器，deployment
</code></pre>
<h2 id="3-查看控制器情况">3.查看控制器情况</h2>
<h3 id="（1）">（1）</h3>
<pre><code>[root@master ~]# kubectl get deployments.
//查看控制器情况
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106093615852.png" alt="image-20200106093615852"></p>
<pre><code>[root@master ~]# kubectl get pod --all-namespaces -o wide
//显示pod的节点信息
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106093922849.png" alt="image-20200106093922849"></p>
<h3 id="（2）">（2）</h3>
<pre><code>[root@master ~]# kubectl get namespaces 
//查看k8s名称空间
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106093850247.png" alt="image-20200106093850247"></p>
<pre><code>[root@master ~]# kubectl describe deployments. test-web
//查看资源详细信息
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106093723330.png" alt="image-20200106093723330"></p>
<p><em><strong>查看某种资源对象，没有指定名称空间，默认是在default名称空间。可以加上-n选项，查看指定名称空间的资源。</strong></em></p>
<pre><code>[root@master ~]# kubectl get pod -n kube-system 
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106094343401.png" alt="image-20200106094343401"></p>
<h2 id="3-删除test-web控制器">3.删除test-web控制器</h2>
<pre><code>[root@master ~]# kubectl delete deployments. test-web 
</code></pre>
<h2 id="4-master创建Pod控制器（web），deployment">4.master创建Pod控制器（web），deployment</h2>
<pre><code>[root@master ~]# kubectl run web --image=nginx --replicas=5
</code></pre>
<h3 id="查看一下pod信息">查看一下pod信息</h3>
<pre><code>[root@master ~]# kubectl get pod -o wide
//查看一下pod的节点信息
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106095722353.png" alt="image-20200106095722353"></p>
<pre><code>[root@master ~]# kubectl describe deployments. web 
//查看资源详细信息
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106100606861.png" alt="image-20200106100606861"></p>
<p><em><strong>注意：直接运行创建的deployment资源对象，是经常使用的一个控制器资源类型，除了deployment，还有rc、rs等等pod控制器，deployment是一个高级的pod控制器。</strong></em></p>
<h3 id="本机测试访问nginx">本机测试访问nginx</h3>
<pre><code>[root@master ~]# curl 10.244.1.7
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106100827131.png" alt="image-20200106100827131"></p>
<h2 id="5-创建service资源类型">5.创建service资源类型</h2>
<pre><code>[root@master ~]# kubectl expose deployment web --name=web-xgp --port=80 --type=NodePort
//创建service资源类型，这里我们设置了映射端口
</code></pre>
<p><em><strong>如果想要外网能够访问服务，可以暴露deployment资源，得到service资源，但svc资源的类型必须为NodePort。</strong></em></p>
<p><strong>映射端口范围：30000-32767</strong></p>
<h3 id="查看service信息">查看service信息</h3>
<pre><code>[root@master ~]# kubectl get svc
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106101443348.png" alt="image-20200106101443348"></p>
<h3 id="浏览器测试访问http-192-168-1-21-30493">浏览器测试访问http://192.168.1.21:30493/</h3>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106101624954.png" alt="image-20200106101624954"></p>
<h1>二、服务的扩容与缩容</h1>
<h2 id="1-查看控制器信息">1. 查看控制器信息</h2>
<pre><code>[root@master ~]# kubectl get deployments. -o wide
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106104638757.png" alt="image-20200106104638757"></p>
<h2 id="2-扩容">2.扩容</h2>
<pre><code>[root@master ~]# kubectl scale deployment web --replicas=8
</code></pre>
<h3 id="查看一下">查看一下</h3>
<pre><code>[root@master ~]# kubectl get deployments. -o wide
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106104757123.png" alt="image-20200106104757123"></p>
<h2 id="3-缩容">3.缩容</h2>
<pre><code>[root@master ~]# kubectl scale deployment web --replicas=4
</code></pre>
<h3 id="查看一下-2">查看一下</h3>
<pre><code>[root@master ~]# kubectl get deployments. -o wide
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106105536316.png" alt="image-20200106105536316"></p>
<h2 id="3-通过修改web的yaml文件进行扩容缩容">3.通过修改web的yaml文件进行扩容缩容</h2>
<h3 id="备份web的yaml文件">备份web的yaml文件</h3>
<pre><code>[root@master ~]# kubectl get deployments. -o yaml &gt; web.yaml
</code></pre>
<h3 id="使用edit修改web的yaml文件">使用edit修改web的yaml文件</h3>
<pre><code>[root@master ~]# kubectl edit deployments. web 
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106105924531.png" alt="image-20200106105924531"></p>
<h3 id="查看一下-3">查看一下</h3>
<pre><code>[root@master ~]# kubectl get deployments. -o wide
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106105816339.png" alt="image-20200106105816339"></p>
<h1>三、服务的升级与回滚</h1>
<h2 id="node01和node02下载1-15版本的nginx">node01和node02下载1.15版本的nginx</h2>
<pre><code>[root@master ~]# docker pull nginx:1.15
</code></pre>
<h2 id="1-master设置服务升级">1.master设置服务升级</h2>
<pre><code>[root@master ~]#  kubectl set image deployment web web=nginx:1.15
</code></pre>
<h3 id="查看一下-4">查看一下</h3>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106111227960.png" alt="image-20200106111227960"></p>
<h2 id="2-master设置服务回滚">2.master设置服务回滚</h2>
<h3 id="（1）修改配置文件回滚">（1）修改配置文件回滚</h3>
<h3 id="使用edit修改web的yaml文件-2">使用edit修改web的yaml文件</h3>
<pre><code>[root@master ~]# kubectl edit deployments. web 
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106111523148.png" alt="image-20200106111523148"></p>
<h3 id="查看一下-5">查看一下</h3>
<pre><code>[root@master ~]# kubectl get deployments. -o wide
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106111319699.png" alt="image-20200106111319699"></p>
<h3 id="（2）命令回滚">（2）命令回滚</h3>
<pre><code>[root@master ~]# kubectl rollout undo deployment web 
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106111733617.png" alt="image-20200106111733617"></p>
<p><em><strong>注意:只能回滚到上一次操作的状态</strong></em></p>
<h1>四、实验环境</h1>
<table>
<thead>
<tr>
<th>主机</th>
<th>IP地址</th>
<th>服务</th>
</tr>
</thead>
<tbody>
<tr>
<td>master</td>
<td>192.168.1.21</td>
<td>registry+Deployment</td>
</tr>
<tr>
<td>node01</td>
<td>192.168.1.22</td>
<td></td>
</tr>
<tr>
<td>node02</td>
<td>192.168.1.23</td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="1-master-基于httpd制作自己的镜像，需要3个版本，v1-v2-v3-并且对应的版本镜像，访问的主目录内容不一样">1.master 基于httpd制作自己的镜像，需要3个版本，v1,v2,v3.并且对应的版本镜像，访问的主目录内容不一样</h2>
<h3 id="（1）master下载httpd镜像">（1）master下载httpd镜像</h3>
<pre><code>[root@master ~]# docker pull httpd
</code></pre>
<h3 id="（2）编写Dockerfile">（2）编写Dockerfile</h3>
<pre><code>[root@master xgp]# vim Dockerfile
FROM httpd
COPY index.html /usr/local/apache2/htdocs/index.html
</code></pre>
<h3 id="（3）创建测试网页v1">（3）创建测试网页v1</h3>
<pre><code>[root@master xgp]#echo "&lt;h1&gt;xgp | test-web | httpd:v1&lt;h1&gt;" &gt; index.html
</code></pre>
<h3 id="（4）基于Dockerfile创建镜像-web1">（4）基于Dockerfile创建镜像 web1</h3>
<pre><code>[root@master xgp]# docker build -t web1 .
</code></pre>
<h3 id="（5）创建测试网页v2">（5）创建测试网页v2</h3>
<pre><code>[root@master xgp]#echo "&lt;h1&gt;xgp | test-web | httpd:v1&lt;h1&gt;" &gt; index.html
</code></pre>
<h3 id="（6）基于Dockerfile创建镜像-web2">（6）基于Dockerfile创建镜像 web2</h3>
<pre><code>[root@master xgp]# docker build -t web2 .
</code></pre>
<h3 id="（7）创建测试网页v3">（7）创建测试网页v3</h3>
<pre><code>[root@master xgp]# echo "&lt;h1&gt;xgp | test-web | httpd:v3&lt;h1&gt;" &gt; index.html
</code></pre>
<h3 id="（8）基于Dockerfile创建镜像-web3">（8）基于Dockerfile创建镜像 web3</h3>
<pre><code>[root@master xgp]# docker build -t web3 .
</code></pre>
<h2 id="2-master部署私有仓库">2.master部署私有仓库</h2>
<h3 id="（1）master下载registry镜像">（1）master下载registry镜像</h3>
<pre><code>[root@master ~]# docker pull registry
</code></pre>
<h3 id="（2）启动registry">（2）启动registry</h3>
<pre><code>[root@master xgp]# docker run -itd --name registry -p 5000:5000 --restart=always registry:latest 
</code></pre>
<h3 id="（3）修改docker配置文件，加入私有仓库（三台）">（3）修改docker配置文件，加入私有仓库（三台）</h3>
<pre><code>[root@master xgp]# vim /usr/lib/systemd/system/docker.service
ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.21:5000
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106120848869.png" alt="image-20200106120848869"></p>
<h3 id="（4）重启docker（三台）">（4）重启docker（三台）</h3>
<pre><code>[root@master xgp]# systemctl daemon-reload 
[root@master xgp]# systemctl restart docker
</code></pre>
<h2 id="3-上传之前创建的三个web镜像到私有仓库">3.上传之前创建的三个web镜像到私有仓库</h2>
<h3 id="（1）修改镜像标签">（1）修改镜像标签</h3>
<pre><code>[root@master xgp]# docker tag web1:latest 192.168.1.21:5000/web1:latest
[root@master xgp]# docker tag web2:latest 192.168.1.21:5000/web2:latest
[root@master xgp]# docker tag web3:latest 192.168.1.21:5000/web3:latest
</code></pre>
<h3 id="（2）将三个web镜像上传到私有仓库">（2）将三个web镜像上传到私有仓库</h3>
<pre><code>[root@master xgp]# docker push  192.168.1.21:5000/web1:latest 
[root@master xgp]# docker push  192.168.1.21:5000/web2:latest
[root@master xgp]# docker push  192.168.1.21:5000/web3:latest 
</code></pre>
<h2 id="4-部署一个Deployment资源对象，要求镜像使用上述私有镜像v1版本。6个副本Pod。">4.部署一个Deployment资源对象，要求镜像使用上述私有镜像v1版本。6个副本Pod。</h2>
<pre><code>[root@master xgp]# kubectl run www1 --image=192.168.1.21:5000/web1:latest --replicas=6
</code></pre>
<h3 id="查看一下-6">查看一下</h3>
<pre><code>[root@master xgp]# kubectl get pod
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106122026271.png" alt="image-20200106122026271"></p>
<h3 id="本地访问一下">本地访问一下</h3>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106122426308.png" alt="image-20200106122426308"></p>
<h3 id="5-将上述Deployment暴露一个service资源对象，使外网能否访问服务。">5.将上述Deployment暴露一个service资源对象，使外网能否访问服务。</h3>
<pre><code>[root@master xgp]#  kubectl expose deployment www1 --name=web-xgp --port=80 --type=NodePort
</code></pre>
<h3 id="查看一下-7">查看一下</h3>
<pre><code>[root@master xgp]# kubectl get svc
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106122313996.png" alt="image-20200106122313996"></p>
<h3 id="浏览器访问一下">浏览器访问一下</h3>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106122340747.png" alt="image-20200106122340747"></p>
<h2 id="6-将上述Deployment进行扩容和缩容操作，扩容为8个副本Pod，然后缩容为4个副本Pod。">6.将上述Deployment进行扩容和缩容操作，扩容为8个副本Pod，然后缩容为4个副本Pod。</h2>
<h2 id="（1）扩容">（1）扩容</h2>
<pre><code>[root@master xgp]# kubectl scale deployment www1 --replicas=8
</code></pre>
<h3 id="查看一下-8">查看一下</h3>
<pre><code>[root@master xgp]# kubectl get deployments. -o wide
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106122722977.png" alt="image-20200106122722977"></p>
<h2 id="（2）缩容">（2）缩容</h2>
<h3 id="修改k8s配置文件">修改k8s配置文件</h3>
<h3 id="备份web的yaml文件-2">备份web的yaml文件</h3>
<pre><code>[root@master ~]# kubectl get deployments. -o yaml &gt; www1.yaml
</code></pre>
<h3 id="使用edit修改web的yaml文件-3">使用edit修改web的yaml文件</h3>
<pre><code>[root@master ~]# kubectl edit deployments. www1
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106105924531.png" alt="image-20200106105924531"></p>
<h3 id="查看一下-9">查看一下</h3>
<pre><code>[root@master xgp]# kubectl get deployments. -o wide
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106122953397.png" alt="image-20200106122953397"></p>
<h2 id="7-将上述Deployment进行升级与回滚操作，将v1版本，升级到v2版本。">7.将上述Deployment进行升级与回滚操作，将v1版本，升级到v2版本。</h2>
<h2 id="（1）升级版本为web2">（1）升级版本为web2</h2>
<pre><code>[root@master ~]# kubectl set image deployment www1 www1=192.168.1.21:5000/web2
</code></pre>
<h3 id="本机测试访问">本机测试访问</h3>
<pre><code>[root@master ~]# curl 127.0.0.1:30996
&lt;h1&gt;xgp | test-web | httpd:v2&lt;h1&gt;
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106125722931.png" alt="image-20200106125722931"></p>
<h3 id="浏览器测试访问">浏览器测试访问</h3>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106125750021.png" alt="image-20200106125750021"></p>
<h2 id="（2）回滚版本到web1">（2）回滚版本到web1</h2>
<h3 id="1-修改配置文件回滚">&lt;1&gt;修改配置文件回滚</h3>
<h3 id="使用edit修改web的yaml文件-4">使用edit修改web的yaml文件</h3>
<pre><code>[root@master ~]# kubectl edit deployments. www1
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106130010344.png" alt="image-20200106130010344"></p>
<h3 id="查看一下-10">查看一下</h3>
<pre><code>[root@master ~]# kubectl get deployments. -o wide
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106130304423.png" alt="image-20200106130304423"></p>
<h3 id="访问一下">访问一下</h3>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106130435212.png" alt="image-20200106130435212"></p>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106130447693.png" alt="image-20200106130447693"></p>
<h3 id="2-命令回滚">&lt;2&gt;命令回滚</h3>
<pre><code>[root@master ~]# kubectl rollout undo deployment www1
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106130317956.png" alt="image-20200106130317956"></p>
<p><em><strong>注意:只能回滚到上一次操作的状态</strong></em></p>
<h3 id="访问一下-2">访问一下</h3>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106130357339.png" alt="image-20200106130357339"></p>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200106130414060.png" alt="image-20200106130414060"></p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script></div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/posts/cd85.html">02 k8s架构，基本概念</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-05-01</time><div class="content"><table>
<thead>
<tr>
<th>主机名</th>
<th>IP地址</th>
<th>服务</th>
</tr>
</thead>
<tbody>
<tr>
<td>master</td>
<td>192.168.1.21</td>
<td></td>
</tr>
<tr>
<td>node01</td>
<td>192.168.1.22</td>
<td></td>
</tr>
<tr>
<td>node02</td>
<td>192.168.1.23</td>
<td></td>
</tr>
</tbody>
</table>
<h1>kubernetes架构</h1>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/qqq.png" alt="image-20200104100759831"></p>
<p><strong>kubectl</strong>：k8s是命令行端，用来发送客户的操作指令。</p>
<h2 id="master节点">master节点</h2>
<p><strong>1. API server[资源操作入口]</strong>：是k8s集群的前端接口，各种各样客户端工具以及k8s的其他组件可以通过它管理k8s集群的各种资源。它提供了HTTP/HTTPS RESTful API,即K8S API。</p>
<blockquote>
<ul>
<li><strong>提供了资源对象的唯一操作入口，其他所有组件都必须通过它提供的API来操作资源数据，只有API Server与存储通信，其他模块通过API Server访问集群状态。</strong></li>
</ul>
<p><strong>第一，是为了保证集群状态访问的安全。</strong></p>
<p><strong>第二，是为了隔离集群状态访问的方式和后端存储实现的方式：API Server是状态访问的方式，不会因为后端存储技术etcd的改变而改变。</strong></p>
<ul>
<li><strong>作为kubernetes系统的入口，封装了核心对象的增删改查操作，以<a href="https://www.centos.bz/tag/restful/" target="_blank" rel="noopener">RESTFul</a>接口方式提供给外部客户和内部组件调用。对相关的资源数据“全量查询”+“变化监听”，实时完成相关的业务功能。</strong></li>
</ul>
</blockquote>
<p><strong>2. Scheduler[集群分发调度器]</strong>：负责决定将Pod放在哪个Node上运行。在调度时，会充分考虑集群的拓扑结构，当前各个节点的负载情况，以及应对高可用、性能、数据亲和性和需求。</p>
<blockquote>
<p><strong>1.Scheduler收集和分析当前Kubernetes集群中所有Minion节点的资源(内存、CPU)负载情况，然后依此分发新建的Pod到Kubernetes集群中可用的节点。</strong></p>
<p><strong>2.实时监测Kubernetes集群中未分发和已分发的所有运行的Pod。</strong></p>
<p><strong>3.Scheduler也监测Minion节点信息，由于会频繁查找Minion节点，Scheduler会缓存一份最新的信息在本地。</strong></p>
<p><strong>4.最后，Scheduler在分发Pod到指定的Minion节点后，会把Pod相关的信息Binding写回API Server。</strong></p>
</blockquote>
<p><strong>4. Controller Manager[内部管理控制中心]</strong>：负责管理集群的各种资源，保证资源处于预期的状态。它由多种Controller组成，包括Replication Controller、Endpoints Controller、Namespace Controller、Serviceaccounts Controller等。</p>
<blockquote>
<p><strong>实现集群故障检测和恢复的自动化工作，负责执行各种控制器，主要有：</strong></p>
<p><strong>1.endpoint-controller：定期关联<a href="https://www.centos.bz/tag/service/" target="_blank" rel="noopener">service</a>和pod(关联信息由endpoint对象维护)，保证service到pod的映射总是最新的。</strong></p>
<p><strong>2.replication-controller：定期关联replicationController和pod，保证replicationController定义的复制数量与实际运行pod的数量总是一致的。</strong></p>
</blockquote>
<p><strong>5. Etcd</strong>：负责保存k8s集群的配置信息和各种资源的状态信息。当数据发生变化时，etcd会快速的通知k8s相关组件。<a href="">（第三方组件）它有可替换方案。Consul、zookeeper</a></p>
<p><strong>6. Pod:</strong> k8s集群的最小组成单位。一个Pod内，可以运行一个或多个容器。大多数情况下，一个Pod内只有一个Container容器。</p>
<p><strong>7. Flanner</strong>：是k8s集群网络，可以保证Pod的跨主机通信。也有替换方案。</p>
<pre><code>[root@master ~]# kubectl get pod --all-namespaces
//查看pod信息
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200104100759831.png" alt="image-20200104100759831"></p>
<pre><code>[root@master ~]# kubectl get pod --all-namespaces -o wide
//显示pod的节点信息
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200104101023909.png" alt="image-20200104101023909"></p>
<h2 id="Node节点">Node节点</h2>
<p><strong>Kubelet[节点上的Pod管家]</strong>：它是Node的agent(代理)，当Scheduler确定某 个Node上运行Pod之后，会将Pod的具体配置信息发送给该节点的kubelet,kubelet会根据这些信息创建和运行容器，并向Master报告运行状态。</p>
<blockquote>
<ul>
<li><strong>负责Node节点上pod的创建、修改、监控、删除等全生命周期的管理</strong></li>
<li><strong>定时上报本Node的状态信息给API Server。</strong></li>
<li><strong>kubelet是Master API Server和Minion之间的桥梁，接收Master API Server分配给它的commands和work，与持久性键值存储etcd、file、server和http进行交互，读取配置信息。</strong></li>
<li><strong>具体的工作如下：</strong></li>
</ul>
<p><strong>设置容器的环境变量、给容器绑定<a href="https://www.centos.bz/tag/volume/" target="_blank" rel="noopener">Volume</a>、给容器绑定Port、根据指定的Pod运行一个单一容器、给指定的Pod创建network 容器。</strong></p>
<p><strong>同步Pod的状态、同步Pod的状态、从<a href="https://www.centos.bz/tag/cadvisor/" target="_blank" rel="noopener">cAdvisor</a>获取<a href="https://www.centos.bz/tag/container/" target="_blank" rel="noopener">Container</a> info、 pod info、 root info、 <a href="https://www.centos.bz/tag/machine/" target="_blank" rel="noopener">machine</a> info。</strong></p>
<p><strong>在容器中运行命令、杀死容器、删除Pod的所有容器。</strong></p>
</blockquote>
<p>**kube-proxy[负载均衡、路由转发]:**负责将访问service的TCP/UDP数据流转发到后端的容器。如果有多个副本，kube-proxy会实现负载均衡。</p>
<blockquote>
<ul>
<li><strong>Proxy是为了解决外部网络能够访问跨机器集群中容器提供的应用服务而设计的，运行在每个Node上。Proxy提供TCP/UDP sockets的proxy，每创建一种Service，Proxy主要从etcd获取Services和Endpoints的配置信息（也可以从file获取），然后根据配置信息在Minion上启动一个Proxy的进程并监听相应的服务端口，当外部请求发生时，Proxy会根据Load Balancer将请求分发到后端正确的容器处理。</strong></li>
<li><strong>Proxy不但解决了同一主宿机相同服务端口冲突的问题，还提供了Service转发服务端口对外提供服务的能力，Proxy后端使用了随机、轮循负载均衡算法。</strong></li>
</ul>
</blockquote>
<h2 id="范例">范例</h2>
<blockquote>
<h3 id="分析各个组件的作用以及架构工作流程">分析各个组件的作用以及架构工作流程:</h3>
<p><strong>1) kubectl发送部署 请求到API server</strong><br>
<strong>2) APIserver通知Controller Manager创建一个Deployment资源。</strong><br>
<strong>3) Scheduler执行调度任务,将两个副本Pod分发到node01和node02. 上。</strong><br>
<strong>4) node01和node02, 上的kubelet在各自节点上创建并运行Pod。</strong></p>
<h3 id="补充">补充</h3>
<p><strong>1.应用的配置和当前的状态信息保存在etcd中，执行kubectl get pod时API server会从etcd中读取这些数据。</strong></p>
<p><strong>2.flannel会为每个Pod分配一个IP。 但此时没有创建Service资源，目前kube-proxy还没有参与进来。</strong></p>
</blockquote>
<h3 id="运行一个例子（创建一个deployment资源对象-pod控制器-）">运行一个例子（创建一个deployment资源对象&lt;pod控制器&gt;）</h3>
<pre><code>[root@master ~]# kubectl run test-web --image=httpd --replicas=2
//创建一个deployment资源对象。
</code></pre>
<p><em><strong>运行完成之后，如果有镜像可直接开启，没有的话需要等待一会儿，node节点要在docker hup上下载</strong></em></p>
<h4 id="查看一下">查看一下</h4>
<pre><code>[root@master ~]# kubectl get  deployments.或 kubectl get  deploy
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200104110812772.png" alt="image-20200104110812772"></p>
<pre><code>[root@master ~]# kubectl get pod
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200104110954406.png" alt="image-20200104110954406"></p>
<pre><code>[root@master ~]# kubectl get pod  -o wide
//显示pod的节点信息
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200104111128779.png" alt="image-20200104111128779"></p>
<p><em><strong>如果，node节点没有运行test-web服务，需要在节点上重启一下<systemctl restart="" kubelet=""></systemctl></strong></em></p>
<h3 id="如果删除一个pod">如果删除一个pod</h3>
<pre><code>[root@master ~]# kubectl delete pod test-web-5b56bdff65-2njqf 
</code></pre>
<h4 id="查看一下-2">查看一下</h4>
<pre><code>[root@master ~]# kubectl get pod -o wide
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200104112418012.png" alt="image-20200104112418012"></p>
<p><em><strong>现在发现容器还存在，因为控制器会自动发现，一旦与之前执行的命令有误差，他会自动补全。</strong></em></p>
<p><a href="https://blog.csdn.net/gongxsh00/article/details/79932136" target="_blank" rel="noopener">https://blog.csdn.net/gongxsh00/article/details/79932136</a></p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script></div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/posts/2cda.html">01 部署k8s集群</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-05-01</time><div class="content"><h2 id="k8s">k8s</h2>
<h4 id="最基本的硬件要求"><strong>最基本的硬件要求</strong></h4>
<p><strong>CPU: 双核</strong><br>
<strong>Mem: 2G</strong><br>
<strong>3台dockerhost</strong><br>
<strong>时间必须同步</strong></p>
<h1>实验环境</h1>
<table>
<thead>
<tr>
<th>主机名</th>
<th>IP地址</th>
<th>服务</th>
</tr>
</thead>
<tbody>
<tr>
<td>master</td>
<td>192.168.1.21</td>
<td>dockerhost</td>
</tr>
<tr>
<td>node01</td>
<td>192.168.1.22</td>
<td>dockerhost</td>
</tr>
<tr>
<td>node02</td>
<td>192.168.1.23</td>
<td>dockerhost</td>
</tr>
</tbody>
</table>
<h2 id="环境准备">环境准备</h2>
<p>分别将3台虚拟机命名，设置好对应IP，并将其写入域名解析/etc/hosts中，关闭防火墙，iptables，禁用selinux。还有要做到，时间必须一致。全部禁用swap</p>
<h3 id="1-给三台docker命名">1.给三台docker命名</h3>
<p><strong>k8.1</strong></p>
<pre><code>[root@localhost ~]# hostnamectl set-hostname master
[root@localhost ~]# su -
</code></pre>
<p><strong>k8.2</strong></p>
<pre><code>[root@localhost ~]# hostnamectl set-hostname node01
[root@localhost ~]# su -
</code></pre>
<p><strong>k8.3</strong></p>
<pre><code>[root@localhost ~]# hostnamectl set-hostname node02
[root@localhost ~]# su -
</code></pre>
<p>验证docker是否能使用及版本是否一样</p>
<pre><code>[root@master ~]# docker -v
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200102093813472.png" alt="image-20200102093813472"></p>
<h3 id="2-关闭防火墙及禁用selinux">2.关闭防火墙及禁用selinux</h3>
<pre><code>[root@master ~]# systemctl stop firewalld
[root@master ~]# systemctl disable firewalld
 [root@master ~]# vim /etc/selinux/config
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200102115453524.png" alt="image-20200102115453524"></p>
<h3 id="3-禁用swap（三台）">3.  禁用swap（三台）</h3>
<pre><code>[root@master ~]# swapoff -a
//临时禁用swap
[root@master ~]# free -h
[root@master ~]# vim /etc/fstab 
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200102094039749.png" alt="image-20200102094039749"></p>
<h3 id="4-添加域名解析（三台）">4.添加域名解析（三台）</h3>
<pre><code>[root@master ~]# echo 192.168.1.21 master &gt;&gt; /etc/hosts
[root@master ~]# echo 192.168.1.22 node01 &gt;&gt; /etc/hosts
[root@master ~]# echo 192.168.1.23 node02 &gt;&gt; /etc/hosts
</code></pre>
<h3 id="5-做免密登陆（三台）">5.做免密登陆（三台）</h3>
<pre><code>[root@master ~]# ssh-keygen -t rsa
//生成密钥
</code></pre>
<p><strong>复制密钥到其他主机</strong></p>
<pre><code>   54  ssh-copy-id node01
   55  ssh-copy-id node02
</code></pre>
<h4 id="把域名解析复制到其他主机"><strong>把域名解析复制到其他主机</strong></h4>
<pre><code>   63  scp /etc/hosts node01:/etc
   64  scp /etc/hosts node02:/etc
</code></pre>
<h3 id="6-打开路由转发和iptables桥接功能（三台）">6.打开路由转发和iptables桥接功能（三台）</h3>
<pre><code>[root@master ~]# vim /etc/sysctl.d/k8s.conf
//开启iptables桥接功能
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1

[root@master ~]# echo net.ipv4.ip_forward = 1 &gt;&gt; /etc/sysctl.conf 
//**打开路由转发

[root@master ~]# sysctl -p /etc/sysctl.d/k8s.conf 
[root@master ~]# sysctl -p 
//刷新一下
</code></pre>
<p><strong>如果以上命令执行失败可能是缺少模块，可执行以下命令</strong></p>
<pre><code>[root@master ~]# modprobe br_netfiler
</code></pre>
<p><strong>把路由转发和iptables桥接复制到其他主机</strong></p>
<pre><code>[root@master ~]# scp /etc/sysctl.d/k8s.conf  node01:/etc/sysctl.d/
[root@master ~]# scp /etc/sysctl.d/k8s.conf  node02:/etc/sysctl.d/
[root@master ~]# scp /etc/sysctl.conf  node02:/etc/
[root@master ~]# scp /etc/sysctl.conf  node01:/etc/
</code></pre>
<p><strong>记得node01和node02也要执行以下命令</strong></p>
<pre><code>[root@master ~]# sysctl -p /etc/sysctl.d/k8s.conf 
[root@master ~]# sysctl -p 
</code></pre>
<h1>master节点安装部署k8s</h1>
<h2 id="指定yum安装kubernetes的yum源（三台）">指定yum安装kubernetes的yum源（三台）</h2>
<pre><code>cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
</code></pre>
<p><strong>下载完成之后，查看一下仓库是否可用</strong></p>
<pre><code>[root@master ~]# yum repolist 
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200102095945934.png" alt="image-20200102095945934"></p>
<p><strong>创建本地缓存（三台）</strong></p>
<pre><code>[root@master ~]# yum makecache fast
</code></pre>
<h2 id="各节点安装所需安装包">各节点安装所需安装包</h2>
<h3 id="master下载"><strong>master下载</strong></h3>
<pre><code>[root@master ~]# yum -y install kubeadm-1.15.0-0 kubelet-1.15.0-0 kubectl-1.15.0-0
</code></pre>
<h3 id="node01和node02下载"><strong>node01和node02下载</strong></h3>
<pre><code>[root@node01 ~]# yum -y install kubeadm-1.15.0-0 kubelet-1.15.0-0
</code></pre>
<h3 id="三台主机把-kubelet加入开机自启"><strong>三台主机把 kubelet加入开机自启</strong></h3>
<pre><code>[root@master ~]# systemctl enable kubelet
</code></pre>
<h2 id="master导入，之前准备好的镜像"><strong>master导入，之前准备好的镜像</strong></h2>
<pre><code>[root@master ~]# mkdir images
[root@master ~]# cd images/
[root@master images]# ls
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200102101531123.png" alt="image-20200102101531123"></p>
<h3 id="创建一个导入镜像的脚本"><strong>创建一个导入镜像的脚本</strong></h3>
<pre><code>[root@master images]# cat &gt; image.sh &lt;&lt;EOF
&gt; #!/bin/bash
&gt; for i in /root/images/*
&gt; do
&gt; docker load &lt; $i 
&gt; done
&gt; EOF
[root@master images]# chmod +x image.sh 
</code></pre>
<h3 id="导入镜像">导入镜像</h3>
<pre><code>[root@master images]# sh image.sh 
</code></pre>
<h3 id="初始化Kubernetes集群">初始化Kubernetes集群</h3>
<pre><code>[root@master ~]#  kubeadm init --kubernetes-version=v1.15.0 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 --ignore-preflight-errors=Swap
</code></pre>
<p><strong>如果以上的命令报错，找出问题后先重置一下（下面的命令），然后再执行以上命令</strong></p>
<pre><code>[root@master ~]# kubeadm reset
//重置kubeadm
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200102122213788.png" alt="image-20200102122213788"></p>
<pre><code>[root@master images]# kubectl get node
//查看当前节点信息
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200102110808239.png" alt="image-20200102110808239"></p>
<p><strong>可以看出master的状态是未就绪（NotReady），之所以是这种状态是因为还缺少一个附件flannel，没有网络各Pod是无法通信的</strong></p>
<h3 id="也可以通过检查组件的健康状态"><strong>也可以通过检查组件的健康状态</strong></h3>
<pre><code>[root@master images]# kubectl get cs
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200102122413443.png" alt="image-20200102122413443"></p>
<h3 id="添加网络组件（flannel）">添加网络组件（flannel）</h3>
<p><strong>组件flannel可以通过https://github.com/coreos/flannel中获取</strong></p>
<pre><code>[root@master ~]# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
</code></pre>
<p>以上只是方式之一，在网络状况良好的情况下建议使用上述方法（调用远端文件执行一下），<strong>若网速较差，建议使用以下方法</strong>：</p>
<pre><code>[root@master images]# wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
#将github官网指定的.yml配置文件下载到本地
[root@master images]# ls | grep flannel.yml   #确定下载到了当前目录
kube-flannel.yml
[root@master images]# kubectl apply -f kube-flannel.yml  #指定下载的.yml文件执行相应命令
</code></pre>
<p><em><strong>上述方法，二选一进行配置即可。</strong></em></p>
<p><strong>看到很多东西被创建是还不够的，还需要查看flannel是否处于正常启动并运行的状态，才算正在的部署完成</strong></p>
<pre><code>[root@master images]# kubectl get pods --all-namespaces
//查看所有的名称空间的pod（可以看到flannel网络运行正常）
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200102122732587.png" alt="image-20200102122732587"></p>
<pre><code>[root@master images]# kubectl get pod -n kube-system
//查看名称空间为kube-system的pod
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200102122826221.png" alt="image-20200102122826221"></p>
<p>查看当前节点信息</p>
<pre><code>kubectl get node
//查看当前节点信息（已经准备好了）
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200102111853910.png" alt="image-20200102111853910"></p>
<h1>node两台节点，导入镜像并加入群集</h1>
<h2 id="导入镜像-2">导入镜像</h2>
<p>上传所需镜像包，也可以使用docker pull下载</p>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200102133744555.png" alt="image-20200102133744555"></p>
<pre><code>[root@node01 images]# docker load &lt; kube-proxy-1-15.tar &amp;&amp; docker load -i myflannel-11-0.tar  &amp;&amp; docker load -i pause-3-1.tar
</code></pre>
<pre><code>[root@node01 images]# docker images
//查看本地镜像
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200102134006724.png" alt="image-20200102134006724"></p>
<h2 id="node01和node02加入群集">node01和node02加入群集</h2>
<p><strong>这时使用的命令是初始化群集之后生成的令牌（只有24小时的时效）</strong></p>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200102134336922.png" alt="image-20200102134336922"></p>
<pre><code>[root@node01 ~]# kubeadm join 192.168.1.21:6443 --token z0vknh.s6ib4eu4f8bre2nu     --discovery-token-ca-cert-hash sha256:8da72cc83f45d1247f42ce888658129b43726fe2af4ffc0c4e79faedb4050359
</code></pre>
<h2 id="加入群集之后查看一下">加入群集之后查看一下</h2>
<pre><code>[root@master images]# kubectl get node
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200102114628989.png" alt="image-20200102114628989"></p>
<h1>各节点优化一下</h1>
<h2 id="设置table键的默认间距；">设置table键的默认间距；</h2>
<pre><code>[root@master ~]# vim .vimrc
set tabstop=2
[root@master ~]# source .vimrc 
</code></pre>
<h2 id="设置kubectl命令自动补全">设置kubectl命令自动补全</h2>
<pre><code>[root@master ~]# yum  -y install bash-completion
[root@master ~]#  source /usr/share/bash-completion/bash_completion 
[root@master ~]# source &lt;(kubectl completion bash)
[root@master ~]# echo "source &lt;(kubectl completion bash)" &gt;&gt; ~/.bashrc
</code></pre>
<h2 id="确认k8s群集没有问题，并设置为开机自启">确认k8s群集没有问题，并设置为开机自启</h2>
<h3 id="master主机操作如下："><strong>master主机操作如下</strong>：</h3>
<pre><code>[root@master ~]# kubectl get pod -n kube-system   
#查看pod资源，类似于docker中的容器，确保返回的信息都是running
#“-n kube-system”：是k8s的名称空间
</code></pre>
<p><img src="http://xgp-cunchu.test.upcdn.net/k8s/image-20200102142028971.png" alt="image-20200102142028971"></p>
<h3 id="master和node节点上都需要进行以下操作，以便设置为开机自启："><strong>master和node节点上都需要进行以下操作，以便设置为开机自启</strong>：</h3>
<pre><code>[root@master ~]# systemctl enable kubelet
[root@master ~]# systemctl enable docker 
</code></pre>
<p><strong>设置为开机自启后，k8s群集的配置基本完成了，现在可以重启一下这三台服务器，如果重启后，执行下面的命令，状态都还是running，则表示绝对没有问题了。</strong></p>
<pre><code>[root@master ~]# kubectl get pod -n kube-system    #重启后验证状态是否还都是running
</code></pre>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script></div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/posts/cfe.html">xgp</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-05-01</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/%E6%B5%8B%E8%AF%95/">测试</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E6%B5%8B%E8%AF%95/">测试</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E5%9B%BE%E7%89%87/">图片</a></span><div class="content"><p><img src="http://xgp-cunchu.test.upcdn.net/docker/image-20191224101214064.png" alt="img"></p>
<p><a href="http://xgp-cunchu.test.upcdn.net/xgp/image-20200102093813472.png" target="_blank" rel="noopener">http://xgp-cunchu.test.upcdn.net/xgp/image-20200102093813472.png</a></p>
<p><a href="http://xgp-cunchu.test.upcdn.net/xgp/image-20200102094039749.png" target="_blank" rel="noopener">http://xgp-cunchu.test.upcdn.net/xgp/image-20200102094039749.png</a></p>
<img src="/posts/cfe/1.png" class="" title="This is an example image">
<img src="/posts/cfe/1.png" class="" title="This is an example image">
<img src="/posts/cfe/1.png" class="" title="This is an example image">
<p>./xgp/1.png</p>
<p><img src="/posts/1.png" alt="1"></p>
<p><img src="/posts/1.png" alt="1"></p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script></div><hr></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/">&lt;i class&#x3D;&quot;fa fa-chevron-left&quot;&gt;&lt;&#x2F;i&gt;</a><a class="page-number" href="/">1</a><span class="page-number current">2</span></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2020 By Wu Shao Dong</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody" target="_blank" rel="noopener"><span>Melody</span></a></div><div class="busuanzi"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.5.6"></script><script src="/js/fancybox.js?version=1.5.6"></script><script src="/js/sidebar.js?version=1.5.6"></script><script src="/js/copy.js?version=1.5.6"></script><script src="/js/fireworks.js?version=1.5.6"></script><script src="/js/transition.js?version=1.5.6"></script><script src="/js/scroll.js?version=1.5.6"></script><script src="/js/head.js?version=1.5.6"></script></body></html>